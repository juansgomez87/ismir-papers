"Authors","Author full names","Author(s) ID","Title","Year","Source title","Cited by","DOI","Link","Affiliations","Authors with affiliations","Abstract","Author Keywords","Index Keywords","Correspondence Address","Sponsors","Conference name","Conference date","Conference location","Conference code"
"Taenzer M.; Abeßer J.; Mimilakis S.I.; Weiß C.; Müller M.; Lukashevich H.","Taenzer, Michael (57208817830); Abeßer, Jakob (36607532300); Mimilakis, Stylianos I. (55378090100); Weiß, Christof (56273046500); Müller, Meinard (7404689873); Lukashevich, Hanna (6508121862)","57208817830; 36607532300; 55378090100; 56273046500; 7404689873; 6508121862","Investigating CNN-based instrument family recognition for western classical music recordings","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094413&partnerID=40&md5=c6991436626fd2085c47d97f045f9ea2","Fraunhofer IDMT, Ilmenau, Germany; International Audio Laboratories Erlangen, Germany","Taenzer M., Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Fraunhofer IDMT, Ilmenau, Germany; Mimilakis S.I., Fraunhofer IDMT, Ilmenau, Germany; Weiß C., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany; Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany","Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Augmentation techniques; Classical musics; Data augmentation; Data normalization; Domain adaptation; Generalization capability; Logarithmic compression; Optimization routine; Audio recordings","M. Taenzer; Fraunhofer IDMT, Ilmenau, Germany; email: michael.taenzer@idmt.fraunhofer.de","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Zangerle E.; Huber R.; Vötter M.; Yang Y.-H.","Zangerle, Eva (36186499400); Huber, Ramona (57217330525); Vötter, Michael (57209103369); Yang, Yi-Hsuan (55218558400)","36186499400; 57217330525; 57209103369; 55218558400","Hit song prediction: Leveraging low- and high-level audio features","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094653&partnerID=40&md5=7502e17f9f434aa0c563b059a3af7477","University of Innsbruck, Austria; Academia Sinica, Taipei, Taiwan","Zangerle E., University of Innsbruck, Austria; Huber R., University of Innsbruck, Austria; Vötter M., University of Innsbruck, Austria; Yang Y.-H., Academia Sinica, Taipei, Taiwan","Assessing the potential success of a given song based on its acoustic characteristics is an important task in the music industry. This task has mostly been approached from an internal perspective, utilizing audio descriptors to predict the success of a given song, where either low- or high-level audio features have been utilized separately. In this work, we aim to jointly exploit low- and high-level audio features and model the prediction as a regression task. Particularly, we make use of a wide and deep neural network architecture that allows for jointly exploiting low- and high-level features. Furthermore, we enrich the set of features with information about the release year of tracks. We evaluate our approach based on the Million Song Dataset and characterize a song as a hit if it is contained in the Billboard Hot 100 at any point in time. Our findings suggest that the proposed approach is able to outperform baseline approaches as well as approaches utilizing low- or high-level features individually. Furthermore, we find that incorporating the release year as well as features describing the mood and vocals of a song improve prediction results. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep neural networks; Information retrieval; Network architecture; Acoustic characteristic; Audio features; Descriptors; High-level features; Music industry; Forecasting","Y.-H. Yang; Academia Sinica, Taipei, Taiwan; email: yang@citi.sinica.edu.tw","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Tsuchida S.; Fukayama S.; Hamasaki M.; Goto M.","Tsuchida, Shuhei (55646446900); Fukayama, Satoru (56407004300); Hamasaki, Masahiro (14066071000); Goto, Masataka (7403505330)","55646446900; 56407004300; 14066071000; 7403505330","Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","55","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096677&partnerID=40&md5=02a801b1660cde92300e5279251a7510","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Tsuchida S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Hamasaki M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We describe the AIST Dance Video Database (AIST Dance DB), a shared database containing original street dance videos with copyright-cleared dance music. Although dancing is highly related to dance music and dance information can be considered an important aspect of music information, research on dance information processing has not yet received much attention in the Music Information Retrieval (MIR) community. We therefore developed the AIST Dance DB as the first large-scale shared database focusing on street dances to facilitate research on a variety of tasks related to dancing to music. It consists of 13,939 dance videos covering 10 major dance genres as well as 60 pieces of dance music composed for those genres. The videos were recorded by having 40 professional dancers (25 male and 15 female) dance to those pieces. We carefully designed this database so that it can cover both solo dancing and group dancing as well as both basic choreography moves and advanced moves originally choreographed by each dancer. Moreover, we used multiple cameras surrounding a dancer to simultaneously shoot from various directions. The AIST Dance DB will foster new MIR tasks such as dance-motion genre classification, dancer identification, and dance-technique estimation. We propose a dance-motion genre-classification task and developed four baseline methods of identifying dance genres of videos in this database. We evaluated these methods by extracting dancer body motions and training their classifiers on the basis of long short-term memory (LSTM) recurrent neural network models and support-vector machine (SVM) models. © Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, Masataka Goto.","","Database systems; Information retrieval; Long short-term memory; Support vector machines; Video cameras; Video recording; Video signal processing; Baseline methods; Genre classification; Multi-cameras; Multiple cameras; Music information; Music information retrieval; Recurrent neural network model; Video database; Classification (of information)","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Cano E.; Beveridge S.","Cano, Estefanía (51161075800); Beveridge, Scott (57218450585)","51161075800; 57218450585","Microtiming analysis in traditional shetland fiddle music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096159&partnerID=40&md5=851ea31c6dd6d45bd736db4957010344","Fraunhofer IDMT, Germany; Songquito, Germany","Cano E., Fraunhofer IDMT, Germany; Beveridge S., Songquito, Germany","This work aims to characterize microtiming variations in traditional Shetland fiddle music. These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt. Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences. Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer. This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Music information retrieval; Rhythmic patterns; Computer music","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Wang C.; Benetos E.; Lostanlen V.; Chew E.","Wang, Changhong (57216729718); Benetos, Emmanouil (16067946900); Lostanlen, Vincent (57170761700); Chew, Elaine (8706714000)","57216729718; 16067946900; 57170761700; 8706714000","Adaptive time-frequency scattering for periodic modulation recognition in music signals","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096038&partnerID=40&md5=2df319cc5a5908e14ed975b551795808","Centre for Digital Music, Queen Mary University of London, United Kingdom; Music and Audio Research Laboratory, New York University, NY, United States; CNRS-UMR9912/STMS IRCAM, Paris, France","Wang C., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Lostanlen V., Music and Audio Research Laboratory, New York University, NY, United States; Chew E., CNRS-UMR9912/STMS IRCAM, Paris, France","Vibratos, tremolos, trills, and flutter-tongue are techniques frequently found in vocal and instrumental music. A common feature of these techniques is the periodic modulation in the time-frequency domain. We propose a representation based on time-frequency scattering to model the interclass variability for fine discrimination of these periodic modulations. Time-frequency scattering is an instance of the scattering transform, an approach for building invariant, stable, and informative signal representations. The proposed representation is calculated around the wavelet subband of maximal acoustic energy, rather than over all the wavelet bands. To demonstrate the feasibility of this approach, we build a system that computes the representation as input to a machine learning classifier. Whereas previously published datasets for playing technique analysis focus primarily on techniques recorded in isolation, for ecological validity, we create a new dataset to evaluate the system. The dataset, named CBF-periDB, contains fulllength expert performances on the Chinese bamboo flute that have been thoroughly annotated by the players themselves. We report F-measures of 99% for flutter-tongue, 82% for trill, 69% for vibrato, and 51% for tremolo detection, and provide explanatory visualisations of scattering coefficients for each of these techniques. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Flutter (aerodynamics); Frequency domain analysis; Information retrieval; Learning systems; Ecological validity; Expert performance; Periodic modulation; Playing techniques; Scattering co-efficient; Scattering transforms; Signal representations; Time frequency domain; Modulation","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Bretan M.; Heck L.","Bretan, Mason (55193092700); Heck, Larry (7005241213)","55193092700; 7005241213","Learning semantic similarity in music via self-supervision","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095586&partnerID=40&md5=63555b89cb41fcaa853a8bd14e406968","Samsung Research America, United States","Bretan M., Samsung Research America, United States; Heck L., Samsung Research America, United States","Neural networks have been used to learn a latent ""musical space"" or ""embedding"" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adversarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Backpropagation; Embeddings; Information retrieval; Semantics; Baseline models; Feature space; Learning semantics; Low-dimensional spaces; Multiple input representations; Semantic Model; Semantic relevance; Semantic similarity; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Maezawa A.; Yamamoto K.; Fujishima T.","Maezawa, Akira (35753591100); Yamamoto, Kazuhiko (55652696000); Fujishima, Takuya (55414444100)","35753591100; 55652696000; 55414444100","Rendering music performance with interpretation variations using conditional variational RNN","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095620&partnerID=40&md5=69363d5f1ea6ab9d528f2391d26129a6","Yamaha Corporation, Japan","Maezawa A., Yamaha Corporation, Japan; Yamamoto K., Yamaha Corporation, Japan; Fujishima T., Yamaha Corporation, Japan","Capturing and generating a wide variety of musical expression is important in music performance rendering, but current methods fail to model such a variation. This paper presents a music performance rendering method that explicitly models the variability in interpretations for a given piece of music. Conditional variational recurrent neural network is used to jointly train, conditioned on the music score, an encoder from a music performance to a latent representation of interpretation and a decoder from the latent interpretation back to the music performance. Evaluation demonstrates the method is capable of predicting and generating an expressive performance, and that the decoder learns a latent space of musical interpretation that is consistent with human perception of interpretation. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Decoding; Information retrieval; Expressive performance; Human perception; Music performance; Music scores; Musical expression; Rendering methods; Recurrent neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Balke S.; Dorfer M.; Carvalho L.; Arzt A.; Widmer G.","Balke, Stefan (56940401000); Dorfer, Matthias (55516844500); Carvalho, Luis (57217331814); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","56940401000; 55516844500; 57217331814; 36681791200; 7004342843","Learning Soft-attention models for Tempo-invariant audio-sheet music retrieval","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097246&partnerID=40&md5=58a80f35caee243e68bc6025429e0aaa","Institute of Computational Perception, Johannes Kepler University Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Balke S., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Carvalho L., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Audio recordings; Audio systems; Deep learning; Deep neural networks; Information retrieval; Learning systems; Attention mechanisms; Attention model; Audio input; Cross-modal; General tools; Music recording; Music retrieval; Retrieval systems; Search engines","S. Balke; Institute of Computational Perception, Johannes Kepler University Linz, Austria; email: stefan.balke@jku.at","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lerch A.; Arthur C.; Pati A.; Gururani S.","Lerch, Alexander (22034963000); Arthur, Claire (57191728327); Pati, Ashis (57217331630); Gururani, Siddharth (57195432595)","22034963000; 57191728327; 57217331630; 57195432595","Music performance analysis: A survey","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096431&partnerID=40&md5=fec7b5d1da0c7fa75c2dc3d986021cf9","Center for Music Technology, Georgia Institute of Technology, Atlanta, United States","Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Arthur C., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Pati A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Gururani S., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States","Music Information Retrieval (MIR) tends to focus on the analysis of audio signals. Often, a single music recording is used as representative of a ""song"" even though different performances of the same song may reveal different properties. A performance is distinct in many ways from a (arguably more abstract) representation of a ""song,"" ""piece,"" or musical score. The characteristics of the (recorded) performance-as opposed to the score or musical idea- can have a major impact on how a listener perceives music. The analysis of music performance, however, has been traditionally only a peripheral topic for the MIR research community. This paper surveys the field of Music Performance Analysis (MPA) from various perspectives, discusses its significance to the field of MIR, and points out opportunities for future research in this field. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Information retrieval; Signal analysis; Surveys; Audio signal; Music information retrieval; Music performance; Music recording; Musical score; Paper surveys; Research communities; Audio recordings","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"De Valk R.; Ahmed R.; Crawford T.","De Valk, Reinier (55979450000); Ahmed, Ryaan (57217330713); Crawford, Tim (15054056900)","55979450000; 57217330713; 15054056900","Josquintab: A dataset for content-based computational analysis of music in lute tablature","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095629&partnerID=40&md5=29f2df0d8716713acc930bb154185302","Department of Computing, Goldsmiths, University of London, United Kingdom; Digital Humanities, MIT, United States","De Valk R., Department of Computing, Goldsmiths, University of London, United Kingdom; Ahmed R., Digital Humanities, MIT, United States; Crawford T., Department of Computing, Goldsmiths, University of London, United Kingdom","An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difficult to understand for non-specialists as it reveals little structural information. In this paper we present JOSQUINTAB, a dataset of automatically created enriched diplomatic transcriptions in MIDI and MEI format of 64 sixteenthcentury lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of fit) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Computational methods; Conformal mapping; Information retrieval; Command line; Computational analysis; Content-based; Goodness of fit; Mapping algorithms; Music information retrieval; Source codes; Structural information; Large dataset","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Condit-Schultz N.; Arthur C.","Condit-Schultz, Nathaniel (56681201600); Arthur, Claire (57191728327)","56681201600; 57191728327","Humdrumr: A new take on an old approach to computational musicology","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093848&partnerID=40&md5=448d2fc6d98ac391270135dc86dd4f10","Georgia Institute of Technology, United States","Condit-Schultz N., Georgia Institute of Technology, United States; Arthur C., Georgia Institute of Technology, United States","Musicology research is a fundamentally humanistic endeavor. However, despite the productive work of a small niche of humanities-trained computational musicologists, most cutting-edge digital music research is pursued by scholars whose primary training is scientific or computational, not humanistic. This unfortunate situation is prolonged, at least in part, by the daunting barrier that computer coding presents to humanities scholars with no technical training. In this paper, we present humdrumR (""hum-drummer""), a software package designed to afford computational musicology research for both advanced and novice computer coders. Humdrum is a powerful and influential existing computational musicology framework including the humdrum syntax-a flexible text data format with tens of thousands of extant scores available-and the Bash-based humdrum toolkit. HumdrumR is a modern replacement for the humdrum toolkit, based in the dataanalysis/ statistical programming language R. By combining the flexibility and transparency of the humdrum syntax with the powerful data analysis tools and concise syntax of R, humdrumR offers an appealing new approach to would-be computational musicologists. HumdrumR leverages R's powerful metaprogramming capabilities to create an extremely expressive and composable syntax, allowing novices to achieve usable analyses quickly while avoiding many coding concepts that are commonly challenging for beginners. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Composable; Computer coding; Cutting edges; Data analysis tool; Digital music; Metaprogramming capabilities; New approaches; Technical training; Syntactics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Pauwels J.; O'Hanlon K.; Gómez E.; Sandler M.B.","Pauwels, Johan (35113648700); O'Hanlon, Ken (55311607600); Gómez, Emilia (14015483200); Sandler, Mark B. (7202740804)","35113648700; 55311607600; 14015483200; 7202740804","20 years of automatic chord recognition from audio","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096966&partnerID=40&md5=e8660a5914a46ee26ca639e84511f02f","Centre for Digital Music, Queen Mary University of London, United Kingdom; Music Technology Group, Universitat Pompeu Fabra, Spain","Pauwels J., Centre for Digital Music, Queen Mary University of London, United Kingdom; O'Hanlon K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","In 1999, Fujishima published Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music. This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Computer music; Information retrieval; Chord recognition; Current direction; Data-driven methods; Musical sounds; Real time; Research topics; Knowledge based systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Fuentes M.; Maia L.S.; Rocamora M.; Biscainho L.W.P.; Crayencour H.C.; Essid S.; Bello J.P.","Fuentes, Magdalena (57190739095); Maia, Lucas S. (57217331856); Rocamora, Martín (55347707700); Biscainho, Luiz W. P. (6603071473); Crayencour, Hélène C. (57209881493); Essid, Slim (16033218700); Bello, Juan P. (7102889110)","57190739095; 57217331856; 55347707700; 6603071473; 57209881493; 16033218700; 7102889110","Tracking beats and microtiming in Afro-latin American music using conditional random fields and deep learning","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094424&partnerID=40&md5=2292d1e040914b7e59bb2835cd0de512","L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France; Federal University of Rio de Janeiro, Brazil; Universidad de la República, Uruguay; Music and Audio Research Laboratory, New York University, United States","Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France; Maia L.S., LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France, Federal University of Rio de Janeiro, Brazil; Rocamora M., Universidad de la República, Uruguay; Biscainho L.W.P., Federal University of Rio de Janeiro, Brazil; Crayencour H.C., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; Essid S., LTCI, Télécom Paris, Institut Polytechnique de Paris 0 France, France; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Events in music frequently exhibit small-scale temporal deviations (microtiming), with respect to the underlying regular metrical grid. In some cases, as in music from the Afro-Latin American tradition, such deviations appear systematically, disclosing their structural importance in rhythmic and stylistic configuration. In this work we explore the idea of automatically and jointly tracking beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular Brazilian samba and Uruguayan candombe. To that end, we propose a language model based on conditional random fields that integrates beat and onset likelihoods as observations. We derive those activations using deep neural networks and evaluate its performance on manually annotated data using a scheme adapted to this task. We assess our approach in controlled conditions suitable for these timekeeper instruments, and study the microtiming profiles' dependency on genre and performer, illustrating promising aspects of this technique towards a more comprehensive understanding of these music traditions. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep neural networks; Information retrieval; Random processes; Conditional random field; Controlled conditions; Language model; Latin americans; Small scale; Structural importance; Deep learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Pacha A.; Calvo-Zaragoza J.; Hajič J., Jr.","Pacha, Alexander (54382080100); Calvo-Zaragoza, Jorge (55847598300); Hajič, Jan (57193414196)","54382080100; 55847598300; 57193414196","Learning notation graph construction for full-pipeline optical music recognition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094788&partnerID=40&md5=8af972845e0918cb776d970ee03c2b19","Institute of Information Systems Engineering, TU Wien, Austria; Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain; Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic","Pacha A., Institute of Information Systems Engineering, TU Wien, Austria; Calvo-Zaragoza J., Pattern Recognition and Artificial Intelligence Group, University of Alicante, Spain; Hajič J., Jr., Institute of Formal and Applied Linguistics, Charles University, Prague, Czech Republic","Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2% precision, 96.0% recall, and an F-score of 95.6% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2% precision, 91.5% recall and an F-score of 92.3%. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep learning; Information retrieval; Semantics; Syntactics; Detecting objects; Graph construction; Machine learning problem; Music information retrieval; Music notation; Object detectors; Optical music recognition; Writing systems; Object detection","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Jeong D.; Kwon T.; Kim Y.; Lee K.; Nam J.","Jeong, Dasaem (57195433126); Kwon, Taegyun (57210194738); Kim, Yoojin (57215321815); Lee, Kyogu (8597995500); Nam, Juhan (35812266500)","57195433126; 57210194738; 57215321815; 8597995500; 35812266500","VirtuosoNet: A hierarchical RNN-based system for modeling expressive piano performance","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094425&partnerID=40&md5=013edc9262b5bd28d6984412396f5b45","Graduate School of Culture Technology, KAIST, South Korea; Graduate School of Convergence Science and Technology, Seoul National University, South Korea","Jeong D., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Kim Y., Graduate School of Culture Technology, KAIST, South Korea; Lee K., Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models.We also share the dataset we used for the experiment. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep neural networks; Information retrieval; Musical instruments; Statistical tests; Auto encoders; Human like; Listening tests; Musical expression; Recurrent neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Simonetta F.; Cancino-Chacón C.; Ntalampiras S.; Widmer G.","Simonetta, Federico (57205640973); Cancino-Chacón, Carlos (55968703500); Ntalampiras, Stavros (24830597800); Widmer, Gerhard (7004342843)","57205640973; 55968703500; 24830597800; 7004342843","A convolutional approach to melody line identification in symbolic scores","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094302&partnerID=40&md5=fe9cb282d4f2a655c9da74e7947c2b7d","Music Informatics Laboratory, Dept. of Computer Science, University of Milano, Italy; Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Dept. of Computational Perception, Johannes Kepler University Linz, Austria","Simonetta F., Music Informatics Laboratory, Dept. of Computer Science, University of Milano, Italy; Cancino-Chacón C., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Ntalampiras S., Music Informatics Laboratory, Dept. of Computer Science, University of Milano, Italy; Widmer G., Austrian Research Institute for Artificial Intelligence, Vienna, Austria, Dept. of Computational Perception, Johannes Kepler University Linz, Austria","In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score - i.e. without listening to the music performed - can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications. In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input. © Federico Simonetta, Carlos Cancino-Chacón, Stavros Ntalampiras, Gerhard Widmer.","","Convolution; Information retrieval; Automated approach; Human listeners; Line identifications; Manual annotation; Music information retrieval; Convolutional neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lemaire Q.; Holzapfel A.","Lemaire, Quentin (57217332142); Holzapfel, Andre (18041818000)","57217332142; 18041818000","Temporal convolutional networks for speech and music detection in radio broadcast","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097259&partnerID=40&md5=7d9a242103c9a6d5f2e5bfc4dbee0169","KTH Royal Institute of Technology, Sweden","Lemaire Q., KTH Royal Institute of Technology, Sweden; Holzapfel A., KTH Royal Institute of Technology, Sweden","The task of speech and music detection aims at the automatic annotation of potentially overlapping speech and music segments in audio recordings. This metadata extraction process finds important applications in royalty collection for broadcast audio. This study focuses on deep neural network architectures made to process sequential data, and a series of recent architectures that have not yet been applied for this task are evaluated, extended and compared with a state-of-the-art architecture. Moreover, different training strategies are evaluated, and we demonstrate the advantages of a pre-training procedure with lowquality data that facilitates the combination of heterogeneous datasets. The study shows that Temporal Convolution Network (TCN) architectures can outperform state-ofthe- art architectures. In specific, the novel non-causal TCN extension introduced in this paper leads to a significant improvement of the accuracy. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Arts computing; Audio acoustics; Audio recordings; Convolution; Deep neural networks; Information retrieval; Neural networks; Radio broadcasting; Speech recognition; Automatic annotation; Convolutional networks; Heterogeneous datasets; Meta-data extractions; Music segments; Sequential data; State of the art; Training strategy; Network architecture","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Porcaro L.; Gomez E.","Porcaro, Lorenzo (57211338743); Gomez, Emilia (14015483200)","57211338743; 14015483200","20 years of playlists: A statistical analysis on popularity and diversity","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095993&partnerID=40&md5=0acf8bbe0234f13efc0a0701918b4055","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Joint Research Centre, European Commission, Seville, Spain","Porcaro L., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gomez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, Joint Research Centre, European Commission, Seville, Spain","Grouping songs together, according to music preferences, mood or other characteristics, is an activity which reflects personal listening behaviours and tastes. In the last two decades, due to the increasing size of music catalogue accessible and to improvements of recommendation algorithms, people have been exposed to new ways for creating playlists. In this work, through the statistical analysis of more than 400K playlists from four datasets, created in different temporal and technological contexts, we aim to understand if it is possible to extract information about the evolution of humans strategies for playlist creation. We focus our analysis on two driving concepts of the Music Information Retrieval literature: popularity and diversity. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Statistical methods; Exposed to; Extract informations; Music information retrieval; Music preferences; Recommendation algorithms; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Hantrakul L.; Engel J.; Roberts A.; Gu C.","Hantrakul, Lamtharn (56258458500); Engel, Jesse (57192111540); Roberts, Adam (57201381304); Gu, Chenjie (57216551134)","56258458500; 57192111540; 57201381304; 57216551134","Fast and flexible neural audio synthesis","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095752&partnerID=40&md5=a1db328ccd3fb2ec15d70d83c883123d","Google Brain, United States; Google DeepMind, United Kingdom","Hantrakul L., Google Brain, United States; Engel J., Google Brain, United States; Roberts A., Google Brain, United States; Gu C., Google DeepMind, United Kingdom","Autoregressive neural networks, such as WaveNet, have opened up new avenues for expressive audio synthesis. High-quality speech synthesis utilizes detailed linguistic features for conditioning, but comparable levels of control have yet to be realized for neural synthesis of musical instruments. Here, we demonstrate an autoregressive model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency. We find the appropriate choice of conditioning features and architectures improves both the quantitative accuracy of audio resynthesis and qualitative responsiveness to creative manipulation of conditioning. While large autoregressive models generate audio much slower than real-time, we achieve these results with a more efficient WaveRNN model, opening the door for exploring real-time interactive audio synthesis with neural networks. © Lamtharn Hantrakul, Jesse Engel, Adam Roberts, Chenjie Gu.","","Information retrieval; Speech synthesis; Audio resynthesis; Auto regressive models; Autoregressive neural networks; Fundamental frequencies; High quality speech synthesis; Interactive audio; Linguistic features; Quantitative accuracy; Linguistics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Ju Y.; Howes S.; McKay C.; Condit-Schultz N.; Calvo-Zaragoza J.; Fujinaga I.","Ju, Yaolong (57200500552); Howes, Samuel (57212690994); McKay, Cory (14033215600); Condit-Schultz, Nathaniel (56681201600); Calvo-Zaragoza, Jorge (55847598300); Fujinaga, Ichiro (9038140900)","57200500552; 57212690994; 14033215600; 56681201600; 55847598300; 9038140900","An interactive workflow for generating chord labels for homorhythmic music in symbolic formats","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095631&partnerID=40&md5=f2693b813dd34d06da8d5f307524613f","Schulich School of Music, McGill University, Canada; Department of Liberal and Creative Arts, Marianopolis College, Canada; School of Music, Georgia Institute of Technology, United States; Department of Software and Computing Systems, University of Alicante, Spain","Ju Y., Schulich School of Music, McGill University, Canada; Howes S., Schulich School of Music, McGill University, Canada; McKay C., Department of Liberal and Creative Arts, Marianopolis College, Canada; Condit-Schultz N., School of Music, Georgia Institute of Technology, United States; Calvo-Zaragoza J., Department of Software and Computing Systems, University of Alicante, Spain; Fujinaga I., Schulich School of Music, McGill University, Canada","Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workflow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5% on the reserved test set, a 24.4% reduction in the number of errors. This versatile interactive workflow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive highquality ground truth for training effective machine learning models. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Domain experts; Ground truth; High quality; Interactive workflow; Machine learning models; Manual analysis; Manual annotation; Rule-based models; Machine learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Elowsson A.; Friberg A.","Elowsson, Anders (56381573300); Friberg, Anders (7006743539)","56381573300; 7006743539","Modeling music modality with a key-class invariant pitch chroma CNN","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094660&partnerID=40&md5=bb2e1c454563d0dce9e0e0fc2be0e673","KTH Royal Institute of Technology, Sweden","Elowsson A., KTH Royal Institute of Technology, Sweden; Friberg A., KTH Royal Institute of Technology, Sweden","This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Chemical activation; Convolutional neural networks; Information retrieval; Class invariants; Harmony analysis; Human listeners; Max-pooling; Parameter sharing; Pitch estimation; Spectral feature; Time-scales; Computer music","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Driedger J.; Schreiber H.; De Haas W.B.; Müller M.","Driedger, Jonathan (55582371700); Schreiber, Hendrik (55586286200); De Haas, W. Bas (51160955300); Müller, Meinard (7404689873)","55582371700; 55586286200; 51160955300; 7404689873","Towards automatically correcting tapped beat annotations for music recordings","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096560&partnerID=40&md5=e370b71974d44c23a2f7244b7520abbb","Chordify; International Audio Laboratories Erlangen","Driedger J., Chordify; Schreiber H., International Audio Laboratories Erlangen; De Haas W.B., Chordify; Müller M., International Audio Laboratories Erlangen","A common method to create beat annotations for music recordings is to let a human annotator tap along with them. However, this method is problematic due to the limited human ability to temporally align taps with audio cues for beats accurately. In order to create accurate beat annotations, it is therefore typically necessary to manually correct the recorded taps in a subsequent step, which is a cumbersome task. In this work we aim to automate this correction step by ""snapping"" the taps to close-by audio cues-a strategy that is often used by beat tracking algorithms to refine their beat estimates. The main contributions of this paper can be summarized as follows. First, we formalize the automated correction procedure mathematically. Second, we introduce a novel visualization method that serves as a tool to analyze the results of the correction procedure for potential errors. Third, we present a new dataset consisting of beat annotations for 101 music recordings. Fourth, we use this dataset to perform a listening experiment as well as a quantitative study to show the effectiveness of our snapping procedure. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Audio cues; Beat tracking; Correction procedure; Human abilities; Music recording; Novel visualizations; Potential errors; Quantitative study; Audio recordings","J. Driedger; Chordify; email: jonathan@chordify.net","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Feisthauer L.; Bigo L.; Giraud M.","Feisthauer, Laurent (57210193101); Bigo, Louis (37057150800); Giraud, Mathieu (8700367400)","57210193101; 37057150800; 8700367400","Modeling and learning structural breaks in sonata forms","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095081&partnerID=40&md5=54f7f988d552cf026762670e856e057e","CRIStAL, UMR 9189, CNRS, Université de Lille, France","Feisthauer L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France","Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts. We introduce highlevel features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Textures; Energy accumulation; Energy gain; High-level features; Structural break; Transition zones; Validation strategies; Long short-term memory","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Waloschek S.; Hadjakos A.; Pacha A.","Waloschek, Simon (57191268983); Hadjakos, Aristotelis (35113080700); Pacha, Alexander (54382080100)","57191268983; 35113080700; 54382080100","Identification and Cross-document alignment of measures in music score images","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095915&partnerID=40&md5=6de799025a105370ef4ff21488d7a1e0","Center of Music and Film Informatics, Detmold University of Music, Germany; Institute of Information Systems Engineering, TU Wien, Austria","Waloschek S., Center of Music and Film Informatics, Detmold University of Music, Germany; Hadjakos A., Center of Music and Film Informatics, Detmold University of Music, Germany; Pacha A., Institute of Information Systems Engineering, TU Wien, Austria","In the course of editing musical works, musicologists regularly compare multiple sources of the same musical piece, such as composers' autographs, handwritten copies, and various prints. For efficient comparison, cross-source navigation is essential, enabling to quickly jump back and forth between multiple sources without losing the current musical position. In practice, measures are first annotated by hand in the individual source images and then related to each other. Our approach automates this time-consuming and error-prone process with the help of deep learning. For this purpose, we train a neural network that automatically finds bounding boxes of all measures in images. A second network is trained to compute the similarity between two measures to determine if they have the same musical content and should, therefore, be linked for navigation. Sequences of outputs from the second network are matched using Dynamic TimeWarping to provide the final proposal of measure relationships, so-called concordances. In addition to cross-source navigation, the results can be used to spot structural differences across the sources which are essential for editorial work, so that musicologists can focus more on analytical tasks. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Navigation; Cross documents; Error-prone process; Multiple source; Music scores; Musical pieces; Source images; Structural differences; Time warping; Deep learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Gadermaier T.; Widmer G.","Gadermaier, Thassilo (57003475800); Widmer, Gerhard (7004342843)","57003475800; 7004342843","A study of annotation and alignment accuracy for performance comparison in complex orchestral music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095759&partnerID=40&md5=aeabcb6cd0f941e1597fa035ef4e7907","Institute of Computational Perception, Johannes Kepler University Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Gadermaier T., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Quantitative analysis of commonalities and differences between recorded music performances is an increasingly common task in computational musicology. A typical scenario involves manual annotation of different recordings of the same piece along the time dimension, for comparative analysis of, e.g., the musical tempo, or for mapping other performance-related information between performances. This can be done by manually annotating one reference performance, and then automatically synchronizing other performances, using audio-to-audio alignment algorithms. In this paper we address several questions related to those tasks. First, we analyze different annotations of the same musical piece, quantifying timing deviations between the respective human annotators. A statistical evaluation of the marker time stamps will provide (a) an estimate of the expected timing precision of human annotations and (b) a ground truth for subsequent automatic alignment experiments. We then carry out a systematic evaluation of different audio features for audio-to-audio alignment, quantifying the degree of alignment accuracy that can be achieved, and relate this to the results from the annotation study. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Alignment; Information retrieval; Alignment accuracy; Automatic alignment; Comparative analysis; Degree of alignments; Human annotations; Performance comparison; Statistical evaluation; Systematic evaluation; Audio recordings","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Li L.; Toda T.; Morikawa K.; Kobayashi K.; Makino S.","Li, Li (57190745945); Toda, Tomoki (7202683282); Morikawa, Kazuho (57203173233); Kobayashi, Kazuhiro (56192181900); Makino, Shoji (7403067619)","57190745945; 7202683282; 57203173233; 56192181900; 7403067619","Improving singing aid system for laryngectomees with statistical voice conversion and VAE-space","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095914&partnerID=40&md5=7e3aa26951f162837e2f5599b881fcda","University of Tsukuba, Japan; Nagoya University, Japan","Li L., University of Tsukuba, Japan; Toda T., Nagoya University, Japan; Morikawa K., Nagoya University, Japan; Kobayashi K., Nagoya University, Japan; Makino S., University of Tsukuba, Japan","This paper proposes an improved singing aid system for laryngectomees that converts electrolaryngeal (EL) speech produced using an electrolarynx to a more naturally sounding singing voice. Although the previously proposed system employing a noise suppression process and a rulebased pitch control approach has achieved preliminary success in converting EL speech into a singing voice, there are still two major limitations. First, the converted singing voice still sounds mechanical and unnatural owing to the adverse impacts of spectrograms extracted from EL speeches, also making the effect of pitch control limited. Second, the capability and flexibility of the rulebased pitch control in modeling various singing styles are insufficient, causing the converted singing voices to lack variety. To address these limitations, this paper proposes an improved system that uses 1) a statistical voice conversion approach to convert spectrograms extracted from EL speeches into those of natural speeches and 2) a deep generative model-based approach called VAE-SPACE for pitch modification, which generates pitch patterns in a data-driven manner instead of following manually designed rules. The experimental results revealed that 1) the conversion of spectrograms was effective in improving the naturalness of singing voices, and 2) the statistical pitch control approach was able to achieve comparable results with the rule-based approach, which was very carefully designed to be specialized in singing. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Spectrographs; Generative model; Laryngectomees; Natural speech; Noise suppression; Pitch modification; Rule-based approach; Singing styles; Voice conversion; Continuous speech recognition","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Weiß C.; Schlecht S.J.; Rosenzweig S.; Müller M.","Weiß, Christof (56273046500); Schlecht, Sebastian J. (55546914800); Rosenzweig, Sebastian (57195432903); Müller, Meinard (7404689873)","56273046500; 55546914800; 57195432903; 7404689873","Towards measuring intonation quality of choir recordings: A case study on bruckner's locus iste","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095518&partnerID=40&md5=158dab249af3f0891f74baf71e801d7c","International Audio Laboratories Erlangen, Germany","Weiß C., International Audio Laboratories Erlangen, Germany; Schlecht S.J., International Audio Laboratories Erlangen, Germany; Rosenzweig S., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Unaccompanied vocal music is a central part of Western art music, yet it requires excellent skills for singers to achieve proper intonation. In this paper, we analyze intonation deficiencies by introducing an intonation cost measure that can be computed from choir recordings and may help to assess the singers' intonation quality. With our approach, we measure the deviation between the recording's local salient frequency content and an adaptive reference grid based on the equal-tempered scale. The adaptivity introduces invariance of the local intonation measure to global intonation drifts. In our experiments, we compute this measure for several recordings of Anton Bruckner's choir piece Locus Iste. We demonstrate the robustness of the proposed measure by comparing scenarios of different complexity regarding the availability of aligned scores and multi-track recordings, as well as the number of singers per part. Even without using score information, our cost measure shows interesting trends, thus indicating the potential of our method for real-world applications. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Adaptivity; Frequency contents; Multi-track recording; Real-world; Reference grids; Vocal music; Audio recordings","C. Weiß; International Audio Laboratories Erlangen, Germany; email: christof.weiss@audiolabs-erlangen.de","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Kelz R.; Widmer G.","Kelz, Rainer (57195436721); Widmer, Gerhard (7004342843)","57195436721; 7004342843","Towards interpretable polyphonic transcription with invertible neural networks","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096814&partnerID=40&md5=5ecb198b255cf61763dcd41959bf65fc","Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Institute of Computational Perception, Johannes Kepler University Linz, Austria","Kelz R., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Austria, Institute of Computational Perception, Johannes Kepler University Linz, Austria","We explore a novel way of conceptualising the task of polyphonic music transcription, using so-called invertible neural networks. Invertible models unify both discriminative and generative aspects in one function, sharing one set of parameters. Introducing invertibility enables the practitioner to directly inspect what the discriminative model has learned, and exactly determine which inputs lead to which outputs. For the task of transcribing polyphonic audio into symbolic form, these models may be especially useful as they allow us to observe, for instance, to what extent the concept of single notes could be learned from a corpus of polyphonic music alone (which has been identified as a serious problem in recent research). This is an entirely new approach to audio transcription, which first of all necessitates some groundwork. In this paper, we begin by looking at the simplest possible invertible transcription model, and then thoroughly investigate its properties. Finally, we will take first steps towards a more sophisticated and capable version. We use the task of piano transcription, and specifically the MAPS dataset, as a basis for these investigations. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Discriminative models; Invertibility; New approaches; Polyphonic music; Polyphonic transcriptions; Recent researches; Audio acoustics","R. Kelz; Austrian Research Institute for Artificial Intelligence (OFAI), Austria; email: rainer.kelz@ofai.at","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Yesiler F.; Tralie C.; Correya A.; Silva D.F.; Tovstogan P.; Gómez E.; Serra X.","Yesiler, Furkan (57211821605); Tralie, Chris (55921094200); Correya, Albin (57217331712); Silva, Diego F. (55585876200); Tovstogan, Philip (57196328177); Gómez, Emilia (14015483200); Serra, Xavier (55892979900)","57211821605; 55921094200; 57217331712; 55585876200; 57196328177; 14015483200; 55892979900","Da-tacos: A dataset for cover song identification and understanding","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095724&partnerID=40&md5=cba491877fe5116f20bcc46ca925d908","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Department of Mathematics and Computer Science, Ursinus College, United States; Departamento de Computação, Universidade Federal de São Carlos, Brazil; Joint Research Centre, European Commission, Seville, Spain","Yesiler F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Tralie C., Department of Mathematics and Computer Science, Ursinus College, United States; Correya A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, Brazil; Tovstogan P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Joint Research Centre, European Commission, Seville, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper focuses on Cover Song Identification (CSI), an important research challenge in content-based Music Information Retrieval (MIR). Although the task itself is interesting and challenging for both academia and industry scenarios, there are a number of limitations for the advancement of current approaches. We specifically address two of them in the present study. First, the number of publicly available datasets for this task is limited, and there is no publicly available benchmark set that is widely used among researchers for comparative algorithm evaluation. Second, most of the algorithms are not publicly shared and reproducible, limiting the comparison of approaches. To overcome these limitations we propose Da-TACOS, a DaTAset for COver Song Identification and Understanding, and two frameworks for feature extraction and benchmarking to facilitate reproducibility. Da-TACOS contains 25K songs represented by unique editorial metadata plus 9 low- and mid-level features pre-computed with open source libraries, and is divided into two subsets. The Cover Analysis subset contains audio features (e.g. key, tempo) that can serve to study how musical characteristics vary for cover songs. The Benchmark subset contains the set of features that have been frequently used in CSI research, e.g. chroma, MFCC, beat onsets etc. Moreover, we provide initial benchmarking results of a selected number of state-of-the-art CSI algorithms using our dataset, and for reproducibility, we share a GitHub repository containing the feature extraction and benchmarking frameworks. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Benchmarking; Extraction; Feature extraction; Information retrieval; Algorithm evaluation; Cover song identifications; Mid-level features; Music information retrieval; Number of state; Open-source libraries; Reproducibilities; Research challenges; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Yang D.; Tanprasert T.; Jenrungrot T.; Shan M.; Tsai T.J.","Yang, Daniel (57217330697); Tanprasert, Thitaree (57213666497); Jenrungrot, Teerapat (57217331562); Shan, Mengyi (57217331401); Tsai, T.J. (55752421100)","57217330697; 57213666497; 57217331562; 57217331401; 55752421100","Midi passage retrieval using cell phone pictures of sheet music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097109&partnerID=40&md5=6e95ad826c3332e79a04b45a5d92fb93","Harvey Mudd College, Claremont, CA, United States","Yang D., Harvey Mudd College, Claremont, CA, United States; Tanprasert T., Harvey Mudd College, Claremont, CA, United States; Jenrungrot T., Harvey Mudd College, Claremont, CA, United States; Shan M., Harvey Mudd College, Claremont, CA, United States; Tsai T.J., Harvey Mudd College, Claremont, CA, United States","This paper investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of a physical page of sheet music. While audio-sheet music retrieval has been explored by a number of works, this scenario is novel in that the query is a cell phone picture rather than a digital scan. To solve this problem, we introduce a midlevel feature representation called a bootleg score which explicitly encodes the rules of Western musical notation. We convert both the MIDI and the sheet music into bootleg scores using deterministic rules of music and classical computer vision techniques for detecting simple geometric shapes. Once the MIDI and cell phone image have been converted into bootleg scores, we estimate the alignment using dynamic programming. The most notable characteristic of our system is that it does test-time adaptation and has no trainable weights at all-only a set of about 30 hyperparameters. On a dataset containing 1000 cell phone pictures taken of 100 scores of classical piano music, our system achieves an F measure score of:869 and outperforms baseline systems based on commercial optical music recognition software. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Cellular telephones; Dynamic programming; Information retrieval; Cell phone pictures; Computer vision techniques; Deterministic rule; F-measure scores; Mid-level features; Musical notation; Optical music recognition; Passage retrieval; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Choi K.; Cho K.","Choi, Keunwoo (57190944352); Cho, Kyunghyun (55722769200)","57190944352; 55722769200","Deep unsupervised drum transcription","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095689&partnerID=40&md5=6b50dc9a384ced8bcf6d345331214477","Spotify, Sweden; New York University, Facebook AI Research, United States","Choi K., Spotify, Sweden; Cho K., New York University, Facebook AI Research, United States","We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Large dataset; Audio signal; Ground truth; Deep neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Luo Y.-J.; Agres K.; Herremans D.","Luo, Yin-Jyun (57204050387); Agres, Kat (25222806800); Herremans, Dorien (55533923200)","57204050387; 25222806800; 55533923200","Learning disentangled representations of timbre and pitch for musical instrument sounds using gaussian mixture variational autoencoders","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097188&partnerID=40&md5=d075e5d929a4e7e0243ab08c2f197c64","Singapore University of Technology and Design, Singapore; Institute of High Performance Computing, A STAR, Singapore; Yong Siew Toh Conservatory of Music, National University of Singapore, Singapore","Luo Y.-J., Singapore University of Technology and Design, Singapore, Institute of High Performance Computing, A STAR, Singapore; Agres K., Institute of High Performance Computing, A STAR, Singapore, Yong Siew Toh Conservatory of Music, National University of Singapore, Singapore; Herremans D., Singapore University of Technology and Design, Singapore, Institute of High Performance Computing, A STAR, Singapore","In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model's efficacy using latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high F-scores when tested on our synthesized sounds, which verifies the model's performance of controllable realistic timbre/pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single encoder-decoder architecture, which is evaluated by measuring the shift in the posterior of instrument classification. Our in-depth evaluation confirms the model's ability to successfully disentangle timbre and pitch. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Acoustic waves; Decoding; Gaussian distribution; Information retrieval; Learning systems; Musical instruments; Signal encoding; Controllable synthesis; Depth evaluations; Encoder-decoder architecture; Mixture components; Multiple instruments; Musical instrument sounds; Space visualization; Synthesized sounds; Classification (of information)","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lee J.H.; Choi H.-S.; Lee K.","Lee, Jie Hwan (57217332391); Choi, Hyeong-Seok (57212853224); Lee, Kyogu (8597995500)","57217332391; 57212853224; 8597995500","Audio Query-based music source separation","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096770&partnerID=40&md5=9025313df061f9d46110ac2aaaa82201","Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","Lee J.H., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Choi H.-S., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Deep learning; Encoding (symbols); Information retrieval; Mixtures; Separation; Separators; Vector spaces; Latent vectors; Multiple source; Music information retrieval; Music source separations; Number of sources; Single networks; Target signals; Training sample; Source separation","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Chen T.-P.; Su L.","Chen, Tsung-Ping (57207993823); Su, Li (55966919100)","57207993823; 55966919100","Harmony transformer: Incorporating chord segmentation into harmony recognition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097185&partnerID=40&md5=81ac54128b45e5f80afcf86ffdabfe77","Institute of Information Science, Academia Sinica, Taiwan","Chen T.-P., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep learning; Information retrieval; Natural language processing systems; Pattern recognition; Computational approach; Hierarchical structures; Highly-correlated; NAtural language processing; Recognition process; Sequential learning; Structural information; Symbol recognition; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Katsiavalos A.; Collins T.; Battey B.","Katsiavalos, Andreas (57191410084); Collins, Tom (37088212600); Battey, Bret (6506877475)","57191410084; 37088212600; 6506877475","An initial computational model for musical schemata theory","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094518&partnerID=40&md5=3528a71690278f53a16a1faae7e6fd4d","Music, Technology and Innovation Research Centre, De Montfort University, United Kingdom; Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom; Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States","Katsiavalos A., Music, Technology and Innovation Research Centre, De Montfort University, United Kingdom; Collins T., Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom, Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States; Battey B., Music, Technology and Innovation Research Centre, De Montfort University, United Kingdom","Musical schemata theory entails the classification of subphrase-length progressions in melodic, harmonic and metric feature-sets as named entities (e.g., 'Romanesca', 'Meyer', 'Cadence', etc.), where a musical schema is characterized by factors such as music content and form, position and tonal function within phrase structure, and interrelation with other schemata. To examine and automate the task of musical schemata classification, we developed a novel musical schemata classifier. First, we tested methods for exact and approximate matching of user-defined schemata prototypes, to establish the notions of identity and similarity between composite music patterns. Next, we examined methods for schemata prototype extraction from collections of same-labelled annotated examples, performing training and testing sessions similar to supervised learning approaches. The performance of the above tasks was verified using the same annotated dataset of 40 keyboard sonata excerpts from pre-Classical and Classical periods. Our evaluation of the classifier sheds light on: (a) ability to parse and interpret music information, (b) similarity methods for composite music patterns, (c) categorization methods for polyphonic music. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Computation theory; Information retrieval; Approximate matching; Categorization methods; Computational model; Music information; Polyphonic music; Supervised learning approaches; Training and testing; User-defined schema; Classification (of information)","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Koutlis C.; Schinas M.; Gkatziaki V.; Papadopoulos S.; Kompatsiaris Y.","Koutlis, Christos (56653508100); Schinas, Manos (57188693848); Gkatziaki, Vasiliki (57189056645); Papadopoulos, Symeon (23095370800); Kompatsiaris, Yiannis (57208472535)","56653508100; 57188693848; 57189056645; 23095370800; 57208472535","Data-driven song recognition estimation using collective memory dynamics models","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095615&partnerID=40&md5=042ba6529aae2f1707659c2cc043856d","CERTH-ITI, Thessaloniki, Greece","Koutlis C., CERTH-ITI, Thessaloniki, Greece; Schinas M., CERTH-ITI, Thessaloniki, Greece; Gkatziaki V., CERTH-ITI, Thessaloniki, Greece; Papadopoulos S., CERTH-ITI, Thessaloniki, Greece; Kompatsiaris Y., CERTH-ITI, Thessaloniki, Greece","Cultural products such as music tracks intend to be appreciated and recognized by a portion of the audience. However, no matter how highly recognized a song might be at the beginning of its life, its recognition will inevitably and progressively decay. The mechanism that governs this decreasing trajectory could be modelled as a forgetting curve or a collective memory decay process. Here, we propose a composite model, termed T-REC, that involves chart data, YouTube views, Spotify popularity of tracks and forgetting curve dynamics with the purpose of estimating song recognition levels. We also present a comparative study, involving state-of-the-art and baseline models based on ground truth data from a survey that we conducted regarding the recognition level of 100 songs in Sweden. Our method is found to perform best among this ensemble of models. A remarkable finding of our study pertains to the role of the number of weeks a song remains in the charts, which is found to be a major factor for the accurate estimation of the song recognition level. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Accurate estimation; Collective memory; Comparative studies; Composite modeling; Ensemble of models; Forgetting curve; Ground truth data; State of the art; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Epure E.V.; Khlif A.; Hennequin R.","Epure, Elena V. (56226375200); Khlif, Anis (57210232867); Hennequin, Romain (36504173400)","56226375200; 57210232867; 36504173400","Leveraging knowledge bases and parallel annotations for music genre translation","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094405&partnerID=40&md5=6d9c190a81f384894448a29acbf06d75","Deezer R&D","Epure E.V., Deezer R&D; Khlif A., Deezer R&D; Hennequin R., Deezer R&D","Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Knowledge based systems; Knowledge management; Maximum likelihood; Petroleum reservoir evaluation; Knowledge based; Knowledge basis; Knowledge sources; Large corpora; Maximum a posteriori; Multi-label classifications; Statistical translation; Taxonomy mappings; Logistic regression","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Donahue C.; Mao H.H.; Li Y.E.; Cottrell G.W.; McAuley J.","Donahue, Chris (35213199400); Mao, Huanru Henry (57214218550); Li, Yiting Ethan (57217332405); Cottrell, Garrison W. (7102792906); McAuley, Julian (14822353500)","35213199400; 57214218550; 57217332405; 7102792906; 14822353500","LakhNES: Improving multi-instrumental music generation with cross-domain pre-training","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093830&partnerID=40&md5=1680add114525a3993be791d414c6091","Department of Music, UC San Diego, United States; Department of Computer Science, UC San Diego, United States","Donahue C., Department of Music, UC San Diego, United States; Mao H.H., Department of Computer Science, UC San Diego, United States; Li Y.E., Department of Computer Science, UC San Diego, United States; Cottrell G.W., Department of Computer Science, UC San Diego, United States; McAuley J., Department of Computer Science, UC San Diego, United States","We are interested in the task of generating multiinstrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation-here we adapt it to the multiinstrumental setting. Transformers are complex, highdimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-instrument scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music, namely the Lakh MIDI dataset. Despite differences between the two corpora, we find that this transfer learning procedure improves both quantitative and qualitative performance for our primary task. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Transfer learning; High-dimensional; Language model; Large amounts of data; Large volumes; Learning procedures; Long-term structures; Sequence data; Symbolic data; Large dataset","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Chowdhury S.; Vall A.; Haunschmid V.; Widmer G.","Chowdhury, Shreyan (57217331670); Vall, Andreu (56286378600); Haunschmid, Verena (57194241196); Widmer, Gerhard (7004342843)","57217331670; 56286378600; 57194241196; 7004342843","Towards explainable music emotion recognition: The route via Mid-level features","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094248&partnerID=40&md5=cc71ebfffe1a6bb3ed17e525e6f36f7f","Institute of Computational Perception, Johannes Kepler University Linz, Austria","Chowdhury S., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Vall A., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Haunschmid V., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria","Emotional aspects play an important part in our interaction with music. However, modelling these aspects in MIR systems have been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its predictions, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features and observe that the loss in predictive performance of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the midlevel features is justified by the gain in explainability of the predictions. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep neural networks; Information retrieval; Emotion predictions; Emotional aspect; Mid-level features; Music emotions; Musical pieces; Perceptual feature; Predictive performance; Subjective experiences; Forecasting","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Verma H.; Thickstun J.","Verma, Harsh (57217331547); Thickstun, John (57204045678)","57217331547; 57204045678","Convolutional composer classification","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094400&partnerID=40&md5=fb32a8003b262b92ca566ed44a049762","Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States","Verma H., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Thickstun J., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States","This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; 20th century; End to end; Learning approach; Musical score; N-grams; Convolution","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Gillick J.; Cella C.-E.; Bamman D.","Gillick, Jon (57195418754); Cella, Carmine-Emanuele (36614069100); Bamman, David (15041728000)","57195418754; 36614069100; 15041728000","Estimating unobserved audio features for Target-based orchestration","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096544&partnerID=40&md5=54719e9d05a725a7db88bbd8e5f068e7","School of Information, University of California, Berkeley, United States; CNMAT, University of California, Berkeley, United States","Gillick J., School of Information, University of California, Berkeley, United States; Cella C.-E., CNMAT, University of California, Berkeley, United States; Bamman D., School of Information, University of California, Berkeley, United States","Target-based assisted orchestration can be thought of as the process of searching for optimal combinations of sounds to match a target sound, given a database of samples, a similarity metric, and a set of constraints. A typical solution to this problem is a proposed orchestral score where candidates are ranked by similarity in some feature space between the target sound and the mixture of audio samples in the database corresponding to the notes in the score; in the orchestral setting, valid scores may contain dozens of instruments sounding simultaneously. Generally, target-based assisted orchestration systems consist of a combinatorial optimization algorithm and a constraint solver that are jointly optimized to find valid solutions. A key step in the optimization involves generating a large number of combinations of sounds from the database and then comparing the features of each mixture of sounds with the target sound. Because of the high computational cost required to synthesize a new audio file and then compute features for every combination of sounds, in practice, existing systems instead estimate the features of each new mixture using precomputed features of the individual source files making up the combination. Currently, state-of-the-art systems use a simple linear combination to make these predictions, even if the features in use are not themselves linear. In this work, we explore neural models for estimating the features of a mixture of sounds from the features of the component sounds, finding that standard features can be estimated with accuracy significantly better than that of the methods currently used in assisted orchestration systems. We present quantitative comparisons and discuss the implications of our findings for target-based orchestration problems. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio systems; Combinatorial optimization; Database systems; Information retrieval; Mixtures; Combinatorial optimization algorithm; Computational costs; Constraint solvers; Linear combinations; Optimal combination; Quantitative comparison; Similarity metrics; State-of-the-art system; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Park S.Y.; Laplante A.; Lee J.H.; Kaneshiro B.","Park, So Yeon (57194687604); Laplante, Audrey (37110930300); Lee, Jin Ha (57190797465); Kaneshiro, Blair (56884405500)","57194687604; 37110930300; 57190797465; 56884405500","Tunes together: Perception and experience of collaborative playlists","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094640&partnerID=40&md5=e9cd4e3a65fac039af8c33c378800428","Center for Computer Research in Music and Acoustics, Stanford University, United States; École de Bibliothéconomie et des Sciences de l'Information, Université de Montréal, Canada; Information School, University of Washington, United States","Park S.Y., Center for Computer Research in Music and Acoustics, Stanford University, United States; Laplante A., École de Bibliothéconomie et des Sciences de l'Information, Université de Montréal, Canada; Lee J.H., Information School, University of Washington, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States","Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP).We conducted an exploratory study of CP users and non-users (N = 65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes-Practical, Cognitive, and Social- and two connotations-Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music's role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Exploratory studies; Interpersonal relationship; Music information retrieval; Social connection; Social interactions; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Watanabe K.; Goto M.","Watanabe, Kento (57191841548); Goto, Masataka (7403505330)","57191841548; 7403505330","Query-by-blending: A music exploration system blending latent vector representations of lyric word, song audio, and artist","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095697&partnerID=40&md5=3fea99410f50f6ac88db3eee68847713","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Watanabe K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents Query-by-Blending, a novel music exploration system that enables users to find unfamiliar music content by flexibly combining three musical aspects: lyric word, song audio, and artist. Although there are various systems for music retrieval based on the similarity between songs or artists and for music browsing based on visualized songs, it is still difficult to explore unfamiliar content by flexibly combining multiple musical aspects. Query-by- Blending overcomes this difficulty by representing each of the aspects as a latent vector representation (called a ""flavor"" in this paper) that is a distinctive quality felt to be characteristic of a given word/song/artist. By giving a lyric word as a query, for example, a user can find songs and artists whose flavors are similar to the flavor of the query word. Moreover, by giving a query combining (blending) lyric-word and song-audio flavors, the user can interactively explore unfamiliar content containing the blended flavor. This multi-aspect blending was achieved by constructing a novel vector space model into which all of the lyric words, song audio tracks, and artist IDs of a collection can be embedded. In our experiments, we embedded 14,505 lyric words, 433,936 songs, and 44,696 artists into the same shared vector space and found that the system can appropriately calculate similarities between different aspects and blend flavors to find related lyric words, songs, and artists. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Blending; Encoding (symbols); Information retrieval; Vector spaces; Vectors; Audio track; Latent vectors; Multi aspects; Music contents; Music exploration systems; Music retrieval; Query words; Vector space models; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Ycart A.; Stoller D.; Benetos E.","Ycart, Adrien (57192301993); Stoller, Daniel (56928086600); Benetos, Emmanouil (16067946900)","57192301993; 56928086600; 16067946900","A comparative study of neural models for polyphonic music sequence transduction","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094121&partnerID=40&md5=54c5e11862c3a49de2b38b6af8198785","Centre for Digital Music, Queen Mary University of London, United Kingdom","Ycart A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Stoller D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Automatic transcription of polyphonic music remains a challenging task in the field of Music Information Retrieval. One under-investigated point is the post-processing of timepitch posteriograms into binary piano rolls. In this study, we investigate this task using a variety of neural network models and training procedures. We introduce an adversarial framework, that we compare against more traditional training losses. We also propose the use of binary neuron outputs and compare them to the usual real-valued outputs in both training frameworks. This allows us to train networks directly using the F-measure as training objective. We evaluate these methods using two kinds of transduction networks and two different multi-pitch detection systems, and compare the results against baseline note-tracking methods on a dataset of classical piano music. Analysis of results indicates that (1) convolutional models improve results over baseline models, but no improvement is reported for recurrent models; (2) supervised losses are superior to adversarial ones; (3) binary neurons do not improve results; (4) cross-entropy loss results in better or equal performance compared to the F-measure loss. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Bacteriophages; Neural networks; Automatic transcription; Comparative studies; Convolutional model; Music information retrieval; Neural network model; Recurrent models; Training framework; Training procedures; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Ducher J.F.; Esling P.","Ducher, Jean-François (57217330527); Esling, Philippe (36052697700)","57217330527; 36052697700","Folded CQT rcnn for real-time recognition of instrument playing techniques","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094122&partnerID=40&md5=e86df6fabe3985f487def2638c647755","CICM - MUSIDANSE, Université Paris 8, IRCAM (UMR9912 STMS), France","Ducher J.F., CICM - MUSIDANSE, Université Paris 8, IRCAM (UMR9912 STMS), France; Esling P., CICM - MUSIDANSE, Université Paris 8, IRCAM (UMR9912 STMS), France","In the past years, deep learning has produced state-of-theart performance in timbre and instrument classification. However, only a few models currently deal with the recognition of advanced Instrument Playing Techniques (IPT). None of them have a real-time approach of this problem. Furthermore, most studies rely on a single sound bank for training and testing. Their methodology provides no assurance as to the generalization of their results to other sounds. In this article, we extend state-ofthe- art convolutional neural networks to the classification of IPTs. We build the first IPT corpus from independent sound banks, annotate it with the JAMS standard and make it freely available. Our models yield consistently high accuracies on a homogeneous subset of this corpus. However, only a proper taxonomy of IPTs and specifically defined input transforms offer proper resilience when addressing the ""minus-1db"" methodology, which assesses the ability of the models to generalize. In particular, we introduce a novel Folded Constant Q-Transform adjusted to the requirements of IPT classification. Finally we discuss the use of our classifier in real-time. © Jean-François Ducher, Philippe Esling.","","Convolutional neural networks; Information retrieval; Constant q transforms; Playing techniques; Real time; Real time recognition; Training and testing; Deep learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Wei I.-C.; Wu C.-W.; Su L.","Wei, I-Chieh (57205426137); Wu, Chih-Wei (57188865070); Su, Li (55966919100)","57205426137; 57188865070; 55966919100","Generating structured drum pattern using variational autoencoder and self-similarity matrix","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096142&partnerID=40&md5=de8ddedd999e9b3efb1351f3185b6eed","Institute of Information Science, Academia Sinica, Taiwan; Netflix, Inc., United States","Wei I.-C., Institute of Information Science, Academia Sinica, Taiwan; Wu C.-W., Netflix, Inc., United States; Su L., Institute of Information Science, Academia Sinica, Taiwan","Drum pattern generation is a task that focuses on the rhythmic aspect of music and aims at generating percussive sequences. With the advancement of machine learning techniques, several models have been proven useful in producing compelling results. However, one of the main challenges is to generate structurally cohesive sequences. In this study, a drum pattern generation model based on Variational Autoencoders (VAEs) is presented; Specifically, the proposed model is built to generate symbolic drum patterns given an accompaniment that consists of melodic sequences. A self-similarity matrix (SSM) is incorporated in the process for encapsulating structural information. Both the objective evaluation and the subjective listening test highlight the model's capability of creating musically meaningful transitions on structural boundaries. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Machine learning techniques; Model-based OPC; Objective evaluation; Pattern Generation; Self-similarity matrix; Structural boundary; Structural information; Subjective listening test; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Shi Z.; Sapp C.S.; Arul K.; McBride J.; Smith J.O.","Shi, Zhengshan (56768547200); Sapp, Craig Stuart (14032002500); Arul, Kumaran (57210209840); McBride, Jerry (39963300600); Smith, Julius O. (7410167523)","56768547200; 14032002500; 57210209840; 39963300600; 7410167523","Supra: Digitizing the stanford university piano roll archive","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096382&partnerID=40&md5=45f693d3810b985d8bb6f3eac6bc47f6","Center for Computer Research in Music and Acoustics, Stanford University, United States; Center for Computer Assisted Research in the Humanities, Stanford University, United States; Department of Music, Stanford University, United States; Music Library and Archive of Recorded Sound, Stanford University, United States","Shi Z., Center for Computer Research in Music and Acoustics, Stanford University, United States; Sapp C.S., Center for Computer Assisted Research in the Humanities, Stanford University, United States; Arul K., Department of Music, Stanford University, United States; McBride J., Music Library and Archive of Recorded Sound, Stanford University, United States; Smith J.O., Center for Computer Research in Music and Acoustics, Stanford University, United States","This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stanford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a ""raw"" MIDI file of hole data, an ""expressive"" MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, ""SUPRA-RW"" from a selection of ""red Welte""-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Large dataset; Audio files; Digital access; Expressive performance; High resolution image; MIDI files; Online database; Stanford University; Twentieth century; Musical instruments","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Janssen B.; Collins T.; Yuping I.R.","Janssen, Berit (56448253700); Collins, Tom (37088212600); Yuping, Iris Ren (57210197214)","56448253700; 37088212600; 57210197214","Algorithmic ability to predict the musical future: Datasets and evaluation","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093835&partnerID=40&md5=bddd2a972592f9433ba4f9b66f050f08","Digital Humanities Lab, Department of Humanities, Utrecht University, Netherlands; Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom; Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States; Department of Information and Computing Sciences, Utrecht University, Netherlands","Janssen B., Digital Humanities Lab, Department of Humanities, Utrecht University, Netherlands; Collins T., Music, Science and Technology Research Cluster, Department of Music, University of York, United Kingdom, Music Artificial Intelligence Algorithms, Inc., Davis, CA, United States; Yuping I.R., Department of Information and Computing Sciences, Utrecht University, Netherlands","Music prediction and generation have been of recurring interest in the field of music informatics: many models that emulate listeners' musical expectancies, or that produce novel musical content have been introduced over the past few decades. So far, these models have mostly been evaluated in isolation, following diverse evaluation strategies. Our paper provides an overview of the new MIREX task Patterns for Prediction. We introduce a dataset, which contains monophonic and polyphonic data, both in symbolic and audio representations. We suggest a standardized evaluation procedure to compare algorithmic musical predictions. We compare two neural network models to a baseline model and show that algorithmic approaches can correctly predict about a third of a monophonic segment, and around half of a polyphonic segment, with one of the neural network models achieving best results. However, other approaches to algorithmic music prediction are needed to achieve a more rounded picture of the potential of state-of-the-art methods of music prediction. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Algorithmic approach; Algorithmic music; Audio representation; Evaluation strategies; Music Informatics; Music prediction; Neural network model; State-of-the-art methods; Forecasting","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Wiggins A.; Kim Y.","Wiggins, Andrew (57217330637); Kim, Youngmoo (58290515000)","57217330637; 58290515000","Guitar tablature estimation with a convolutional neural network","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095304&partnerID=40&md5=aa87f623b26c66a68e916b8b33581a99","Drexel University, Dept. of Electrical and Computer Engineering, United States","Wiggins A., Drexel University, Dept. of Electrical and Computer Engineering, United States; Kim Y., Drexel University, Dept. of Electrical and Computer Engineering, United States","Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation, followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset [24], and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Audio recordings; Convolution; Information retrieval; Musical instruments; Statistical tests; Acoustic guitar; Audio data; Multi-pitch estimations; Pitch estimation; Playability; State of the art; Convolutional neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Akama T.","Akama, Taketo (57224648722)","57224648722","Controlling symbolic music generation based on concept learning from domain knowledge","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094730&partnerID=40&md5=c10209e7e7e6519a49e2dc1f812e51c6","Sony Computer Science Laboratories, Tokyo, Japan","Akama T., Sony Computer Science Laboratories, Tokyo, Japan","Machine learning allows automatic construction of generative models for music. However, they are learned from only the succession of notes itself without explicitly employing domain knowledge of musical concepts such as rhythm, contour, and fragmentation & consolidation. We approximate such musical domain knowledge as a function, and feed it into our model. Then, two decoupled spaces are learned: the extraction space that captures the target concept, and the residual space that captures the remainder. For monophonic symbolic music, our model exhibits high decoupling/modeling performance. Controllability in generation is improved: (i) our interpolation enables concept-aware flexible control over blending two musical fragments, and (ii) our variation generation enables users to make concept-aware adjustable variations. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Blending; Automatic construction; Concept learning; Domain knowledge; Flexible control; Generative model; Musical concepts; Musical domains; Residual spaces; Information retrieval","T. Akama; Sony Computer Science Laboratories, Tokyo, Japan; email: taketo.akama@sony.com","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Huang J.; Lerch A.","Huang, Jiawen (58603563100); Lerch, Alexander (22034963000)","58603563100; 22034963000","Automatic assessment of sight-reading exercises","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096996&partnerID=40&md5=579bdaa350b175f5b0c7208433cb01e1","Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, United States","Huang J., Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, United States","Sight-reading requires a musician to decode, process, and perform a musical score quasi-instantaneously and without rehearsal. Due to the complexity of this task, it is difficult to assess the proficiency of a sight-reading performance, and it is even more challenging to model its human assessment. This study aims at evaluating and identifying effective features for automatic assessment of sight-reading performance. The evaluated set of features comprises taskspecific, hand-crafted, and interpretable features designed to represent various aspect of sight-reading performance covering parameters such as intonation, timing, dynamics, and score continuity. The most relevant features are identified by Principal Component Analysis and forward feature selection. For context, the same features are also applied to the assessment of rehearsed student music performances and compared across different assessment categories. The results show potential of automatic assessment models for sight-reading and the relevancy of different features as well as the contribution of different feature groups to different assessment categories. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Automatic assessment; Feature groups; Forward feature selections; Human assessment; Music performance; Musical score; Reading performance; Relevant features; Principal component analysis","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Grachten M.; Deruty E.; Tanguy A.","Grachten, Maarten (8974600000); Deruty, Emmanuel (35503270700); Tanguy, Alexandre (57217331148)","8974600000; 35503270700; 57217331148","Auto-adaptive resonance equalization using dilated residual networks","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097445&partnerID=40&md5=6d6b22c5f3ea3c4bccb215e2ae9c38db","Contractor for Sony CSL Paris, France; Sony CSL Paris, France; Yascore, Paris, France","Grachten M., Contractor for Sony CSL Paris, France; Deruty E., Sony CSL Paris, France; Tanguy A., Yascore, Paris, France","In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances, to be attenuated by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from a listening experiment. We test two distinct network architectures for the predictive model and find that an agnostic network architecture operating directly on the audio signal is on a par with a network architecture that relies on hand-designed features. Both architectures significantly improve a baseline approach to predicting human-preferred resonance attenuation factors. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Deep neural networks; Equalizers; Information retrieval; Resonance; Adaptive resonance; Attenuation factors; Audio signal; Dynamic equalizers; Empirical data; Predictive modeling; Resonance attenuation; Two component systems; Network architecture","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Nuttall T.; Casado M.G.; Tarifa V.N.; Repetto R.C.; Serra X.","Nuttall, Thomas (57217330524); Casado, Miguel García (57217332182); Tarifa, Víctor Núñez (57217331446); Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57217330524; 57217332182; 57217331446; 57200495266; 55892979900","Contributing to new musicological theories with computational methods: The case of centonization in arab-andalusian music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097203&partnerID=40&md5=ab903a208cb3b79d23bd615d0c1a9ce2","Music Technology Group, Universitat Pompeu Fabra, Spain","Nuttall T., Music Technology Group, Universitat Pompeu Fabra, Spain; Casado M.G., Music Technology Group, Universitat Pompeu Fabra, Spain; Tarifa V.N., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Arab-Andalusian music was formed in the medieval Islamic territories of the Iberian Peninsula, drawing on local traditions and assuming Arabic influences. The expert performer and researcher of the Moroccan tradition of this music, Amin Chaachoo, is developing a theory whose last formulation was recently published in La Musique Hispano-Arabe, al-Ala (2016), which argues that centonization, a melodic composition technique used in Gregorian chant, was also utilized for the creation of this repertoire. In this paper we aim to contribute to Chaachoo's theory by means of tf-idf analysis. A high-order n-gram model is applied to a corpus of 149 prescriptive transcriptions of heterophonic recordings, representing each as an unordered multiset of patterns. Computing the tf-idf statistic of each pattern in this corpus provides a means by which we can rank and compare motivic content across nawabat, distinct musical forms of the tradition. For each nawba, an empirical comparison is made between patterns identified as significant via our approach and those proposed by Chaachoo. Ultimately we observe considerable agreement between the two pattern sets and go further in proposing new, unique and as yet undocumented patterns that occur at least as frequently and with at least as much importance as those in Chaachoo's proposals. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Composition technique; Empirical - comparisons; Gregorian; High-order; Iberian Peninsula; Multiset; N-gram modeling; Pattern set; Computation theory","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Ens J.; Pasquier P.","Ens, Jeff (57210203647); Pasquier, Philippe (8850202000)","57210203647; 8850202000","Quantifying musical style: Ranking symbolic music based on similarity to a style","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095386&partnerID=40&md5=d92bd09e0808b9b97f9f8dba3b9880c4","Simon Fraser University, Canada","Ens J., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada","Modelling human perception of musical similarity is critical for the evaluation of generative music systems, musicological research, and many Music Information Retrieval tasks. Although human similarity judgments are the gold standard, computational analysis is often preferable, since results are often easier to reproduce, and computational methods are much more scalable. Moreover, computation based approaches can be calculated quickly and on demand, which is a prerequisite for use with an online system. We propose StyleRank, a method to measure the similarity between a MIDI file and an arbitrary musical style delineated by a collection of MIDI files. MIDI files are encoded using a novel set of features and an embedding is learned using Random Forests. Experimental evidence demonstrates that StyleRank is highly correlated with human perception of stylistic similarity, and that it is precise enough to rank generated samples based on their similarity to the style of a corpus. In addition, similarity can be measured with respect to a single feature, allowing specific discrepancies between generated samples and a particular musical style to be identified. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Computational methods; Computer music; Decision trees; Information retrieval; Computational analysis; Experimental evidence; Generative musics; Highly-correlated; Human perception; Music information retrieval; Musical similarity; Stylistic similarity; Search engines","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Böck S.; Davies M.E.P.; Knees P.","Böck, Sebastian (55413719000); Davies, Matthew E.P. (55349903900); Knees, Peter (8219023200)","55413719000; 55349903900; 8219023200","Multi-task learning of tempo and beat: Learning one to improve the other","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093880&partnerID=40&md5=696893156b001e35005534e129bd962c","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; INESC TEC, Porto, Portugal; TU Wien, Vienna, Austria","Böck S., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, TU Wien, Vienna, Austria; Davies M.E.P., INESC TEC, Porto, Portugal; Knees P., TU Wien, Vienna, Austria","We propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolutional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Information retrieval; Multi-task learning; Beat tracking; Convolutional networks; Musical audio; Mutual informations; Shared representations; State-of-the-art performance; Tempo estimations; Training data; Learning systems","S. Böck; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: sebastian.boeck@ofai.at","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lattner S.; Dorfler M.; Arzt A.","Lattner, Stefan (56989751900); Dorfler, Monika (13609544300); Arzt, Andreas (36681791200)","56989751900; 13609544300; 36681791200","Learning complex basis functions for invariant representations of audio","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094175&partnerID=40&md5=eb7a137853f3948ff15fa106b4e4761d","Sony Computer Science Laboratories (CSL), Paris, France; NuHAG, Faculty of Mathematics, University Vienna, Austria; Institute of Computational Perception, JKU Linz, Austria","Lattner S., Sony Computer Science Laboratories (CSL), Paris, France; Dorfler M., NuHAG, Faculty of Mathematics, University Vienna, Austria; Arzt A., Institute of Computational Perception, JKU Linz, Austria","Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant ""magnitude space"" and a transformation-variant ""phase space"". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method, is available online. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Functions; Information retrieval; Object oriented programming; Phase space methods; Auto encoders; Complex basis; Invariant representation; Music information retrieval; Orthogonal transformations; Spectrograms; State of the art; Transformation invariants; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Tanprasert T.; Jenrungrot T.; Müller M.; Tsai T.J.","Tanprasert, Thitaree (57213666497); Jenrungrot, Teerapat (57217331562); Müller, Meinard (7404689873); Tsai, T.J. (55752421100)","57213666497; 57217331562; 7404689873; 55752421100","Midi-sheet music alignment using bootleg score synthesis","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094219&partnerID=40&md5=b7227cc6d3bc836c41b70c84eb970e21","Harvey Mudd College, Claremont, CA, United States; International Audio Laboratories Erlangen, Germany","Tanprasert T., Harvey Mudd College, Claremont, CA, United States; Jenrungrot T., Harvey Mudd College, Claremont, CA, United States; Müller M., International Audio Laboratories Erlangen, Germany; Tsai T.J., Harvey Mudd College, Claremont, CA, United States","MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images. Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain. Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis. Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead. Finally, we align the bootleg representations using a simple variant of dynamic time warping. On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97:3% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Baseline systems; Bounding box; Dynamic time warping; Error tolerance; Filling in; Image domain; Optical music recognition; Alignment","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"De Reuse T.; Fujinaga I.","De Reuse, Timothy (57217332346); Fujinaga, Ichiro (9038140900)","57217332346; 9038140900","Pattern clustering in monophonic music by learning a non-linear embedding from human annotations","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096639&partnerID=40&md5=fa763b64196c865e2eca87ca9affca42","Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Canada","De Reuse T., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Canada","Musical pattern discovery algorithms find instances of repetition in symbolic music, allowing for some userspecifiable amount of variation between identified repetitions; however, they can yield an intractably large number of discovered patterns when allowing for even small amounts of variation. This is commonly addressed by defining some heuristic notion of pattern significance, and returning only the most significant patterns. This paper develops a method of pattern discovery that models human judgement of what constitutes a significant pattern by incorporating annotations of repeated patterns, avoiding the need to design heuristics. We take pattern discovery as a clustering task, where the input is a set of passages of monophonic music, represented as vectors of extracted features, and the output clusters correspond to discovered patterns. The human annotations are used to train a neural network to learn a lowdimensional embedding of the feature space that maps passages of music close together when they are occurrences of the same ground-truth pattern. The results of this approach match up with the annotations significantly better than the results of an approach using clustering without subspace learning. We provide examples of the types of patterns that this method tends to discover and discuss its feasibility and practicality as a tool for extracting useful information about repetitive structure in music. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Embeddings; Information retrieval; Design heuristics; Human annotations; Pattern clustering; Pattern discovery; Repeated patterns; Repetitive structure; Significant patterns; Subspace learning; Heuristic methods","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Holzapfel A.; Benetos E.","Holzapfel, Andre (18041818000); Benetos, Emmanouil (16067946900)","18041818000; 16067946900","Automatic music transcription and ethnomusicology: A user study","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096647&partnerID=40&md5=3b951f0fdce645f74f36b81d683ae2c2","KTH Royal Institute of Technology, Sweden; Queen Mary University of London, United Kingdom","Holzapfel A., KTH Royal Institute of Technology, Sweden; Benetos E., Queen Mary University of London, United Kingdom","Converting an acoustic music signal into music notation using a computer program has been at the forefront of music information research for several decades, as a task referred to as automatic music transcription (AMT). However, current AMT research is still constrained to system development followed by quantitative evaluations; it is still unclear whether the performance of AMT methods is considered sufficient to be used in the everyday practice of music scholars. In this paper, we propose and carry out a user study on evaluating the usefulness of automatic music transcription in the context of ethnomusicology. As part of the study, we recruited 16 participants who were asked to transcribe short musical excerpts either from scratch or using the output of an AMT system as a basis. We collect and analyze quantitative measures such as transcription time and effort, and a range of qualitative feedback from study participants, which includes user needs, criticisms of AMT technologies, and links between perceptual and quantitative evaluations on AMT outputs. The results show no quantitative advantage of using AMT, but important indications regarding appropriate user groups and evaluation measures are provided. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Automatic music transcription; Evaluation measures; Music information; Music notation; Qualitative feedback; Quantitative evaluation; Quantitative measures; System development; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Harasim D.; O'Donnell T.J.; Rohrmeier M.","Harasim, Daniel (57192074560); O'Donnell, Timothy J. (56368153500); Rohrmeier, Martin (6507901506)","57192074560; 56368153500; 6507901506","Harmonic syntax in time rhythm improves grammatical models of harmony","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096645&partnerID=40&md5=983d49e900b8f8049c018a95b872606f","Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland; Department of Linguistics, McGill University, Canada","Harasim D., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland; O'Donnell T.J., Department of Linguistics, McGill University, Canada; Rohrmeier M., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland","Music is hierarchically structured, both in how it is perceived by listeners and how it is composed. Such structure can be elegantly captured using probabilistic grammatical models similar to those used to study natural language. They address the complexity of the structure using abstract categories in a recursive formalism. Most existing grammatical models of musical structure focus on one single dimension of music - such as melody, harmony, or rhythm. While these grammar models often work well on short musical excerpts, accurate analysis of longer pieces requires taking into account the constraints from multiple domains of structure. The present paper proposes abstract product grammars - a formalism which integrates multiple dimensions of musical structure into a single grammatical model - along with efficient parsing and inference algorithms for this formalism. We use this model to study the combination of hierarchically-structured harmonic syntax and hierarchically-structured rhythmic information. The latter is modeled by a novel grammar of rhythm that is capable of expressing temporal regularities in musical phrases. It integrates grouping structure and meter. The combined model of harmony and rhythm outperforms both single-dimension models in computational experiments. All models are trained and evaluated on a treebank of hand-annotated Jazz standards. © Daniel Harasim, Timothy J. O'Donnell, Martin Rohrmeier .","","Inference engines; Information retrieval; Accurate analysis; Computational experiment; Grammatical models; Inference algorithm; Multiple dimensions; Multiple domains; Musical structures; Natural languages; Syntactics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lee J.H.; Pritchard L.; Hubbles C.","Lee, Jin Ha (57190797465); Pritchard, Liz (57196460602); Hubbles, Chris (57193931567)","57190797465; 57196460602; 57193931567","Can we listen to it together?: Factors influencing reception of music recommendations and post-recommendation behavior","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095313&partnerID=40&md5=3dce03abf980ee0303e0aa89a7a6094f","University of Washington, United States","Lee J.H., University of Washington, United States; Pritchard L., University of Washington, United States; Hubbles C., University of Washington, United States","Few prior studies on music recommendations investigate the context in which users receive the recommendations, and what impact the recommendation has on the user. In this paper, we aim to better understand the factors that affect people's decisions as to whether they choose to listen to music recommendations and how the recommendations impact their music-listening behaviors. We conducted an online survey asking about people's past experiences on giving and receiving music recommendations. We found that in addition to the aesthetic qualities of music and the respondent's taste, expectations regarding the delivery (e.g., timing, persistence) of the recommendations, familiarity, trust in the recommender's abilities, and the rationale for suggestions were important factors. We discuss the implications for the design of music recommenders based on the findings, including better rationale for and accessibility of recommended music, improved saving options, and more targeted delivery at specific times. The data also suggests disparities in how people wish to receive music recommendations and what will influence them to listen to recommendations, versus how they would like to offer recommendations to others. In addition, the findings highlight the importance of music recommendations in people's existing social relationships and their role in building/improving new relationships. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Aesthetic qualities; In-buildings; Music recommendation; Online surveys; Social relationships; Targeted delivery; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Thickstun J.; Harchaoui Z.; Foster D.P.; Kakade S.M.","Thickstun, John (57204045678); Harchaoui, Zaid (22234232100); Foster, Dean P. (7403219373); Kakade, Sham M. (6603824936)","57204045678; 22234232100; 7403219373; 6603824936","Coupled recurrent models for polyphonic music composition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095276&partnerID=40&md5=ab7de4eb33c91039ede91fd9aa4c1433","Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Department of Statistics, University of Washington, Seattle, WA, United States; Amazon, NY, United States","Thickstun J., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States; Harchaoui Z., Department of Statistics, University of Washington, Seattle, WA, United States; Foster D.P., Amazon, NY, United States; Kakade S.M., Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, United States, Department of Statistics, University of Washington, Seattle, WA, United States","This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Conditional distribution; Music composition; Musical score; Neural models; Polyphonic music; Recurrent models; Temporal structures; Train model; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Chen W.; Keast J.; Moody J.; Moriarty C.; Villalobos F.; Winter V.; Zhang X.; Lyu X.; Freeman E.; Wang J.; Cai S.; Kinnaird K.M.","Chen, Wenqin (57217331996); Keast, Jessica (57217332319); Moody, Jordan (57217330838); Moriarty, Corinne (57217330064); Villalobos, Felicia (57217332383); Winter, Virtue (57217331439); Zhang, Xueqi (57217332246); Lyu, Xuanqi (57217331289); Freeman, Elizabeth (57217330213); Wang, Jessie (57217332420); Cai, Sherry (57217330688); Kinnaird, Katherine M. (57205699026)","57217331996; 57217332319; 57217330838; 57217330064; 57217332383; 57217331439; 57217332246; 57217331289; 57217330213; 57217332420; 57217330688; 57205699026","Data usage in MIR: History & future recommendations","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094220&partnerID=40&md5=25ac68120cda610b1db1cf291f7b8163","Smith College, Northampton, MA, United States","Chen W., Smith College, Northampton, MA, United States; Keast J., Smith College, Northampton, MA, United States; Moody J., Smith College, Northampton, MA, United States; Moriarty C., Smith College, Northampton, MA, United States; Villalobos F., Smith College, Northampton, MA, United States; Winter V., Smith College, Northampton, MA, United States; Zhang X., Smith College, Northampton, MA, United States; Lyu X., Smith College, Northampton, MA, United States; Freeman E., Smith College, Northampton, MA, United States; Wang J., Smith College, Northampton, MA, United States; Cai S., Smith College, Northampton, MA, United States; Kinnaird K.M., Smith College, Northampton, MA, United States","The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility. Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Copyright law; Data access; Data usage; Large companies; Large parts; Music data; Reproducibilities; Research communities; Information retrieval","K.M. Kinnaird; Smith College, Northampton, United States; email: kkinnaird@smith.edu","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Falcão F.; Bozkurt B.; Serra X.; Andrade N.; Baysal O.","Falcão, Felipe (57210190038); Bozkurt, Baris (23476648700); Serra, Xavier (55892979900); Andrade, Nazareno (7003429497); Baysal, Ozan (57217331858)","57210190038; 23476648700; 55892979900; 7003429497; 57217331858","A dataset of rhythmic pattern reproductions and baseline automatic assessment system","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096761&partnerID=40&md5=528c64465e5be0b8cb1a2c8a6b6e7bb6","Universidade Federal de Campina Grande, Brazil; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Izmir Demokrasi University, Turkey; Istanbul Technical University, Turkey","Falcão F., Universidade Federal de Campina Grande, Brazil, Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bozkurt B., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, Izmir Demokrasi University, Turkey; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Andrade N., Universidade Federal de Campina Grande, Brazil; Baysal O., Istanbul Technical University, Turkey","This work presents a novel dataset comprised of audio and jury evaluations for rhythmic pattern reproduction performances by students applying for a conservatory. Data was collected in-loco during entrance exams where students were asked to imitate a set of rhythmic patterns played by teachers. In addition to the pass or fail grades provided by the members of the jury during the exam sessions, a subset of the data was also evaluated by external annotators on a 4-level scale. A baseline automatic assessment system is presented to demonstrate the usefulness of the dataset. Preliminary results deliver an accuracy of 76% for a simple pass/fail logistic regression classifier and a mean average error of 0.59 for a linear regression grade estimator. The implementation is also made publicly available to serve as baseline for alternative assessments systems that may leverage the dataset. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Cell proliferation; Information retrieval; 4-level; Alternative assessment; Automatic assessment; Average errors; Logistic regression classifier; Rhythmic patterns; Logistic regression","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Nieto O.; McCallum M.; Davies M.E.P.; Robertson A.; Stark A.; Egozy E.","Nieto, Oriol (55583364500); McCallum, Matthew (55551882200); Davies, Matthew E. P. (55349903900); Robertson, Andrew (56301105400); Stark, Adam (36629517800); Egozy, Eran (57194519843)","55583364500; 55551882200; 55349903900; 56301105400; 36629517800; 57194519843","The harmonix set: Beats, downbeats, and functional segment annotations of western popular music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095398&partnerID=40&md5=69e01d6c9b04cb915c21333dd39b1565","Pandora Media, Inc., Oakland, CA, United States; INESC TEC, Porto, Portugal; Ableton AG, Berlin, Germany; MIMU, London, United Kingdom; MIT, Cambridge, MA, United States","Nieto O., Pandora Media, Inc., Oakland, CA, United States; McCallum M., Pandora Media, Inc., Oakland, CA, United States; Davies M.E.P., INESC TEC, Porto, Portugal; Robertson A., Ableton AG, Berlin, Germany; Stark A., MIMU, London, United Kingdom; Egozy E., MIT, Cambridge, MA, United States","We introduce the Harmonix set: a collection of annotations of beats, downbeats, and functional segmentation for over 900 full tracks that covers a wide range of western popular music. Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we seek to foster research that focuses on multiple retrieval tasks at once. The dataset includes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party information or audio data when available. We describe the methodology employed in acquiring this set, including the annotation process and song selection. In addition, an initial data exploration of the annotations and actual dataset content is conducted. Finally, we provide a series of baselines of the Harmonix set with reference beat-trackers, downbeat estimation, and structural segmentation algorithms. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio data; Data exploration; Functional segments; Music information; Popular music; Segmentation algorithms; Third parties; Information retrieval","O. Nieto; Pandora Media, Inc., Oakland, United States; email: onieto@pandora.com","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Parmer T.; Ahn Y.Y.","Parmer, Thomas (57217330641); Ahn, Yong-Yeol (8454031800)","57217330641; 8454031800","Evolution of the informational complexity of contemporary western music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095277&partnerID=40&md5=721060746755e34d402c63c85bf1cb1a","School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States","Parmer T., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States; Ahn Y.Y., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States","We measure the complexity of songs in the Million Song Dataset (MSD) in terms of pitch, timbre, loudness, and rhythm to investigate their evolution from 1960 to 2010. By comparing the Billboard Hot 100 with random samples, we find that the complexity of popular songs tends to be more narrowly distributed around the mean, supporting the idea of an inverted U-shaped relationship between complexity and hedonistic value. We then examine the temporal evolution of complexity, reporting consistent changes across decades, such as a decrease in average loudness complexity since the 1960s, and an increase in timbre complexity overall but not for popular songs. We also show, in contrast to claims that popular songs sound more alike over time, that they are not more similar than they were 50 years ago in terms of pitch or rhythm, although similarity in timbre shows distinctive patterns across eras and similarity in loudness has been increasing. Finally, we show that musical genres can be differentiated by their distinctive complexity profiles. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Musical genre; Popular song; Random sample; Temporal evolution; U-shaped; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Jiang J.; Chen K.; Li W.; Xia G.","Jiang, Junyan (57217331141); Chen, Ke (57214859448); Li, Wei (56209264100); Xia, Gus (57208212250)","57217331141; 57214859448; 56209264100; 57208212250","Large-vocabulary chord transcription via chord structure decomposition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096760&partnerID=40&md5=b35e745d612d917cf77160dee0519d8a","Computer Science Department, Fudan University, China; Music X Lab, NYU Shanghai, China; Machine Learning Department, Carnegie Mellon University, United States","Jiang J., Computer Science Department, Fudan University, China, Music X Lab, NYU Shanghai, China, Machine Learning Department, Carnegie Mellon University, United States; Chen K., Computer Science Department, Fudan University, China, Music X Lab, NYU Shanghai, China; Li W., Computer Science Department, Fudan University, China; Xia G., Music X Lab, NYU Shanghai, China","While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., major/ minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training. In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Petroleum reservoir evaluation; Vocabulary control; Audio-recognition; Chord recognition; Evaluation metrics; Individual components; Large vocabulary; Long-tail distribution; State of the art; Structure decomposition; Audio systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Foroughmand H.; Peeters G.","Foroughmand, Hadrien (57217330857); Peeters, Geoffroy (22433836000)","57217330857; 22433836000","Deep-rhythm for tempo estimation and rhythm pattern recognition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093890&partnerID=40&md5=0b6040b8707040a0b89530d145343cfd","IRCAM Lab, CNRS, Sorbonne Université, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Foroughmand H., IRCAM Lab, CNRS, Sorbonne Université, France; Peeters G., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","It has been shown that the harmonic series at the tempo frequency of the onset-strength-function of an audio signal accurately describes its rhythm pattern and can be used to perform tempo or rhythm pattern estimation. Recently, in the case of multi-pitch estimation, the depth of the input layer of a convolutional network has been used to represent the harmonic series of pitch candidates. We use a similar idea here to represent the harmonic series of tempo candidates. We propose the Harmonic-Constant-Q-Modulation which represents, using a 4D-tensors, the harmonic series of modulation frequencies (considered as tempo frequencies) in several acoustic frequency bands over time. This representation is used as input to a convolutional network which is trained to estimate tempo or rhythm pattern classes. Using a large number of datasets, we evaluate the performance of our approach and compare it with previous approaches. We show that it slightly increases Accuracy-1 for tempo estimation but not the average-mean-Recall for rhythm pattern recognition. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Convolution; Convolutional neural networks; Frequency estimation; Harmonic analysis; Information retrieval; Large dataset; Modulation; Acoustic frequency; Convolutional networks; Harmonic constants; Harmonic series; Modulation frequencies; Multi-pitch estimations; Strength function; Tempo estimations; Pattern recognition","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Li B.; Kumar A.","Li, Bochen (57191924185); Kumar, Aparna (57205359931)","57191924185; 57205359931","Query by video: Cross-modal music retrieval","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096929&partnerID=40&md5=a12c9ab681ee3a332b9705820c115c1e","University of Rochester, Rochester, NY, United States; Spotify, New York, NY, United States","Li B., University of Rochester, Rochester, NY, United States; Kumar A., Spotify, New York, NY, United States","Cross-modal retrieval learns the relationship between the two types of data in a common space so that an input from one modality can retrieve data from a different modality. We focus on modeling the relationship between two highly diverse data, music and real-world videos. We learn crossmodal embeddings using a two-stream network trained with music-video pairs. Each branch takes one modality as the input and it is constrained with emotion tags. Then the constraints allow the cross-modal embeddings to be learned with significantly fewer music-video pairs. To retrieve music for an input video, the trained model ranks tracks in the music database by cross-modal distances to the query video. Quantitative evaluations show high accuracy of audio/video emotion tagging when evaluated on each branch independently and high performance for cross-modal music retrieval. We also present crossmodal music retrieval experiments on Spotify music using user-generated videos from Instagram and Youtube as queries, and subjective evaluations show that the proposed model can retrieve relevant music. We present the music retrieval results at: http://www.ece.rochester. edu/~bli23/projects/query.html. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Embeddings; Information retrieval; Query processing; Common spaces; High-accuracy; Music database; Music retrieval; Quantitative evaluation; Real world videos; Subjective evaluations; User-generated video; Computer music","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Doras G.; Peeters G.","Doras, Guillaume (57208214373); Peeters, Geoffroy (22433836000)","57208214373; 22433836000","Cover detection using dominant melody embeddings","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096926&partnerID=40&md5=2a914a267a14d6dc0bfb24320421fe7a","Sacem & Ircam Lab, CNRS, Sorbonne Université, France; LTCI, Telecom Paris, Institut Polytechnique de Paris, France","Doras G., Sacem & Ircam Lab, CNRS, Sorbonne Université, France; Peeters G., LTCI, Telecom Paris, Institut Polytechnique de Paris, France","Automatic cover detection - the task of finding in an audio database all the covers of one or several query tracks- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use. In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction - that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Embeddings; Information retrieval; Large dataset; Query processing; Audio database; Computation burden; Euclidean distance computations; Original algorithms; Pair-wise comparison; Practical problems; Small data set; State of the art; Network architecture","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Flexer A.; Lallai T.","Flexer, Arthur (7004555682); Lallai, Taric (57217332347)","7004555682; 57217332347","Can we increase inter- and intra-rater agreement in modeling general music similarity?","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096736&partnerID=40&md5=ccb08895e949ecc84c77ac2a4942693e","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Lallai T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We present a pilot study on ways to increase inter- and intra-rater agreement in quantification of general similarity between pieces of music. By using a more controlled group of human subjects and carefully curating song material, we try to increase overall agreement between raters concerning the perceived general similarity of songs. Repeated conduction of the experiment with a two week lag shows that intra-rater agreement is higher than inter-rater agreement. Analysis of the results and interviews with test subjects suggests that the genre of songs was a major factor in judging similarity between songs. We discuss the impacts of our results on evaluation of respective machine learning models and question the validity of experiments on general music similarity. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Human subjects; Inter-rater agreements; Machine learning models; Major factors; Music similarity; Pilot studies; Computer music","A. Flexer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: arthur.flexer@ofai.at","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Kim J.W.; Bello J.P.","Kim, Jong Wook (57195433649); Bello, Juan Pablo (7102889110)","57195433649; 7102889110","Adversarial learning for improved onsets and frames music transcription","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095765&partnerID=40&md5=e66e619ce896b4dcbc82970ebd0555ad","Music and Audio Research Lab, New York University, United States","Kim J.W., Music and Audio Research Lab, New York University, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements on transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the timefrequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both framelevel and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep learning; Information retrieval; Adversarial learning; Automatic music transcription; Conditional independences; Music information retrieval; Music signal analysis; Music transcription; Output distribution; Time-frequency representations; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Bittner R.; Bosch J.J.","Bittner, Rachel (55659619600); Bosch, Juan J. (36760347300)","55659619600; 36760347300","Generalized metrics for Single-F0 estimation evaluation","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087097028&partnerID=40&md5=281a2dffe18de0638312a88e1ec167de","Spotify, United States","Bittner R., Spotify, United States; Bosch J.J., Spotify, United States","Single-f0 estimation methods, including pitch trackers and melody estimators, have historically been evaluated using a set of common metrics which score estimates frame-wise in terms of pitch and voicing accuracy. ""Voicing"" refers to whether or not a pitch is active, and has historically been regarded as a binary value. However, this has limitations because it is often ambiguous whether a pitch is present or absent, making a binary choice difficult for humans and algorithms alike. For example, when a source fades out or reverberates, the exact point where the pitch is no longer present is unclear. Many single-f0 estimation algorithms select a threshold for when a pitch is active or not, and different choices of threshold drastically affect the results of standard metrics. In this paper, we present a refinement on the existing single-f0 metrics, by allowing the estimated voicing to be represented as a continuous likelihood, and introducing a weighting on frame level pitch accuracy, which considers the energy of the source producing the f0 relative to the energy of the rest of the signal. We compare these metrics experimentally with the previous metrics using a number of algorithms and datasets and discuss the fundamental differences. We show that, compared to the previous metrics, our proposed metrics allow threshold-independent algorithm comparisons. © 2020 International Society for Music Information Retrieval. All rights reserved.","","A-weighting; Algorithm comparison; Binary values; F0 estimations; Standard metrics; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Madhusudhan S.T.; Chowdhary G.","Madhusudhan, Sathwik Tejaswi (57217331321); Chowdhary, Girish (8724723800)","57217331321; 8724723800","Deepsrgm - Sequence classification and ranking in Indian classical music with deep learning","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096972&partnerID=40&md5=d1304545153ee30a9da6b2723c24a337","University of Illinois, Urbana Champaign, United States","Madhusudhan S.T., University of Illinois, Urbana Champaign, United States; Chowdhary G., University of Illinois, Urbana Champaign, United States","A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient prepossessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on the Comp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music data base that are closely related to the presented query sequence. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Long short-term memory; Downstream applications; Indian classical music; Learning-based approach; Music collection; Music information retrieval; Music recommendation; Sequence classification; Temporal sequences; Deep learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Muller M.; Zalkow F.","Muller, Meinard (7404689873); Zalkow, Frank (57192297460)","7404689873; 57192297460","FMP notebooks: Educational material for teaching and learning fundamentals of music processing","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096730&partnerID=40&md5=3e40bd3fafe6c56693938ad2c7b94014","International Audio Laboratories Erlangen, Germany","Muller M., International Audio Laboratories Erlangen, Germany; Zalkow F., International Audio Laboratories Erlangen, Germany","In this paper, we introduce a novel collection of educational material for teaching and learning fundamentals of music processing (FMP) with a particular focus on the audio domain. This collection, referred to as FMP notebooks, discusses well-established topics in Music Information Retrieval (MIR) as motivating application scenarios. The FMP notebooks provide detailed textbook-like explanations of central techniques and algorithms in combination with Python code examples that illustrate how to implement the theory. All components including the introductions of MIR scenarios, illustrations, sound examples, technical concepts, mathematical details, and code examples are integrated into a consistent and comprehensive framework based on Jupyter notebooks. The FMP notebooks are suited for studying the theory and practice, for generating educational material for lectures, as well as for providing baseline implementations for many MIR tasks, thus addressing students, teachers, and researchers. © Meinard Müller, Frank Zalkow.","","Information retrieval; Teaching; Application scenario; Educational materials; Mathematical details; Music information retrieval; Python code; Teaching and learning; Theory and practice; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Zhou Y.; Chu W.; Young S.; Chen X.","Zhou, Yichao (57215773909); Chu, Wei (57204048264); Young, Sam (57217329950); Chen, Xin (57204049368)","57215773909; 57204048264; 57217329950; 57204049368","Bandnet: A neural network-based, multi-instrument beatles-style midi music composition machine","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094399&partnerID=40&md5=bd038e7ceafde648265fe52d5ada76b0","Snap Inc., 63 Market St, Venice, 90291, CA, United States; Department of EECS, University of California, Berkeley, United States; Herb Alpert School of Music, University of California, Los Angeles, United States","Zhou Y., Snap Inc., 63 Market St, Venice, 90291, CA, United States, Department of EECS, University of California, Berkeley, United States; Chu W., Snap Inc., 63 Market St, Venice, 90291, CA, United States; Young S., Snap Inc., 63 Market St, Venice, 90291, CA, United States, Herb Alpert School of Music, University of California, Los Angeles, United States; Chen X., Snap Inc., 63 Market St, Venice, 90291, CA, United States","In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness. The generated music samples can be downloaded at https://goo.gl/uaLXoB. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Human intervention; Interestingness; Multiple channels; Music composition; Music scores; Music theory; Recurrent neural network (RNN); Subjective listening test; Recurrent neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Behrooz M.; Mennicken S.; Thom J.; Cramer R.K.H.","Behrooz, Morteza (56336016100); Mennicken, Sarah (36095716600); Thom, Jennifer (57205547315); Cramer, Rohit Kumar Henriette (57217330818)","56336016100; 36095716600; 57205547315; 57217330818","Augmenting music listening experiences on voice assistants","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095122&partnerID=40&md5=44c7870d3f0a600178333bccd2793ef6","University of California Santa Cruz, Santa Cruz, United States; Spotify USA, Somerville, MA, United States","Behrooz M., University of California Santa Cruz, Santa Cruz, United States; Mennicken S., Spotify USA, Somerville, MA, United States; Thom J., Spotify USA, Somerville, MA, United States; Cramer R.K.H., Spotify USA, Somerville, MA, United States","Voice interfaces have rapidly gained popularity, introducing the opportunity for new ways to explore new interaction paradigms for music. However, most interactions with music in current consumer voice devices are still relatively transactional; primarily allowing for keyword-based commands and basic content playback controls. They are less likely to contextualize content or support content discovery beyond what users think to ask for. We present an approach to dynamically augment the voice-based music experience with background information using story generation techniques. Our findings indicate that augmentation can have positive effects on voice-based music experiences, given the right user context and mindset. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Background information; Content discoveries; Contextualize; Interaction paradigm; Keyword-based; Story generations; User context; Voice interfaces; User experience","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Meseguer-Brocal G.; Peeters G.","Meseguer-Brocal, Gabriel (57210191728); Peeters, Geoffroy (22433836000)","57210191728; 22433836000","Conditioned-U-Net: Introducing a control mechanism in the U-net for multiple source separations","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094154&partnerID=40&md5=7641e990bd7b4a9e5926a5293077f772","Ircam Lab, CNRS, Sorbonne Université, Paris, 75004, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, 75013, France","Meseguer-Brocal G., Ircam Lab, CNRS, Sorbonne Université, Paris, 75004, France; Peeters G., LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, 75013, France","Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The CU- Net decides the instrument to isolate according to a onehot- encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Separation; Affine transformations; Audio source separation; Control features; Control mechanism; Data-driven model; Input vector; Linear modulations; Single models; Source separation","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Parada-Cabaleiro E.; Batliner A.; Schuller B.","Parada-Cabaleiro, Emilia (57195938576); Batliner, Anton (6602152015); Schuller, Björn (6603767415)","57195938576; 6602152015; 6603767415","Diplomatic edition of Il Lauro secco: Ground truth for OMR of white mensural notation","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096057&partnerID=40&md5=7b72a28157995f28e9730c69465ad0bb","ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Instituto Complutense de Ciencias Musicales (ICCMU), Universidad Complutense de Madrid, Spain; GLAM - Group on Language, Audio & Music, Imperial College London, United Kingdom","Parada-Cabaleiro E., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, Instituto Complutense de Ciencias Musicales (ICCMU), Universidad Complutense de Madrid, Spain; Batliner A., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schuller B., ZD.B Dipartment of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM - Group on Language, Audio & Music, Imperial College London, United Kingdom","Early musical sources in white mensural notation-the most common notation in European printed music during the Renaissance-are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of imagebased musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in ∗∗mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the ∗∗mens files. © Emilia Parada-Cabaleiro, Anton Batliner, Björn Schuller.","","Image enhancement; Semantics; Ground truth; Image-based; Music information retrieval; Optical music recognition; Symbolic representation; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Román M.A.; Pertusa A.; Calvo-Zaragoza J.","Román, Miguel A. (57210187945); Pertusa, Antonio (8540257800); Calvo-Zaragoza, Jorge (55847598300)","57210187945; 8540257800; 55847598300","Holistic approach to polyphonic music transcription with neural networks","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095528&partnerID=40&md5=1e59b95d558ce169e6afb25b9793e0b4","U.I. for Computing Research, University of Alicante, Spain","Román M.A., U.I. for Computing Research, University of Alicante, Spain; Pertusa A., U.I. for Computing Research, University of Alicante, Spain; Calvo-Zaragoza J., U.I. for Computing Research, University of Alicante, Spain","We present a framework based on neural networks to extract music scores directly from polyphonic audio in an end-to-end fashion. Most previous Automatic Music Transcription (AMT) methods seek a piano-roll representation of the pitches, that can be further transformed into a score by incorporating tempo estimation, beat tracking, key estimation or rhythm quantization. Unlike these methods, our approach generates music notation directly from the input audio in a single stage. For this, we use a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function which does not require annotated alignments of audio frames with the score rhythmic information. We trained our model using as input Haydn, Mozart, and Beethoven string quartets and Bach chorales synthesized with different tempos and expressive performances. The output is a textual representation of four-voice music scores based on ∗∗kern format. Although the proposed approach is evaluated in a simplified scenario, results show that this model can learn to transcribe scores directly from audio signals, opening a promising avenue towards complete AMT. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Classification (of information); Convolutional neural networks; Information retrieval; Recurrent neural networks; Automatic music transcription; Expressive performance; Holistic approach; Loss functions; Polyphonic music; Tempo estimations; Temporal classification; Textual representation; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Park S.; Kwon T.; Lee J.; Kim J.; Nam J.","Park, Saebyul (56239519200); Kwon, Taegyun (57210194738); Lee, Jongpil (57192303939); Kim, Jeounghoon (11541017900); Nam, Juhan (35812266500)","56239519200; 57210194738; 57192303939; 11541017900; 35812266500","A Cross-scape plot representation for visualizing symbolic melodic similarity","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096627&partnerID=40&md5=28b6a381b35d5de0164482a56d1b6c71","Graduate School of Culture Technology, KAIST, South Korea","Park S., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Kim J., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Symbolic melodic similarity is based on measuring a pairwise distance between two songs from diverse perspectives. The distance is usually summarized as a single value for song retrieval. This obscures observing the details of similarity patterns within the two songs. In this paper, we propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music encodings. The cross-scape plot is computed by stacking up a minimum local distance between two segments from each of the two songs. As the layer goes up, the segment size increases and it computes incrementally more long-term distances. This hierarchical representation allows for capturing the location and length of similar segments between two songs in a visually intuitive manner. We show the effectiveness of the cross-scape plot by evaluating it on examples from folk music collections with similarity-based categories and plagiarism cases. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Encodings; Hierarchical representation; Melodic similarity; Melody similarity; Music collection; Pairwise distances; Similarity patterns; Single-value; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Huang Y-F.; Chen T.-P.; Moran N.; Coleman S.; Su L.","Huang, Yu-Fen (57217330138); Chen, Tsung-Ping (57207993823); Moran, Nikki (36005292000); Coleman, Simon (7201402711); Su, Li (55966919100)","57217330138; 57207993823; 36005292000; 7201402711; 55966919100","Identifying expressive semantics in orchestral conducting kinematics","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096506&partnerID=40&md5=55314d2e04b4a634efbed2e552a4022e","Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; Reid School of Music, University of Edinburgh, United Kingdom; Institute for Sport, Physical Education and Health Sciences, University of Edinburgh, United Kingdom","Huang Y-F., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; Chen T.-P., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; Moran N., Reid School of Music, University of Edinburgh, United Kingdom; Coleman S., Institute for Sport, Physical Education and Health Sciences, University of Edinburgh, United Kingdom; Su L., Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan","Existing kinematic research on orchestral conducting movement contributes to beat-tracking and the delivery of performance dynamics. Methodologically, such movement cues have been treated as distinct, isolated events. Yet as practicing musicians and music pedagogues know, conductors' expressive instructions are highly flexible and dependent on the musical context. We seek to demonstrate an approach to search for effective descriptors to express musical features in conducting movement in a valid music context, and to extract complex expressive semantics from elementary conducting kinematic variations. This study therefore proposes a multi-task learning model to jointly identify dynamic, articulation, and phrasing cues from conducting kinematics. A professional conducting movement dataset is compiled using a high-resolution motion capture system. The ReliefF algorithm is applied to select significant features from conducting movement, and recurrent neural network (RNN) is implemented to identify multiple movement cues. The experimental results disclose key elements in conducting movement which communicate musical expressiveness; the results also highlight the advantage of multi-task learning in the complete musical context over single-task learning. To the best of our knowledge, this is the first attempt to use recurrent neural network to explore multiple semantic expressive cuing in conducting movement kinematics. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Kinematics; Learning systems; Multi-task learning; Semantics; Expressive semantics; Motion capture system; Movement kinematics; Musical expressiveness; Performance dynamics; Recurrent neural network (RNN); Relieff algorithms; Single task learning; Recurrent neural networks","Y-F. Huang; Music and Culture Technology Lab, Institute of Information Science, Academia Sinica, Taiwan; email: yfhuang@iis.sinica.edu.tw","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lee K.; Nam J.","Lee, Kyungyun (57210184383); Nam, Juhan (35812266500)","57210184383; 35812266500","Learning a joint embedding space of monophonic and mixed music signals for singing voice","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096988&partnerID=40&md5=02a013fa7cf720ee5af0d40f014154ae","Graduate School of Culture Technology, KAIST, South Korea","Lee K., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and query-by-singer in both the in-domain and cross-domain tasks. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Embeddings; Information retrieval; Large dataset; Learning systems; Query processing; Semantics; Source separation; Cross-domain; Metric learning; Multiple instruments; Music recording; Music signals; Semantic gap; Singing voices; Two domains; Audio recordings","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"MacKinlay D.; Botev Z.","MacKinlay, Dan (57201381370); Botev, Zdravko (8336989800)","57201381370; 8336989800","Mosaic style transfer using sparse autocorrelograms","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095657&partnerID=40&md5=033acde8f374b7dba2d0ecfbd0de080f","School of Mathematics and Statistics, UNSW Sydney, Australia","MacKinlay D., School of Mathematics and Statistics, UNSW Sydney, Australia; Botev Z., School of Mathematics and Statistics, UNSW Sydney, Australia","We introduce a novel mosaic synthesis algorithm for musical style transfer using the autocorrelogram as a feature map. We decompose the autocorrelogram feature map sparsely in a decaying sinusoid basis, using that decomposition as an interpolation scheme in feature space. This efficiently provides gradient information in the mosaicing optimization, including gradients of the challenging time-scale parameters, which are usually computationally intractable for discretely sampled signals. The required calculations are straightforward to parallelize on vectorprocessing hardware. Our implementation of the method provides good quality output and novel musical effects in example tasks by itself and can also be integrated into alternative mosaicing methods. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Autocorrelograms; Feature map; Feature space; Gradient informations; Interpolation schemes; Mosaicing; Synthesis algorithms; Time-scales; Information retrieval","D. MacKinlay; School of Mathematics and Statistics, UNSW Sydney, Australia; email: dan@danmackinlay.name","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Rosenzweig S.; Scherbaum F.; Müller M.","Rosenzweig, Sebastian (57195432903); Scherbaum, Frank (7004310500); Müller, Meinard (7404689873)","57195432903; 7004310500; 7404689873","Detecting stable regions in frequency trajectories for tonal analysis of traditional georgian vocal music","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096985&partnerID=40&md5=52a47ca274d620d895d97b0f5be364fb","International Audio Laboratories Erlangen, Germany; University of Potsdam, Potsdam, Germany","Rosenzweig S., International Audio Laboratories Erlangen, Germany; Scherbaum F., University of Potsdam, Potsdam, Germany; Müller M., International Audio Laboratories Erlangen, Germany","While Georgia has a long history of orally transmitted polyphonic singing, there is still an ongoing controversial discussion among ethnomusicologists on the tuning system underlying this type of music. First attempts have been made to analyze tonal properties (e. g., harmonic and melodic intervals) based on fundamental frequency (F0) trajectories. One major challenge in F0-based tonal analysis is introduced by unstable regions in the trajectories due to pitch slides and other frequency fluctuations. In this paper, we describe two approaches for detecting stable regions in frequency trajectories: the first algorithm uses morphological operations inspired by image processing, and the second one is based on suitably defined binary time-frequency masks. To avoid undesired distortions in subsequent analysis steps, both approaches keep the original F0-values unmodified, while only removing F0-values in unstable trajectory regions. We evaluate both approaches against manually annotated stable regions and discuss their potential in the context of interval analysis for traditional three-part Georgian singing. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Image processing; Information retrieval; Mathematical morphology; Frequency fluctuation; Fundamental frequencies; Interval analysis; Melodic intervals; Morphological operations; Stable region; Time frequency; Unstable regions; Trajectories","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Shibata G.; Nishikimi R.; Nakamura E.; Yoshii K.","Shibata, Go (57217330836); Nishikimi, Ryo (57207989997); Nakamura, Eita (24587601200); Yoshii, Kazuyoshi (7103400120)","57217330836; 57207989997; 24587601200; 7103400120","Statistical music structure analysis based on a Homogeneity-, Repetitiveness-, and regularity-aware hierarchical hidden Semi-markov model","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096105&partnerID=40&md5=9771ef48f007564005bd7b0b157954ad","Graduate School of Informatics, Kyoto University, Japan","Shibata G., Graduate School of Informatics, Kyoto University, Japan; Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper describes a music structure analysis method that splits music audio signals into meaningful segments such as musical sections and clusters them. In this task, how to model the four fundamental aspects of musical sections, i.e., homogeneity, repetitiveness, novelty, and regularity, in a unified way is still an open problem. Here we propose a solid statistical approach based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model. The higher-level semi-Markov chain represents a sequence of sections that tend to have regularly spaced boundaries. The timbral features in each section are assumed to follow emission distributions that are homogeneous over time. The lower-level left-to-right Markov chain in each section represents a chord sequence whose sequential order is constrained to be a repetition of a chord sequence in another section of the same cluster. The whole model can be trained unsupervisedly based on Bayesian sparse learning where unnecessary sections automatically degenerate. The proposed method outperformed representative methods in segmentation and clustering accuracies with estimated sections having similar statistical properties as the ground truth data. © G. Shibata, R. Nishikimi, E. Nakamura, and K. Yoshii.","","Audio acoustics; Information retrieval; Emission distribution; Hidden semi-Markov modeling; Music structure analysis; Segmentation and clustering; Semi-Markov chain; Sequential ordering; Statistical approach; Statistical properties; Markov chains","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Choi J.; Lee J.; Park J.; Nam J.","Choi, Jeong (57217331776); Lee, Jongpil (57192303939); Park, Jiyoung (57210173812); Nam, Juhan (35812266500)","57217331776; 57192303939; 57210173812; 35812266500","Zero-shot learning for audio-based music classification and tagging","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094321&partnerID=40&md5=7aee708efe36f96743500d8040fae2de","Graduate School of Culture Technology, KAIST, South Korea; NAVER Corp, South Korea","Choi J., Graduate School of Culture Technology, KAIST, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Park J., NAVER Corp, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where side information about the labels is used to unveil the relationship between each other. In this work, we investigate the zero-shot learning in the music domain and organize two different setups of side information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations. Considering a music track is usually multilabeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning. Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in the music domain. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Classification (of information); Information retrieval; Information use; Motion compensation; Semantics; Attribute information; Audio-based; Multi-label; Music classification; Music retrieval; Semantic information; Semantic Space; Side information; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Karsdorp F.; Van Kranenburg P.; Manjavacas E.","Karsdorp, Folgert (56297352000); Van Kranenburg, Peter (35108158000); Manjavacas, Enrique (57210376459)","56297352000; 35108158000; 57210376459","Learning similarity metrics for melody retrieval","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094564&partnerID=40&md5=acdd16aa53ad3c8c52f8f38615a7d0bf","KNAW Meertens Instituut, Netherlands; Utrecht University, Netherlands; University of Antwerp, Belgium","Karsdorp F., KNAW Meertens Instituut, Netherlands; Van Kranenburg P., KNAW Meertens Instituut, Netherlands, Utrecht University, Netherlands; Manjavacas E., University of Antwerp, Belgium","Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic timecomplexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep learning; Deep neural networks; Dynamic programming; Information retrieval; Vector spaces; Learning similarity; Melodic similarity; Music information retrieval; Neural architectures; Sequence alignments; Similarity measure; Similarity metric learning; Similarity metrics; Long short-term memory","F. Karsdorp; KNAW Meertens Instituut, Netherlands; email: folgert.karsdorp@meertens.knaw.nl","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Finkensiep C.; Widdess R.; Rohrmeier M.","Finkensiep, Christoph (57210197364); Widdess, Richard (26038663600); Rohrmeier, Martin (6507901506)","57210197364; 26038663600; 6507901506","Modelling the syntax of north indian melodies with a generalized graph grammar","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094289&partnerID=40&md5=f90b583179067d59d3ffdaa51fc969f3","EPFL, Switzerland; SOAS University of London, United Kingdom","Finkensiep C., EPFL, Switzerland; Widdess R., SOAS University of London, United Kingdom; Rohrmeier M., EPFL, Switzerland","Hierarchical models of music allow explanation of highly complex musical structure based on the general principle of recursive elaboration and a small set of orthogonal operations. Recent approaches to melodic elaboration have converged to a representation based on intervals, which allows the elaboration of pairs of notes. However, two problems remain: First, an interval-first representation obscures one-sided operations like neighbor notes. Second, while models of Western melody styles largely agree on stepwise operations such as neighbors and passing notes, larger intervals are either attributed to latent harmonic properties or left unexplained. This paper presents a grammar for melodies in North Indian raga music, showing not only that recursively applied neighbor and passing note operations underlie this style as well, but that larger intervals are generated as generalized neighbors, based on the tonal hierarchy of the underlying scale structure. The notion of a generalized neighbor is not restricted to ragas but can be transferred to other musical styles, opening new perspectives on latent structure behind melodies and music in general. The presented grammar is based on a graph representation that allows one to express elaborations on both notes and intervals, unifying and generalizing previous graphand tree-based approaches. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Formal languages; Hierarchical systems; Information retrieval; Generalized graphs; Graph representation; Harmonic properties; Hierarchical model; Latent structures; Musical structures; Orthogonal operations; Tree-based approach; Trees (mathematics)","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Maia L.S.; Fuentes M.; Biscainho L.W.P.; Rocamora M.; Essid S.","Maia, Lucas S. (57217331856); Fuentes, Magdalena (57190739095); Biscainho, Luiz W. P. (6603071473); Rocamora, Martín (55347707700); Essid, Slim (16033218700)","57217331856; 57190739095; 6603071473; 55347707700; 16033218700","Sambaset: A dataset of historical samba de enredo recordings for computational music analysis","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096223&partnerID=40&md5=4801fcc978ab2020621c02f29e0de32c","Federal University of Rio de Janeiro, Brazil; LTCI, Télécom Paris, Institut Polytechnique de Paris, France; L2S, CNRS-Université Paris-Sud-Centrale Supélec, France; Universidad de la República, Uruguay","Maia L.S., Federal University of Rio de Janeiro, Brazil, LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Fuentes M., LTCI, Télécom Paris, Institut Polytechnique de Paris, France, L2S, CNRS-Université Paris-Sud-Centrale Supélec, France; Biscainho L.W.P., Federal University of Rio de Janeiro, Brazil; Rocamora M., Universidad de la República, Uruguay; Essid S., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","In the last few years, several datasets have been released to meet the requirements of ""hungry"" yet promising datadriven approaches in music technology research. Since, for historical reasons, most investigations conducted in the field still revolve around music of the so-called ""Western"" tradition, the corresponding data, methodology and conclusions carry a strong cultural bias. Music of non- ""Western"" background, whenever present, is usually underrepresented, poorly labeled, or even mislabeled, the exception being projects that aim at specifically describing such music. In this paper we present SAMBASET, a dataset of Brazilian samba music that contains over 40 hours of historical and modern samba de enredo commercial recordings. To the best of our knowledge, this is the first dataset of this genre. We describe the collection of metadata (e.g. artist, composer, release date) and outline our semiautomatic approach to the challenging task of annotating beats in this large dataset, which includes the assessment of the performance of state-of-the-art beat tracking algorithms for this specific case. Finally, we present a study on tempo and beat tracking that illustrates SAMBASET's value, and we comment on other tasks for which it could be used. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Large dataset; Beat tracking; Cultural bias; Data-driven approach; Music analysis; Music technologies; Release date; State of the art; Audio recordings","L.S. Maia; Federal University of Rio de Janeiro, Brazil; email: lucas.maia@smt.ufrj.br","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Rigaux P.; Travers N.","Rigaux, Philippe (57204372163); Travers, Nicolas (23391144800)","57204372163; 23391144800","Scalable searching and ranking for melodic pattern queries","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095650&partnerID=40&md5=57dd03b0a326db8c6e9672520614738d","Cedric Lab, CNAM, France; Leonard de Vinci, Research Center, Cedric Lab, CNAM, France","Rigaux P., Cedric Lab, CNAM, France; Travers N., Leonard de Vinci, Research Center, Cedric Lab, CNAM, France","We present the design and implementation of a scalable search engine for large Digital Score Libraries. It covers the core features expected from an information retrieval system. Music representation is pre-processed, simplified and normalized. Collections are searched for scores that match a melodic pattern, results are ranked on their similarity with the pattern, and matching fragments are finally identified on the fly. Moreover, all these features are designed to be integrated in a standard search engine and thus benefit from the horizontal scalability of such systems. Our method is fully implemented, and relies on ELASTICSEARCH for collection indexing. We describe its main components, report and study its performances. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Digital libraries; Core features; Design and implementations; Digital score; Music representation; On the flies; Pattern query; Searching and ranking; Search engines","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Smith J.B.L.; Kawasaki Y.; Goto M.","Smith, Jordan B. L. (55582613300); Kawasaki, Yuta (57217332161); Goto, Masataka (7403505330)","55582613300; 57217332161; 7403505330","Unmixer: An interface for extracting and remixing loops","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087096067&partnerID=40&md5=d01f287f713b75a60d3186620e52f603","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Kawasaki Y., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mashups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Web services; Mashups; Nonnegative tucker decompositions; Sparseness constraints; User evaluations; Source separation","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Park J.; Choi K.; Jeon S.; Kim D.; Park J.","Park, Jonggwon (57217332195); Choi, Kyoyun (57217330871); Jeon, Sungwook (57217330649); Kim, Dokyun (57217331797); Park, Jonghun (14833327500)","57217332195; 57217330871; 57217330649; 57217331797; 14833327500","A Bi-directional transformer for musical chord recognition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095098&partnerID=40&md5=4d5fd2adc9517635d878caa5a61453cf","Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea","Park J., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Choi K., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Jeon S., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Kim D., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea; Park J., Department of Industrial Engineering, Center for Superintelligence, Seoul National University, Seoul, South Korea","Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model. In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Convolutional neural networks; Information retrieval; Attention mechanisms; Bi-directional; Chord recognition; Competitive performance; Long-term dependencies; Machine learning models; Receptive fields; Recurrent neural network (RNNs); Recurrent neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Ferreira L.N.; Whitehead J.","Ferreira, Lucas N. (56313310300); Whitehead, Jim (8307167100)","56313310300; 8307167100","Learning to generate music with sentiment","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094164&partnerID=40&md5=b2d2008d0b9de3429f6e72c801abab8d","University of California, Santa Cruz, Department of Computational Media, United States","Ferreira L.N., University of California, Santa Cruz, Department of Computational Media, United States; Whitehead J., University of California, Santa Cruz, Department of Computational Media, United States","Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Classification (of information); Deep learning; Information retrieval; Sentiment analysis; Human subjects; Learning models; Polyphonic music; Prediction accuracy; User study; Video game; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Cífka O.; Şimşekli U.; Richard G.","Cífka, Ondřej (57207859450); Şimşekli, Umut (36622388800); Richard, Gaël (57195915952)","57207859450; 36622388800; 57195915952","Supervised symbolic music style translation using synthetic data","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095493&partnerID=40&md5=8b453fef8d3b9cfe92f45dc35d5b6ac3","LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Cífka O., LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Şimşekli U., LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Richard G., LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the 'style' of a piece while keeping its original 'content'. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of 'aligned' data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Deep learning; Information retrieval; Data generation; Encoder-decoder; Learning-based algorithms; Music accompaniments; Musical pieces; Supervised algorithm; Synthetic data; Synthetic data generations; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Hu X.; Que Y.; Kando N.; Lian W.","Hu, Xiao (55496358400); Que, Ying (57215926050); Kando, Noriko (6602320153); Lian, Wenwei (57217331094)","55496358400; 57215926050; 6602320153; 57217331094","Analyzing user interactions with music information retrieval system: An eye-tracking approach","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095095&partnerID=40&md5=5f3e9348bf00b092e8bbb1b3a7653144","University of Hong Kong, Hong Kong; National Institute of Informatics, Japan","Hu X., University of Hong Kong, Hong Kong; Que Y., University of Hong Kong, Hong Kong; Kando N., National Institute of Informatics, Japan; Lian W., University of Hong Kong, Hong Kong","There has been little research considering eye movement as a measure when assessing user interactions with music information retrieval (MIR) systems, whereas many studies have adopted conventional user-centered measures such as user effectiveness and user perception. To bridge this research gap, this study investigates users' eye movement patterns and measures with two music retrieval tasks and two interface presentation modes. A user experiment was conducted with 16 participants whose eye movement and mouse click behaviors were recorded through professional eye trackers. Through analyzing visual patterns of eye gazes and movements as well as various metrics in prominent Areas of Interest (AOI), it is found that users' eye movement behaviors were related to task type. Besides, the results also disclosed that some eye movement metrics were related to both user effectiveness and user perception, and influenced by user characteristics. It is also found that some eye movement and user effectiveness metrics can be used to predict user perception. This study allows researchers to gain a deeper insight into user interactions with MIR systems from the perspective of eye movement measure. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Eye tracking; Mammals; Search engines; Effectiveness metrics; Eye movement patterns; Eye-movement measures; Movement behavior; Music information retrieval; Presentation modes; User characteristics; User-centered measures; Eye movements","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"McFee B.; Kinnaird K.M.","McFee, Brian (34875379700); Kinnaird, Katherine M. (57205699026)","34875379700; 57205699026","Improving structure evaluation through automatic hierarchy expansion","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093919&partnerID=40&md5=41ae65872090ab34e73629208e5781fc","Music and Audio Research Lab, Center for Data Science, New York University, United States; Department of Computer Science, Statistical and Data Sciences Program, Smith College, United States","McFee B., Music and Audio Research Lab, Center for Data Science, New York University, United States; Kinnaird K.M., Department of Computer Science, Statistical and Data Sciences Program, Smith College, United States","Structural segmentation is the task of partitioning a recording into non-overlapping time intervals, and labeling each segment with an identifying marker such as A, B, or verse. Hierarchical structure annotation expands this idea to allow an annotator to segment a song with multiple levels of granularity. While there has been recent progress in developing evaluation criteria for comparing two hierarchical annotations of the same recording, the existing methods have known deficiencies when dealing with inexact label matchings and sequential label repetition. In this article, we investigate methods for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. The proposed method complements existing techniques for comparing hierarchical structural annotations by coarsening or refining labels with variation markers to either collapse similarly labeled segments together, or separate identically labeled segments from each other. Using the multi-level structure annotations provided in the SALAMI dataset, we demonstrate that automatic hierarchy expansion allows structure comparison methods to more accurately assess similarity between annotations. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Coarsening; Information retrieval; Evaluation criteria; Hierarchical information; Hierarchical structures; Multi-level structures; Multiple levels; Recent progress; Structure comparisons; Structure evaluations; Audio recordings","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Low T.; Hentschel C.; Polley S.; Das A.; Sack H.; Nürnberger A.; Stober S.","Low, Thomas (48761661000); Hentschel, Christian (57198277665); Polley, Sayantan (57217332326); Das, Anustup (57217330569); Sack, Harald (7102918498); Nürnberger, Andreas (14027288100); Stober, Sebastian (14027561800)","48761661000; 57198277665; 57217332326; 57217330569; 7102918498; 14027288100; 14027561800","The Ismir explorer - A visual interface for exploring 20 years of Ismir publications","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095814&partnerID=40&md5=a8ce6f4d76ba3ef028eb2ca94c3c5321","Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Hasso Plattner Institute for IT Systems Engineering, University of Potsdam, Germany; FIZ Karlsruhe, Leibniz Institute for Information Infrastructure, Karlsruhe, Germany","Low T., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Hentschel C., Hasso Plattner Institute for IT Systems Engineering, University of Potsdam, Germany; Polley S., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Das A., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Sack H., FIZ Karlsruhe, Leibniz Institute for Information Infrastructure, Karlsruhe, Germany; Nürnberger A., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany","Ever since the first International SymposiumonMusic InformationRetrieval in 2000, the proceedings have beenmade publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nnmaps, the interface creates the impression of panning a large global map. Evaluation results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code are made publicly available. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Nearest neighbor search; Animated transitions; Document collection; Evaluation results; Generic approach; K-nearest neighbors; Scientific researches; Search interfaces; Visual Interface; User interfaces","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Fu Z.-S.; Su L.","Fu, Zih-Sing (57217331620); Su, Li (55966919100)","57217331620; 55966919100","Hierarchical classification networks for singing voice segmentation and transcription","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087095418&partnerID=40&md5=16ee32256c9860dd21b6939b78cc553f","Dept. EE, National Taiwan University, Taiwan; IIS, Academia Sinica, Taiwan","Fu Z.-S., Dept. EE, National Taiwan University, Taiwan; Su L., IIS, Academia Sinica, Taiwan","Identifying the onset and offset time of a note is a challenging step in singing voice transcription, as the soft onset/ offset, portamento, and vibrato phenomena are rich in singing voice signals. In this work, we utilize various types of signal representations with deep learning for onset and offset detection of monophonic singing voice. We consider onset and offset detection as a hierarchical classification problem, where every input segment is classified into one of all the possible states in monophonic singing, namely the silence, activation, and transition states,where the transition state is further classified into the onset and offset states. An objective function based on this hierarchical taxonomy nicely guides the model to capture complicated temporal dynamics of note sequences. Multiple input signal representations containing spectral differences and pitch saliency are employed to jointly enhance such temporal patterns. The proposed method implemented with residual networks provides improved performance over prior art in onset and offset detection. Moreover, by integrating with a pitch detection framework, the proposed method also outperforms previous singing voice transcription methods. This result emphasizes the importance of note segmentation in singing voice transcription. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Classification (of information); Deep learning; Information retrieval; Hierarchical classification; Hierarchical taxonomy; Monophonic singing; Objective functions; Signal representations; Spectral differences; Temporal dynamics; Transcription methods; Speech recognition","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Sioros G.; Câmara G.S.; Danielsen A.","Sioros, George (23091906900); Câmara, Guilherme Schmidt (57206893360); Danielsen, Anne (7004505364)","23091906900; 57206893360; 7004505364","Mapping timing strategies in drum performance","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087094847&partnerID=40&md5=e524b8943bc6886e73b08b9a20007ec8","RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway","Sioros G., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway; Câmara G.S., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway; Danielsen A., RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, Department of Musicology, University of Oslo, Norway","How do drummers express different timing styles? We conducted an experiment in which we asked twenty-two professional drummers to perform a simple rhythmic pattern while listening to a metronome. Here, we investigate the strategies they employed to express three different instructed timing profiles for the same pattern: ""On"" ""Pushed"" and Laid-back. Our analysis of the recordings follows three stages. First, we compute sixteen boolean features that capture the microtiming relations of the kick, snare and hi-hat drum onsets, between each other and with regards to the metrical grid. Second, we construct a microtiming profile (mtP) for every performance by averaging the boolean features across the recording. An mtP codifies the frequency with which the various features were found in a performance. Third, through a ""similarity profiles"" hierarchical clustering analysis, we identify groups of recordings with significant similarities in their mtPs. We found distinct strategies to express each intended timing profile that employ specific combinations of relations between the instruments and with regards to the meter. Finally, we created a map that summarizes the main characteristics of the strategies and their relations using a phylogenetic tree visualization. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Hierarchical clustering; Information retrieval; Timing circuits; Boolean features; Hierarchical clustering analysis; Phylogenetic trees; Rhythmic patterns; Timing profiles; Timing strategy; Audio recordings","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Tymoczko D.; Gotham M.; Cuthbert M.S.; Ariza C.","Tymoczko, Dmitri (14038447800); Gotham, Mark (55987597200); Cuthbert, Michael Scott (15724599200); Ariza, Christopher (57204340082)","14038447800; 55987597200; 15724599200; 57204340082","The romantext format: A flexible and standard method for representing roman numeral analyses","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076722082&partnerID=40&md5=1de6fd68b9ff4a66d90078b979185bf7","Princeton University, NJ, United States; Cornell University, NY, United States; M.I.T., MA, United States","Tymoczko D., Princeton University, NJ, United States; Gotham M., Cornell University, NY, United States; Cuthbert M.S., M.I.T., MA, United States; Ariza C.","Roman numeral analysis has been central to the Western musician's toolkit since its emergence in the early nineteenth century: it is an extremely popular method for recording subjective analytical decisions about the chords and keys implied by a passage of music. Disagreements about these judgments have led to extensive theoretical debates and ongoing controversies. Such debates are exacerbated by the absence of a public corpus of expert Roman numeral analyses, and by the more fundamental lack of an agreed-upon, computer-readable syntax in which those analyses might be expressed. This paper specifies such a standard, along with an associated code library in music21, and a preliminary set of example corpora. To frame the project, we review some of the motivations for doing harmonic analysis, some reasons why it resists automation, and some prospective uses for our tools. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Analytical decisions; Code libraries; Roman numerals; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Manzelli R.; Thakkar V.; Siahkamari A.; Kulis B.","Manzelli, Rachel (57210182224); Thakkar, Vijay (57210185560); Siahkamari, Ali (57210189203); Kulis, Brian (12141279600)","57210182224; 57210185560; 57210189203; 12141279600","Conditioning deep generative raw audio models for structured automatic music","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067130314&partnerID=40&md5=45f8c9d17e572c0c79dacd1e9eadfd76","ECE Department, Boston University, United States","Manzelli R., ECE Department, Boston University, United States; Thakkar V., ECE Department, Boston University, United States; Siahkamari A., ECE Department, Boston University, United States; Kulis B., ECE Department, Boston University, United States","Existing automatic music generation approaches that feature deep learning can be broadly classified into two types: raw audio models and symbolic models. Symbolic models, which train and generate at the note level, are currently the more prevalent approach; these models can capture long-range dependencies of melodic structure, but fail to grasp the nuances and richness of raw audio generations. Raw audio models, such as DeepMind’s WaveNet, train directly on sampled audio waveforms, allowing them to produce realistic-sounding, albeit unstructured music. In this paper, we propose an automatic music generation methodology combining both of these approaches to create structured, realistic-sounding compositions. We consider a Long Short Term Memory network to learn the melodic structure of different styles of music, and then use the unique symbolic generations from this model as a conditioning input to a WaveNet-based raw audio generator, creating a model for automatic, novel music. We then evaluate this approach by showcasing results of this work. © Rachel Manzelli, Vijay Thakkar, Ali Siahkamari, Brian Kulis.","","Deep learning; Information retrieval; Audio waveforms; Long-range dependencies; Melodic structure; Short term memory; Symbolic model; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"McGuirl M.R.; Kinnaird K.M.; Savard C.; Bugbee E.H.","McGuirl, Melissa R. (57193558433); Kinnaird, Katherine M. (57205699026); Savard, Claire (57210195997); Bugbee, Erin H. (57210193520)","57193558433; 57205699026; 57210195997; 57210193520","SE and SNL diagrams: Flexible data structures for MIR","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069872104&partnerID=40&md5=00ecac4542bfb81e37db3fedd38cb3d1","Division of Applied Mathematics, Brown University, United States; Department of Mathematics, University of Michigan, United States; Department of Biostatistics, Brown University, United States","McGuirl M.R., Division of Applied Mathematics, Brown University, United States; Kinnaird K.M., Division of Applied Mathematics, Brown University, United States; Savard C., Department of Mathematics, University of Michigan, United States; Bugbee E.H., Department of Biostatistics, Brown University, United States","The matrix-based representations commonly used in MIR tasks are often difficult to interpret. This work introduces start-end (SE) diagrams and start(normalized)length (SNL) diagrams, two novel structure-based representations for sequential music data. Inspired by methods from topological data analysis, both SE and SNL diagrams come equipped with efficiently computable and stable metrics. Utilizing SE or SNL diagrams as input, we address the cover song task for score-based data with high accuracy. While both representations are concisely defined and flexible, SNL diagrams in particular address issues introduced by commonly used resampling methods. © Melissa R. McGuirl, Katherine M. Kinnaird, Claire Savard, Erin H. Bugbee.","","Information retrieval; Cover songs; High-accuracy; Matrix based representation; Music data; Novel structures; Resampling method; Stable metrics; Topological data analysis; Graphic methods","M.R. McGuirl; Division of Applied Mathematics, Brown University, United States; email: melissa_mcguirl@brown.edu","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Hu X.; Li F.; Ng J.T.D.","Hu, Xiao (55496358400); Li, Fanjie (57207732941); Ng, Jeremy T.D. (57201810212)","55496358400; 57207732941; 57201810212","On the relationships between music-induced emotion and physiological signals","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069877632&partnerID=40&md5=f95811834b5b3f1f500ddcfb97a0958e","University of Hong Kong, Shenzhen Institute of Research and Innovation, Hong Kong; Sichuan University, China; University of Hong Kong, Hong Kong","Hu X., University of Hong Kong, Shenzhen Institute of Research and Innovation, Hong Kong; Li F., Sichuan University, China; Ng J.T.D., University of Hong Kong, Hong Kong","Emotion-aware music information retrieval (MIR) has been difficult due to the subjectivity and temporality of emotion responses to music. Physiological signals are regarded as related to emotion and thus could potentially be exploited in emotion-aware music discovery. This study explored the possibility of using physiological signals to detect users’ emotion responses to music, with consideration of individual characteristics (personality, music preferences, etc.). A user experiment was conducted with 23 participants who searched for music in a novel MIR system. Users’ listening behaviors and self-reported emotion responses to a total of 628 music pieces were collected. During music listening, a series of peripheral physiological signals (e.g., heart rate, skin conductance) were recorded from participants unobtrusively using a research-grade wearable wristband. A set of features in the time- and frequency- domains were extracted from the physiological signals and analyzed using statistical and machine learning methods. Results reveal 1) significant differences in some physiological features between positive and negative arousal and mood categories, and 2) effective classification of emotion responses based on physiological signals for some individuals. The findings can contribute to further improvement of emotion-aware intelligent MIR systems exploiting physiological signals as an objective and personalized input. © Hu, X. Li, F. Ng, J.","","Information retrieval; Learning systems; Physiology; Classification of emotions; Individual characteristics; Machine learning methods; Music information retrieval; Music preferences; Physiological features; Physiological signals; Time and frequency domains; Physiological models","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lattner S.; Grachten M.; Widmer G.","Lattner, Stefan (56989751900); Grachten, Maarten (8974600000); Widmer, Gerhard (7004342843)","56989751900; 8974600000; 7004342843","A predictive model for music based on learned interval representations","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858678&partnerID=40&md5=87366d8dbf02b2082681f8bede336da3","Institute of Computational Perception, JKU Linz, Austria; Sony Computer Science Laboratories (CSL), Paris, France","Lattner S., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Grachten M., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Widmer G., Institute of Computational Perception, JKU Linz, Austria","Connectionist sequence models (e.g., RNNs) applied to musical sequences suffer from two known problems: First, they have strictly “absolute pitch perception”. Therefore, they fail to generalize over musical concepts which are commonly perceived in terms of relative distances between pitches (e.g., melodies, scale types, modes, cadences, or chord types). Second, they fall short of capturing the concepts of repetition and musical form. In this paper we introduce the recurrent gated autoencoder (RGAE), a recurrent neural network which learns and operates on interval representations of musical sequences. The relative pitch modeling increases generalization and reduces sparsity in the input data. Furthermore, it can learn sequences of copy-and-shift operations (i.e. chromatically transposed copies of musical fragments)—a promising capability for learning musical repetition structure. We show that the RGAE improves the state of the art for general connectionist sequence models in learning to predict monophonic melodies, and that ensembles of relative and absolute music processing models improve the results appreciably. Furthermore, we show that the relative pitch processing of the RGAE naturally facilitates the learning and the generation of sequences of copy-and-shift operations, wherefore the RGAE greatly outperforms a common absolute pitch recurrent neural network on this task. © Stefan Lattner, Maarten Grachten, Gerhard Widmer.","","Information retrieval; Recurrent neural networks; Musical concepts; Predictive modeling; Processing model; Relative distances; Relative pitch; Sequence models; Shift operations; State of the art; Learning systems","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Panda R.; Malheiro R.; Paiva R.P.","Panda, Renato (55354413900); Malheiro, Ricardo (6508214140); Paiva, Rui Pedro (7003358437)","55354413900; 6508214140; 7003358437","Musical texture and expressivity features for music emotion recognition","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857501&partnerID=40&md5=3a693b38be058f8246e6ac38f44ead08","CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal","Panda R., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal; Malheiro R., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal; Paiva R.P., CISUC – Centre for Informatics and Systems, University of Coimbra, Portugal","We present a set of novel emotionally-relevant audio features to help improving the classification of emotions in audio music. First, a review of the state-of-the-art regarding emotion and music was conducted, to understand how the various music concepts may influence human emotions. Next, well known audio frameworks were analyzed, assessing how their extractors relate with the studied musical concepts. The intersection of this data showed an unbalanced representation of the eight musical concepts. Namely, most extractors are low-level and related with tone color, while musical form, musical texture and expressive techniques are lacking. Based on this, we developed a set of new algorithms to capture information related with musical texture and expressive techniques, the two most lacking concepts. To validate our work, a public dataset containing 900 30-second clips, annotated in terms of Russell’s emotion quadrants was created. The inclusion of our features improved the F1-score obtained using the best 100 features by 8.6% (to 76.0%), using support vector machines and 20 repetitions of 10-fold cross-validation. © Renato Panda, Ricardo Malheiro, Rui Pedro Paiva.","","Information retrieval; Support vector machines; Textures; 10-fold cross-validation; Audio features; Classification of emotions; Human emotion; Music emotions; Musical concepts; Public dataset; State of the art; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Schreiber H.; Müller M.","Schreiber, Hendrik (55586286200); Müller, Meinard (7404689873)","55586286200; 7404689873","A single-step approach to musical tempo estimation using a convolutional neural network","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068964919&partnerID=40&md5=1cf2d643dda9761eb9b6f030d440498c","Tagtraum Industries Incorporated, Germany; International Audio Laboratories, Erlangen, Germany","Schreiber H., Tagtraum Industries Incorporated, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","We present a single-step musical tempo estimation system based solely on a convolutional neural network (CNN). Contrary to existing systems, which typically first identify onsets or beats and then derive a tempo, our system estimates the tempo directly from a conventional mel-spectrogram in a single step. This is achieved by framing tempo estimation as a multi-class classification problem using a network architecture that is inspired by conventional approaches. The system’s CNN has been trained with the union of three datasets covering a large variety of genres and tempi using problem-specific data augmentation techniques. Two of the three ground-truths are novel and will be released for research purposes. As input the system requires only 11.9 s of audio and is therefore suitable for local as well as global tempo estimation. When used as a global estimator, it performs as well as or better than other state-of-the-art algorithms. Especially the exact estimation of tempo without tempo octave confusion is significantly improved. As local estimator it can be used to identify and visualize tempo drift in musical performances. © Hendrik Schreiber, Meinard Müller.","","Convolution; Information retrieval; Large dataset; Network architecture; Conventional approach; Convolutional neural network; Data augmentation; Existing systems; Multiclass classification problems; Musical performance; State-of-the-art algorithms; Tempo estimations; Neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Viraraghavan V.S.; Murthy H.A.; Aravind R.","Viraraghavan, Venkata Subramanian (57200146255); Murthy, Hema A. (57200197348); Aravind, R. (55930722800)","57200146255; 57200197348; 55930722800","Precision of sung notes in carnatic music","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862016&partnerID=40&md5=4221d577ca0ac260e2470459441c5837","TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India; Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India","Viraraghavan V.S., TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India, Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India; Aravind R., Department of Electrical Engineering, Indian Institute of Technology, Madras, India","Carnatic music is replete with continuous pitch movement called gamakas and can be viewed as consisting of constant-pitch notes (CPNs) and transients. The stationary points (STAs) of transients – points where the pitch curve changes direction – also carry melody information. In this paper, the precision of sung notes in Carnatic music is studied in detail by treating CPNs and STAs separately. There is variation among the nineteen musicians considered, but on average, the precision of CPNs increases exponentially with duration and settles at about 10 cents for CPNs longer than 0.5 seconds. For analyzing STAs, in contrast to Western music, rāga (melody) information is found to be necessary, and errors in STAs show a significantly larger standard deviation of about 60 cents. To corroborate these observations, the music was automatically transcribed and re-synthesized using CPN and STA information using two interpolation techniques. The results of perceptual tests clearly indicate that the grammar is highly flexible. We also show that the precision errors are not due to poor pitch tracking, singer deficiencies or delay in auditory feedback. © V S Viraraghavan, H A Murthy, R Aravind.","","Auditory feedback; Interpolation techniques; Perceptual test; Pitch curve; Pitch-tracking; Precision errors; Standard deviation; Stationary points; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Eremenko V.; Demirel E.; Bozkurt B.; Serra X.","Eremenko, Vsevolod (57210184345); Demirel, Emir (57210187153); Bozkurt, Baris (23476648700); Serra, Xavier (55892979900)","57210184345; 57210187153; 23476648700; 55892979900","Audio-aligned jazz harmony dataset for automatic chord transcription and corpus-based research","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883510&partnerID=40&md5=924b9f758a7c370c44cacea998a9d065","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Eremenko V., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Demirel E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bozkurt B., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present a new dataset of time-aligned jazz harmony transcriptions. This dataset is a useful resource for content-based analysis, especially for training and evaluating chord transcription algorithms. Most of the available chord transcription datasets only contain annotations for rock and pop, and the characteristics of jazz, such as the extensive use of seventh chords, are not represented. Our dataset consists of annotations of 113 tracks selected from “The Smithsonian Collection of Classic Jazz” and “Jazz: The Smithsonian Anthology,” covering a range of performers, subgenres, and historical periods. Annotations were made by a jazz musician and contain information about the meter, structure, and chords for entire audio tracks. We also present evaluation results of this dataset using state-of-the-art chord estimation algorithms that support seventh chords. The dataset is valuable for jazz scholars interested in corpus-based research. To demonstrate this, we extract statistics for symbolic data and chroma features from the audio tracks. © Vsevolod Eremenko, Emir Demirel, Baris Bozkurt, Xavier Serra.","","Information retrieval; Chroma features; Content-based analysis; Corpus-based; Estimation algorithm; Evaluation results; Historical periods; State of the art; Symbolic data; Transcription","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Vatolkin I.; Rudolph G.","Vatolkin, Igor (25652236100); Rudolph, Günter (15021491400)","25652236100; 15021491400","Comparison of audio features for recognition of western and ethnic instruments in polyphonic mixtures","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858324&partnerID=40&md5=135018cdc72030d379166fe4df5af471","TU Dortmund, Department of Computer Science, Germany","Vatolkin I., TU Dortmund, Department of Computer Science, Germany; Rudolph G., TU Dortmund, Department of Computer Science, Germany","Studies on instrument recognition are almost always restricted to either Western or ethnic music. Only little work has been done to compare both musical worlds. In this paper, we analyse the performance of various audio features for recognition of Western and ethnic instruments in chords. The feature selection is done with the help of a minimum redundancy - maximum relevance strategy and a multi-objective evolutionary algorithm. We compare the features found to be the best for individual categories and propose a novel strategy based on non-dominated sorting to evaluate and select trade-off features which may contribute as best as possible to the recognition of individual and all instruments. © Igor Vatolkin, Günter Rudolph.","","Economic and social effects; Evolutionary algorithms; Information retrieval; Audio features; Instrument recognition; Minimum redundancy-maximum relevances; Multi objective evolutionary algorithms; Non-dominated Sorting; Novel strategies; Trade off; Feature extraction","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Yan Y.; Lustig E.; VanderStel J.; Duan Z.","Yan, Yujia (57210192422); Lustig, Ethan (57210310402); VanderStel, Joseph (57207819753); Duan, Zhiyao (24450312900)","57210192422; 57210310402; 57207819753; 24450312900","Part-invariant model for music generation and harmonization","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862810&partnerID=40&md5=e791d92f474abea76c8a215aebff7d99","Electrical and Computer Engineering, University of Rochester, United States; Eastman School of Music, University of Rochester, United States","Yan Y., Electrical and Computer Engineering, University of Rochester, United States; Lustig E., Eastman School of Music, University of Rochester, United States; VanderStel J., Eastman School of Music, University of Rochester, United States; Duan Z., Electrical and Computer Engineering, University of Rochester, United States","Automatic music generation has been gaining more attention in recent years. Existing approaches, however, are mostly ad hoc to specific rhythmic structures or instrumentation layouts, and lack music-theoretic rigor in their evaluations. In this paper, we present a neural language (music) model that tries to model symbolic multi-part music. Our model is part-invariant, i.e., it can process/generate any part (voice) of a music score consisting of an arbitrary number of parts, using a single trained model. For better incorporating structural information of pitch spaces, we use a structured embedding matrix to encode multiple aspects of a pitch into a vector representation. The generation is performed by Gibbs Sampling. Meanwhile, our model directly generates note spellings to make outputs human-readable. We performed objective (grading) and subjective (listening) evaluations by recruiting music theorists to compare the outputs of our algorithm with those of music students on the task of bassline harmonization (a traditional pedagogical task). Our experiment shows that errors of our algorithm and students are differently distributed, and the range of ratings for generated pieces overlaps with students’ to varying extents for our three provided basslines. This experiment suggests some future research directions. © Yujia Yan, Ethan Lustig, Joseph VanderStel, Zhiyao Duan .","","Grading; Information retrieval; Vector spaces; Arbitrary number; Embedding matrices; Future research directions; Gibbs sampling; Pedagogical tasks; Rhythmic structures; Structural information; Vector representations; Students","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Cumming J.E.; McKay C.; Stuchbery J.; Fujinaga I.","Cumming, Julie E. (53463213000); McKay, Cory (14033215600); Stuchbery, Jonathan (57210190501); Fujinaga, Ichiro (9038140900)","53463213000; 14033215600; 57210190501; 9038140900","Methodologies for creating symbolic corpora of Western music before 1600","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069843974&partnerID=40&md5=502b02a4425e917f9fe768b6d5f6e4b5","McGill University, Canada; Marianopolis College, Canada","Cumming J.E., McGill University, Canada; McKay C., Marianopolis College, Canada; Stuchbery J., McGill University, Canada; Fujinaga I., McGill University, Canada","The creation of a corpus of compositions in symbolic formats is an essential step for any project in systematic research. There are, however, many potential pitfalls, especially in early music, where scores are edited in different ways: variables include clefs, note values, types of barline, and editorial accidentals. Different score editors and optical music recognition software have their own ways of storing and exporting musical data. Choice of software and file formats, and their various parameters, can thus unintentionally bias data, as can decisions on how to interpret potentially ambiguous markings in original sources. This becomes especially problematic when data from different corpora are combined for computational processing, since observed regularities and irregularities may in fact be linked with inconsistent corpus collection methodologies, internal and external, rather than the underlying music. This paper proposes guidelines, templates, and work-flows for the creation of consistent early music corpora, and for detecting encoding biases in existing corpora. We have assembled a corpus of Renaissance duos as a sample implementation, and present machine learning experiments demonstrating how inconsistent or naïve encoding methodologies for corpus collection can distort results. © Julie E. Cumming, Cory McKay, Jonathan Stuchbery, Ichiro Fujinaga.","","Encoding (symbols); Information retrieval; Computational processing; Early musics; File formats; Optical music recognition; Systematic research; Work-flows; Signal encoding","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Schreiber H.; Müller M.","Schreiber, Hendrik (55586286200); Müller, Meinard (7404689873)","55586286200; 7404689873","A crowdsourced experiment for tempo estimation of electronic dance music","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804549&partnerID=40&md5=1196739f405131e9d1f5681c0bdf674e","tagtraum industries incorporated, United States; International Audio Laboratories Erlangen, Germany","Schreiber H., tagtraum industries incorporated, United States; Müller M., International Audio Laboratories Erlangen, Germany","Relative to other datasets, state-of-the-art tempo estimation algorithms perform poorly on the GiantSteps Tempo dataset for electronic dance music (EDM). In order to investigate why, we conducted a large-scale, crowdsourced experiment involving 266 participants from two distinct groups. The quality of the collected data was evaluated with regard to the participants’ input devices and background. In the data itself we observed significant tempo ambiguities, which we attribute to annotator subjectivity and tempo instability. As a further contribution, we then constructed new annotations consisting of tempo distributions for each track. Using these annotations, we reevaluated two recent state-of-the-art tempo estimation systems achieving significantly improved results. The main conclusions of this investigation are that current tempo estimation systems perform better than previously thought and that evaluation quality needs to be improved. The new crowdsourced annotations will be released for evaluation purposes. © Hendrik Schreiber, Meinard Müller.","","Crowdsourcing; Electronic musical instruments; Information retrieval; Input devices; Recent state; State of the art; Tempo estimations; Quality control","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Dorfer M.; Henkel F.; Widmer G.","Dorfer, Matthias (55516844500); Henkel, Florian (57210190747); Widmer, Gerhard (7004342843)","55516844500; 57210190747; 7004342843","Learning to listen, read, and follow: Score following as a reinforcement learning game","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069801375&partnerID=40&md5=5d87093dfa8032d617301902a4f72f74","Institute of Computational Perception, Johannes Kepler University Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Dorfer M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Henkel F., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Score following is the process of tracking a musical performance (audio) with respect to a known symbolic representation (a score). We start this paper by formulating score following as a multimodal Markov Decision Process, the mathematical foundation for sequential decision making. Given this formal definition, we address the score following task with state-of-the-art deep reinforcement learning (RL) algorithms such as synchronous advantage actor critic (A2C). In particular, we design multimodal RL agents that simultaneously learn to listen to music, read the scores from images of sheet music, and follow the audio along in the sheet, in an end-to-end fashion. All this behavior is learned entirely from scratch, based on a weak and potentially delayed reward signal that indicates to the agent how close it is to the correct position in the score. Besides discussing the theoretical advantages of this learning paradigm, we show in experiments that it is in fact superior compared to previously proposed methods for score following in raw sheet music images. © Matthias Dorfer, Florian Henkel, Gerhard Widmer.","","Audio acoustics; Behavioral research; Decision making; Deep learning; Information retrieval; Machine learning; Markov processes; Formal definition; Learning paradigms; Markov Decision Processes; Mathematical foundations; Musical performance; Sequential decision making; State of the art; Symbolic representation; Reinforcement learning","M. Dorfer; Institute of Computational Perception, Johannes Kepler University Linz, Austria; email: matthias.dorfer@jku.at","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Tsukuda K.; Fukayama S.; Goto M.","Tsukuda, Kosetsu (36609397600); Fukayama, Satoru (56407004300); Goto, Masataka (7403505330)","36609397600; 56407004300; 7403505330","Listener anonymizer: Camouflaging play logs to preserve user’s demographic anonymity","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069851815&partnerID=40&md5=6b4d313e7a549b059b2d3b53dc9dc324","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Tsukuda K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","When a user signs up with an online music service, she is often requested to register her demographic attributes such as age, gender, and nationality. Even if she does not input such information, it has been reported that user attributes can be predicted with high accuracy by using her play log. How can users enjoy music when using an online music service while preserving their demographic anonymity? To solve this problem, we propose a system called Listener Anonymizer. Listener Anonymizer monitors the user’s play log. When it detects that her confidential attributes can be predicted, it selects songs that can decrease the prediction accuracy and recommends them to her. The user can camouflage her play logs by playing these songs to preserve her demographic anonymity. Since such songs do not always match her music taste, selecting as few songs as possible that can effectively anonymize her attributes is required. Listener Anonymizer realizes this by selecting songs based on feature ablation analysis. Our experimental results using Last.fm play logs showed that Listener Anonymizer was able to preserve anonymity with fewer songs than a method that randomly selected songs. © Kosetsu Tsukuda, Satoru Fukayama, Masataka Goto.","","Information retrieval; Anonymizer; High-accuracy; Last.fm; Online music services; Prediction accuracy; Population statistics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Spinelli L.; Lau J.; Pritchard L.; Lee J.H.","Spinelli, Louis (57203945988); Lau, Josephine (57209060717); Pritchard, Liz (57196460602); Lee, Jin Ha (57190797465)","57203945988; 57209060717; 57196460602; 57190797465","Influences on the social practices surrounding commercial music services: A model for rich interactions","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069865386&partnerID=40&md5=0473760382aadd9b84bfd8459e42c3ef","Information School, University of Washington, Seattle, United States","Spinelli L., Information School, University of Washington, Seattle, United States; Lau J., Information School, University of Washington, Seattle, United States; Pritchard L., Information School, University of Washington, Seattle, United States; Lee J.H., Information School, University of Washington, Seattle, United States","Music can play an important role in social experiences and interactions. Technologies in-use affect these experiences and interactions and as they continue to evolve, social behaviors and norms surrounding them also evolve. In this paper, we explore the social aspects of commercial music services through focus group observation and interview data. We seek to better understand how existing services are used for social music practices and can be improved. We identified 9 social practices and 24 influences surrounding commercial music services. Based on the user data, we created a model of these practices and influences that provides a lens through which social experiences surrounding commercial music services can be understood. An understanding of these social practices within their contextual ecosystem help inform what influences should be considered when designing new technologies. Our findings include the identification of: the underlying relationships between practices and their influences; practices and influences that inform the weight of relationships in social networks; social norms to be considered when designing social features; influences that add additional insight to previously observed behaviors; and a detailed explanation of how music selection and listening practices can be supported by commercial music services.[1] L. Barrington, R. Oda, and G.R.G. Lanckriet. Smarter than Genius? Human Evaluation of Music Recom-mender Systems. In Proc. ISMIR, pages 357–362, 2009. © Louis Spinelli, Josephine Lau, Liz Pritchard, Jin Ha Lee.","","Information retrieval; Social aspects; Focus groups; Human evaluation; Social behavior; Social norm; Social practices; User data; Economic and social effects","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Krasanakis E.; Schinas E.; Papadopoulos S.; Kompatsiaris Y.; Mitkas P.","Krasanakis, Emmanouil (57209312190); Schinas, Emmanouil (55319302900); Papadopoulos, Symeon (23095370800); Kompatsiaris, Yiannis (57208472535); Mitkas, Pericles (7003488915)","57209312190; 55319302900; 23095370800; 57208472535; 7003488915","VenueRank: Identifying venues that contribute to artist popularity","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069864049&partnerID=40&md5=01069617b622f38ab74591deab6b6c4e","CERTH-ITI, Thessaloniki, Greece; AUTH, Thessaloniki, Greece","Krasanakis E., CERTH-ITI, Thessaloniki, Greece; Schinas E., CERTH-ITI, Thessaloniki, Greece, AUTH, Thessaloniki, Greece; Papadopoulos S., CERTH-ITI, Thessaloniki, Greece; Kompatsiaris Y., CERTH-ITI, Thessaloniki, Greece; Mitkas P., AUTH, Thessaloniki, Greece","An important problem in the live music industry is finding venues that help expose artists to wider audiences. However, it is often difficult to obtain live music audience data to tackle this task. In this work, we investigate whether important venues can instead be inferred through social media data. Our approach consists of employing bipartite graph ranking algorithms to help discover important venues in artist-venue graphs mined from Facebook. We use both well-established algorithms, such as BiRank, and a modification of their common iterative scheme that avoids the impact of possibly erroneous heuristics to the ranking, which we call VenueRank. Resulting venue ranks are compared to those obtained from feature extraction for predicting the most listened artists and large listener increments in Spotify. This comparison yields high correlation between venue importance for listener prediction and bipartite graph ranking algorithms, with VenueRank found more robust against overfitting. © Emmanouil Krasanakis, Emmanouil Schinas, Symeon Papadopoulos, Yiannis Kompatsiaris, Pericles Mitkas.","","Graph theory; Information retrieval; Social networking (online); Bipartite graphs; Facebook; Iterative schemes; Music industry; Overfitting; Ranking algorithm; Social media datum; Iterative methods","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Arzt A.; Lattner S.","Arzt, Andreas (36681791200); Lattner, Stefan (56989751900)","36681791200; 56989751900","Audio-to-score alignment using transposition-invariant features","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069809162&partnerID=40&md5=8de70ca97538629875f9d098090c1d85","Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Sony Computer Science Laboratories (CSL), Paris, France","Arzt A., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Lattner S., Institute of Computational Perception, Johannes Kepler University, Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France","Audio-to-score alignment is an important pre-processing step for in-depth analysis of classical music. In this paper, we apply novel transposition-invariant audio features to this task. These low-dimensional features represent local pitch intervals and are learned in an unsupervised fashion by a gated autoencoder. Our results show that the proposed features are indeed fully transposition-invariant and enable accurate alignments between transposed scores and performances. Furthermore, they can even outperform widely used features for audio-to-score alignment on ‘untransposed data’, and thus are a viable and more flexible alternative to well-established features for music alignment and matching. © Andreas Arzt, Stefan Lattner.","","Alignment; Information retrieval; Audio features; Auto encoders; Classical musics; In-depth analysis; Invariant features; Low dimensional; Pre-processing step; Audio acoustics","A. Arzt; Institute of Computational Perception, Johannes Kepler University, Linz, Austria; email: andreas.arzt@jku.at","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gebhardt R.B.; Lykartsis A.; Stein M.","Gebhardt, Roman B. (57189270631); Lykartsis, Athanasios (57140810900); Stein, Michael (57225710530)","57189270631; 57140810900; 57225710530","A confidence measure for key labelling","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883464&partnerID=40&md5=c2320a2c29b20bb3eaa86af824ae8b34","Audio Communication Group, TU Berlin, Germany; Native Instruments GmbH, Germany","Gebhardt R.B., Audio Communication Group, TU Berlin, Germany; Lykartsis A., Audio Communication Group, TU Berlin, Germany; Stein M., Native Instruments GmbH, Germany","We present a new measure for automatically estimating the confidence of musical key classification. Our approach leverages the degree of harmonic information held within a musical audio signal (its “keyness”) as well as the steadiness of local key detections across the its duration (its “stability”). Using this confidence measure, musical tracks which are likely to be misclassified, i.e. those with low confidence, can then be handled differently from those analysed by standard, fully automatic key detection methods. By means of a listening test, we demonstrate that our developed features significantly correlate with listeners’ ratings of harmonic complexity, steadiness and the uniqueness of key. Furthermore, we demonstrate that tracks which are incorrectly labelled using an existing key detection system obtain low confidence values. Finally, we introduce a new method called “root note heuristics” for the special treatment of tracks with low confidence. We show that by applying these root note heuristics, key detection results can be improved for minimalistic music. © Roman B. Gebhardt, Athanasios Lykartsis, Michael Stein.","","Heuristic methods; Information retrieval; Confidence Measure; Confidence values; Detection methods; Detection system; Listening tests; Musical audio signal; Special treatments; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Carsault T.; Nika J.; Esling P.","Carsault, Tristan (57210197206); Nika, Jérôme (56330694900); Esling, Philippe (36052697700)","57210197206; 56330694900; 36052697700","Using musical relationships between chord labels in automatic chord extraction tasks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850608&partnerID=40&md5=d6331c0115d183f60b8d14e175f203eb","Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France; L3i Lab, University of La Rochelle, France","Carsault T., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France; Nika J., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France, L3i Lab, University of La Rochelle, France; Esling P., Ircam, CNRS, Sorbonne Université, UMR 9912 STMS, France","Recent research on Automatic Chord Extraction (ACE) has focused on the improvement of models based on machine learning. However, most models still fail to take into account the prior knowledge underlying the labeling alphabets (chord labels). Furthermore, recent works have shown that ACE performances have reached a glass ceiling. Therefore, this prompts the need to focus on other aspects of the task, such as the introduction of musical knowledge in the representation, the improvement of the models towards more complex chord alphabets and the development of more adapted evaluation methods. In this paper, we propose to exploit specific properties and relationships between chord labels in order to improve the learning of statistical ACE models. Hence, we analyze the interdependence of the representations of chords and their associated distances, the precision of the chord alphabets, and the impact of performing alphabet reduction before or after training the model. Furthermore, we propose new training losses based on musical theory. We show that these improve the results of ACE systems based on Convolutional Neural Networks. By analyzing our results, we uncover a set of related insights on ACE tasks based on statistical models, and also formalize the musical meaning of some classification errors. © Tristan Carsault, Jérôme Nika, Philippe Esling.","","Extraction; Information retrieval; Neural networks; Classification errors; Convolutional neural network; Evaluation methods; Glass ceiling; On-machines; Prior knowledge; Recent researches; Specific properties; Learning systems","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Kelkar T.; Roy U.; Jensenius A.R.","Kelkar, Tejaswinee (56131884600); Roy, Udit (55613397900); Jensenius, Alexander Refsum (14026820200)","56131884600; 55613397900; 14026820200","Evaluating a collection of sound-tracing data of melodic phrases","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065563922&partnerID=40&md5=f1f1ab594e3192e859ac7865ac6047b5","RITMO, Dept. of Musicology, University of Oslo, Norway","Kelkar T., RITMO, Dept. of Musicology, University of Oslo, Norway; Roy U., RITMO, Dept. of Musicology, University of Oslo, Norway; Jensenius A.R., RITMO, Dept. of Musicology, University of Oslo, Norway","Melodic contour, the ‘shape’ of a melody, is a common way to visualize and remember a musical piece. The purpose of this paper is to explore the building blocks of a future ‘gesture-based’ melody retrieval system. We present a dataset containing 16 melodic phrases from four musical styles and with a large range of contour variability. This is accompanied by full-body motion capture data of 26 participants performing sound-tracing to the melodies. The dataset is analyzed using canonical correlation analysis (CCA), and its neural network variant (Deep CCA), to understand how melodic contours and sound tracings relate to each other. The analyses reveal non-linear relationships between sound and motion. The link between pitch and verticality does not appear strong enough for complex melodies. We also find that descending melodic contours have the least correlation with tracings. © Tejaswinee Kelkar, Udit Roy, Alexander Refsum Jensenius.","","Large dataset; Building blockes; Canonical correlation analysis; Full-body motions; Musical pieces; Non-linear relationships; Retrieval systems; Sound tracings; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gómez J.S.; Abeßer J.; Cano E.","Gómez, Juan S. (57210196969); Abeßer, Jakob (36607532300); Cano, Estefanía (51161075800)","57210196969; 36607532300; 51161075800","Jazz solo instrument classification with convolutional neural networks, source separation, and transfer learning","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069006100&partnerID=40&md5=b04928c84667dc2cb4b790b39914554c","Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Gómez J.S., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Cano E., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Predominant instrument recognition in ensemble recordings remains a challenging task, particularly if closely-related instruments such as alto and tenor saxophone need to be distinguished. In this paper, we build upon a recently-proposed instrument recognition algorithm based on a hybrid deep neural network: a combination of convolutional and fully connected layers for learning characteristic spectral-temporal patterns. We systematically evaluate harmonic/percussive and solo/accompaniment source separation algorithms as pre-processing steps to reduce the overlap among multiple instruments prior to the instrument recognition step. For the particular use-case of solo instrument recognition in jazz ensemble recordings, we further apply transfer learning techniques to fine-tune a previously trained instrument recognition model for classifying six jazz solo instruments. Our results indicate that both source separation as pre-processing step as well as transfer learning clearly improve recognition performance, especially for smaller subsets of highly similar instruments. © Juan S. Gómez, Jakob Abeßer, Estefanía Cano.","","Audio recordings; Convolution; Deep neural networks; Information retrieval; Multilayer neural networks; Separation; Convolutional neural network; Instrument recognition; Multiple instruments; Pre-processing step; Separation algorithms; Temporal pattern; Transfer learning; Source separation","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Xi Q.; Bittner R.M.; Pauwels J.; Ye X.; Bello J.P.","Xi, Qingyang (57210186195); Bittner, Rachel M. (55659619600); Pauwels, Johan (35113648700); Ye, Xuzhou (57210188386); Bello, Juan P. (7102889110)","57210186195; 55659619600; 35113648700; 57210188386; 7102889110","Guitarset: A dataset for guitar transcription","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069828888&partnerID=40&md5=9fad38df11b933c11790ebd05a3cb366","Music and Audio Research Lab, New York University, United States; Center for Digital Music, Queen Mary University of London, United Kingdom","Xi Q., Music and Audio Research Lab, New York University, United States; Bittner R.M., Music and Audio Research Lab, New York University, United States; Pauwels J., Center for Digital Music, Queen Mary University of London, United Kingdom; Ye X., Music and Audio Research Lab, New York University, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","The guitar is a popular instrument for a variety of reasons, including its ability to produce polyphonic sound and its musical versatility. The resulting variability of sounds, however, poses significant challenges to automated methods for analyzing guitar recordings. As data driven methods become increasingly popular for difficult problems like guitar transcription, sets of labeled audio data are highly valuable resources. In this paper we present GuitarSet, a dataset that provides high quality guitar recordings alongside rich annotations and metadata. In particular, by recording guitars using a hexaphonic pickup, we are able to not only provide recordings of the individual strings but also to largely automate the expensive annotation process. The dataset contains recordings of a variety of musical excerpts played on an acoustic guitar, along with time-aligned annotations of string and fret positions, chords, beats, downbeats, and playing style. We conclude with an analysis of new challenges presented by this data, and see that it is interesting for a wide variety of tasks in addition to guitar transcription, including performance analysis, beat/downbeat tracking, and chord estimation. © Qingyang Xi, Rachel Bittner, Johan Pauwels, Xuzhou Ye, Juan Bello.","","Information retrieval; Musical instruments; Transcription; Acoustic guitar; Audio data; Automated methods; Data-driven methods; High quality; Performance analysis; Playing style; Polyphonic sounds; Audio recordings","Q. Xi; Music and Audio Research Lab, New York University, United States; email: tom.xi@nyu.edu","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Figueiredo F.; Andrade N.","Figueiredo, Flavio (35317374600); Andrade, Nazareno (7003429497)","35317374600; 7003429497","Quantifying disruptive influence in the allmusic guide","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093667&partnerID=40&md5=9c9a17c137cac2256fc62ec76214c1ad","Universidade Federal de Minas Gerais, Brazil; Universidade Federal de Campina Grande, Brazil","Figueiredo F., Universidade Federal de Minas Gerais, Brazil; Andrade N., Universidade Federal de Campina Grande, Brazil","Understanding how influences shape musical creation provides rich insight into cultural trends. As such, there have been several efforts to create quantitative complex network methods that support the analysis of influence networks among artists in a music corpus. We contribute to this body of work by examining how disruption happens in a corpus about music influence from the All Music Guide. A disruptive artist is one that creates a new stream of influences; this artist builds on prior efforts but influences subsequent artists that do not build on the same prior efforts. We leverage methods devised to study disruption in Science and Technology and apply them to the context of music creation. Our results point that such methods identify innovative artists and that disruption is mostly uncorrelated with network centrality. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Complex networks; Cultural trends; Influence networks; Music creation; Network centralities; Science and Technology; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Bauer C.; Schedl M.","Bauer, Christine (24491705100); Schedl, Markus (8684865900)","24491705100; 8684865900","Investigating cross-country relationship between users’ social ties and music mainstreaminess","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069877333&partnerID=40&md5=34cb4ff88a61298e0fb0b56f548bf59c","Johannes Kepler University Linz, Austria","Bauer C., Johannes Kepler University Linz, Austria; Schedl M., Johannes Kepler University Linz, Austria","We investigate the complex relationship between the factors (i) preference for music mainstream, (ii) social ties in an online music platform, and (iii) demographics. We define (i) on a global and a country level, (ii) by several network centrality measures such as Jaccard index among users’ connections, closeness centrality, and betweenness centrality, and (iii) by country and age information. Using the LFM-1b dataset of listening events of Last.fm users, we are able to uncover country-dependent differences in consumption of mainstream music as well as in user behavior with respect to social ties and users’ centrality. We could identify that users inclined to mainstream music tend to have stronger connections than the group of less mainstreamy users. Furthermore, our analysis revealed that users typically have less connections within a country than cross-country ones, with the first being stronger social ties, though. Results will help building better user models of listeners and in turn improve personalized music retrieval and recommendation algorithms. © Christine Bauer, Markus Schedl.","","Information retrieval; Betweenness centrality; Closeness centralities; Complex relationships; Jaccard index; Music retrieval; Network centralities; Recommendation algorithms; User behaviors; Behavioral research","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Cheng T.; Fukayama S.; Goto M.","Cheng, Tian (57206575362); Fukayama, Satoru (56407004300); Goto, Masataka (7403505330)","57206575362; 56407004300; 7403505330","Comparing RNN parameters for melodic similarity","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069829696&partnerID=40&md5=774ee016a53f8a2b8542459dd8b1fe56","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Cheng T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Melodic similarity is an important task in the Music Information Retrieval (MIR) domain, with promising applications including query by example, music recommendation and visualisation. Most current approaches compute the similarity between two melodic sequences by comparing their local features (distance between pitches, intervals, etc.) or by comparing the sequences after aligning them. In order to find a better feature representing global characteristics of a melody, we propose to represent the melodic sequence of each musical piece by the parameters of a generative Recurrent Neural Network (RNN) trained on its sequence. Because the trained RNN can generate the identical melodic sequence of each piece, we can expect that the RNN parameters contain the temporal information within the melody. In our experiment, we first train an RNN on all melodic sequences, and then use it as an initialisation to train an individual RNN on each melodic sequence. The similarity between two melodies is computed by using the distance between their individual RNN parameters. Experimental results showed that the proposed RNN-based similarity outperformed the baseline similarity obtained by directly comparing melodic sequences. © Tian Cheng, Satoru Fukayama, Masataka Goto.","","Information retrieval; Its sequences; Melodic similarity; Music information retrieval; Music recommendation; Musical pieces; Query-by example; Recurrent neural network (RNN); Temporal information; Recurrent neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Jeong D.; Kwon T.; Nam J.","Jeong, Dasaem (57195433126); Kwon, Taegyun (57210194738); Nam, Juhan (35812266500)","57195433126; 57210194738; 35812266500","A timbre-based approach to estimate key velocity from polyphonic piano recordings","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069873811&partnerID=40&md5=95e5b8d30b3b48a2b48322287728431c","Graduate School of Culture Technology, KAIST, South Korea","Jeong D., Graduate School of Culture Technology, KAIST, South Korea; Kwon T., Graduate School of Culture Technology, KAIST, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Estimating the key velocity of each note from polyphonic piano music is a highly challenging task. Previous work addressed the problem by estimating note intensity using a polyphonic note model. However, they are limited because the note intensity is vulnerable to various factors in a recording environment. In this paper, we propose a novel method to estimate the key velocity focusing on timbre change which is another cue associated with the key velocity. To this end, we separate individual notes of polyphonic piano music using non-negative matrix factorization (NMF) and feed them into a neural network that is trained to discriminate the timbre change according to the key velocity. Combining the note intensity from the separated notes with the statistics of the neural network prediction, the proposed method estimates the key velocity in the dimension of MIDI note velocity. The evaluation on Saarland Music Data and the MAPS dataset shows promising results in terms of robustness to changes in the recording environment. © Dasaem Jeong, Taegyun Kwon, Juhan Nam.","","Factorization; Information retrieval; Matrix algebra; Musical instruments; Velocity; Music data; Neural network predictions; Nonnegative matrix factorization; Piano music; Recording environment; Audio recordings","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Tralie C.J.","Tralie, Christopher J. (55921094200)","55921094200","Cover song synthesis by analogy","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837883&partnerID=40&md5=dbf6b7af7297b0a69e134fd25a0b997f","Duke University, Department of Mathematics, United States","Tralie C.J., Duke University, Department of Mathematics, United States","In this work, we pose and address the following “cover song analogies” problem: given a song A by artist 1 and a cover song A’ of this song by artist 2, and given a different song B by artist 1, synthesize a song B’ which is a cover of B in the style of artist 2. Normally, such a polyphonic style transfer problem would be quite challenging, but we show how the cover songs example constrains the problem, making it easier to solve. First, we extract the longest common beat-synchronous subsequence between A and A’, and we time stretch the corresponding beat intervals in A’ so that they align with A. We then derive a version of joint 2D convolutional NMF, which we apply to the constant-Q spectrograms of the synchronized segments to learn a translation dictionary of sound templates from A to A’. Finally, we apply the learned templates as filters to the song B, and we mash up the translated filtered components into the synthesized song B’ using audio mosaicing. We showcase our algorithm on several examples, including a synthesized cover version of Michael Jackson’s “Bad” by Alien Ant Farm, learned from the latter’s “Smooth Criminal” cover. © Christopher J. Tralie.","","Cover songs; Jackson; Mash up; Mosaicing; Spectrograms; Time stretch; Transfer problems; Information retrieval","C.J. Tralie; Duke University, Department of Mathematics, United States; email: ctralie@alumni.princeton.edu","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"de Valk R.; Weyde T.","de Valk, Reinier (55979450000); Weyde, Tillman (24476899500)","55979450000; 24476899500","Deep neural networks with voice entry estimation heuristics for voice separation in symbolic music representations","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842543&partnerID=40&md5=3ba72db0c8766844cc88d12fb0eb10f3","Jukedeck Ltd, United Kingdom; Department of Computer Science City, University of London, United Kingdom","de Valk R., Jukedeck Ltd, United Kingdom; Weyde T., Department of Computer Science City, University of London, United Kingdom","In this study we explore the use of deep feedforward neural networks for voice separation in symbolic music representations. We experiment with different network architectures, varying the number and size of the hidden layers, and with dropout. We integrate two voice entry estimation heuristics that estimate the entry points of the individual voices in the polyphonic fabric into the models. These heuristics serve to reduce error propagation at the beginning of a piece, which, as we have shown in previous work, can seriously hamper model performance. The models are evaluated on the 48 fugues from Johann Sebastian Bach’s The Well-Tempered Clavier and his 30 inventions—a dataset that we curated and make publicly available. We find that a model with two hidden layers yields the best results. Using more layers does not lead to a significant performance improvement. Furthermore, we find that our voice entry estimation heuristics are highly effective in the reduction of error propagation, improving performance significantly. Our best-performing model outperforms our previous models, where the difference is significant, and, depending on the evaluation metric, performs close to or better than the reported state of the art. © Reinier de Valk, Tillman Weyde.","","Backpropagation; Feedforward neural networks; Information retrieval; Network architecture; Error propagation; Evaluation metrics; Improving performance; Model performance; Reduction of errors; State of the art; Symbolic music representation; Voice separation; Deep neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lu W.-T.; Su L.","Lu, Wei-Tsung (57207994024); Su, Li (55966919100)","57207994024; 55966919100","Transferring the style of homophonic music using recurrent neural networks and autoregressive models","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069003444&partnerID=40&md5=095223d6ba07d585b36e60460c82e486","Institute of Information Science, Academia Sinica, Taiwan","Lu W.-T., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Utilizing deep learning techniques to generate musical contents has caught wide attention in recent years. Within this context, this paper investigates a specific problem related to music generation, music style transfer. This practical problem aims to alter the style of a given music piece from one to another while preserving the essence of that piece, such as melody and chord progression. In particular, we discuss the style transfer of homophonic music, composed of a predominant melody part and an accompaniment part, where the latter is modified through Gibbs sampling on a generative model combining recurrent neural networks and autoregressive models. Both objective and subjective test experiment are performed to assess the performance of transferring the style of an arbitrary music piece having a homophonic texture into two different distinct styles, Bachs chorales and Jazz. © Wei-Tsung Lu and Li Su.","","Deep learning; Information retrieval; Textures; Auto regressive models; Generative model; Gibbs sampling; Learning techniques; Practical problems; Specific problems; Recurrent neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bigoni F.; Dahl S.","Bigoni, Francesco (57205490068); Dahl, Sofia (7102199864)","57205490068; 7102199864","Timbre discrimination for brief instrument sounds","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856128&partnerID=40&md5=9db954ad57a674b76ee69ae6c8a1c002","Sound and Music Computing Aalborg University, Copenhagen, Denmark; Dept. of Architecture, Design and Media Technology, Aalborg University, Copenhagen, Denmark","Bigoni F., Sound and Music Computing Aalborg University, Copenhagen, Denmark; Dahl S., Dept. of Architecture, Design and Media Technology, Aalborg University, Copenhagen, Denmark","Timbre discrimination, even for very brief sounds, allows identification and separation of different sound sources. The existing literature on the effect of duration on timbre recognition shows high performance for remarkably short time window lengths, but does not address the possible effect of musical training. In this study, we applied an adaptive procedure to investigate the effect of musical training on individual thresholds for instrument identification. A timbre discrimination task consisting of a 4-alternative forced choice (4AFC) of brief instrument sounds with varying duration was assigned to 16 test subjects using an adaptive staircase method. The effect of musical training has been investigated by dividing the participants into two groups: musicians and non-musicians. The experiment showed lowest thresholds for the guitar sound and highest for the violin sound, with a high overall performance level, but no significant difference between the two groups. It is suggested that the test subjects adjust the weightings of the perceptual dimensions of timbre according to different degrees of acoustic degradation of the stimuli, which are evaluated both by plotting extracted audio features in a feature space and by considering the timbral specificities of the four instruments. © Francesco Bigoni, Sofia Dahl.","","Acoustic waves; Information retrieval; Adaptive procedure; Audio features; Discrimination tasks; Instrument identification; Perceptual dimensions; Performance level; Short time windows; Staircase method; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gouvert O.; Oberlin T.; Févotte C.","Gouvert, Olivier (57210196294); Oberlin, Thomas (55193333000); Févotte, Cédric (14031507800)","57210196294; 55193333000; 14031507800","Matrix co-factorization for cold-start recommendation","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069849853&partnerID=40&md5=3ca127bbab4189deff4ab91ec238f618","IRIT, Université de Toulouse, CNRS, France","Gouvert O., IRIT, Université de Toulouse, CNRS, France; Oberlin T., IRIT, Université de Toulouse, CNRS, France; Févotte C., IRIT, Université de Toulouse, CNRS, France","Song recommendation from listening counts is now a classical problem, addressed by different kinds of collaborative filtering (CF) techniques. Among them, Poisson matrix factorization (PMF) has raised a lot of interest, since it seems well-suited to the implicit data provided by listening counts. Additionally, it has proven to achieve state-of-the-art performance while being scalable to big data. Yet, CF suffers from a critical issue, usually called cold-start problem: the system cannot recommend new songs, i.e., songs which have never been listened to. To alleviate this, one should complement the listening counts with another modality. This paper proposes a multi-modal extension of PMF applied to listening counts and tag labels extracted from the Million Song Dataset. In our model, every song is represented by the same activation pattern in each modality but with possibly different scales. As such, the method is not prone to the cold-start problem, i.e., it can learn from a single modality when the other one is not informative. Our model is symmetric (it equally uses both modalities) and we evaluate it on two tasks: new songs recommendation and tag labeling. © Olivier Gouvert, Thomas Oberlin, Cédric Févotte.","","Collaborative filtering; Factorization; Information retrieval; Activation patterns; Classical problems; Cold start problems; Cold-start Recommendations; Critical issues; Matrix factorizations; Multi-modal; State-of-the-art performance; Matrix algebra","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Hennequin R.; Royo-Letelier J.; Moussallam M.","Hennequin, Romain (36504173400); Royo-Letelier, Jimena (36915600300); Moussallam, Manuel (36608879000)","36504173400; 36915600300; 36608879000","Audio based disambiguation of music genre tags","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804969&partnerID=40&md5=f25c465b60612921495749536dc496f1","Deezer R and D, Paris, France","Hennequin R., Deezer R and D, Paris, France; Royo-Letelier J., Deezer R and D, Paris, France; Moussallam M., Deezer R and D, Paris, France","In this paper, we propose to infer music genre embeddings from audio datasets carrying semantic information about genres. We show that such embeddings can be used for disambiguating genre tags (identification of different labels for the same genre, tag translation from a tag system to another, inference of hierarchical taxonomies on these genre tags). These embeddings are built by training a deep convolutional neural network genre classifier with large audio datasets annotated with a flat tag system. We show empirically that they makes it possible to retrieve the original taxonomy of a tag system, spot duplicates tags and translate tags from a tag system to another. © Romain Hennequin, Jimena Royo-Letelier, Manuel Moussallam.","","Deep neural networks; Embeddings; Information retrieval; Large dataset; Neural networks; Semantics; Taxonomies; Audio-based; Convolutional neural network; Hierarchical taxonomy; Music genre; Semantic information; Tag systems; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bigo L.; Feisthauer L.; Giraud M.; Levé F.","Bigo, Louis (37057150800); Feisthauer, Laurent (57210193101); Giraud, Mathieu (8700367400); Levé, Florence (55893852300)","37057150800; 57210193101; 8700367400; 55893852300","Relevance of musical features for cadence detection","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069812504&partnerID=40&md5=e6dbeb86bc9e7add5148cc9f30f5fdd7","CRIStAL, UMR 9189, CNRS, Université de Lille, France; MIS, Université de Picardie Jules Verne, Amiens, France","Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Feisthauer L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Levé F., CRIStAL, UMR 9189, CNRS, Université de Lille, France, MIS, Université de Picardie Jules Verne, Amiens, France","Cadences, as breaths in music, are felt by the listener or studied by the theorist by combining harmony, melody, texture and possibly other musical aspects. We formalize and discuss the significance of 44 cadential features, correlated with the occurrence of cadences in scores. These features describe properties at the arrival beat of a cadence and its surroundings, but also at other onsets heuristically identified to pinpoint chords preparing the cadence. The representation of each beat of the score as a vector of cadential features makes it possible to reformulate cadence detection as a classification task. An SVM classifier was run on two corpora from Bach and Haydn totaling 162 perfect authentic cadences and 70 half cadences. In these corpora, the classifier correctly identified more than 75% of perfect authentic cadences and 50% of half cadences, with low false positive rates. The experiment results are consistent with common knowledge that classification is more complex for half cadences than for authentic cadences. © Louis Bigo, Laurent Feisthauer, Mathieu Giraud, Florence Levé.","","Information retrieval; Support vector machines; Textures; Classification tasks; Common knowledge; False positive rates; Musical features; SVM classifiers; Feature extraction","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Tsushima H.; Nakamura E.; Itoyama K.; Yoshii K.","Tsushima, Hiroaki (57201298652); Nakamura, Eita (24587601200); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120)","57201298652; 24587601200; 18042499100; 7103400120","Interactive arrangement of chords and melodies based on a tree-structured generative model","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069828371&partnerID=40&md5=e0a4ffb1a7858976601d1b760c6ecf62","Graduate School of Informatics, Kyoto University, Japan","Tsushima H., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","We describe an interactive music composition system that assists a user in refining chords and melodies by generating chords for melodies (harmonization) and vice versa (melodization). Since these two tasks have been dealt with independently, it is difficult to jointly estimate chords and melodies that are optimal in both tasks. Another problem is developing an interactive GUI that enables a user to partially update chords and melodies by considering the latent tree structure of music. To solve these problems, we propose a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chord symbols, (2) a metrical Markov model for chord boundaries, (3) a Markov model for melody pitches, and (4) a metrical Markov model for melody onsets. The harmonic functions (syntactic roles) and repetitive structure of chords are learned by the PCFG. Any variables specified by a user can be optimized or sampled in a principled manner according to a unified posterior distribution. For improved melodization, a long short-term memory (LSTM) network can also be used. The subjective experimental result showed the effectiveness of the proposed system. © Hiroaki Tsushima, Katsutoshi Itoyama, Eita Nakamura, Kazuyoshi Yoshii.","","Context free grammars; Forestry; Harmonic functions; Information retrieval; Markov processes; Trees (mathematics); Generative model; Interactive music; Markov model; Posterior distributions; Probabilistic context free grammars; Repetitive structure; Tree structures; Tree-structured; Long short-term memory","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"McLeod A.; Steedman M.","McLeod, Andrew (57114559100); Steedman, Mark (6602901918)","57114559100; 6602901918","Evaluating automatic polyphonic music transcription","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069839804&partnerID=40&md5=188c425fbacd254dbde234072e4d9662","University of Edinburgh, United Kingdom","McLeod A., University of Edinburgh, United Kingdom; Steedman M., University of Edinburgh, United Kingdom","Automatic Music Transcription (AMT) is an important task in music information retrieval. Prior work has focused on multiple fundamental frequency estimation (multi-pitch detection), the conversion of an audio signal into a time-frequency representation such as a MIDI file. It is less common to annotate this output with musical features such as voicing information, metrical structure, and harmonic information, though these are important aspects of a complete transcription. Evaluation of these features is most often performed separately and independent of multi-pitch detection; however, these features are non-independent. We therefore introduce MV 2H, a quantitative, automatic, joint evaluation metric based on musicological principles, and show its effectiveness through the use of specific examples. The metric is modularised in such a way that it can still be used with partially performed annotation—for example, when the transcription process has been applied to some transduced format such as MIDI (which may itself be the result of multi-pitch detection). The code for the evaluation metric described here is available at https://www.github.com/apmcleod/MV2H. © Andrew McLeod, Mark Steedman.","","Frequency estimation; Information retrieval; Automatic music transcription; Evaluation metrics; Multiple fundamental frequency estimations; Music information retrieval; Musical features; Polyphonic music; Time-frequency representations; Transcription process; Transcription","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ren I.Y.; Volk A.; Swierstra W.; Veltkamp R.C.","Ren, Iris Yuping (57210197214); Volk, Anja (30567849900); Swierstra, Wouter (23486958400); Veltkamp, Remco C. (7003421646)","57210197214; 30567849900; 23486958400; 7003421646","Analysis by classification: A comparative study of annotated and algorithmically extracted patterns in symbolic music data","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069814533&partnerID=40&md5=72eb6d250e4c94141ba1a895c1b8e3eb","Utrecht University, Netherlands","Ren I.Y., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Swierstra W., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","Musical patterns are salient passages that repeatedly appear in music. Such passages are vital for compression, classification and prediction tasks in MIR, and algorithms employing different techniques have been proposed to find musical patterns automatically. Human-annotated patterns have been collected and used to evaluate pattern discovery algorithms, e.g., in the Discovery of Repeated Themes & Sections MIREX task. However, state-of-the-art algorithms are not yet able to reproduce human-annotated patterns. To understand what gives rise to the discrepancy between algorithmically extracted patterns and human-annotated patterns, we use jSymbolic2 to extract features from patterns, visualise the feature space using PCA and perform a comparative analysis using classification techniques. We show that it is possible to classify algorithmically extracted patterns, human-annotated patterns and randomly sampled passages. This implies: (a) Algorithmically extracted patterns possess different properties than human-annotated patterns (b) Algorithmically extracted patterns have different structures than randomly sampled passages (c) Human-annotated patterns contain more information than randomly sampled passages despite subjectivity involved in the annotation process. We further discover that rhythmic features are of high importance in the classification process, which should influence future research on automatic pattern discovery. © Iris Yuping Ren, Anja Volk, Wouter Swierstra, Remco C. Veltkamp.","","Classification process; Classification technique; Comparative analysis; Comparative studies; Different structure; Pattern discovery; Rhythmic features; State-of-the-art algorithms; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Park J.; Lee J.; Park J.; Ha J.-W.; Nam J.","Park, Jiyoung (57210173812); Lee, Jongpil (57192303939); Park, Jangyeon (57195613621); Ha, Jung-Woo (55430349200); Nam, Juhan (35812266500)","57210173812; 57192303939; 57195613621; 55430349200; 35812266500","Representation learning of music using artist labels","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069814372&partnerID=40&md5=08c39aadfe4c07c867c14c3d3be2d141","NAVER Corp, South Korea; Graduate School of Culture Technology, KAIST, South Korea","Park J., NAVER Corp, South Korea; Lee J., Graduate School of Culture Technology, KAIST, South Korea; Park J., NAVER Corp, South Korea; Ha J.-W., NAVER Corp, South Korea; Nam J., Graduate School of Culture Technology, KAIST, South Korea","In music domain, feature learning has been conducted mainly in two ways: unsupervised learning based on sparse representations or supervised learning by semantic labels such as music genre. However, finding discriminative features in an unsupervised way is challenging and supervised feature learning using semantic labels may involve noisy or expensive annotation. In this paper, we present a supervised feature learning approach using artist labels annotated in every single track as objective meta data. We propose two deep convolutional neural networks (DCNN) to learn the deep artist features. One is a plain DCNN trained with the whole artist labels simultaneously, and the other is a Siamese DCNN trained with a subset of the artist labels based on the artist identity. We apply the trained models to music classification and retrieval tasks in transfer learning settings. The results show that our approach is comparable to previous state-of-the-art methods, indicating that the proposed approach captures general music audio features as much as the models learned with semantic labels. Also, we discuss the advantages and disadvantages of the two models. © Jiyoung Park, Jongpil Lee, Jangyeon Park, Jung-Woo Ha, Juhan Nam.","","Deep neural networks; Information retrieval; Machine learning; Neural networks; Semantics; Convolutional neural network; Discriminative features; Feature learning; Music classification; Semantic labels; Sparse representation; State-of-the-art methods; Transfer learning; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"de Padua R.; Oliveira de Carvalho V.; de Oliveira Rezende S.; Silva D.F.","de Padua, Renan (55235541600); Oliveira de Carvalho, Verônica (57205632781); de Oliveira Rezende, Solange (7005154055); Silva, Diego Furtado (55585876200)","55235541600; 57205632781; 7005154055; 55585876200","Exploring musical relations using association rule networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837234&partnerID=40&md5=8ad3c36d1db5075224680a5814ae22ff","Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil; Data Science Team, Itaú-Unibanco, São Paulo, Brazil; Instituto de Geociências e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, Brazil; Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil","de Padua R., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil, Data Science Team, Itaú-Unibanco, São Paulo, Brazil; Oliveira de Carvalho V., Instituto de Geociências e Ciências Exatas, Universidade Estadual Paulista, Rio Claro, Brazil; de Oliveira Rezende S., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil; Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil","Music information retrieval (MIR) has been gaining increasing attention in both industry and academia. While many algorithms for MIR rely on assessing feature subsequences, the user normally has no resources to interpret the significance of these patterns. Interpreting the relations between these temporal patterns and some aspects of the assessed songs can help understanding not only some algorithms’ outcomes but the kind of patterns which better defines a set of similarly labeled recordings. In this work, we present a novel method to assess these relations, constructing an association rule network from temporal patterns obtained by a simple quantization process. With an empirical evaluation, we illustrate how we can use our method to explore these relations in a varied set of data and labels. © Renan de Padua, Vernica Oliveira de Carvalho, Solange Rezende, Diego Furtado Silva.","","Information retrieval; Empirical evaluations; Music information retrieval; Temporal pattern; Association rules","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bogdanov D.; Porter A.; Schreiber H.; Urbano J.; Oramas S.","Bogdanov, Dmitry (35748642000); Porter, Alastair (55582507300); Schreiber, Hendrik (55586286200); Urbano, Julián (36118414700); Oramas, Sergio (55582112200)","35748642000; 55582507300; 55586286200; 36118414700; 55582112200","The acousticbrainz genre dataset: Multi-source, multi-level, multi-label, and large-scale","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076102558&partnerID=40&md5=4692a4371583b34fadafa4813882feeb","Music Technology Group, Universitat Pompeu Fabra, Spain; Tagtraum Industries Incorporated, United States; Multimedia Computing Group, Delft University of Technology, Netherlands; Pandora, United States","Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Spain; Schreiber H., Tagtraum Industries Incorporated, United States; Urbano J., Multimedia Computing Group, Delft University of Technology, Netherlands; Oramas S., Pandora, United States","This paper introduces the AcousticBrainz Genre Dataset, a large-scale collection of hierarchical multi-label genre annotations from different metadata sources. It allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how this could be addressed by genre recognition systems. Genre labels for the dataset are sourced from both expert annotations and crowds, permitting comparisons between strict hierarchies and folksonomies. Music features are available via the Acoustic- Brainz database. To guide research, we suggest a concrete research task and provide a baseline as well as an evaluation method. This task may serve as an example of the development and validation of automatic annotation algorithms on complementary datasets with different taxonomies and coverage. With this dataset, we hope to contribute to developments in content-based music genre recognition as well as cross-disciplinary studies on genre metadata analysis. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Metadata; Petroleum reservoir evaluation; Taxonomies; Automatic annotation; Concrete research; Content-based; Cross-disciplinary; Expert annotations; Metadata analysis; Multi-Sources; Recognition systems; Large dataset","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Subramanian V.; Lerch A.","Subramanian, Vinod (57218450042); Lerch, Alexander (22034963000)","57218450042; 22034963000","Concert stitch: Organization and synchronization of crowd-sourced recordings","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881019&partnerID=40&md5=b0a1c42de5334ea5e30f8c4d285e00fe","Center for Music Technology, Georgia Institute of Technology, United States","Subramanian V., Center for Music Technology, Georgia Institute of Technology, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","The number of audience recordings of concerts on the internet has exploded with the advent of smartphones. This paper proposes a method to organize and align these recordings in order to create one or more complete renderings of the concert. The process comprises two steps: first, using audio fingerprints to represent the recordings, identify overlapping segments, and compute an approximate alignment using a modified Dynamic Time Warping (DTW) algorithm and second, applying a cross-correlation around the approximate alignment points in order to improve the accuracy of the alignment. The proposed method is compared to two baseline systems using approaches previously proposed for similar tasks. One baseline cross-correlates the audio fingerprints directly without DTW. The second baseline replaces the audio fingerprints with pitch chroma in the DTW algorithm. A new dataset annotating real-world data obtained from the Live Music Archive is presented and used for evaluation of the three systems. © Vinod Subramanian, Alexander Lerch.","","Alignment; Crowdsourcing; Information retrieval; Alignment points; Audio fingerprint; Baseline systems; Cross correlations; Dtw algorithms; Dynamic time warping algorithms; Real-world; Audio recordings","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bemman B.; Meredith D.","Bemman, Brian (56989782300); Meredith, David (57125703100)","56989782300; 57125703100","Backtracking search heuristics for solving the all-partition array problem","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093761&partnerID=40&md5=746e2893bc792aa57247ca4783d2425f","Aalborg University, Aalborg, Denmark","Bemman B., Aalborg University, Aalborg, Denmark; Meredith D., Aalborg University, Aalborg, Denmark","Recent efforts to model the compositional processes of Milton Babbitt have yielded a number of computationally challenging problems. One of these problems, known as the all-partition array problem, is a particularly hard variant of set covering, and several different approaches, including mathematical optimization, constraint satisfaction, and greedy backtracking, have been proposed for solving it. Of these previous approaches, only constraint programming has led to a successful solution. Unfortunately, this solution is expensive in terms of computation time. We present here two new search heuristics and a modification to a previously proposed heuristic, that, when applied to a greedy backtracking algorithm, allow the all-partition array problem to be solved in a practical running time. We demonstrate the success of our heuristics by solving for three different instances of the problem found in Babbitt's music, including one previously solved with constraint programming and one Babbitt himself was unable to solve. Use of the new heuristics allows each instance of the problem to be solved more quickly than was possible with previous approaches. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Computer programming; Constraint theory; Heuristic algorithms; Information retrieval; Optimization; Backtracking algorithm; Backtracking search; Computation time; Constraint programming; Constraint Satisfaction; Mathematical optimizations; Practical running time; Search heuristics; Constraint satisfaction problems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Pati A.; Lerch A.; Hadjeres G.","Pati, Ashis (57217331630); Lerch, Alexander (22034963000); Hadjeres, Gaëtan (57009038800)","57217331630; 22034963000; 57009038800","Learning to traverse latent spaces for musical score inpainting","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083466364&partnerID=40&md5=9a56b4e2cbd0393af640f73c64a74766","Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Sony CSL, Paris, France","Pati A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, Atlanta, United States; Hadjeres G., Sony CSL, Paris, France","Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Recurrent neural networks; Auto encoders; Complex trajectories; Designed models; Generative model; Interactive music; Learning-based approach; Musical score; Objective and subjective evaluations; Learning systems","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Huang C.-Z.A.; Hawthorne C.; Roberts A.; Dinculescu M.; Wexler J.; Hong L.; Howcroft J.","Huang, Cheng-Zhi Anna (56104404600); Hawthorne, Curtis (57204813140); Roberts, Adam (57201381304); Dinculescu, Monica (57217331672); Wexler, James (57195548331); Hong, Leon (57217329988); Howcroft, Jacob (57217329973)","56104404600; 57204813140; 57201381304; 57217331672; 57195548331; 57217329988; 57217329973","The bach doodle: Approachable music composition with machine learning at scale","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082396519&partnerID=40&md5=da8675f209460ffafe1e7a602281bb9d","Google Brain Magenta; Google Brain Pair; Google Doodle","Huang C.-Z.A., Google Brain Magenta; Hawthorne C., Google Brain Magenta; Roberts A., Google Brain Magenta; Dinculescu M., Google Brain Magenta; Wexler J., Google Brain Pair; Hong L., Google Doodle; Howcroft J., Google Doodle","To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle [1], where users can create their own melody and have it harmonized by a machine learning model (Coconet [22]) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js [32] to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depthwise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Input melodies; Machine learning models; Music composition; Music education; Partial modeling; Public dataset; Runtimes; Speed test; Machine learning","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Sarfati M.; Hu A.; Donier J.","Sarfati, Marc (57217331581); Hu, Anthony (57217330585); Donier, Jonathan (56672943900)","57217331581; 57217330585; 56672943900","Community-based cover song detection","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079218664&partnerID=40&md5=4720e9c6188b6ccb5011257eafff5a85","Spotify, Sweden","Sarfati M., Spotify, Sweden; Hu A., Spotify, Sweden; Donier J., Spotify, Sweden","Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective. Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio signal; Audio-based; Binary decision; Community-based; Cover songs; Ensemble-based method; Industrial scale; Many to many; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Korzeniowski F.; Widmer G.","Korzeniowski, Filip (56328099700); Widmer, Gerhard (7004342843)","56328099700; 7004342843","Genre-agnostic key classification with convolutional neural networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842568&partnerID=40&md5=df04e8bb8bb4df69d2cf0bb23744b342","Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","We propose modifications to the model structure and training procedure to a recently introduced Convolutional Neural Network for musical key classification. These modifications enable the network to learn a genre-independent model that performs better than models trained for specific music styles, which has not been the case in existing work. We analyse this generalisation capability on three datasets comprising distinct genres. We then evaluate the model on a number of unseen data sets, and show its superior performance compared to the state of the art. Finally, we investigate the model’s performance on short excerpts of audio. From these experiments, we conclude that models need to consider the harmonic coherence of the whole piece when classifying the local key of short segments of audio. © Filip Korzeniowski and Gerhard Widmer.","","Information retrieval; Neural networks; Convolutional neural network; Generalisation; Independent model; Short segments; State of the art; Training procedures; Convolution","F. Korzeniowski; Institute of Computational Perception, Johannes Kepler University, Linz, Austria; email: filip.korzeniowski@jku.at","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Wu C.-W.; Lerch A.","Wu, Chih-Wei (57188865070); Lerch, Alexander (22034963000)","57188865070; 22034963000","From labeled to unlabeled data – On the data challenge in automatic drum transcription","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842320&partnerID=40&md5=7cb64d5dc368ef8901a69bc40a6bb995","Georgia Institute of Technology, Center for Music Technology, United States","Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","Automatic Drum Transcription (ADT), like many other music information retrieval tasks, has made progress in the past years through the integration of machine learning and audio signal processing techniques. However, with the increasing popularity of data-hungry approaches such as deep learning, the insufficient amount of data becomes more and more a challenge that concerns the generality of the resulting models and the validity of the evaluation. To address this challenge in ADT, this paper first examines the existing labeled datasets and how representative they are of the research problem. Next, possibilities of using unlabeled data to improve general ADT systems are explored. Specifically, two paradigms that harness information from unlabeled data, namely feature learning and student-teacher learning, are applied to two major types of ADT systems. All systems are evaluated on four different drum datasets. The results highlight the necessity of more and larger annotated datasets and indicate the feasibility of exploiting unlabeled data for improving ADT systems. © Chih-Wei Wu, Alexander Lerch.","","Audio acoustics; Audio signal processing; Information retrieval; Machine learning; Transcription; Annotated datasets; Data challenges; Feature learning; Labeled datasets; Music information retrieval; Research problems; Student teachers; Unlabeled data; Deep learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Kinnaird K.M.","Kinnaird, Katherine M. (57205699026)","57205699026","Aligned sub-hierarchies: A structure-based approach to the cover song task","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856585&partnerID=40&md5=52423734ab002b56fb4c2d055476da17","Data Sciences Initiative, Division of Applied Mathematics, Brown University, United States","Kinnaird K.M., Data Sciences Initiative, Division of Applied Mathematics, Brown University, United States","Extending previous structure-based approaches to the song comparison tasks such as the fingerprint and cover song tasks, this paper introduces the aligned sub-hierarchies (AsH) representation. Built by applying a post-processing technique to the aligned hierarchies of a song, the AsH representation is the set of unique aligned hierarchies for repeats (called AHR) encoded in the original aligned hierarchies of the whole song. Effectively each AHR within AsH is a section of the aligned hierarchies for the original song. Like aligned hierarchies, the AsH representation can be embedded into a classification space with a natural metric that makes inter-song comparisons based on sections of the songs. Experiments addressing a version of the cover song task on score-based data using AsH as the basis of inter-song comparison demonstrate potential of AsH-based approaches for MIR tasks. © Katherine M. Kinnaird.","","Cover songs; Original songs; Post-processing techniques; Structure-based; Information retrieval","K.M. Kinnaird; Data Sciences Initiative, Division of Applied Mathematics, Brown University, United States; email: katherine_kinnaird@brown.edu","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Parada-Cabaleiro E.; Schmitt M.; Batliner A.; Schuller B.W.","Parada-Cabaleiro, Emilia (57195938576); Schmitt, Maximilian (57191862153); Batliner, Anton (6602152015); Schuller, Björn W. (6603767415)","57195938576; 57191862153; 6602152015; 6603767415","Musical-linguistic annotations of Il Lauro Secco","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069823889&partnerID=40&md5=b2b30dd77eb77a393dcf0110301c8cb2","Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","Parada-Cabaleiro E., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schmitt M., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Batliner A., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schuller B.W., Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","The Italian madrigal, a polyphonic secular a cappella composition of the 16th century, is characterised by a strong musical-linguistic relationship, which has made it an icon of the ‘Renaissance humanism’. In madrigals, lyrical meaning is mimicked by the music, through the utilisation of a composition technique known as madrigalism. The synergy between Renaissance music and poetry makes madrigals of great value to musicologists, linguists, and historians—thus, it is a promising repertoire for computational musicology. However, the application of computational techniques for automatic detection of madrigalisms within scores of such repertoire is limited by the lack of annotations to refer to. In this regard, we present 30 madrigals of the anthology Il Lauro Secco encoded in two symbolic formats, MEI and **kern, with hand-encoded annotations of madrigalisms. This work aims to encourage the development of algorithms for madrigalism detection, a composition procedure typical of early music, but still underrepresented in music information retrieval research. © Emilia Parada-Cabaleiro, Maximilian Schmitt, Anton Batliner, Björn W. Schuller.","","Information retrieval; Automatic Detection; Composition technique; Computational technique; Early musics; Linguistic annotations; Music information retrieval; Linguistics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Manilow E.; Seetharaman P.; Pardo B.","Manilow, Ethan (57200754341); Seetharaman, Prem (55391829300); Pardo, Bryan (10242155400)","57200754341; 55391829300; 10242155400","The northwestern university source separation library","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069852021&partnerID=40&md5=9debebfb15d7805595437852775ee34b","Northwestern University, United States","Manilow E., Northwestern University, United States; Seetharaman P., Northwestern University, United States; Pardo B., Northwestern University, United States","Audio source separation is the process of isolating individual sonic elements from a mixture or auditory scene. We present the Northwestern University Source Separation Library, or nussl for short. nussl (pronounced ‘nuzzle’) is an open-source, object-oriented audio source separation library implemented in Python. nussl provides implementations for many existing source separation algorithms and a platform for creating the next generation of source separation algorithms. By nature of its design, nussl easily allows new algorithms to be benchmarked against existing algorithms on established data sets and facilitates development of new variations on algorithms. Here, we present the design methodologies in nussl, two experiments using it, and use nussl to showcase benchmarks for some algorithms contained within. © Ethan Manilow, Prem Seetharaman, Bryan Pardo.","","Information retrieval; Separation; Audio source separation; Design Methodology; Northwestern University; Object oriented; Open sources; Separation algorithms; Source separation","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Korzeniowski F.; Widmer G.","Korzeniowski, Filip (56328099700); Widmer, Gerhard (7004342843)","56328099700; 7004342843","Improved chord recognition by combining duration and harmonic language models","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069841915&partnerID=40&md5=c892295e22bc4e16f6d49675ba67dc22","Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Chord recognition systems typically comprise an acoustic model that predicts chords for each audio frame, and a temporal model that casts these predictions into labelled chord segments. However, temporal models have been shown to only smooth predictions, without being able to incorporate musical information about chord progressions. Recent research discovered that it might be the low hierarchical level such models have been applied to (directly on audio frames) which prevents learning musical relationships, even for expressive models such as recurrent neural networks (RNNs). However, if applied on the level of chord sequences, RNNs indeed can become powerful chord predictors. In this paper, we disentangle temporal models into a harmonic language model—to be applied on chord sequences—and a chord duration model that connects the chord-level predictions of the language model to the frame-level predictions of the acoustic model. In our experiments, we explore the impact of each model on the chord recognition score, and show that using harmonic language and duration models improves the results. © Filip Korzeniowski and Gerhard Widmer.","","Audio systems; Computational linguistics; Forecasting; Harmonic analysis; Information retrieval; Recurrent neural networks; Chord recognition; Duration modeling; Hierarchical level; Improved chords; Musical information; Recent researches; Recurrent neural network (RNNs); Temporal modeling; Audio acoustics","F. Korzeniowski; Institute of Computational Perception, Johannes Kepler University, Linz, Austria; email: filip.korzeniowski@jku.at","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Luo Y.-J.; Su L.","Luo, Yin-Jyun (57226047667); Su, Li (55966919100)","57226047667; 55966919100","Learning domain-adaptive latent representations of music signals using variational autoencoders","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069883438&partnerID=40&md5=d3f2b1e8c1f67ee7fa2a9d6ff7b09821","Institute of Information Science, Academia Sinica, Taiwan","Luo Y.-J., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","In this paper, we tackle the problem of domain-adaptive representation learning for music processing. Domain adaptation is an approach aiming to eliminate the distributional discrepancy of the modeling data, so as to transfer learnable knowledge from one domain to another. With its great success in the fields of computer vision and natural language processing, domain adaptation also shows great potential in music processing, for music is essentially a highly-structured semantic system having domain-dependent information. Our proposed model contains a Variational Autoencoder (VAE) that encodes the training data into a latent space, and the resulting latent representations along with its model parameters are then reused to regularize the representation learning of the downstream task where the data are in the other domain. The experiments on cross-domain music alignment, namely an audio-to-MIDI alignment, and a monophonic-to-polyphonic music alignment of singing voice show that the learned representations lead to better higher alignment accuracy than that using conventional features. Furthermore, a preliminary experiment on singing voice source separation, by regarding the mixture and the voice as two distinct domains, also demonstrates the capability to solve music processing problems from the perspective of domain-adaptive representation learning. © Yin-Jyun Luo and Li Su.","","Alignment; Audio acoustics; Information retrieval; Natural language processing systems; Semantics; Source separation; Adaptive representations; Alignment accuracy; Domain adaptation; Model parameters; NAtural language processing; Polyphonic music; Processing problems; Semantic systems; Learning systems","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Crawford T.; Badkobeh G.; Lewis D.","Crawford, Tim (15054056900); Badkobeh, Golnaz (44260904300); Lewis, David (55742498400)","15054056900; 44260904300; 55742498400","Searching page-images of early music scanned with OMR: A scalable solution using minimal absent words","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","11","10.1002/acp.1463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067862161&doi=10.1002%2facp.1463&partnerID=40&md5=36a6560b3365673a6f01b4bb7ab35649","Goldsmiths, University of London, United Kingdom; Oxford eResearch Centre, United Kingdom","Crawford T., Goldsmiths, University of London, United Kingdom; Badkobeh G., Goldsmiths, University of London, United Kingdom; Lewis D., Oxford eResearch Centre, United Kingdom","We define three retrieval tasks requiring efficient search of the musical content of a collection of ~32k page-images of 16th-century music to find: duplicates; pages with the same musical content; pages of related music. The images are subjected to Optical Music Recognition (OMR), introducing inevitable errors. We encode pages as strings of diatonic pitch intervals, ignoring rests, to reduce the effect of such errors. We extract indices comprising lists of two kinds of ‘word’. Approximate matching is done by counting the number of common words between a query page and those in the collection. The two word-types are (a) normal ngrams and (b) minimal absent words (MAWs). The latter have three important properties for our purpose: they can be built and searched in linear time, the number of MAWs generated tends to be smaller, and they preserve the structure and order of the text, obviating the need for expensive sorting operations. We show that retrieval performance of MAWs is comparable with ngrams, but with a marked speed improvement. We also show the effect of word length on retrieval. Our results suggest that an index of MAWs of mixed length provides a good method for these tasks which is scalable to larger collections. © Tim Crawford, Golnaz Badkobeh, David Lewis.","","Approximate matching; Early musics; Good methods; Linear time; Optical music recognition; Retrieval performance; Scalable solution; Speed improvement; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Condit-Schultz N.; Ju Y.; Fujinaga I.","Condit-Schultz, Nathaniel (56681201600); Ju, Yaolong (57200500552); Fujinaga, Ichiro (9038140900)","56681201600; 57200500552; 9038140900","A flexible approach to automated harmonic analysis: Multiple annotations of chorales by Bach and Prætorius","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866814&partnerID=40&md5=2c06de5c771c572b7839c04d699badea","McGill University, Canada","Condit-Schultz N., McGill University, Canada; Ju Y., McGill University, Canada; Fujinaga I., McGill University, Canada","Despite being a core component of Western music theory, harmonic analysis remains a subjective endeavor, resistant automation. This subjectivity arises from disagreements regarding, among other things, the interpretation of contrapuntal figures, the set of “legal” harmonies, and how harmony relates to more abstract features like tonal function. In this paper, we provide a formal specification of harmonic analysis. We then present a novel approach to computational harmonic analysis: rather than computing harmonic analyses based on one specific set of rules, we compute all possible analyses which satisfy only basic, uncontroversial constraints. These myriad interpretations can later be filtered to extract preferred analyses; for instance, to forbid 7th chords or to prefer analyses with fewer non-chord tones. We apply this approach to two concrete musical datasets: existing encodings of 371 chorales by J.S. Bach and new encodings of 200 chorales by M. Prætorius. Through an online API users can filter and download numerous harmonic interpretations of these 571 chorales. This dataset will serve as a useful resource in the study of harmonic/functional progression, voice-leading, and the relationship between melody and harmony, and as a stepping stone towards automated harmonic analysis of more complex music. © Nathaniel Condit-Schultz, Yaolong Ju, Ichiro Fujinaga.","","Automation; Computation theory; Encoding (symbols); Information retrieval; Core components; Encodings; Music theory; Set of rules; Stepping stone; Harmonic analysis","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lattner S.; Grachten M.; Widmer G.","Lattner, Stefan (56989751900); Grachten, Maarten (8974600000); Widmer, Gerhard (7004342843)","56989751900; 8974600000; 7004342843","Learning transposition-invariant interval features from symbolic music and audio","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069853194&partnerID=40&md5=389e2eac20fa4864b77755ba551323aa","Institute of Computational Perception, JKU Linz, Austria; Sony Computer Science Laboratories (CSL), Paris, France","Lattner S., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Grachten M., Institute of Computational Perception, JKU Linz, Austria, Sony Computer Science Laboratories (CSL), Paris, France; Widmer G., Institute of Computational Perception, JKU Linz, Austria","Many music theoretical constructs (such as scale types, modes, cadences, and chord types) are defined in terms of pitch intervals—relative distances between pitches. Therefore, when computer models are employed in music tasks, it can be useful to operate on interval representations rather than on the raw musical surface. Moreover, interval representations are transposition-invariant, valuable for tasks like audio alignment, cover song detection and music structure analysis. We employ a gated autoencoder to learn fixed-length, invertible and transposition-invariant interval representations from polyphonic music in the symbolic domain and in audio. An unsupervised training method is proposed yielding an organization of intervals in the representation space which is musically plausible. Based on the representations, a transposition-invariant self-similarity matrix is constructed and used to determine repeated sections in symbolic music and in audio, yielding competitive results in the MIREX task”Discovery of Repeated Themes and Sections”. © Stefan Lattner, Maarten Grachten, Gerhard Widmer.","","Information retrieval; Audio alignments; Invariant intervals; Music structure analysis; Polyphonic music; Relative distances; Representation space; Self-similarity matrix; Unsupervised training; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gotham M.","Gotham, Mark (55987597200)","55987597200","Taking form: A representation standard, conversion code, and example corpus for recording, visualizing, and studying analyses of musical form","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076737785&partnerID=40&md5=0e23e8738e586d910e3438460684460b","Cornell University, Department of Music, United States; University of Cambridge, Sidney Sussex College, United Kingdom","Gotham M., Cornell University, Department of Music, United States, University of Cambridge, Sidney Sussex College, United Kingdom","We report on new specification standards for representing human analyses of musical form which enable musicians to represent their analytical view of a piece either on the score (where an encoded version is available) or on a spreadsheet. Both of these representations are simple, intuitive, and highly human-readable. Further, we provide code for converting between these formats, as well as a nested bracket representation adopted from computational linguistics which, in turn, can be visualised in familiar tree diagrams to provide 'at a glance' introductions to works. Finally, we provide an initial corpus of analyses/ annotations in these formats, report on the practicalities of amassing them, and offer tools for automatic comparison of the works in the corpus based on the content and structure of the annotations. We intend for this resource to be useful to computational musicologists, enabling study of form at scale, and also useful pedagogically to all teachers, students, and appreciators of music from whom projects of this kind can be rather disconnected. The code and corpus can be found at https://github.com/MarkGotham/Taking-Form. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Content and structure; Corpus-based; Human analysis; Human-readable; Tree diagram; Codes (symbols)","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Abeßer J.; Balke S.; Müller M.","Abeßer, Jakob (36607532300); Balke, Stefan (56940401000); Müller, Meinard (7404689873)","36607532300; 56940401000; 7404689873","Improving bass saliency estimation using label propagation and transfer learning","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850763&partnerID=40&md5=e7fd0c18aa858a49a626f7986de3a9d1","Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; International Audio Laboratories, Erlangen, Germany","Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","In this paper, we consider two methods to improve an algorithm for bass saliency estimation in jazz ensemble recordings which are based on deep neural networks. First, we apply label propagation to increase the amount of training data by transferring pitch labels from our labeled dataset to unlabeled audio recordings using a spectral similarity measure. Second, we study in several transfer learning experiments, whether isolated note recordings can be beneficial for pre-training a model which is later fine-tuned on ensemble recordings. Our results indicate that both strategies can improve the performance on bass saliency estimation by up to five percent in accuracy. © Jakob Abeßer, Stefan Balke, Meinard Müller.","","Backpropagation; Deep neural networks; Information retrieval; Label propagation; Labeled dataset; Pre-training; Spectral similarity; Training data; Transfer learning; Audio recordings","J. Abeßer; Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; email: jakob.abesser@idmt.fraunhofer.de","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Delbouys R.; Hennequin R.; Piccoli F.; Royo-Letelier J.; Moussallam M.","Delbouys, Rémi (57210189121); Hennequin, Romain (36504173400); Piccoli, Francesco (57521134800); Royo-Letelier, Jimena (36915600300); Moussallam, Manuel (36608879000)","57210189121; 36504173400; 57521134800; 36915600300; 36608879000","Music mood detection based on audio and lyrics with deep neural net","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069855379&partnerID=40&md5=b4436be475571caff4965ae8959a6c80","Deezer, 12 rue d’Athènes, Paris, 75009, France","Delbouys R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Hennequin R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Piccoli F., Deezer, 12 rue d’Athènes, Paris, 75009, France; Royo-Letelier J., Deezer, 12 rue d’Athènes, Paris, 75009, France; Moussallam M., Deezer, 12 rue d’Athènes, Paris, 75009, France","We consider the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. We reproduce the implementation of traditional feature engineering based approaches and propose a new model based on deep learning. We compare the performance of both approaches on a database containing 18,000 tracks with associated valence and arousal values and show that our approach outperforms classical models on the arousal detection task, and that both approaches perform equally on the valence prediction task. We also compare the a poste-riori fusion with fusion of modalities optimized simultaneously with each unimodal model, and observe a significant improvement of valence prediction. We release part of our database for comparison purposes. © Rémi Delbouys, Romain Hennequin, Francesco Piccoli, Jimena Royo-Letelier, Manuel Moussallam.","","Audio acoustics; Forecasting; Information retrieval; Audio signal; Classical model; Detection tasks; Feature engineerings; Model-based OPC; Mood detection; Prediction tasks; Prediction-based; Deep neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Frieler K.; Höger F.; Pfleiderer M.; Dixon S.","Frieler, Klaus (32667638400); Höger, Frank (57210183609); Pfleiderer, Martin (57094855700); Dixon, Simon (7201479437)","32667638400; 57210183609; 57094855700; 7201479437","Two web applications for exploring melodic patterns in jazz solos","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850925&partnerID=40&md5=7b9b24c202f6783dd250d62a0bf6575a","Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Center for Digital Music, Queen Mary University of London, United Kingdom","Frieler K., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Höger F., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Pfleiderer M., Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; Dixon S., Center for Digital Music, Queen Mary University of London, United Kingdom","This paper presents two novel user interfaces for investigating the pattern content in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation. In jazz improvisation, patterns are of particular interest for the analysis of improvisation styles, the oral transmission of musical language, the practice of improvisation, and the psychology of creative processes. The ongoing project “Dig That Lick” is devoted to addressing these questions with the help of a large database of jazz solo transcriptions generated by automated melody extraction algorithms. To expose these transcriptions to jazz researchers, two prototypes of user interfaces were designed that work currently with the 456 manually transcribed jazz solos of the Weimar Jazz Database. The first one is a Shiny application that allows exploring a set of 653 of the most common patterns by eminent players. The second one is a web interface for a general two-staged pattern search in the Weimar Jazz Database featuring regular expressions. These applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans. © Klaus Frieler, Frank Höger, Martin Pfleiderer, Simon Dixon.","","Database systems; Information retrieval; Transcription; Creative process; Large database; Melody extractions; Pattern search; Regular expressions; Testing hypothesis; WEB application; Web interface; User interfaces","K. Frieler; Institute for Musicology, University of Music “Franz Liszt”, Weimar, Germany; email: klaus.frieler@hfm-weimar.de","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Mishra S.; Sturm B.L.; Dixon S.","Mishra, Saumitra (57195518532); Sturm, Bob L. (14014190500); Dixon, Simon (7201479437)","57195518532; 14014190500; 7201479437","Understanding a deep machine listening model through feature inversion","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068994776&partnerID=40&md5=c0824b9329b09073d5f844a05666582d","Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom","Mishra S., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom","Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input. © Saumitra Mishra, Bob L. Sturm, Simon Dixon.","","Convolution; Information retrieval; Convolutional neural network; Decision functions; Harmonic structures; Machine learning models; Machine listening; Qualitative analysis; Singing voice detection; State of the art; Multilayer neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"McKay C.; Cumming J.E.; Fujinaga I.","McKay, Cory (14033215600); Cumming, Julie E. (53463213000); Fujinaga, Ichiro (9038140900)","14033215600; 53463213000; 9038140900","jSymbolic 2.2: Extracting features from symbolic music for use in musicological and MIR research","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069809082&partnerID=40&md5=c34da26245ae7902215179cb5550e00d","Marianopolis College, Canada; McGill University, Canada","McKay C., Marianopolis College, Canada; Cumming J.E., McGill University, Canada; Fujinaga I., McGill University, Canada","jSymbolic is an open-source platform for extracting features from symbolic music. These features can serve as inputs to machine learning algorithms, or they can be analyzed statistically to derive musicological insights. jSymbolic implements 246 unique features, comprising 1497 different values, making it by far the most extensive symbolic feature extractor to date. These features are designed to be applicable to a diverse range of musics, and may be extracted from both symbolic music files as a whole and from windowed subsets of them. Researchers can also use jSymbolic as a platform for developing and distributing their own bespoke features, as it has an easily extensible plug-in architecture. In addition to implementing 135 new unique features, version 2.2 of jSymbolic places a special focus on functionality for avoiding biases associated with how symbolic music is encoded. In addition, new interface elements and documentation improve convenience, ease-of-use and accessibility to researchers with diverse ranges of technical expertise. jSymbolic now includes a GUI, command-line interface, API, flexible configuration file format, extensive manual and detailed tutorial. The enhanced effectiveness of jSymbolic 2.2’s features is demonstrated in two sets of experiments: 1) genre classification and 2) Renaissance composer attribution. © Cory McKay, Julie E. Cumming, Ichiro Fujinaga.","","Application programming interfaces (API); Information retrieval; Learning algorithms; Command line interface; Configuration files; Extracting features; Genre classification; Interface elements; Open source platforms; Plug-in architectures; Technical expertise; Machine learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Sears D.R.W.; Korzeniowski F.; Widmer G.","Sears, David R.W. (55874470000); Korzeniowski, Filip (56328099700); Widmer, Gerhard (7004342843)","55874470000; 56328099700; 7004342843","Evaluating language models of tonal harmony","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862591&partnerID=40&md5=91a52b68f33ebb04f97520d35d9e0d12","College of Visual and Performing Arts, Texas Tech University, Lubbock, United States; Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Sears D.R.W., College of Visual and Performing Arts, Texas Tech University, Lubbock, United States; Korzeniowski F., Institute of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","This study borrows and extends probabilistic language models from natural language processing to discover the syntactic properties of tonal harmony. Language models come in many shapes and sizes, but their central purpose is always the same: to predict the next event in a sequence of letters, words, notes, or chords. However, few studies employing such models have evaluated the most state-of-the-art architectures using a large-scale corpus of Western tonal music, instead preferring to use relatively small datasets containing chord annotations from contemporary genres like jazz, pop, and rock. Using symbolic representations of prominent instrumental genres from the common-practice period, this study applies a flexible, data-driven encoding scheme to (1) evaluate Finite Context (or n-gram) models and Recurrent Neural Networks (RNNs) in a chord prediction task; (2) compare predictive accuracy from the best-performing models for chord onsets from each of the selected datasets; and (3) explain differences between the two model architectures in a regression analysis. We find that Finite Context models using the Prediction by Partial Match (PPM) algorithm outperform RNNs, particularly for the piano datasets, with the regression model suggesting that RNNs struggle with particularly rare chord types. © Sears, Korzeniowski, Widmer.","","Forecasting; Information retrieval; Large dataset; Natural language processing systems; Network architecture; Recurrent neural networks; Regression analysis; Syntactics; Model architecture; NAtural language processing; Prediction by partial matches; Predictive accuracy; Probabilistic language; Recurrent neural network (RNNs); Symbolic representation; Syntactic properties; Computational linguistics","D.R.W. Sears; College of Visual and Performing Arts, Texas Tech University, Lubbock, United States; email: david.sears@ttu.edu","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Castellanos F.J.; Calvo-Zaragoza J.; Vigliensoni G.; Fujinaga I.","Castellanos, Francisco J. (57200208997); Calvo-Zaragoza, Jorge (55847598300); Vigliensoni, Gabriel (55217696900); Fujinaga, Ichiro (9038140900)","57200208997; 55847598300; 55217696900; 9038140900","Document analysis of music score images with selectional auto-encoders","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069842280&partnerID=40&md5=848e94ed88f658846c4df86895025540","Software and Computing Systems, University of Alicante, Spain; PRHLT Research Center, Universitat Politècnica de València, Spain; Schulich School of Music, McGill University, Canada","Castellanos F.J., Software and Computing Systems, University of Alicante, Spain; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain; Vigliensoni G., Schulich School of Music, McGill University, Canada; Fujinaga I., Schulich School of Music, McGill University, Canada","The document analysis of music score images is a key step in the development of successful Optical Music Recognition systems. The current state of the art considers the use of deep neural networks trained to classify every pixel of the image according to the image layer it belongs to. This process, however, involves a high computational cost that prevents its use in interactive machine learning scenarios. In this paper, we propose the use of a set of deep selectional auto-encoders, implemented as fully-convolutional networks, to perform image-to-image categorizations. This strategy retains the advantages of using deep neural networks, which have demonstrated their ability to perform this task, while dramatically increasing the efficiency by processing a large number of pixels in a single step. The results of an experiment performed with a set of high-resolution images taken from Medieval manuscripts successfully validate this approach, with a similar accuracy to that of the state of the art but with a computational time orders of magnitude smaller, making this approach appropriate for being used in interactive applications. © Francisco J. Castellanos, Jorge Calvo-Zaragoza, Gabriel Vigliensoni, Ichiro Fujinaga.","","Deep neural networks; Information retrieval; Multilayer neural networks; Pixels; Signal encoding; Convolutional networks; High resolution image; Image Categorization; Interactive applications; Interactive machine learning; Medieval manuscript; Optical music recognition; Orders of magnitude; Image analysis","F.J. Castellanos; Software and Computing Systems, University of Alicante, Spain; email: fcastellanos@dlsi.ua.es","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gururani S.; Sharma M.; Lerch A.","Gururani, Siddharth (57195432595); Sharma, Mohit (57217331377); Lerch, Alexander (22034963000)","57195432595; 57217331377; 22034963000","An attention mechanism for musical instrument recognition","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086245656&partnerID=40&md5=b346594bfe647bc8b3e1c3d148098fd8","Center for Music Technology, Georgia Institute of Technology, United States; School of Interactive Computing, Georgia Institute of Technology, United States","Gururani S., Center for Music Technology, Georgia Institute of Technology, United States; Sharma M., School of Interactive Computing, Georgia Institute of Technology, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or 'attend to') specific time segments in the audio relevant to each instrument label leading to interpretable results. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Data handling; Decision trees; Information retrieval; Labeled data; Musical instruments; Attention mechanisms; Automatic recognition; Binary relevances; Classification accuracy; Fully connected neural network; Instrument recognition; Multiple instruments; Musical instrument recognition; Recurrent neural networks","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Parada-Cabaleiro E.; Schmitt M.; Batliner A.; Hantke S.; Costantini G.; Scherer K.; Schuller B.W.","Parada-Cabaleiro, Emilia (57195938576); Schmitt, Maximilian (57191862153); Batliner, Anton (6602152015); Hantke, Simone (55413414500); Costantini, Giovanni (35309726600); Scherer, Klaus (57202413462); Schuller, Björn W. (6603767415)","57195938576; 57191862153; 6602152015; 55413414500; 35309726600; 57202413462; 6603767415","Identifying emotions in opera singing: Implications of adverse acoustic conditions","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069882450&partnerID=40&md5=fb4e6b031e10f8b2432cf9b63f125c68","ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Machine Intelligence and Signal Processing Group, Technische Universität München, Germany; Department of Electronic Engineering, University of Rome Tor Vergata, Italy; Department of Psychology, University of Geneva, Switzerland; GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","Parada-Cabaleiro E., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Schmitt M., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Batliner A., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; Hantke S., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, Machine Intelligence and Signal Processing Group, Technische Universität München, Germany; Costantini G., Department of Electronic Engineering, University of Rome Tor Vergata, Italy; Scherer K., Department of Psychology, University of Geneva, Switzerland; Schuller B.W., ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany, GLAM – Group on Language, Audio and Music, Imperial College London, United Kingdom","The expression of emotion is an inherent aspect in singing, especially in operatic voice. Yet, adverse acoustic conditions, as, e. g., a performance in open-air, or a noisy analog recording, may affect its perception. State-of-the art methods for emotional speech evaluation have been applied to operatic voice, such as perception experiments, acoustic analyses, and machine learning techniques. Still, the extent to which adverse acoustic conditions may impair listeners’ and machines’ identification of emotion in vocal cues has only been investigated in the realm of speech. For our study, 132 listeners evaluated 390 nonsense operatic sung instances of five basic emotions, affected by three noises (brown, pink, and white), each at four Signal-to-Noise Ratios (-1 dB, -0.5 dB, +1 dB, and +3 dB); the performance of state-of-the-art automatic recognition methods was evaluated as well. Our findings show that the three noises affect similarly female and male singers and that listeners’ gender did not play a role. Human perception and automatic classification display similar confusion and recognition patterns: sadness is identified best, fear worst; low aroused emotions display higher confusion. © Emilia Parada-Cabaleiro, Maximilian Schmitt, Anton Batliner, Simone Hantke, Giovanni Costantini, Klaus Scherer, Björn W. Schuller.","","Acoustic noise; Information retrieval; Learning systems; Pattern recognition; Signal to noise ratio; Acoustic analysis; Acoustic conditions; Automatic classification; Automatic recognition method; Machine learning techniques; Perception experiment; Recognition patterns; State-of-the-art methods; Behavioral research","E. Parada-Cabaleiro; ZD.B Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany; email: emilia.parada-cabaleiro@informatik.uni-augsburg.de","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Harasim D.; Rohrmeier M.; O’Donnell T.J.","Harasim, Daniel (57192074560); Rohrmeier, Martin (6507901506); O’Donnell, Timothy J. (57205739686)","57192074560; 6507901506; 57205739686","A generalized parsing framework for generative models of harmonic syntax","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837251&partnerID=40&md5=a1e79474513d6701d253f80a7c9847d4","Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland; Institut für Kunst- und Musikwissenschaft, TU Dresden, Germany; Department of Linguistics, McGill University, Canada","Harasim D., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland, Institut für Kunst- und Musikwissenschaft, TU Dresden, Germany; Rohrmeier M., Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland, Institut für Kunst- und Musikwissenschaft, TU Dresden, Germany; O’Donnell T.J., Department of Linguistics, McGill University, Canada","Modeling the structure of musical pieces constitutes a central research problem for music information retrieval, music generation, and musicology. At the present, models of harmonic syntax face challenges on the tasks of detecting local and higher-level modulations (most previous models assume a priori knowledge of key), computing connected parse trees for long sequences, and parsing sequences that do not end with tonic chords, but in turnarounds. This paper addresses those problems by proposing a new generative formalism Probabilistic Abstract Context-Free Grammars (PACFGs) to address these issues, and presents variants of standard parsing algorithms that efficiently enumerate all possible parses of long chord sequences and to estimate their probabilities. PACFGs specifically allow for structured non-terminal symbols in rich and highly flexible feature spaces. The inference procedure moreover takes advantage of these abstractions by sharing probability mass between grammar rules over joint features. The paper presents a model of the harmonic syntax of Jazz using this formalism together with stochastic variational inference to learn the probabilistic parameters of a grammar from a corpus of Jazz-standards. The PACFG model outperforms the standard context-free approach while reducing the number of free parameters and performing key finding on the fly. © Daniel Harasim, Martin Rohrmeier, Timothy J. O’Donnell.","","Computer music; Context free grammars; Harmonic analysis; Information retrieval; Stochastic models; Stochastic systems; Free parameters; Generative model; Music information retrieval; Non-terminal symbols; Parsing algorithm; Priori knowledge; Research problems; Variational inference; Syntactics","D. Harasim; Digital and Cognitive Musicology Lab, École Polytechnique Fédérale de Lausanne, Switzerland; email: daniel.harasim@epfl.ch","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Li B.; Maezawa A.; Duan Z.","Li, Bochen (57191924185); Maezawa, Akira (35753591100); Duan, Zhiyao (24450312900)","57191924185; 35753591100; 24450312900","Skeleton plays piano: Online generation of pianist body movements from MIDI performance","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805601&partnerID=40&md5=1ca341be9eabece48f888d7b71990ca0","University of Rochester, United States; Yamaha Corporation, Japan","Li B., University of Rochester, United States; Maezawa A., Yamaha Corporation, Japan; Duan Z., University of Rochester, United States","Generating expressive body movements of a pianist for a given symbolic sequence of key depressions is important for music interaction, but most existing methods cannot incorporate musical context information and generate movements of body joints that are further away from the fingers such as head and shoulders. This paper addresses such limitations by directly training a deep neural network system to map a MIDI note stream and additional metric structures to a skeleton sequence of a pianist playing a keyboard instrument in an online fashion. Experiments show that (a) incorporation of metric information yields in 4% smaller error, (b) the model is capable of learning the motion behavior of a specific player, and (c) no significant difference between the generated and real human movements is observed by human subjects in 75% of the pieces. © Bochen Li, Akira Maezawa, Zhiyao Duan.","","Information retrieval; Musculoskeletal system; Musical instruments; Context information; Metric information; Motion behavior; Music interaction; Neural network systems; On-line fashion; On-line generation; Symbolic sequence; Deep neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Donahue C.; Mao H.H.; McAuley J.","Donahue, Chris (35213199400); Mao, Huanru Henry (57214218550); McAuley, Julian (14822353500)","35213199400; 57214218550; 14822353500","The NES music database: A multi-instrumental dataset with expressive performance attributes","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069846035&partnerID=40&md5=eb0cc71e1b8ae6d19698b133ad6a7d03","UC San Diego, United States","Donahue C., UC San Diego, United States; Mao H.H., UC San Diego, United States; McAuley J., UC San Diego, United States","Existing research on music generation focuses on composition, but often ignores the expressive performance characteristics required for plausible renditions of resultant pieces. In this paper, we introduce the Nintendo Entertainment System Music Database (NES-MDB), a large corpus allowing for separate examination of the tasks of composition and performance. NES-MDB contains thousands of multi-instrumental songs composed for playback by the compositionally-constrained NES audio synthesizer. For each song, the dataset contains a musical score for four instrument voices as well as expressive attributes for the dynamics and timbre of each voice. Unlike datasets comprised of General MIDI files, NES-MDB includes all of the information needed to render exact acoustic performances of the original compositions. Alongside the dataset, we provide a tool that renders generated compositions as NES-style audio by emulating the device’s audio processor. Additionally, we establish baselines for the tasks of composition, which consists of learning the semantics of composing for the NES synthesizer, and performance, which involves finding a mapping between a composition and realistic expressive attributes. © Chris Donahue, Huanru Henry Mao, Julian McAuley.","","Semantics; Acoustic performance; Audio processors; Expressive performance; Large corpora; MIDI files; Music database; Musical score; Nintendo entertainment systems; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Shih S.-Y.; Chi H.-Y.","Shih, Shun-Yao (57209327026); Chi, Heng-Yu (57210183575)","57209327026; 57210183575","Automatic, personalized, and flexible playlist generation using reinforcement learning","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069876448&partnerID=40&md5=b8331ad4d9685b715a131a79a4bec1a4","National Taiwan University, Taiwan; KKBOX Inc., Taipei, Taiwan","Shih S.-Y., National Taiwan University, Taiwan; Chi H.-Y., KKBOX Inc., Taipei, Taiwan","Songs can be well arranged by professional music curators to form a riveting playlist that creates engaging listening experiences. However, it is time-consuming for curators to timely rearrange these playlists for fitting trends in future. By exploiting the techniques of deep learning and reinforcement learning, in this paper, we consider music playlist generation as a language modeling problem and solve it by the proposed attention language model with policy gradient. We develop a systematic and interactive approach so that the resulting playlists can be tuned flexibly according to user preferences. Considering a playlist as a sequence of words, we first train our attention RNN language model on baseline recommended playlists. By optimizing suitable imposed reward functions, the model is thus refined for corresponding preferences. The experimental results demonstrate that our approach not only generates coherent playlists automatically but is also able to flexibly recommend personalized playlists for diversity, novelty and freshness. © Shun-Yao Shih, Heng-Yu Chi.","","Computational linguistics; Computer music; Deep learning; Information retrieval; Machine learning; Modeling languages; Sound recording; Interactive approach; Language model; Policy gradient; Reward function; Reinforcement learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bittner R.M.; Fuentes M.; Rubinstein D.; Jansson A.; Choi K.; Kell T.","Bittner, Rachel M. (55659619600); Fuentes, Magdalena (57190739095); Rubinstein, David (57217330203); Jansson, Andreas (57189200331); Choi, Keunwoo (57190944352); Kell, Thor (56407130600)","55659619600; 57190739095; 57217330203; 57189200331; 57190944352; 56407130600","Mirdata: Software for reproducible usage of datasets","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087093741&partnerID=40&md5=598e0700db937030d6dce36f39ccc3bb","Spotify, United States; L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, France","Bittner R.M., Spotify, United States; Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom Paris, Institut Polytechnique de Paris, France; Rubinstein D., Spotify, United States; Jansson A., Spotify, United States; Choi K., Spotify, United States; Kell T., Spotify, United States","There are a number of efforts in the MIR community towards increased reproducibility, such as creating more open datasets, publishing code, and the use of common software libraries, e.g. for evaluation. However, when it comes to datasets, there is usually little guarantee that researchers are using the exact same data in the same way, which among other issues, makes comparisons of different methods on the ""same"" datasets problematic. In this paper, we first show how (often unknown) differences in datasets can lead to significantly different experimental results. We propose a solution to these problems in the form of an open source library, mirdata, which handles datasets in their current distribution modes, but controls for possible variability. In particular, it contains tools which: (1) validate if the user's data (e.g. audio, annotations) is consistent with a canonical version of the dataset; (2) load annotations in a consistent manner; (3) download or give instructions for obtaining data; and (4) make it easy to perform track metadata-specific analysis. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Information retrieval; Common software; Current distribution; Open-source libraries; Reproducibilities; Open source software","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Lee J.; Kim S.; Lee K.","Lee, Juheon (57204050508); Kim, Seohyun (57217332361); Lee, Kyogu (8597995500)","57204050508; 57217332361; 8597995500","Automatic choreography generation with convolutional encoder-decoder network","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082393507&partnerID=40&md5=ee1dd1e93d25b9553e2516d19c19fd3e","Music & Audio Research Group, Seoul National University, South Korea","Lee J., Music & Audio Research Group, Seoul National University, South Korea; Kim S., Music & Audio Research Group, Seoul National University, South Korea; Lee K., Music & Audio Research Group, Seoul National University, South Korea","Automatic choreography generation is a challenging task because it often requires an understanding of two abstract concepts - music and dance - which are realized in the two different modalities, namely audio and video, respectively. In this paper, we propose a music-driven choreography generation system using an auto-regressive encoderdecoder network. To this end, we first collected a set of multimedia clips that include both music and corresponding dance motion. We then extract the joint coordinates of the dancer from video and the mel-spectrogram of music from audio and train our network using musicchoreography pairs as input. Finally, a novel dance motion is generated at the inference time when only music is given as an input. We performed a user study for a qualitative evaluation of the proposed method, and the results show that the proposed model is able to generate musically meaningful and natural dance movements given an unheard song. We also revealed through quantitative evaluation that the network has created a movement that correlates with the beat of music. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Information retrieval; Abstract concept; Auto-regressive; Convolutional encoders; Encoder-decoder; Generation systems; Joint coordinates; Qualitative evaluations; Quantitative evaluation; Network coding","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Southall C.; Stables R.; Hockman J.","Southall, Carl (57201882762); Stables, Ryan (55322057300); Hockman, Jason (36730968100)","57201882762; 55322057300; 36730968100","Player vs transcriber: A game approach to data manipulation for automatic drum transcription","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069851541&partnerID=40&md5=9e16d4a474c67a1a36c1f6dbf0178d17","DMT Lab, Birmingham City University, Birmingham, United Kingdom","Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","State-of-the-art automatic drum transcription (ADT) approaches utilise deep learning methods reliant on time-consuming manual annotations and require congruence between training and testing data. When these conditions are not held, they often fail to generalise. We propose a game approach to ADT, termed player vs transcriber (PvT), in which a player model aims to reduce transcription accuracy of a transcriber model by manipulating training data in two ways. First, existing data may be augmented, allowing the transcriber to be trained using recordings with modified timbres. Second, additional individual recordings from sample libraries are included to generate rare combinations. We present three versions of the PvT model: AugExist, which augments pre-existing recordings; AugAddExist, which adds additional samples of drum hits to the AugExist system; and Generate, which generates training examples exclusively from individual drum hits from sample libraries. The three versions are evaluated alongside a state-of-the-art deep learning ADT system using two evaluation strategies. The results demonstrate that including the player network improves the ADT performance and suggests that this is due to improved generalisability. The results also indicate that although the Generate model achieves relatively low results, it is a viable choice when annotations are not accessible. © Carl Southall, Ryan Stables and Jason Hockman.","","Audio recordings; Information retrieval; Libraries; Transcription; Additional samples; Data manipulations; Evaluation strategies; Learning methods; Manual annotation; Sample libraries; Training and testing; Training example; Deep learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"McLeod A.; Steedman M.","McLeod, Andrew (57114559100); Steedman, Mark (6602901918)","57114559100; 6602901918","Meter detection and alignment of MIDI performance","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069443617&partnerID=40&md5=dc635a34cf83a07dea536c11585ebf77","University of Edinburgh, United Kingdom","McLeod A., University of Edinburgh, United Kingdom; Steedman M., University of Edinburgh, United Kingdom","Metrical alignment is an integral part of any complete automatic music transcription (AMT) system. In this paper, we present an HMM for both detecting the metrical structure of given live performance MIDI data, and aligning that structure with the underlying notes. The model takes as input only a list of the notes present in a performance, and labels bars, beats, and sub beats in time. We also present an incremental algorithm which can perform inference on the model efficiently using a modified Viterbi search. We propose a new metric designed for the task, and using it, we show that our model achieves state-of-the-art performance on a corpus of metronomically aligned MIDI data, as well as a second corpus of live performance MIDI data. The code for the model described in this paper is available at https://www.github.com/apmcleod/met-align. © Andrew McLeod, Mark Steedman.","","Information retrieval; Viterbi algorithm; Automatic music transcription; Incremental algorithm; Integral part; State-of-the-art performance; Viterbi search; Inference engines","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Hajič J., Jr.; Dorfer M.; Widmer G.; Pecina P.","Hajič, Jan (57418161900); Dorfer, Matthias (55516844500); Widmer, Gerhard (7004342843); Pecina, Pavel (23393602100)","57418161900; 55516844500; 7004342843; 23393602100","Towards full-pipeline handwritten OMR with musical symbol detection by U-NETS","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069830366&partnerID=40&md5=4054c1072a8e469ef7e70acf260eb853","Institute of Formal and Applied Linguistics, Charles University, Czech Republic; Institute of Computational Perception, Johannes Kepler University, Austria","Hajič J., Jr., Institute of Formal and Applied Linguistics, Charles University, Czech Republic; Dorfer M., Institute of Computational Perception, Johannes Kepler University, Austria; Widmer G., Institute of Computational Perception, Johannes Kepler University, Austria; Pecina P., Institute of Formal and Applied Linguistics, Charles University, Czech Republic","Detecting music notation symbols is the most immediate unsolved subproblem in Optical Music Recognition for musical manuscripts. We show that a U-Net architecture for semantic segmentation combined with a trivial detector already establishes a high baseline for this task, and we propose tricks that further improve detection performance: training against convex hulls of symbol masks, and multichannel output models that enable feature sharing for semantically related symbols. The latter is helpful especially for clefs, which have severe impacts on the overall OMR result. We then integrate the networks into an OMR pipeline by applying a subsequent notation assembly stage, establishing a new baseline result for pitch inference in handwritten music at an f-score of 0.81. Given the automatically inferred pitches we run retrieval experiments on handwritten scores, providing first empirical evidence that utilizing the powerful image processing models brings content-based search in large musical manuscript archives within reach. © Jan Hajič jr., Matthias Dorfer, Gerhard Widmer, Pavel Pecina.","","Image processing; Information retrieval; Optical data processing; Semantics; Signal detection; Baseline results; Content-based search; Detection performance; Feature Sharing; Musical symbols; NET architecture; Optical music recognition; Semantic segmentation; Pipelines","J. Hajič; Institute of Formal and Applied Linguistics, Charles University, Czech Republic; email: hajicj@ufal.mff.cuni.cz","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Gupta C.; Tong R.; Li H.; Wang Y.","Gupta, Chitralekha (36023604600); Tong, Rong (55400871000); Li, Haizhou (8615868400); Wang, Ye (36103845200)","36023604600; 55400871000; 8615868400; 36103845200","Semi-supervised lyrics and solo-singing alignment","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068989719&partnerID=40&md5=53182bda445245c2aec1219126b3822a","NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; Electrical and Computer Engineering Dept., National University of Singapore, Singapore; Alibaba Inc., Singapore R and D Center, Singapore","Gupta C., NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore; Tong R., Alibaba Inc., Singapore R and D Center, Singapore; Li H., Electrical and Computer Engineering Dept., National University of Singapore, Singapore; Wang Y., NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, School of Computing, National University of Singapore, Singapore","We propose a semi-supervised algorithm to align lyrics to the corresponding singing vocals. The proposed method transcribes and aligns lyrics to solo-singing vocals using the imperfect transcripts from an automatic speech recognition (ASR) system and the published lyrics. The ASR provides time alignment between vocals and hypothesized lyrical content, while the non-aligned published lyrics correct the hypothesized lyrical content. The effectiveness of the proposed method is validated through three experiments. First, a human listening test shows that 73.32% of our automatically aligned sentence-level transcriptions are correct. Second, the automatically aligned sung segments are used for singing acoustic model adaptation, which reduces the word error rate (WER) of automatic transcription of sung lyrics from 72.08% to 37.15% in an open test. Third, another iteration of decoding and model adaptation increases the amount of reliably decoded segments from 44.40% to 91.96% and further reduces the WER to 36.32%. The proposed framework offers an automatic way to generate reliable alignments between lyrics and solo-singing. A large-scale solo-singing and lyrics aligned corpus can be derived with the proposed method, which will be beneficial for music and singing voice related research. © Chitralekha Gupta, Rong Tong, Haizhou, Ye Wang.","","Alignment; Information retrieval; Iterative decoding; Transcription; Acoustic model adaptation; Aligned sentences; Automatic speech recognition system; Automatic transcription; Listening tests; Model Adaptation; Semi-supervised algorithm; Word error rate; Speech recognition","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lagrange M.; Rossignol M.; Lafay G.","Lagrange, Mathieu (18042165200); Rossignol, Mathias (22942426200); Lafay, Grégoire (56974442600)","18042165200; 22942426200; 56974442600","Visualization of audio data using stacked graphs","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069858615&partnerID=40&md5=55fb0905a6433262515fe79cb69f33d3","LS2N, CNRS, École Centrale de Nantes, France","Lagrange M., LS2N, CNRS, École Centrale de Nantes, France; Rossignol M.; Lafay G., LS2N, CNRS, École Centrale de Nantes, France","In this paper, we study the benefit of considering stacked graphs to display audio data. Thanks to a careful use of layering of the spectral information, the resulting display is both concise and intuitive. Compared to the spectrogram display, it allows the reader to focus more on the temporal aspect of the time/frequency decomposition while keeping an abstract view of the spectral information. The use of such a display is validated using two perceptual experiments that demonstrate the potential of the approach. The first considers the proposed display to perform an identification task of the musical instrument and the second considers the proposed display to evaluate the technical level of a musical performer. Both experiments show the potential of the display and potential applications scenarios in musical training are discussed. © Mathieu Lagrange∗, Mathias Rossignol, Grégoire Lafay∗.","","Information retrieval; Audio data; Spectral information; Spectrogram display; Technical levels; Temporal aspects; Data visualization","M. Lagrange; LS2N, CNRS, École Centrale de Nantes, France; email: mathieu.lagrange@cnrs.fr","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Román M.A.; Pertusa A.; Calvo-Zaragoza J.","Román, Miguel A. (57210187945); Pertusa, Antonio (8540257800); Calvo-Zaragoza, Jorge (55847598300)","57210187945; 8540257800; 55847598300","An end-to-end framework for audio-to-score music transcription on monophonic excerpts","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065988943&partnerID=40&md5=50156a8652dbb089f3930a0dc63d7a9b","U.I. for Computing Research, University of Alicante, Alicante, Spain; PRHLT Research Center, Universitat Politècnica de València, Valencia, Spain","Román M.A., U.I. for Computing Research, University of Alicante, Alicante, Spain; Pertusa A., U.I. for Computing Research, University of Alicante, Alicante, Spain; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Valencia, Spain","In this work, we present an end-to-end framework for audio-to-score transcription. To the best of our knowledge, this is the first automatic music transcription approach which obtains directly a symbolic score from audio, instead of performing separate stages for piano-roll estimation (pitch detection and note tracking), meter detection or key estimation. The proposed method is based on a Convolutional Recurrent Neural Network architecture directly trained with pairs of spectrograms and their corresponding symbolic scores in Western notation. Unlike standard pitch estimation methods, the proposed architecture does not need the music symbols to be aligned with their audio frames thanks to a Connectionist Temporal Classification loss function. Training and evaluation were performed using a large dataset of short monophonic scores (incipits) from the RISM collection, that were synthesized to get the ground-truth data. Although there is still room for improvement, most musical symbols were correctly detected and the evaluation results validate the proposed approach. We believe that this end-to-end framework opens new avenues for automatic music transcription. © Miguel A. Román, Antonio Pertusa, Jorge Calvo-Zaragoza.","","Information retrieval; Large dataset; Network architecture; Recurrent neural networks; Transcription; Automatic music transcription; Evaluation results; Ground truth data; Monophonic score; Music transcription; Pitch estimation; Proposed architectures; Temporal classification; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lisena P.; Todorov K.; Cecconi C.; Leresche F.; Canno I.; Puyrenier F.; Voisin M.; Le Meur T.; Troncy R.","Lisena, Pasquale (56497126300); Todorov, Konstantin (24774051300); Cecconi, Cécile (57078816200); Leresche, Françoise (55980814100); Canno, Isabelle (57210188923); Puyrenier, Frédéric (57210196088); Voisin, Martine (57210196850); Le Meur, Thierry (57210188438); Troncy, Raphaël (23986650400)","56497126300; 24774051300; 57078816200; 55980814100; 57210188923; 57210196088; 57210196850; 57210188438; 23986650400","Controlled vocabularies for music metadata","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069830249&partnerID=40&md5=7594e58f5ef462ab63f9faddf62f0bb8","EURECOM, Sophia Antipolis, France; LIRMM, University of Montpellier, CNRS, France; Philharmonie de Paris, France; Bibliothèque Nationale de France, France; Radio France, France","Lisena P., EURECOM, Sophia Antipolis, France; Todorov K., LIRMM, University of Montpellier, CNRS, France; Cecconi C., Philharmonie de Paris, France; Leresche F., Bibliothèque Nationale de France, France; Canno I., Radio France, France; Puyrenier F., Bibliothèque Nationale de France, France; Voisin M., Radio France, France; Le Meur T., Philharmonie de Paris, France; Troncy R., EURECOM, Sophia Antipolis, France","We present a set of music-specific controlled vocabularies, formalized using Semantic Web languages, describing topics like musical genres, keys, or medium of performance. We have collected a number of existing vocabularies in various formats, converted them to SKOS and performed the interconnection of their equivalent terms. In addition, novel vocabularies, not available online before, have been designed by an editorial team. Next to multilingual labels and definitions, we provide hierarchical relations as well as links to external resources. We also show the application of those vocabularies for the production of vector embeddings, allowing for the calculation of distances between keys or between instruments. © Lisena et al.","","Information retrieval; AS-links; External resources; Hierarchical relations; Musical genre; Semantic web languages; Vocabulary control","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Lee K.; Choi K.; Nam J.","Lee, Kyungyun (57210184383); Choi, Keunwoo (57190944352); Nam, Juhan (35812266500)","57210184383; 57190944352; 35812266500","Revisiting singing voice detection: A quantitative review and the future outlook","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068955461&partnerID=40&md5=498af8dc20563758a396c6072302fca4","School of Computing, KAIST, South Korea; Spotify Inc., United States; Graduate School of Culture Technology, KAIST, South Korea","Lee K., School of Computing, KAIST, South Korea; Choi K., Spotify Inc., United States; Nam J., Graduate School of Culture Technology, KAIST, South Korea","Since the vocal component plays a crucial role in popular music, singing voice detection has been an active research topic in music information retrieval. Although several proposed algorithms have shown high performances, we argue that there is still room for improving the singing voice detection system. In order to identify the area of improvement, we first perform an error analysis on three recent singing voice detection systems. Based on the analysis, we design novel methods to test the systems on multiple sets of internally curated and generated data to further examine the pitfalls, which are not clearly revealed with the currently available datasets. From the experiment results, we also propose several directions towards building a more robust singing voice detector. © Kyungyun Lee, Keunwoo Choi, Juhan Nam.","","Information retrieval; Multiple set; Music information retrieval; Novel methods; Popular music; Research topics; Singing voice detection; Singing voices; Speech recognition","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Mustaine M.; Ibrahim K.M.; Gupta C.; Wang Y.","Mustaine, Michael (57210193627); Ibrahim, Karim M. (57204045658); Gupta, Chitralekha (36023604600); Wang, Ye (36103845200)","57210193627; 57204045658; 36023604600; 36103845200","Empirically weighing the importance of decision factors when selecting music to sing","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069864335&partnerID=40&md5=399b6d91282468bda5aa9a1850b1f580","School of Computing, National University of Singapore, Singapore; NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore","Mustaine M., School of Computing, National University of Singapore, Singapore; Ibrahim K.M., School of Computing, National University of Singapore, Singapore; Gupta C., School of Computing, National University of Singapore, Singapore, NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Although music cognition and music information retrieval have many common areas of research interest, relatively little work utilizes a combination of signal- and human-centric approaches when assessing complex cognitive phenomena. This work explores the importance of four cognitive decision-making factors (familiarity, genre preference, ease of vocal reproducibility, and overall preference) influence in the perception of “singability”, how attractive a song is to sing. In Experiment One, we develop a model to validate and empirically determine to what degree these factors are important when evaluating its singability. Results indicate that evaluations of how these four factors impact singability strongly correlate with pairwise evaluations (ρ = 0.692, p < 0.0001), supporting the notion that singability is a measurable cognitive process. Experiment Two examines the degree to which timbral and rhythmic features contribute to singability. Regression and random forest analysis find that some selected features are more significant than others. We discuss the method we use to empirically assess the complex decisions, and provide a preliminary exploration regarding what acoustic features may motivate these choices. © Michael Mustaine, Karim M. Ibrahim, Chitralekha Gupta, Ye Wang.","","Decision making; Decision trees; Acoustic features; Cognitive decision makings; Cognitive process; Decision factors; Music information retrieval; Reproducibilities; Research interests; Rhythmic features; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Stoller D.; Ewert S.; Dixon S.","Stoller, Daniel (56928086600); Ewert, Sebastian (32667575400); Dixon, Simon (7201479437)","56928086600; 32667575400; 7201479437","Wave-U-Net: A multi-scale neural network for end-to-end audio source separation","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","208","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069811693&partnerID=40&md5=cf793041e06ab9c01b06bddb0212daae","Queen Mary University of London, United Kingdom; Spotify, Sweden","Stoller D., Queen Mary University of London, United Kingdom; Ewert S., Spotify, Sweden; Dixon S., Queen Mary University of London, United Kingdom","Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyper-parameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids fixed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difficult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a state-of-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem. © Daniel Stoller, Sebastian Ewert, Simon Dixon.","","Information retrieval; Network architecture; Separation; Architectural improvements; Audio source separation; Different time scale; High-quality separation; Separation performance; Singing voice separations; Spectral transformations; Temporal correlations; Source separation","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Weiß C.; Balke S.; Abeßer J.; Müller M.","Weiß, Christof (56273046500); Balke, Stefan (56940401000); Abeßer, Jakob (36607532300); Müller, Meinard (7404689873)","56273046500; 56940401000; 36607532300; 7404689873","Computational corpus analysis: A case study on jazz solos","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069836077&partnerID=40&md5=91da748135f152dda08eaff51c8c1741","International Audio Laboratories, Erlangen, Germany; Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Weiß C., International Audio Laboratories, Erlangen, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","For musicological studies on large corpora, the compilation of suitable data constitutes a time-consuming step. In particular, this is true for high-quality symbolic representations that are generated manually in a tedious process. A recent study on Western classical music has shown that musical phenomena such as the evolution of tonal complexity over history can also be analyzed on the basis of audio recordings. As our first contribution, we transfer this corpus analysis method to jazz music using the Weimar Jazz Database, which contains high-level symbolic transcriptions of jazz solos along with the audio recordings. Second, we investigate the influence of the input representation type on the corpus-level observations. In our experiments, all representation types led to qualitatively similar results. We conclude that audio recordings can build a reasonable basis for conducting such type of corpus analysis. © Christof Weiß, Stefan Balke, Jakob Abeßer, Meinard Müller.","","Audio acoustics; Information retrieval; Classical musics; Corpus analysis; High quality; Large corpora; Representation type; Symbolic representation; Audio recordings","C. Weiß; International Audio Laboratories, Erlangen, Germany; email: christof.weiss@audiolabs-erlangen.de","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Takahashi T.; Fukayama S.; Goto M.","Takahashi, Takumi (57198399069); Fukayama, Satoru (56407004300); Goto, Masataka (7403505330)","57198399069; 56407004300; 7403505330","Instrudive: A music visualization system based on automatically recognized instrumentation","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069820716&partnerID=40&md5=2c3a42d2f9e82f7bfcdaf0ce790536d7","University of Tsukuba, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Takahashi T., University of Tsukuba, Japan, National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","A music visualization system called Instrudive is presented that enables users to interactively browse and listen to musical pieces by focusing on instrumentation. Instrumentation is a key factor in determining musical sound characteristics. For example, a musical piece performed with vocals, electric guitar, electric bass, and drums can generally be associated with pop/rock music but not with classical or electronic. Therefore, visualizing instrumentation can help listeners browse music more efficiently. Instrudive visualizes musical pieces by illustrating instrumentation with multi-colored pie charts and displays them on a map in accordance with the similarity in instrumentation. Users can utilize three functions. First, they can browse musical pieces on a map by referring to the visualized instrumentation. Second, they can interactively edit a playlist that showing the items to be played later. Finally, they can discern the temporal changes in instrumentation and skip to a preferable part of a piece with a multi-colored graph. The instruments are identified using a deep convolutional neural network that has four convolutional layers with different filter shapes. Evaluation of the proposed model against conventional and state-of-the-art methods showed that it has the best performance. © Takumi Takahashi, Satoru Fukayama, Masataka Goto.","","Convolution; Deep neural networks; Electronic musical instruments; Information retrieval; Multilayer neural networks; Visualization; Convolutional neural network; Electric guitar; Key factors; Music visualization; Musical pieces; Musical sounds; State-of-the-art methods; Temporal change; Acoustic measuring instruments","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ycart A.; McLeod A.; Benetos E.; Yoshii K.","Ycart, Adrien (57192301993); McLeod, Andrew (57114559100); Benetos, Emmanouil (16067946900); Yoshii, Kazuyoshi (7103400120)","57192301993; 57114559100; 16067946900; 7103400120","Blending acoustic and language model predictions for automatic music transcription","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084934683&partnerID=40&md5=55e51c2d0d5012fe546d38a81f91db7e","Queen Mary University of London, United Kingdom; Kyoto University, Japan","Ycart A., Queen Mary University of London, United Kingdom; McLeod A., Kyoto University, Japan; Benetos E., Queen Mary University of London, United Kingdom; Yoshii K., Kyoto University, Japan","In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multipitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps. © Ycart, McLeod, Benetos, Yoshii.","","Audio recordings; Audio systems; Blending; Computational linguistics; Hidden Markov models; Information retrieval; Long short-term memory; Musical instruments; Acoustic and language models; Automatic music transcription; Blending models; Detection models; Language model; Multi pitches; Training methods; Two-component; Audio acoustics","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Esling P.; Chemla–Romeu-Santos A.; Bitton A.","Esling, Philippe (36052697700); Chemla–Romeu-Santos, Axel (57210187574); Bitton, Adrien (57210183043)","36052697700; 57210187574; 57210183043","Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069823548&partnerID=40&md5=6202826c7ae7f050525b45e8da84c886","Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France","Esling P., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France; Chemla–Romeu-Santos A., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France; Bitton A., Institut de Recherche et Coordination Acoustique-Musique (IRCAM), CNRS - UMR 9912, UPMC - Sorbonne Universite, 1 Place Igor Stravinsky, Paris, F-75004, France","Generative models aim to understand the properties of data, through the construction of latent spaces that allow classification and generation. However, as the learning is unsupervised, the latent dimensions are not related to perceptual properties. In parallel, music perception research has aimed to understand timbre based on human dissimilarity ratings. These lead to timbre spaces which exhibit perceptual similarities between sounds. However, they do not generalize to novel examples and do not provide an invertible mapping, preventing audio synthesis. Here, we show that Variational Auto-Encoders (VAE) can bridge these lines of research and alleviate their weaknesses by regularizing the latent spaces to match perceptual distances collected from timbre studies. Hence, we propose three types of regularization and show that they lead to spaces that are simultaneously coherent with signal properties and perceptual similarities. We show that these spaces can be used for efficient audio classification. We study how audio descriptors are organized along the latent dimensions and show that even though descriptors behave in a non-linear way across the space, they still exhibit a locally smooth evolution. We also show that, as this space generalizes to novel samples, it can be used to predict perceptual similarities of novel instruments. Finally, we exhibit the generative capabilities of our spaces, that can directly synthesize sounds with continuous evolution of timbre perception. © Philippe Esling, Axel Chemla, Adrien Bitton.","","Audio acoustics; Audio classification; Generative model; Music perception; Perceptual distance; Perceptual properties; Perceptual similarity; Signal properties; Smooth evolutions; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Royo-Letelier J.; Hennequin R.; Tran V.-A.; Moussallam M.","Royo-Letelier, Jimena (36915600300); Hennequin, Romain (36504173400); Tran, Viet-Anh (57210191917); Moussallam, Manuel (36608879000)","36915600300; 36504173400; 57210191917; 36608879000","Disambiguating music artists at scale with audio metric learning","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069850621&partnerID=40&md5=d27f45ec1b718cf5fa71ddf60dab9bd1","Deezer, 12 rue d’Athènes, Paris, 75009, France","Royo-Letelier J., Deezer, 12 rue d’Athènes, Paris, 75009, France; Hennequin R., Deezer, 12 rue d’Athènes, Paris, 75009, France; Tran V.-A., Deezer, 12 rue d’Athènes, Paris, 75009, France; Moussallam M., Deezer, 12 rue d’Athènes, Paris, 75009, France","We address the problem of disambiguating large scale catalogs through the definition of an unknown artist clustering task. We explore the use of metric learning techniques to learn artist embeddings directly from audio, and using a dedicated homonym artists dataset, we compare our method with a recent approach that learn similar embeddings using artist classifiers. While both systems have the ability to disambiguate unknown artists relying exclusively on audio, we show that our system is more suitable in the case when enough audio data is available for each artist in the train dataset. We also propose a new negative sampling method for metric learning that takes advantage of side information such as music genre during the learning phase and shows promising results for the artist clustering task. © First author, Second author, Third author, Fourth author.","","Audio systems; Classification (of information); Embeddings; Information retrieval; Learning systems; Audio data; Learning phase; Metric learning; Music genre; Sampling method; Side information; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Andreux M.; Mallat S.","Andreux, Mathieu (56568151700); Mallat, Stéphane (7006576695)","56568151700; 7006576695","Music generation and transformation with moment matching-scattering inverse networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866092&partnerID=40&md5=407f1ac7d2d84e9dedf9bffd2950b042","Département d’informatique de l’ENS, École normale supérieure, CNRS, PSL Research University, Paris, 75005, France","Andreux M., Département d’informatique de l’ENS, École normale supérieure, CNRS, PSL Research University, Paris, 75005, France; Mallat S., Département d’informatique de l’ENS, École normale supérieure, CNRS, PSL Research University, Paris, 75005, France","We introduce a Moment Matching-Scattering Inverse Network (MM-SIN) to generate and transform musical sounds. The MM-SIN generator is similar to a variational autoencoder or an adversarial network. However, the encoder or the discriminator are not learned, but computed with a scattering transform defined from prior information on sparse time-frequency audio properties. The generator is trained by jointly minimizing the reconstruction loss of an inverse problem, and a generation loss which computes a distance over scattering moments. It has a similar causal architecture as a WaveNet and provides a simpler mathematical model related to time-frequency decompositions. Numerical experiments demonstrate that this MM-SIN generates new realistic musical signals. It can transform low-level musical attributes such as pitch with a linear transformation in the embedding space of scattering coefficients. © Mathieu Andreux and Stéphane Mallat.","","Information retrieval; Linear transformations; Mathematical transformations; Adversarial networks; Moment-matching; Musical signals; Numerical experiments; Prior information; Scattering co-efficient; Scattering transforms; Time-frequency decomposition; Inverse problems","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Silva D.F.; Falcão F.V.; Andrade N.","Silva, Diego Furtado (55585876200); Falcão, Felipe Vieira (57210190038); Andrade, Nazareno (7003429497)","55585876200; 57210190038; 7003429497","Summarizing and comparing music data and its application on cover song identification","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069856327&partnerID=40&md5=9a645180ad19a1a429002cbc6885481a","Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil; Departamento de Sistemas e Computação, Universidade Federal de Campina Grande, Campina Grande, Brazil","Silva D.F., Departamento de Computação, Universidade Federal de São Carlos, São Carlos, Brazil; Falcão F.V., Departamento de Sistemas e Computação, Universidade Federal de Campina Grande, Campina Grande, Brazil; Andrade N., Departamento de Sistemas e Computação, Universidade Federal de Campina Grande, Campina Grande, Brazil","While there is a multitude of music information retrieval algorithms that have distance functions as their core procedure, comparing the similarity between recordings is a costly procedure. At the same, the recent growth of digital music repositories makes necessary the development of novel time- and memory-efficient algorithms to deal with music data. One particularly interesting idea on the literature is transforming the music data into reduced representations, improving the memory usage and reducing the time necessary to assess the similarity. However, these techniques usually add other issues, such as an expensive preprocessing or a reduced retrieval performance. In this paper, we propose a novel method to summarize a recording in small snippets based on its self-similarity information. Besides, we present a simple way to compare other recordings to these summaries. We demonstrate, in the scenario of cover song identification, that our method is more than one order of magnitude faster than state-of-the-art adversaries, at the same time that the retrieval performance is not affected significantly. Additionally, our method is incremental, which allows the easy and fast update of the database when a new song needs to be inserted into the retrieval system. © Diego Furtado Silva, Felipe Vieira Falcão, Nazareno Andrade.","","Audio recordings; Information retrieval; Metadata; Cover song identifications; Distance functions; Memory-efficient algorithms; Music information retrieval; Reduced representation; Retrieval performance; Retrieval systems; Self-similarities; Data reduction","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Harrison P.M.C.; Pearce M.T.","Harrison, Peter M.C. (56999712100); Pearce, Marcus T. (8674558800)","56999712100; 8674558800","An energy-based generative sequence model for testing sensory theories of Western harmony","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069853333&partnerID=40&md5=664ca62846d3c81df92bf3f9af32e05b","Queen Mary University of London, Cognitive Science Research Group, United Kingdom","Harrison P.M.C., Queen Mary University of London, Cognitive Science Research Group, United Kingdom; Pearce M.T., Queen Mary University of London, Cognitive Science Research Group, United Kingdom","The relationship between sensory consonance and Western harmony is an important topic in music theory and psychology. We introduce new methods for analysing this relationship, and apply them to large corpora representing three prominent genres of Western music: classical, popular, and jazz music. These methods centre on a generative sequence model with an exponential-family energy-based form that predicts chord sequences from continuous features. We use this model to investigate one aspect of instantaneous consonance (harmonicity) and two aspects of sequential consonance (spectral distance and voice-leading distance). Applied to our three musical genres, the results generally support the relationship between sensory consonance and harmony, but lead us to question the high importance attributed to spectral distance in the psychological literature. We anticipate that our methods will provide a useful platform for future work linking music psychology to music theory. © Peter M. C. Harrison, Marcus T. Pearce.","","Chord sequence; Continuous features; Energy-based; Exponential family; Large corpora; Musical genre; Sequence modeling; Spectral distances; Information retrieval","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Fuentes M.; McFee B.; Crayencour H.C.; Essid S.; Bello J.P.","Fuentes, Magdalena (57190739095); McFee, Brian (34875379700); Crayencour, Hélène C. (57209881493); Essid, Slim (16033218700); Bello, Juan P. (7102889110)","57190739095; 34875379700; 57209881493; 16033218700; 7102889110","Analysis of common design choices in deep learning systems for downbeat tracking","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065981409&partnerID=40&md5=77c171ee98a5b550d7533dea69eba1c3","L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; LTCI, Télécom ParisTech, Univ. Paris-Saclay, France; Music and Audio Research Laboratory, New York University, United States; Center of Data Science, New York University, United States","Fuentes M., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France, LTCI, Télécom ParisTech, Univ. Paris-Saclay, France; McFee B., Music and Audio Research Laboratory, New York University, United States, Center of Data Science, New York University, United States; Crayencour H.C., L2S, CNRS-Univ.Paris-Sud-CentraleSupélec, France; Essid S., LTCI, Télécom ParisTech, Univ. Paris-Saclay, France; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Downbeat tracking consists of annotating a piece of musical audio with the estimated position of the first beat of each bar. In recent years, increasing attention has been paid to applying deep learning models to this task, and various architectures have been proposed, leading to a significant improvement in accuracy. However, there are few insights about the role of the various design choices and the delicate interactions between them. In this paper we offer a systematic investigation of the impact of largely adopted variants. We study the effects of the temporal granularity of the input representation (i.e. beat-level vs tatum-level) and the encoding of the networks outputs. We also investigate the potential of convolutional-recurrent networks, which have not been explored in previous downbeat tracking systems. To this end, we exploit a state-of-the-art recurrent neural network where we introduce those variants, while keeping the training data, network learning parameters and post-processing stages fixed. We find that temporal granularity has a significant impact on performance, and we analyze its interaction with the encoding of the networks outputs. © Magdalena Fuentes, Brian McFee, Hélène C. Crayencour, Slim Essid, Juan P. Bello.","","Audio acoustics; Encoding (symbols); Information retrieval; Network coding; Recurrent neural networks; Learning models; Network learning; Post-processing stages; Recurrent networks; State of the art; Temporal granularity; Tracking system; Training data; Deep learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Pons J.; Nieto O.; Prockup M.; Schmidt E.; Ehmann A.; Serra X.","Pons, Jordi (57190284265); Nieto, Oriol (55583364500); Prockup, Matthew (37089471200); Schmidt, Erik (36053813000); Ehmann, Andreas (8988651500); Serra, Xavier (55892979900)","57190284265; 55583364500; 37089471200; 36053813000; 8988651500; 55892979900","End-to-end learning for music audio tagging at scale","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","73","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065977708&partnerID=40&md5=8e4ab4351e7290a6c22723594079e75f","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Pandora Media Inc., Oakland, CA, United States","Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Nieto O., Pandora Media Inc., Oakland, CA, United States; Prockup M., Pandora Media Inc., Oakland, CA, United States; Schmidt E., Pandora Media Inc., Oakland, CA, United States; Ehmann A., Pandora Media Inc., Oakland, CA, United States; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The lack of data tends to limit the outcomes of deep learning research, particularly when dealing with end-to-end learning stacks processing raw data such as waveforms. In this study, 1.2M tracks annotated with musical labels are available to train our end-to-end models. This large amount of data allows us to unrestrictedly explore two different design paradigms for music auto-tagging: assumption-free models – using waveforms as input with very small convolutional filters; and models that rely on domain knowledge – log-mel spectrograms with a convolutional neural network designed to learn timbral and temporal features. Our work focuses on studying how these two types of deep architectures perform when datasets of variable size are available for training: the MagnaTagATune (25k songs), the Million Song Dataset (240k songs), and a private dataset of 1.2M songs. Our experiments suggest that music domain assumptions are relevant when not enough training data are available, thus showing how waveform-based models outperform spectrogram-based ones in large-scale data scenarios. © Jordi Pons, Oriol Nieto, Matthew Prockup, Erik Schmidt, Andreas Ehmann, Xavier Serra.","","Convolution; Data handling; Deep learning; Information retrieval; Neural networks; Spectrographs; Convolutional neural network; Deep architectures; Design paradigm; Domain knowledge; End-to-end models; Large scale data; Temporal features; Variable sizes; Audio acoustics","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Aljanaki A.; Soleymani M.","Aljanaki, Anna (56410808700); Soleymani, Mohammad (57188866370)","56410808700; 57188866370","A data-driven approach to mid-level perceptual musical feature modeling","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069866131&partnerID=40&md5=6c09f65bafa636da83e7530c9dcd1334","Institute of Computational Perception, Johannes Kepler University, Austria; Swiss Center for Affective Sciences, University of Geneva, Switzerland","Aljanaki A., Institute of Computational Perception, Johannes Kepler University, Austria; Soleymani M., Swiss Center for Affective Sciences, University of Geneva, Switzerland","Musical features and descriptors could be coarsely divided into three levels of complexity. The bottom level contains the basic building blocks of music, e.g., chords, beats and timbre. The middle level contains concepts that emerge from combining the basic blocks: tonal and rhythmic stability, harmonic and rhythmic complexity, etc. High-level descriptors (genre, mood, expressive style) are usually modeled using the lower level ones. The features belonging to the middle level can both improve automatic recognition of high-level descriptors, and provide new music retrieval possibilities. Mid-level features are subjective and usually lack clear definitions. However, they are very important for human perception of music, and on some of them people can reach high agreement, even though defining them and therefore, designing a hand-crafted feature extractor for them can be difficult. In this paper, we derive the mid-level descriptors from data. We collect and release a dataset1 of 5000 songs annotated by musicians with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic stability, modality, rhythmic complexity, dissonance and articulation. We then compare several approaches to predicting these descriptors from spectrograms using deep-learning. We also demonstrate the usefulness of these mid-level features using music emotion recognition as an application. © Anna Aljanaki,, Mohammad Soleymani.","","Information retrieval; Automatic recognition; Basic building block; Data-driven approach; Feature extractor; Human perception; Mid-level features; Music retrieval; Musical features; Deep learning","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Yang R.; Wang D.; Wang Z.; Chen T.; Jiang J.; Xia G.","Yang, Ruihan (57217330130); Wang, Dingsu (57217056688); Wang, Ziyu (57209885565); Chen, Tianyao (57217331209); Jiang, Junyan (57217331141); Xia, Gus (57208212250)","57217330130; 57217056688; 57209885565; 57217331209; 57217331141; 57208212250","Deep music analogy via latent representation disentanglement","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083435998&partnerID=40&md5=ed4cb69e1c72deb9f0287edfaf51e710","Music X Lab, NYU Shanghai, China; Machine Learning Department, Carnegie Mellon University, United States","Yang R., Music X Lab, NYU Shanghai, China; Wang D., Music X Lab, NYU Shanghai, China; Wang Z., Music X Lab, NYU Shanghai, China; Chen T., Music X Lab, NYU Shanghai, China; Jiang J., Music X Lab, NYU Shanghai, China, Machine Learning Department, Carnegie Mellon University, United States; Xia G., Music X Lab, NYU Shanghai, China","Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitlyconstrained variational autoencoder (EC2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of ""what if"" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Semantics; Auto encoders; Music clips; Music representation; Objective measurement; Pitch contours; Semantic interpretation; Sub-problems; Unified solutions; Information retrieval","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Liebman E.; White C.N.; Stone P.","Liebman, Elad (55324775700); White, Corey N. (23096672900); Stone, Peter (7203001213)","55324775700; 23096672900; 7203001213","On the impact of music on decision making in cooperative tasks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857537&partnerID=40&md5=5555737395f3467ec673c8836b6cccc4","University of Texas at Austin, Computer Science Department, United States; Missouri Western State University, Department of Psychology, United States","Liebman E., University of Texas at Austin, Computer Science Department, United States; White C.N., Missouri Western State University, Department of Psychology, United States; Stone P., University of Texas at Austin, Computer Science Department, United States","Numerous studies have demonstrated that mood affects emotional and cognitive processing. Previous work has established that music-induced mood can measurably alter people’s behavior in different contexts. However, the nature of how decision-making is affected by music in social settings hasn’t been sufficiently explored. The goal of this study is to examine which aspects of people’s decision making in inter-social tasks are affected when exposed to music. For this purpose, we devised an experiment in which people drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results indicate that music indeed alters people’s behavior with respect to this social task. To further understand the correspondence between auditory features and decision making, we have also studied how individual aspects of music affected response patterns. © Elad Liebman, Corey N. White, Peter Stone.","","Behavioral research; Information retrieval; Auditory feature; Cognitive processing; Cooperative tasks; Exposed to; Response patterns; Simulated vehicles; Social settings; Decision making","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Southall C.; Stables R.; Hockman J.","Southall, Carl (57201882762); Stables, Ryan (55322057300); Hockman, Jason (36730968100)","57201882762; 55322057300; 36730968100","Improving peak-picking using multiple time-step loss functions","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068974002&partnerID=40&md5=304530281415beb7624f2345e3072442","DMT Lab, Birmingham City University, Birmingham, United Kingdom","Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","The majority of state-of-the-art methods for music information retrieval (MIR) tasks now utilise deep learning methods reliant on minimisation of loss functions such as cross entropy. For tasks that include framewise binary classification (e.g., onset detection, music transcription) classes are derived from output activation functions by identifying points of local maxima, or peaks. However, the operating principles behind peak picking are different to that of the cross entropy loss function, which minimises the absolute difference between the output and target values for a single frame. To generate activation functions more suited to peak-picking, we propose two versions of a new loss function that incorporates information from multiple time-steps: 1) multi-individual, which uses multiple individual time-step cross entropies; and 2) multi-difference, which directly compares the difference between sequential time-step outputs. We evaluate the newly proposed loss functions alongside standard cross entropy in the popular MIR tasks of onset detection and automatic drum transcription. The results highlight the effectiveness of these loss functions in the improvement of overall system accuracies for both MIR tasks. Additionally, directly comparing the output from sequential time-steps in the multi-difference approach achieves the highest performance. © Carl Southall, Ryan Stables and Jason Hockman.","","Chemical activation; Deep learning; Information retrieval; Time series analysis; Transcription; Absolute difference; Activation functions; Binary classification; Individual time steps; Music information retrieval; Music transcription; Operating principles; State-of-the-art methods; Entropy","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Zangerle E.; Pichl M.","Zangerle, Eva (36186499400); Pichl, Martin (56453385900)","36186499400; 56453385900","Content-based user models: Modeling the many faces of musical preference","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069857301&partnerID=40&md5=944bb3e16c41e9499e64367444589aee","Universität Innsbruck, Department of Computer Science, Austria","Zangerle E., Universität Innsbruck, Department of Computer Science, Austria; Pichl M., Universität Innsbruck, Department of Computer Science, Austria","User models that capture the musical preferences of users are central for many tasks in music information retrieval and music recommendation, yet, it has not been fully explored and exploited. To this end, the musical preferences of users in the context of music recommender systems have mostly been captured in collaborative filtering-based approaches. Alternatively, users can be characterized by their average listening behavior and hence, by the mean values of a set of content descriptors of tracks the users listened to. However, a user may listen to highly different tracks and genres. Thus, computing the average of all tracks does not capture the user’s listening behavior well. We argue that each user may have many different preferences that depend on contextual aspects (e.g., listening to classical music when working and hard rock when doing sports) and that user models should account for these different sets of preferences. In this paper, we provide a detailed analysis and evaluation of different user models that describe a user’s musical preferences based on acoustic features of tracks the user has listened to. © Eva Zangerle, Martin Pichl.","","Collaborative filtering; Acoustic features; Analysis and evaluation; Classical musics; Content descriptors; Content-based; Music information retrieval; Music recommendation; Music recommender systems; Recommender systems","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ofner A.; Stober S.","Ofner, André (57200936395); Stober, Sebastian (14027561800)","57200936395; 14027561800","Shared generative representation of auditory concepts and EEG to reconstruct perceived and imagined music","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069852890&partnerID=40&md5=2a9387c1269a25309e34e1fc585745a8","Research Focus Cognitive Sciences, University of Potsdam, Germany","Ofner A., Research Focus Cognitive Sciences, University of Potsdam, Germany; Stober S., Research Focus Cognitive Sciences, University of Potsdam, Germany","Retrieving music information from brain activity is a challenging and still largely unexplored research problem. In this paper we investigate the possibility to reconstruct perceived and imagined musical stimuli from electroencephalography (EEG) recordings based on two datasets. One dataset contains multichannel EEG of subjects listening to and imagining rhythmical patterns presented both as sine wave tones and short looped spoken utterances. These utterances leverage the well-known speech-to-song illusory transformation which results in very catchy and easy to reproduce motifs. A second dataset provides EEG recordings for the perception of 10 full length songs. Using a multi-view deep generative model we demonstrate the feasibility of learning a shared latent representation of brain activity and auditory concepts, such as rhythmical motifs appearing across different instrumentations. Introspection of the model trained on the rhythm dataset reveals disentangled rhythmical and timbral features within and across subjects. The model allows continuous interpolation between representations of different observed variants of the presented stimuli. By decoding the learned embeddings we were able to reconstruct both perceived and imagined music. Stimulus complexity and the choice of training data shows strong effect on the reconstruction quality. © André Ofner, Sebastian Stober.","","Brain; Electroencephalography; Electrophysiology; Information retrieval; Neurophysiology; Brain activity; Continuous interpolations; EEG recording; Generative model; Multichannel EEG; Music information; Reconstruction quality; Research problems; Audio recordings","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Finkensiep C.; Neuwirth M.; Rohrmeier M.","Finkensiep, Christoph (57210197364); Neuwirth, Markus (36537590900); Rohrmeier, Martin (6507901506)","57210197364; 36537590900; 6507901506","Generalized skipgrams for pattern discovery in polyphonic streams","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805622&partnerID=40&md5=c1548ff266aa1b49fa53727f9bc18e88","École Polytechnique Fédérale de Lausanne, Switzerland","Finkensiep C., École Polytechnique Fédérale de Lausanne, Switzerland; Neuwirth M., École Polytechnique Fédérale de Lausanne, Switzerland; Rohrmeier M., École Polytechnique Fédérale de Lausanne, Switzerland","The discovery of patterns using a minimal set of assumptions constitutes a central challenge in the modeling of polyphonic music and complex streams in general. Skipgrams have been found to be a powerful model for capturing semi-local dependencies in sequences of entities when dependencies may not be directly adjacent (see, for instance, the problems of modeling sequences of words or letters in computational linguistics). Since common skipgrams define locality based on indices, they can only be applied to a single stream of non-overlapping entities. This paper proposes a generalized skipgram model that allows arbitrary cost functions (defining locality), efficient filtering, recursive application (skipgrams over skipgrams), and memory efficient streaming. Further, a sampling mechanism is proposed that flexibly controls runtime and output size. These generalizations and optimizations make it possible to employ skipgrams for the discovery of repeated patterns of close, nonsimultaneous events or notes. The extensions to the skipgram model provided here do not only apply to musical notes but to any list of entities that is monotonic with respect to a given cost function. © Christoph Finkensiep, Markus Neuwirth, Martin.","","Cost functions; Information retrieval; Arbitrary costs; Memory efficient; Non-simultaneous; Pattern discovery; Polyphonic music; Recursive applications; Repeated patterns; Sampling mechanisms; Computer music","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Demetriou A.; Jansson A.; Kumar A.; Bittner R.M.","Demetriou, Andrew (57192921545); Jansson, Andreas (57189200331); Kumar, Aparna (57205359931); Bittner, Rachel M. (55659619600)","57192921545; 57189200331; 57205359931; 55659619600","Vocals in music matter: The relevance of vocals in the minds of listeners","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067311732&partnerID=40&md5=6f34a519a7f6def1956e962e2fc44373","Multimedia Computing Group, TU Delft, Netherlands; Spotify Inc., New York City, United States","Demetriou A., Multimedia Computing Group, TU Delft, Netherlands, Spotify Inc., New York City, United States; Jansson A., Spotify Inc., New York City, United States; Kumar A., Spotify Inc., New York City, United States; Bittner R.M., Spotify Inc., New York City, United States","In music information retrieval, we often make assertions about what features of music are important to study, one of which is vocals. While the importance of vocals in music preference is both intuitive and anticipated by psychological theory, we have not found any survey studies that confirm this commonly held assertion. We address two questions: (1) what components of music are most salient to people’s musical taste, and (2) how do vocals rank relative to other components of music, in regards to whether people like or dislike a song. Lastly, we explore the aspects of the voice that listeners find important. Two surveys of Spotify users were conducted. The first gathered open-format responses that were then card-sorted into semantic categories by the team of researchers. The second asked respondents to rank the semantic categories derived from the first survey. Responses indicate that vocals were a salient component in the minds of listeners. Further, vocals ranked high as a self-reported factor for a listener liking or disliking a track, among a statistically significant ranking of musical attributes. In addition, we open several new interesting problem areas that have yet to be explored in MIR. © Andrew Demetriou, Andreas Jansson, Aparna Kumar, Rachel M. Bittner.","","Information retrieval; Semantics; Music information retrieval; Music preferences; Problem areas; Psychological theory; Semantic category; Surveys","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Knees P.; Schedl M.; Goto M.","Knees, Peter (8219023200); Schedl, Markus (8684865900); Goto, Masataka (7403505330)","8219023200; 8684865900; 7403505330","Intelligent user interfaces for music discovery: The past 20 years and what's to come","2019","Proceedings of the 20th International Society for Music Information Retrieval Conference, ISMIR 2019","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085359140&partnerID=40&md5=64d28be6e457ea5ab4b4f7a52e0c8d93","Faculty of Informatics, TU Wien, Vienna, Austria; Institute of Computational Perception, Johannes Kepler University Linz, Austria; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Knees P., Faculty of Informatics, TU Wien, Vienna, Austria; Schedl M., Institute of Computational Perception, Johannes Kepler University Linz, Austria; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Providing means to assist the user in finding music is one of the original motivations underlying the research field known as Music Information Retrieval (MIR). Therefore, already the first edition of ISMIR in the year 2000 called for papers addressing the topic of ""User interfaces for music IR"". Since then, the way humans interact with technology to access and listen to music has substantially changed, not least driven by the advances of MIR and related research fields such as machine learning and recommender systems. In this paper, we reflect on the evolution of MIR-driven user interfaces for music browsing and discovery over the past two decades. We argue that three major developments have transformed and shaped user interfaces during this period, each connected to a phase of new listening practices: first, connected to personal music collections, intelligent audio processing and content description algorithms that facilitate the automatic organization of repositories and finding music according to sound qualities; second, connected to collective web platforms, the exploitation of user-generated metadata pertaining to semantic descriptions; and third, connected to streaming services, the collection of online music interaction traces on a large scale and their exploitation in recommender systems. We review and contextualize work from ISMIR and related venues from all three phases and extrapolate current developments to outline possible scenarios of music recommendation and listening interfaces of the future. © 2020 International Society for Music Information Retrieval. All rights reserved.","","Audio acoustics; Audio systems; Online systems; Recommender systems; Semantics; Content description; Intelligent User Interfaces; Music information retrieval; Music recommendation; Personal music collection; Semantic descriptions; Streaming service; User-generated metadatas; User interfaces","","","20th International Society for Music Information Retrieval Conference, ISMIR 2019","4 November 2019 through 8 November 2019","Delft","160244"
"Kedyte V.; Panteli M.; Weyde T.; Dixon S.","Kedyte, Vytaute (57210212707); Panteli, Maria (55915922100); Weyde, Tillman (24476899500); Dixon, Simon (7201479437)","57210212707; 55915922100; 24476899500; 7201479437","Geographical origin prediction of folk music recordings from the United Kingdom","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053915605&partnerID=40&md5=aaa785d172b91ae8c622e56bfc574b1d","Department of Computer Science, City University of London, United Kingdom; Centre for Digital Music, Queen Mary University of London, United Kingdom","Kedyte V., Department of Computer Science, City University of London, United Kingdom; Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Weyde T., Department of Computer Science, City University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Field recordings from ethnomusicological research since the beginning of the 20th century are available today in large digitised music archives. The application of music information retrieval and data mining technologies can aid large-scale data processing leading to a better understanding of the history of cultural exchange. In this paper we focus on folk and traditional music from the United Kingdom and study the correlation between spatial origins and musical characteristics. In particular, we investigate whether the geographical location of music recordings can be predicted solely from the content of the audio signal. We build a neural network that takes as input a feature vector capturing musical aspects of the audio signal and predicts the latitude and longitude of the origins of the music recording. We explore the performance of the model for different sets of features and compare the prediction accuracy between geographical regions of the UK. Our model predicts the geographical coordinates of music recordings with an average error of less than 120 km. The model can be used in a similar manner to identify the origins of recordings in large unlabelled music collections and reveal patterns of similarity in music from around the world. © 2019 Vytaute Kedyte, Maria Panteli, Tillman Weyde, Simon Dixon.","","Audio acoustics; Data handling; Data mining; Geographical regions; History; Information retrieval; Data mining technology; Geographical coordinates; Geographical locations; Geographical origins; Large-scale data processing; Music collection; Music information retrieval; Prediction accuracy; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Ross J.C.; Mishra A.; Ganguli K.K.; Bhattacharyya P.; Rao P.","Ross, Joe Cheri (55321796400); Mishra, Abhijit (56349872900); Ganguli, Kaustuv Kanti (56094514100); Bhattacharyya, Pushpak (7101803108); Rao, Preeti (35180193500)","55321796400; 56349872900; 56094514100; 7101803108; 35180193500","Identifying raga similarity through embeddings learned from compositions' notation","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068065865&partnerID=40&md5=d2be6db29c90d2c4e2ce577e7d841eb4","Dept. of Computer Science and Engineering, India; Dept. of Electrical Engineering, Indian Institute of Technology Bombay, India; IBM Research India, India","Ross J.C., Dept. of Computer Science and Engineering, India; Mishra A., IBM Research India, India; Ganguli K.K., Dept. of Electrical Engineering, Indian Institute of Technology Bombay, India; Bhattacharyya P., Dept. of Computer Science and Engineering, India; Rao P., Dept. of Electrical Engineering, Indian Institute of Technology Bombay, India","Identifying similarities between ragas in Hindustani music impacts tasks like music recommendation, music information retrieval and automatic analysis of large-scale musical content. Quantifying raga similarity becomes extremely challenging as it demands assimilation of both intrinsic (viz., notes, tempo) and extrinsic (viz. raga singingtime, emotions conveyed) properties of ragas. This paper introduces novel frameworks for quantifying similarities between ragas based on their melodic attributes alone, available in the form of bandish (composition) notation. Based on the hypothesis that notes in a particular raga are characterized by the company they keep, we design and train several deep recursive neural network variants with Long Short-term Memory (LSTM) units to learn distributed representations of notes in ragas from bandish notations. We refer to these distributed representations as note-embeddings. Note-embeddings, as we observe, capture a raga's identity, and thus the similarity between note-embeddings signifies the similarity between the ragas. Evaluations with perplexity measure and clustering based method show the performance improvement in identifying similarities using note-embeddings over n-gram and unidirectional LSTM baselines. While our metric may not capture similarity between ragas in their entirety, it could be quite useful in various computational music settings that heavily rely on melodic information. © 2019 Joe Cheri Ross, Abhijit Mishra, Kaustuv Kanti Ganguli, Pushpak Bhattacharyya, Preeti Rao.","","Information retrieval; Long short-term memory; Automatic analysis; Distributed representation; Melodic information; Music information retrieval; Music recommendation; N-grams; Recursive neural networks; Embeddings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Oliveira R.S.; Nóbrega C.; Marinho L.B.; Andrade N.","Oliveira, Ricardo S. (57204068082); Nóbrega, Caio (55485708600); Marinho, Leandro B. (23393414800); Andrade, Nazareno (7003429497)","57204068082; 55485708600; 23393414800; 7003429497","A multiobjective music recommendation approach for aspect-based diversification","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942935&partnerID=40&md5=56f8557a455506b3ec2fbab3c7860fa7","UFCG - Federal University of Campina Grande, Brazil","Oliveira R.S., UFCG - Federal University of Campina Grande, Brazil; Nóbrega C., UFCG - Federal University of Campina Grande, Brazil; Marinho L.B., UFCG - Federal University of Campina Grande, Brazil; Andrade N., UFCG - Federal University of Campina Grande, Brazil","Many successful recommendation approaches are based on the optimization of some explicit utility function defined in terms of the misfit between the predicted and the actual items of the user. Although effective, this approach may lead to recommendations that are relevant but obvious and uninteresting. Many approaches investigate this problem by trying to avoid recommendation lists in which items are very similar to each other (aka diversification) with respect to some aspect of the item. However, users may have very different preferences concerning what aspects should be diversified and what should match their past/current preferences. In this paper we take this into consideration by proposing a solution based on multiobjective optimization for generating recommendation lists featuring the optimal balance between the aspects that should be held fixed (maximize similarity with users actual items) and the ones that should be diversified (minimize similarity with other items in the recommendation list). We evaluate our proposed approach on real data from Last.fm and demonstrate its effectiveness in contrast to state-of-the-art approaches. © 2019 Ricardo S. Oliveira, Caio Nóbrega, Leandro B. Marinho, Nazareno Andrade.","","Multiobjective optimization; Last.fm; Multi objective; Music recommendation; Optimal balance; State-of-the-art approach; Utility functions; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Cogliati A.; Duan Z.","Cogliati, Andrea (56940539000); Duan, Zhiyao (24450312900)","56940539000; 24450312900","A metric for music notation transcription accuracy","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053807815&partnerID=40&md5=7e3f896faa8b50687883c9fba888d1bb","University of Rochester, Electrical and Computer Engineering, United States","Cogliati A., University of Rochester, Electrical and Computer Engineering, United States; Duan Z., University of Rochester, Electrical and Computer Engineering, United States","Automatic music transcription aims at transcribing musical performances into music notation. However, most existing transcription systems only focus on parametric transcription, i.e., they output a symbolic representation in absolute terms, showing frequency and absolute time (e.g., a pianoroll representation), but not in musical terms, with spelling distinctions (e.g., A versus G#) and quantized meter. Recent attempts at producing full music notation output have been hindered by the lack of an objective metric to measure the adherence of the results to the ground truth music score, and had to rely on time-consuming human evaluation by music theorists. In this paper, we propose an edit distance, similar to the Levenshtein Distance used for measuring the difference between two sequences, typically strings of characters. The metric treats a music score as a sequence of sets of musical objects, ordered by their onsets. The metric reports the differences between two music scores based on twelve aspects: barlines, clefs, key signatures, time signatures, notes, note spelling, note durations, stem directions, groupings, rests, rest duration, and staff assignment. We also apply a linear regression model to the metric in order to predict human evaluations on a dataset of short music excerpts automatically transcribed into music notation. © 2019 Andrea Cogliati, Zhiyao Duan.","","Information retrieval; Regression analysis; Automatic music transcription; Human evaluation; Levenshtein distance; Linear regression models; Music notation; Musical performance; Staff assignments; Symbolic representation; Transcription","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Defferrard M.; Benzi K.; Vandergheynst P.; Bresson X.","Defferrard, Michaël (7801535249); Benzi, Kirell (57189593335); Vandergheynst, Pierre (7004114381); Bresson, Xavier (6506291911)","7801535249; 57189593335; 7004114381; 6506291911","FMA: A dataset for music analysis","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","134","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068955365&partnerID=40&md5=52ec35cfef56f2b0a0851dd8d52c1502","LTS2, EPFL, Switzerland; SCSE, NTU, Singapore, Singapore","Defferrard M., LTS2, EPFL, Switzerland; Benzi K., LTS2, EPFL, Switzerland; Vandergheynst P., LTS2, EPFL, Switzerland; Bresson X., SCSE, NTU, Singapore, Singapore","We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commonslicensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma. © 2019 Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, Xavier Bresson.","","Information retrieval; Mobile telecommunication systems; A-train; End to end; Freeforms; Hierarchical taxonomy; High-quality audio; Large music collections; Music analysis; User levels; Large dataset","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Dai J.; Dixon S.","Dai, Jiajie (57205391597); Dixon, Simon (7201479437)","57205391597; 7201479437","Analysis of interactive intonation in unaccompanied SATB ensembles","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059844430&partnerID=40&md5=2b9a6516411eab5208d0a33e1e8feada","Centre for Digital Music, Queen Mary University of London, United Kingdom","Dai J., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Unaccompanied ensemble singing is common in many musical cultures, yet it requires great skill for singers to listen to each other and adjust their pitch to stay in tune. The aim of this research is to investigate interaction in four-part (SATB) singing from the point of view of pitch accuracy (intonation). In particular we compare intonation accuracy of individual singers and collaborative ensembles. 20 participants (five groups of four) sang two pieces of music in three different listening conditions: solo, with one vocal part missing and with all vocal parts. After semi-automatic pitch extraction and manual correction, we annotated the recordings and calculated the pitch error, melodic interval error, harmonic interval error and note stability. We observed significant differences between individual and interactional intonation, more specifically: 1) Singing without the bass part has less mean absolute pitch error than singing with all vocal parts; 2) Mean absolute melodic interval error increases when participants can hear the other parts; 3) Mean absolute harmonic interval error is higher in the one-way interaction condition than the two-way interaction condition; and 4) Singers produce more stable notes when singing solo than with their partners. © 2019 Jiajie Dai, Simon Dixon.","","Information retrieval; Absolute pitch; Interval errors; Melodic intervals; Pitch errors; Pitch extraction; Semi-automatics; Two-way interaction; Errors","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hawthorne C.; Elsen E.; Song J.; Roberts A.; Simon I.; Raffel C.; Engel J.; Oore S.; Eck D.","Hawthorne, Curtis (57204813140); Elsen, Erich (24722660400); Song, Jialin (57210184854); Roberts, Adam (57201381304); Simon, Ian (57205897781); Raffel, Colin (55354986300); Engel, Jesse (57192111540); Oore, Sageev (6507904334); Eck, Douglas (12141444300)","57204813140; 24722660400; 57210184854; 57201381304; 57205897781; 55354986300; 57192111540; 6507904334; 12141444300","Onsets and frames: Dual-objective piano transcription","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","105","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059752323&partnerID=40&md5=8eee573801f54d440814958b249a119b","Google Brain Team, Mountain View, CA, United States","Hawthorne C., Google Brain Team, Mountain View, CA, United States; Elsen E., Google Brain Team, Mountain View, CA, United States; Song J., Google Brain Team, Mountain View, CA, United States; Roberts A., Google Brain Team, Mountain View, CA, United States; Simon I., Google Brain Team, Mountain View, CA, United States; Raffel C., Google Brain Team, Mountain View, CA, United States; Engel J., Google Brain Team, Mountain View, CA, United States; Oore S., Google Brain Team, Mountain View, CA, United States; Eck D., Google Brain Team, Mountain View, CA, United States","We advance the state of the art in polyphonic piano music transcription by using a deep convolutional and recurrent neural network which is trained to jointly predict onsets and frames. Our model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe this correlates better with human musical perception. Our approach results in over a 100% relative improvement in note F1 score (with offsets) on the MAPS dataset. Furthermore, we extend the model to predict relative velocities of normalized audio which results in more natural-sounding transcriptions. © Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, Douglas Eck.","","Forecasting; Information retrieval; Recurrent neural networks; Transcription; F1 scores; Human musical perception; Piano music; Relative velocity; State of the art; Audio acoustics","C. Hawthorne; Google Brain Team, Mountain View, United States; email: fjord@google.com","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bogdanov D.; Serra X.","Bogdanov, Dmitry (35748642000); Serra, Xavier (55892979900)","35748642000; 55892979900","Quantifying music trends and facts using editorial metadata from the discogs database","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069834588&partnerID=40&md5=5e4a4c81d1a6b99d9a619349ebc94524","Music Technology Group, Universitat Pompeu Fabra, Spain","Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","While a vast amount of editorial metadata is being actively gathered and used by music collectors and enthusiasts, it is often neglected by music information retrieval and musicology researchers. In this paper we propose to explore Discogs, one of the largest databases of such data available in the public domain. Our main goal is to show how largescale analysis of its editorial metadata can raise questions and serve as a tool for musicological research on a number of example studies. The metadata that we use describes music releases, such as albums or EPs. It includes information about artists, tracks and their durations, genre and style, format (such as vinyl, CD, or digital files), year and country of each release. Using this data we study correlations between different genre and style labels, assess their specificity and analyze typical track durations. We estimate trends in prevalence of different genres, styles, and formats across different time periods. In our analysis of styles we use electronic music as an example. Our contribution also includes the tools we developed for our analysis and the generated datasets that can be re-used by MIR researchers and musicologists. © 2019 Dmitry Bogdanov, Xavier Serra.","","Computer music; Electronic musical instruments; Information retrieval; Digital files; Electronic music; Large-scale analysis; Music information retrieval; Public domains; Time-periods; Metadata","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Gururani S.; Lerch A.","Gururani, Siddharth (57195432595); Lerch, Alexander (22034963000)","57195432595; 22034963000","Automatic sample detection in polyphonic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069859408&partnerID=40&md5=00178da32a1564171d91b210377ea5c3","Georgia Institute of Technology, Center for Music Technology, United States","Gururani S., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","The term 'sampling' refers to the usage of snippets or loops from existing songs or sample libraries in new songs, mashups, or other music productions. The ability to automatically detect sampling in music is, for instance, beneficial for studies tracking artist influences geographically and temporally. We present a method based on Non-negative Matrix Factorization (NMF) and Dynamic Time Warping (DTW) for the automatic detection of a sample in a pool of songs. The method comprises of two processing steps: first, the DTW alignment path between NMF activations of a song and query sample is computed. Second, features are extracted from this path and used to train a Random Forest classifier to detect the presence of the sample. The method is able to identify samples that are pitch shifted and/or time stretched with approximately 63% F-measure. We evaluate this method against a new publicly available dataset of real-world sample and song pairs. © 2019 Siddharth Gururani, Alexander Lerch.","","Decision trees; Factorization; Information retrieval; Automatic Detection; Dynamic time warping; Music production; Nonnegative matrix factorization; Polyphonic music; Processing steps; Random forest classifier; Sample libraries; Matrix algebra","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hu X.; Choi K.; Hao Y.; Cunningham S.J.; Lee J.H.; Laplante A.; Bainbridge D.; Downie J.S.","Hu, Xiao (55496358400); Choi, Kahyun (56452038000); Hao, Yun (56147494900); Cunningham, Sally Jo (7201937110); Lee, Jin Ha (57190797465); Laplante, Audrey (37110930300); Bainbridge, David (8756864800); Downie, J. Stephen (7102932568)","55496358400; 56452038000; 56147494900; 7201937110; 57190797465; 37110930300; 8756864800; 7102932568","Exploring the music library association mailing list: A text mining approach","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062741595&partnerID=40&md5=e8dd9bde3918517143d72517048cdd61","University of Hong Kong, Hong Kong, Hong Kong; University of Illinois, United States; University of Waikato, New Zealand; Univeristy of Washington, United States; Université de Montréal, Canada","Hu X., University of Hong Kong, Hong Kong, Hong Kong; Choi K., University of Illinois, United States; Hao Y., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Lee J.H., Univeristy of Washington, United States; Laplante A., Université de Montréal, Canada; Bainbridge D., University of Waikato, New Zealand; Downie J.S., University of Illinois, United States","Music librarians and people pursuing music librarianship have exchanged emails via the Music Library Association Mailing List (MLA-L) for decades. The list archive is an invaluable resource to discover new insights on music information retrieval from the perspective of the music librarian community. This study analyzes a corpus of 53,648 emails posted on MLA-L from 2000 to 2016 by using text mining and quantitative analysis methods. In addition to descriptive analysis, main topics of discussions and their trends over the years are identified through topic modeling. We also compare messages that stimulated discussions to those that did not. Inspection of semantic topics reveals insights complementary to previous topic analyses of other Music Information Retrieval (MIR) related resources. © 2019 Xiao Hu, Kahyun Choi, Yun Hao, Sally Jo Cunningham, Jin Ha Lee, Audrey Laplante, David Bainbridge and J. Stephen Downie.","","Electronic mail; Information retrieval; Libraries; Semantics; Descriptive analysis; Mailing lists; Music information retrieval; Music library; Text mining; Topic analysis; Topic Modeling; Data mining","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Schlüter J.; Lehner B.","Schlüter, Jan (55063593300); Lehner, Bernhard (7003282869)","55063593300; 7003282869","Zero-mean convolutions for level-invariant singing voice detection","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057115155&partnerID=40&md5=29e8164278e76e4d1ea9fcd5cb1328c5","Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Institute of Computational Perception, Johannes Kepler University, Linz, Austria","Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Lehner B., Institute of Computational Perception, Johannes Kepler University, Linz, Austria","State-of-the-art singing voice detectors are based on classifiers trained on annotated examples. As recently shown, such detectors have an important weakness: Since singing voice is correlated with sound level in training data, classifiers learn to become sensitive to input magnitude, and give different predictions for the same signal at different sound levels. Starting from a Convolutional Neural Network (CNN) trained on logarithmic-magnitude mel spectrogram excerpts, we eliminate this dependency by forcing each first-layer convolutional filter to be zero-mean – that is, to have its coefficients sum to zero. In contrast to four other methods – data augmentation, instance normalization, spectral delta features, and per-channel energy normalization (PCEN) – that we evaluated on a large-scale public dataset, zero-mean convolutions achieve perfect sound level invariance without any impact on prediction accuracy or computational requirements. We assume that zero-mean convolutions would be useful for other machine listening tasks requiring robustness to level changes. © Jan Schlüter, Bernhard Lehner.","","Classification (of information); Convolution; Correlation detectors; Information retrieval; Large dataset; Multilayer neural networks; Computational requirements; Convolutional neural network; Data augmentation; Machine listening; Prediction accuracy; Public dataset; Singing voice detection; State of the art; Speech recognition","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Kinnaird K.M.","Kinnaird, Katherine M. (57205699026)","57205699026","Examining musical meaning in similarity thresholds","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069826319&partnerID=40&md5=be00e1ca527451c69d6b4121726e64bb","Brown University, United States","Kinnaird K.M., Brown University, United States","Many approaches to Music Information Retrieval tasks rely on correctly determining if two segments of a given musical recording are repeats of each other. Repetitions in recordings are rarely exact, and identifying the appropriate threshold for these pairwise decisions is crucial for tuning MIR algorithms. However, current approaches for determining and reporting this threshold parameter are devoid of contextual meaning and interpretations, which makes comparing previous results difficult and which requires access to specific datasets. This paper highlights weaknesses in current approaches to choosing similarity thresholds, provides a framework using the proportion of orthogonal musical change to tie thresholds back to feature spaces with the cosine dissimilarity measure, and introduces new research possibilities given a music-centered approach for selecting similarity thresholds. © 2019 Katherine M. Kinnaird.","","Information retrieval; Dissimilarity measures; Feature space; Music information retrieval; Similarity threshold; Threshold parameters; Audio recordings","K.M. Kinnaird; Brown University, United States; email: katherine_kinnaird@brown.edu","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tsaptsinos A.","Tsaptsinos, Alexandros (57210208344)","57210208344","Lyrics-based music genre classification using a hierarchical attention network","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055006420&partnerID=40&md5=35e86b38936a8004a8160f22d88ace00","ICME, Stanford University, United States","Tsaptsinos A., ICME, Stanford University, United States","Music genre classification, especially using lyrics alone, remains a challenging topic in Music Information Retrieval. In this study we apply recurrent neural network models to classify a large dataset of intact song lyrics. As lyrics exhibit a hierarchical layer structure-in which words combine to form lines, lines form segments, and segments form a complete song-we adapt a hierarchical attention network (HAN) to exploit these layers and in addition learn the importance of the words, lines, and segments. We test the model over a 117-genre dataset and a reduced 20-genre dataset. Experimental results show that the HAN outperforms both non-neural models and simpler neural models, whilst also classifying over a higher number of genres than previous research. Through the learning process we can also visualise which words or lines in a song the model believes are important to classifying the genre. As a result the HAN provides insights, from a computational perspective, into lyrical structure and language features that differentiate musical genres. © 2019 Alexandros Tsaptsinos.","","Information retrieval; Large dataset; Learning systems; Recurrent neural networks; Statistical tests; Language features; Layer structures; Learning process; Music genre classification; Music information retrieval; Musical genre; Neural models; Recurrent neural network model; Classification (of information)","A. Tsaptsinos; ICME, Stanford University, United States; email: alextsap@stanford.edu","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Cazau D.; Wang Y.; Adam O.; Wang Q.; Nuel G.","Cazau, Dorian (55745364100); Wang, Yuancheng (57191205640); Adam, Olivier (7005030491); Wang, Qiao (58442898300); Nuel, Grégory (6506476101)","55745364100; 57191205640; 7005030491; 58442898300; 6506476101","Improving note segmentation in automatic piano music transcription systems with a two-state pitch-wise HMM method","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068963394&partnerID=40&md5=10988ae7d9f3c079217185f09258949f","Lab-STICC, ENSTA-Bretagne, France; School of Information Science and Engineering, Southeast, China; Institut d'Alembert, UPMC, France; LPMA, UPMC, France","Cazau D., Lab-STICC, ENSTA-Bretagne, France; Wang Y., School of Information Science and Engineering, Southeast, China; Adam O., Institut d'Alembert, UPMC, France; Wang Q., School of Information Science and Engineering, Southeast, China; Nuel G., LPMA, UPMC, France","Many methods for automatic piano music transcription involve a multi-pitch estimation method that estimates an activity score for each pitch. A second processing step, called note segmentation, has to be performed for each pitch in order to identify the time intervals when the notes are played. In this study, a pitch-wise two-state on/off first-order Hidden Markov Model (HMM) is developed for note segmentation. A complete parametrization of the HMM sigmoid function is proposed, based on its original regression formulation, including a parameter α of slope smoothing and β of thresholding contrast. A comparative evaluation of different note segmentation strategies was performed, differentiated according to whether they use a fixed threshold, called ""Hard Thresholding"" (HT), or a HMM-based thresholding method, called ""Soft Thresholding"" (ST). This evaluation was done following MIREX standards and using the MAPS dataset. Also, different transcription and recording scenarios were tested using three units of the Audio Degradation toolbox. Results show that note segmentation through a HMM soft thresholding with a data-based optimization of the {α, β} parameter couple significantly enhances transcription performance. © 2019 Dorian Cazau, Yuancheng Wang, Olivier Adam, Qiao Wang, Grégory Nuel.","","Information retrieval; Transcription; Comparative evaluations; Data-based optimization; First-order hidden Markov models; Hard thresholding; Multi-pitch estimations; Note segmentation; Soft thresholding; Thresholding methods; Hidden Markov models","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Basaran D.; Essid S.; Peeters G.","Basaran, Dogac (43261015000); Essid, Slim (16033218700); Peeters, Geoffroy (22433836000)","43261015000; 16033218700; 22433836000","Main melody extraction with source-filter NMF and CRNN","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064135681&partnerID=40&md5=e2cf64d437a92902fded7f33a7ac83f9","CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; LTCI, Télécom ParisTech, Université Paris Saclay, Paris, France","Basaran D., CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Essid S., LTCI, Télécom ParisTech, Université Paris Saclay, Paris, France; Peeters G., CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France","Estimating the main melody of a polyphonic audio recording remains a challenging task. We approach the task from a classification perspective and adopt a convolutional recurrent neural network (CRNN) architecture that relies on a particular form of pretraining by source-filter nonnegative matrix factorisation (NMF). The source-filter NMF decomposition is chosen for its ability to capture the pitch and timbre content of the leading voice/instrument, providing a better initial pitch salience than standard time-frequency representations. Starting from such a musically motivated representation, we propose to further enhance the NMF-based salience representations with CNN layers, then to model the temporal structure by an RNN network and to estimate the dominant melody with a final classification layer. The results show that such a system achieves state-of-the-art performance on the MedleyDB dataset without any augmentation methods or large training sets. © Dogac Basaran, Slim Essid, Geoffroy Peeters.","","Audio recordings; Information retrieval; Large dataset; Augmentation methods; Melody extractions; Non-negative matrix factorisation; Source filters; Standard time; State-of-the-art performance; Temporal structures; Training sets; Recurrent neural networks","D. Basaran; CNRS, Ircam Lab, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; email: dogac.basaran@ircam.fr","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Bittner R.M.; McFee B.; Salamon J.; Li P.; Bello J.P.","Bittner, Rachel M. (55659619600); McFee, Brian (34875379700); Salamon, Justin (55184866100); Li, Peter (57200759464); Bello, Juan P. (7102889110)","55659619600; 34875379700; 55184866100; 57200759464; 7102889110","Deep salience representations for F0 estimation in polyphonic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","136","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069924285&partnerID=40&md5=b305708df8196d6969b417b8c9f42431","Music and Audio Research Laboratory, New York University, United States; Center for Data Science, New York University, United States","Bittner R.M., Music and Audio Research Laboratory, New York University, United States; McFee B., Music and Audio Research Laboratory, New York University, United States, Center for Data Science, New York University, United States; Salamon J., Music and Audio Research Laboratory, New York University, United States; Li P., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Estimating fundamental frequencies in polyphonic music remains a notoriously difficult task in Music Information Retrieval. While other tasks, such as beat tracking and chord recognition have seen improvement with the application of deep learning models, little work has been done to apply deep learning methods to fundamental frequency related tasks including multi-f0 and melody tracking, primarily due to the scarce availability of labeled data. In this work, we describe a fully convolutional neural network for learning salience representations for estimating fundamental frequencies, trained using a large, semi-automatically generated f0 dataset. We demonstrate the effectiveness of our model for learning salience representations for both multi-f0 and melody tracking in polyphonic audio, and show that our models achieve state-of-the-art performance on several multi-f0 and melody datasets. We conclude with directions for future research. © 2019 Rachel M. Bittner, Brian McFee, Justin Salamon, Peter Li, Juan P. Bello.","","Information retrieval; Large dataset; Natural frequencies; Neural networks; Automatically generated; Chord recognition; Convolutional neural network; Fundamental frequencies; Learning methods; Music information retrieval; Polyphonic music; State-of-the-art performance; Deep learning","R.M. Bittner; Music and Audio Research Laboratory, New York University, United States; email: rachel.bittner@nyu.edu","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Heo H.; Kim H.J.; Kim W.S.; Lee K.","Heo, Hoon (55354324700); Kim, Hyunwoo J. (56336378000); Kim, Wan Soo (57220896598); Lee, Kyogu (8597995500)","55354324700; 56336378000; 57220896598; 8597995500","Cover song identification with metric learning using distance as a feature","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054236711&partnerID=40&md5=89c431190a732f2ade3b6e25b39b643a","Music and Audio Research Group, Seoul National University, South Korea; Department of Computer Sciences, University of Wisconsin-Madison, United States","Heo H., Music and Audio Research Group, Seoul National University, South Korea; Kim H.J., Department of Computer Sciences, University of Wisconsin-Madison, United States; Kim W.S., Music and Audio Research Group, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Seoul National University, South Korea","Most of cover song identification algorithms are based on the pairwise (dis)similarity between two songs which are represented by harmonic features such as chroma, and therefore the choice of a distance measure and a feature has a significant impact on performance. Furthermore, since the similarity measure is query-dependent, it cannot represent an absolute distance measure. In this paper, we present a novel approach to tackle the cover song identification problem from a new perspective. We first construct a set of core songs, and represent each song in a high-dimensional space where each dimension indicates the pairwise distance between the given song and the other in the pre-defined core set. There are several advantages to this. First, using a number of reference songs in the core set, we make the most of relative distances to many other songs. Second, as all songs are transformed into the same high-dimensional space, kernel methods and metric learning are exploited for distance computation. Third, our approach does not depend on the computation method for the pairwise distance, and thus can use any existing algorithms. Experimental results confirm that the proposed approach achieved a large performance gain compared to the state-of-the-art methods. © 2019 Hoon Heo, Hyunwoo J. Kim,Wan Soo Kim, Kyogu Lee.","","Clustering algorithms; Computation methods; Cover song identifications; Distance computation; High dimensional spaces; Pairwise distances; Relative distances; Similarity measure; State-of-the-art methods; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Vigliensoni G.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); Fujinaga, Ichiro (9038140900)","55217696900; 9038140900","The music listening histories dataset","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066910167&partnerID=40&md5=1c498c50ef6bf9dbd25c551a875183c1","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","We introduce the Music Listening Histories Dataset (MLHD), a large-scale collection of music listening events assembled from more than 27 billion time-stamped logs extracted from Last.fm. The logs are organized in the form of listening histories per user, and have been conveniently preprocessed and cleaned. Attractive features of the MLHD are the self-declared metadata provided by users at the moment of registration whose identities have been anonymized, MusicBrainz identifiers for the music entities in each of the logs that allows for an easy linkage to other existing resources, and a set of user profiling features designed to describe aspects of their music listening behavior and activity. We describe the process of assembling the dataset, its content, its demographic characteristics, and discuss about the possible uses of this collection, which, currently, is the largest research dataset of this kind in the field. © 2019 Gabriel Vigliensoni and Ichiro Fujinaga.","","Information retrieval; Demographic characteristics; Last.fm; Listening history; User profiling; Large dataset","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tsushima H.; Nakamura E.; Itoyama K.; Yoshii K.","Tsushima, Hiroaki (57201298652); Nakamura, Eita (24587601200); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120)","57201298652; 24587601200; 18042499100; 7103400120","Function- And rhythm-aware melody harmonization based on tree-structured parsing and split-merge sampling of chord sequences","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051998287&partnerID=40&md5=6dbad753bb5804715e1798ddf06eeb33","Graduate School of Informatics, Kyoto University, Japan; RIKEN AIP, Japan","Tsushima H., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan, RIKEN AIP, Japan","This paper presents an automatic harmonization method that, for a given melody (sequence of musical notes), generates a sequence of chord symbols in the style of existing data. A typical way is to use hidden Markov models (HMMs) that represent chord transitions on a regular grid (e.g., bar or beat grid). This approach, however, cannot explicitly describe the rhythms, harmonic functions (e.g., tonic, dominant, and subdominant), and the hierarchical structure of chords, which are supposedly important in traditional harmony theories. To solve this, we formulate a hierarchical generative model consisting of (1) a probabilistic context-free grammar (PCFG) for chords incorporating their syntactic functions, (2) a metrical Markov model describing chord rhythms, and (3) a Markov model generating melodies conditionally on a chord sequence. To estimate a variable-length chord sequence for a given melody, we iteratively refine the latent tree structure and the chord symbols and rhythms using a Metropolis-Hastings sampler with split-merge operations. Experimental results show that the proposed method outperformed the HMM-based method in terms of predictive abilities. © 2019 Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii.","","Context free grammars; Forestry; Harmonic functions; Information retrieval; Iterative methods; Syntactics; Trees (mathematics); Automatic harmonizations; Generative model; Hidden markov models (HMMs); Hierarchical structures; Metropolis Hastings; Predictive abilities; Probabilistic context free grammars; Syntactic functions; Hidden Markov models","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Liang F.; Gotham M.; Johnson M.; Shotton J.","Liang, Feynman (57219623557); Gotham, Mark (55987597200); Johnson, Matthew (57211616500); Shotton, Jamie (23019722900)","57219623557; 55987597200; 57211616500; 23019722900","Automatic stylistic composition of bach chorales with deep LSTM","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055992374&partnerID=40&md5=eebbfebf781bab11de68bc83996ad11f","Department of Engineering, University of Cambridge, United Kingdom; Faculty of Music, University of Cambridge, United Kingdom; Microsoft, United States","Liang F., Department of Engineering, University of Cambridge, United Kingdom; Gotham M., Faculty of Music, University of Cambridge, United Kingdom; Johnson M., Microsoft, United States; Shotton J., Microsoft, United States","This paper presents ""BachBot"": an end-to-end automatic composition system for composing and completing music in the style of Bach's chorales using a deep long short-term memory (LSTM) generative model. We propose a new sequential encoding scheme for polyphonic music and a model for both composition and harmonization which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot's success, we conducted one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1% better than random guessing. © 2019 Feynman Liang, Mark Gotham, Matthew Johnson, Jamie Shotton.","","Information retrieval; Markov processes; Automatic composition; Discrimination tests; Encoding schemes; End to end; Generative model; Markov chain Monte Carlo; Polyphonic music; Prior knowledge; Long short-term memory","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Van Der Wel E.; Ullrich K.","Van Der Wel, Eelco (57210207921); Ullrich, Karen (57202060292)","57210207921; 57202060292","Optical Music Recognition with Convolutional Sequence-to-Sequence models","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052817526&partnerID=40&md5=9168ca43222c17b6e48ee8d548bb3f9c","University of Amsterdam, Netherlands","Van Der Wel E., University of Amsterdam, Netherlands; Ullrich K., University of Amsterdam, Netherlands","Optical Music Recognition (OMR) is an important technology within Music Information Retrieval. Deep learning models show promising results on OMR tasks, but symbol-level annotated data sets of sufficient size to train such models are not available and difficult to develop. We present a deep learning architecture called a Convolutional Sequence-to-Sequence model to both move towards an end-to-end trainable OMR pipeline, and apply a learning process that trains on full sentences of sheet music instead of individually labeled symbols. The model is trained and evaluated on a human generated data set, with various image augmentations based on real-world scenarios. This data set is the first publicly available set in OMR research with sufficient size to train and evaluate deep learning models. With the introduced augmentations a pitch recognition accuracy of 81% and a duration accuracy of 94% is achieved, resulting in a note level accuracy of 80%. Finally, the model is compared to commercially available methods, showing a large improvements over these applications. © 2019 Eelco van derWel, Karen Ullrich.","","Convolution; Information retrieval; Learning architectures; Learning process; Music information retrieval; Optical music recognition; Pitch recognition; Real-world scenario; Sequence modeling; Sequence models; Deep learning","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Pacha A.; Calvo-Zaragoza J.","Pacha, Alexander (54382080100); Calvo-Zaragoza, Jorge (55847598300)","54382080100; 55847598300","Optical music recognition in mensural notation with region-based convolutional neural networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062446867&partnerID=40&md5=873254c405ad63072fad568054685977","Institute of Visual Computing and Human-Centered Technology, TU Wien, Austria; PRHLT Research Center, Universitat Politècnica de València, Spain","Pacha A., Institute of Visual Computing and Human-Centered Technology, TU Wien, Austria; Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain","In this work, we present an approach for the task of optical music recognition (OMR) using deep neural networks. Our intention is to simultaneously detect and categorize musical symbols in handwritten scores, written in mensural notation. We propose the use of region-based convolutional neural networks, which are trained in an end-to-end fashion for that purpose. Additionally, we make use of a convolutional neural network that predicts the relative position of a detected symbol within the staff, so that we cover the entire image-processing part of the OMR pipeline. This strategy is evaluated over a set of 60 ancient scores in mensural notation, with more than 15000 annotated symbols belonging to 32 different classes. The results reflect the feasibility and capability of this approach, with a weighted mean average precision of around 76% for symbol detection, and over 98% accuracy for predicting the position. © Alexander Pacha, Jorge Calvo-Zaragoza.","","Convolution; Image processing; Information retrieval; Neural networks; Optical data processing; Convolutional neural network; Different class; Musical symbols; Optical music recognition; Region-based; Relative positions; Symbol detection; Weighted mean; Deep neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Choi K.; Fazekas G.; Sandler M.; Cho K.","Choi, Keunwoo (57190944352); Fazekas, György (37107520200); Sandler, Mark (7202740804); Cho, Kyunghyun (55722769200)","57190944352; 37107520200; 7202740804; 55722769200","Transfer learning for music classification and regression tasks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","95","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069930310&partnerID=40&md5=63fc69c5a839b0e691d27bd198f46419","Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Center for Data Science, New York University, New York, NY, United States","Choi K., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Cho K., Center for Data Science, New York University, New York, NY, United States","In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as low- and high-level music features. © 2019 Keunwoo Choi, György Fazekas, Mark Sandler, Kyunghyun Cho.","","Information retrieval; Convnet; Convolutional networks; Feature map; Feature vectors; Multiple layers; Music classification; Music representation; Transfer learning; Regression analysis","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tsukuda K.; Ishida K.; Goto M.","Tsukuda, Kosetsu (36609397600); Ishida, Keisuke (14066059500); Goto, Masataka (7403505330)","36609397600; 14066059500; 7403505330","Lyric Jumper: A lyrics-based music exploratory web service by modeling lyrics generative process","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069930276&partnerID=40&md5=c099849e1a743e2cb5f851754fd1e2d5","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Tsukuda K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Ishida K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Each artist has their own taste for topics of lyrics such as ""love"" and ""friendship."" Considering such artist's taste brings new applications in music information retrieval: choosing an artist based on topics of lyrics and finding un- familiar artists who have similar taste to a favorite artist. Although previous studies applied latent Dirichlet allocation (LDA) to lyrics to analyze topics, LDA was not able to capture the artist's taste. In this paper, we propose a topic model that can deal with the artist's taste for topics of lyrics. Our model assumes each artist has a topic distribution and a topic is assigned to each song according to the distribution. Our experimental results using a real- world dataset show that our model outperforms LDA in terms of the perplexity. By applying our model to estimate topics of 147,990 lyrics by 3,722 artists, we implement a web service called Lyric Jumper that enables users to explore lyrics based on the estimated topics. Lyric Jumper provides functions such as artist's topic taste visualization and topic-similarity-based artist recommendation. We also analyze operation logs obtained from 12,353 users on Lyric Jumper and show the usefulness of Lyric Jumper especially in recommending topic-related phrases in lyrics. © 2019 Kosetsu Tsukuda, Keisuke Ishida, Masataka Goto.","","Computer music; Information retrieval; Statistics; Websites; Generative process; Latent dirichlet allocations; Music information retrieval; New applications; Real-world; Topic distributions; Topic Modeling; Topic similarity; Web services","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Louboutin C.; Bimbot F.","Louboutin, Corentin (57126336000); Bimbot, Frédéric (6701567957)","57126336000; 6701567957","Modeling the multiscale structure of chord sequences using polytopic graphs","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069899562&partnerID=40&md5=c2ade27a8802210f4c2ac31977d49de2","Université Rennes 1, IRISA, France; CNRS - UMR 6074, IRISA, France","Louboutin C., Université Rennes 1, IRISA, France; Bimbot F., CNRS - UMR 6074, IRISA, France","Chord sequences are an essential source of information in a number of MIR tasks. However, beyond the sequential nature of musical content, relations and dependencies within a music segment can be more efficiently modeled as a graph. Polytopic Graphs have been recently introduced to model music structure so as to account for multiscale relationships between events located at metrically homologous instants. In this paper, we focus on the description of chord sequences and we study a specific set of graph configurations, called Primer Preserving Permutations (PPP). For sequences of 16 chords, PPPs account for 6 different latent systems of relations, corresponding to 6 main structural patterns (Prototypical Carrier Sequences or PCS). Observed chord sequences can be viewed as distorted versions of these PCS and the corresponding optimal PPP is estimated by minimizing a description cost over the latent relations. After presenting the main concepts of this approach, the article provides a detailed study of PPPs across a corpus of 727 chord sequences annotated from the RWC POP database (100 pop songs). Our results illustrate both qualitatively and quantitatively the potential of the proposed model for capturing long-term multiscale structure in musical data, which remains a challenge in computational music modeling and in Music Information Retrieval. © 2019 Corentin Louboutin, Frédéric Bimbot.","","Information retrieval; Chord sequence; Multi-scale structures; Music information retrieval; MUSIC model; Music segments; Music structures; Structural pattern; Computer music","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Jansson A.; Humphrey E.; Montecchio N.; Bittner R.; Kumar A.; Weyde T.","Jansson, Andreas (57189200331); Humphrey, Eric (55060792500); Montecchio, Nicola (25929429200); Bittner, Rachel (55659619600); Kumar, Aparna (57205359931); Weyde, Tillman (24476899500)","57189200331; 55060792500; 25929429200; 55659619600; 57205359931; 24476899500","Singing voice separation with deep U-Net convolutional networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","247","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069911577&partnerID=40&md5=38cca5edb51e737d961412edb9db91c6","City, University of London, United Kingdom; Spotify, Sweden","Jansson A., City, University of London, United Kingdom, Spotify, Sweden; Humphrey E., Spotify, Sweden; Montecchio N., Spotify, Sweden; Bittner R., Spotify, Sweden; Kumar A., Spotify, Sweden; Weyde T., City, University of London, United Kingdom","The decomposition of a music audio signal into its vocal and backing track components is analogous to image-toimage translation, where a mixed spectrogram is transformed into its constituent sources. We propose a novel application of the U-Net architecture - initially developed for medical imaging - for the task of source separation, given its proven capacity for recreating the fine, low-level detail required for high-quality audio reproduction. Through both quantitative evaluation and subjective assessment, experiments demonstrate that the proposed algorithm achieves state-of-the-art performance. © 2019 Andreas Jansson, Eric Humphrey, Nicola Montecchio, Rachel Bittner, Aparna Kumar, Tillman Weyde.","","Audio acoustics; Information retrieval; Medical imaging; Convolutional networks; High-quality audio; NET architecture; Novel applications; Quantitative evaluation; Singing voice separations; State-of-the-art performance; Subjective assessments; Source separation","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Dzhambazov G.; Holzapfel A.; Srinivasamurthy A.; Serra X.","Dzhambazov, Georgi (56271863800); Holzapfel, Andre (18041818000); Srinivasamurthy, Ajay (55583336200); Serra, Xavier (55892979900)","56271863800; 18041818000; 55583336200; 55892979900","Metrical-accent aware vocal onset detection in polyphonic audio","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069903194&partnerID=40&md5=1fc63dddad78f535bc6bd4e77aa249b9","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Media Technology and Interaction Design, KTH Royal Institute of Technology, Stockholm, Sweden","Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Media Technology and Interaction Design, KTH Royal Institute of Technology, Stockholm, Sweden; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The goal of this study is the automatic detection of onsets of the singing voice in polyphonic audio recordings. Starting with a hypothesis that the knowledge of the current position in a metrical cycle (i.e. metrical accent) can improve the accuracy of vocal note onset detection, we propose a novel probabilistic model to jointly track beats and vocal note onsets. The proposed model extends a state of the art model for beat and meter tracking, in which a-priori probability of a note at a specific metrical accent interacts with the probability of observing a vocal note onset. We carry out an evaluation on a varied collection of multi-instrument datasets from two music traditions (English popular music and Turkish makam) with different types of metrical cycles and singing styles. Results confirm that the proposed model reasonably improves vocal note onset detection accuracy compared to a baseline model that does not take metrical position into account. © 2019 Georgi Dzhambazov, Andre Holzapfel, Ajay Srinivasamurthy, Xavier Serra.","","Information retrieval; A-priori probabilities; Automatic Detection; Baseline models; Note onset detections; Onset detection; Probabilistic modeling; Singing voices; State of the art; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Peperkamp J.; Hildebrandt K.; Liem C.C.S.","Peperkamp, Jeroen (57194080473); Hildebrandt, Klaus (6701493600); Liem, Cynthia C. S. (21733247100)","57194080473; 6701493600; 21733247100","A formalization of relative local tempo variations in collections of performances","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936515&partnerID=40&md5=05d15da2630f810a1f8e5d6fe83cd40a","Delft University of Technology, Delft, Netherlands","Peperkamp J., Delft University of Technology, Delft, Netherlands; Hildebrandt K., Delft University of Technology, Delft, Netherlands; Liem C.C.S., Delft University of Technology, Delft, Netherlands","Multiple performances of the same piece share similarities, but also show relevant dissimilarities. With regard to the latter, analyzing and quantifying variations in collections of performances is useful to understand how a musical piece is typically performed, how naturally sounding new interpretations could be rendered, or what is peculiar about a particular performance. However, as there is no formal ground truth as to what these variations should look like, it is a challenge to provide and validate analysis methods for this. In this paper, we focus on relative local tempo variations in collections of performances. We propose a way to formally represent relative local tempo variations, as encoded in warping paths of aligned performances, in a vector space. This enables using statistics for analyzing tempo variations in collections of performances. We elaborate the computation and interpretation of the mean variation and the principal modes of variation. To validate our analysis method despite the absence of a ground truth, we present results on artificially generated data, representing several categories of local tempo variations. Finally, we show how our method can be used for analyzing to realworld data and discuss potential applications. © 2019 Jeroen Peperkamp, Klaus Hildebrandt, Cynthia C. S.","","Information retrieval; Analysis method; Ground truth; Musical pieces; Real-world; Vector spaces","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Zhang S.; Repetto R.C.; Serra X.","Zhang, Shuo (57195678445); Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57195678445; 57200495266; 55892979900","Understanding the expressive functions of jingju metrical patterns through lyrics text mining","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056652704&partnerID=40&md5=d37f7468ec710a04cab8f60afc903f02","Music Technology Group, Universitat Pompeu Fabra, Spain","Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","The emotional content of jingju (aka Beijing or Peking opera) arias is conveyed through pre-defined metrical patterns known as banshi, each of them associated with a specific expressive function. In this paper, we first report the work on a comprehensive corpus of jingju lyrics that we built, suitable for text mining and text analysis in a data-driven framework. Utilizing this corpus, we propose a novel approach to study the expressive functions of banshi by applying text analysis techniques on lyrics. First we apply topic modeling techniques to jingju lyrics text documents grouped at different levels according to the banshi they are associated with. We then experiment with several different document vector representations of lyrics in a series of document classification experiments. The topic modeling results showed that sentiment polarity (positive or negative) is better distinguished between different shengqiang-banshi (a more fine grained partition of banshi) than banshi alone, and we are able to achieve high accuracy scores in classifying lyrics documents into different banshi categories. We discuss the technical and musicological implications and possible future improvements. © 2019 Shuo Zhang, Rafael Caro Repetto, Xavier Serra.","","Information retrieval; Information retrieval systems; Document Classification; Document vectors; Fine grained; High-accuracy; Possible futures; Text analysis; Text document; Topic Modeling; Data mining","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Mishra S.; Sturm B.L.; Dixon S.","Mishra, Saumitra (57195518532); Sturm, Bob L. (14014190500); Dixon, Simon (7201479437)","57195518532; 14014190500; 7201479437","Local interpretable model-agnostic explanations for music content analysis","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","67","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063089874&partnerID=40&md5=de590301375d4acbd16a48a5aaed2c98","Centre for Digital Music, Queen Mary University of London, United Kingdom","Mishra S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","The interpretability of a machine learning model is essential for gaining insight into model behaviour. While some machine learning models (e.g., decision trees) are transparent, the majority of models used today are still black-boxes. Recent work in machine learning aims to analyse these models by explaining the basis of their decisions. In this work, we extend one such technique, called local interpretable model-agnostic explanations, to music content analysis. We propose three versions of explanations: one version is based on temporal segmentation, and the other two are based on frequency and time-frequency segmentation. These explanations provide meaningful ways to understand the factors that influence the classification of specific input data. We apply our proposed methods to three singing voice detection systems: the first two are designed using decision tree and random forest classifiers, respectively; the third system is based on convolutional neural network. The explanations we generate provide insights into the model behaviour. We use these insights to demonstrate that despite achieving 71.4% classification accuracy, the decision tree model fails to generalise. We also demonstrate that the model-agnostic explanations for the neural network model agree in many cases with the model-dependent saliency maps. The experimental code and results are available online.1 © 2019 Saumitra Mishra, Bob L. Sturm, Simon Dixon.","","Information retrieval; Machine learning; Neural networks; Classification accuracy; Convolutional neural network; Decision tree modeling; Machine learning models; Music content analysis; Random forest classifier; Singing voice detection; Temporal segmentations; Decision trees","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Huang C.-Z.A.; Cooijmans T.; Roberts A.; Courville A.; Eck D.","Huang, Cheng-Zhi Anna (56104404600); Cooijmans, Tim (56730948200); Roberts, Adam (57201381304); Courville, Aaron (6507291186); Eck, Douglas (12141444300)","56104404600; 56730948200; 57201381304; 6507291186; 12141444300","Counterpoint by convolution","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","58","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055966041&partnerID=40&md5=15ee7ac379aaf084211bb8b3dc1cfa96","MILA, Université de Montréal, Canada; Google Brain","Huang C.-Z.A., MILA, Université de Montréal, Canada; Cooijmans T., MILA, Université de Montréal, Canada; Roberts A., Google Brain; Courville A., MILA, Université de Montréal, Canada; Eck D., Google Brain","Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE [36], which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from [40] yields better samples than ancestral sampling, based on both log-likelihood and human evaluation. © 2019 Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck.","","Information retrieval; Neural networks; Conditional distribution; Convolutional neural network; Generative procedures; Gibbs sampling; Human evaluation; Log likelihood; Machine learning models; Sample quality; Convolution","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Humphrey E.J.; Montecchio N.; Bittner R.; Jansson A.; Jehan T.","Humphrey, Eric J. (55060792500); Montecchio, Nicola (25929429200); Bittner, Rachel (55659619600); Jansson, Andreas (57189200331); Jehan, Tristan (6504476037)","55060792500; 25929429200; 55659619600; 57189200331; 6504476037","Mining labeled data from web-scale collections for vocal activity detection in music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058518447&partnerID=40&md5=dc5971a63012a034d5105acd9cfc0552","Spotify, New York, NY, United States; Music, Audio and Research Lab (MARL), New York University, United States; City University, London, United Kingdom","Humphrey E.J., Spotify, New York, NY, United States; Montecchio N., Spotify, New York, NY, United States; Bittner R., Spotify, New York, NY, United States, Music, Audio and Research Lab (MARL), New York University, United States; Jansson A., Spotify, New York, NY, United States, City University, London, United Kingdom; Jehan T., Spotify, New York, NY, United States","This work demonstrates an approach to generating strongly labeled data for vocal activity detection by pairing instrumental versions of songs with their original mixes. Though such pairs are rare, we find ample instances in a massive music collection for training deep convolutional networks at this task, achieving state of the art performance with a fraction of the human effort required previously. Our error analysis reveals two notable insights: imperfect systems may exhibit better temporal precision than human annotators, and should be used to accelerate annotation; and, machine learning from mined data can reveal subtle biases in the data source, leading to a better understanding of the problem itself. We also discuss future directions for the design and evolution of benchmarking datasets to rigorously evaluate AI systems. © 2019 Eric J. Humphrey, Nicola Montecchio, Rachel Bittner, Andreas Jansson, Tristan Jehan.","","Activity detection; Convolutional networks; Data-source; Imperfect systems; Labeled data; Music collection; State-of-the-art performance; Temporal precision; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Zalkow F.; Weiß C.; Müller M.","Zalkow, Frank (57192297460); Weiß, Christof (56273046500); Müller, Meinard (7404689873)","57192297460; 56273046500; 7404689873","Exploring tonal-dramatic relationships in richard Wagner's ring cycle","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069920600&partnerID=40&md5=032a612d32cbdca5adad559351fba115","International Audio Laboratories Erlangen, Germany","Zalkow F., International Audio Laboratories Erlangen, Germany; Weiß C., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Richard Wagner's cycle Der Ring des Nibelungen, consisting of four music dramas, constitutes a comprehensive work of high importance forWestern music history. In this paper, we indicate how MIR methods can be applied to explore this large-scale work with respect to tonal properties. Our investigations are based on a data set that contains 16 audio recordings of the entire Ring as well as extensive annotations including measure positions, singer activities, and leitmotif regions. As a basis for the tonal analysis, we make use of common audio features, which capture local chord and scale information. Employing a crossversion approach, we show that global histogram representations can reflect certain tonal relationships in a robust way. Based on our annotations, a musicologist may easily select and compare passages associated with dramatic aspects, for example, the appearance of specific characters or the presence of particular leitmotifs. Highlighting and investigating such passages may provide insights into the role of tonality for the dramatic conception of Wagner's Ring. By giving various concrete examples, we indicate how our approach may open up new ways for exploring large musical corpora in an intuitive and interactive way. © 2019 Frank Zalkow, Christof Weiß, Meinard Müller.","","Information retrieval; Audio features; Data set; Interactive way; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"McFee B.; Bello J.P.","McFee, Brian (34875379700); Bello, Juan Pablo (7102889110)","34875379700; 7102889110","Structured training for large-vocabulary chord recognition","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","74","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053714526&partnerID=40&md5=90af5c2dc10aecb4914dc23913b94745","Center for Data Science, New York University, United States; Music and Audio Research Laboratory, New York University, United States","McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Automatic chord recognition systems operating in the large-vocabulary regime must overcome data scarcity: certain classes occur much less frequently than others, and this presents a significant challenge when estimating model parameters. While most systems model the chord recognition task as a (multi-class) classification problem, few attempts have been made to directly exploit the intrinsic structural similarities between chord classes. In this work, we develop a deep convolutional-recurrent model for automatic chord recognition over a vocabulary of 170 classes. To exploit structural relationships between chord classes, the model is trained to produce both the time-varying chord label sequence as well as binary encodings of chord roots and qualities. This binary encoding directly exposes similarities between related classes, allowing the model to learn a more coherent representation of simultaneous pitch content. Evaluations on a corpus of 1217 annotated recordings demonstrate substantial improvements compared to previous models. © 2019 Brian McFee, Juan Pablo Bello.","","Computer music; Information retrieval; Vocabulary control; Binary encodings; Chord recognition; Coherent representations; Large vocabulary; Recurrent models; Structural relationship; Structural similarity; Systems modeling; Encoding (symbols)","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Schreiber H.; Müller M.","Schreiber, Hendrik (55586286200); Müller, Meinard (7404689873)","55586286200; 7404689873","A post-processing procedure for improving music tempo estimates using supervised learning","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069870428&partnerID=40&md5=9f82a4b963797f8de12fd08fec12a2bf","Tagtraum Industries Incorporated; International Audio Laboratories Erlangen, Germany","Schreiber H., Tagtraum Industries Incorporated; Müller M., International Audio Laboratories Erlangen, Germany","Tempo estimation is a fundamental problem in music information retrieval and has been researched extensively. One problem still unsolved is the tendency of tempo estimation algorithms to produce results that are wrong by a small number of known factors (so-called octave errors). We propose a method that uses supervised learning to predict such tempo estimation errors. In a post-processing step, these predictions can then be used to correct an algorithm's tempo estimates. While being simple and relying only on a small number of features, our proposed method significantly increases accuracy for state-of-the-art tempo estimation methods. © 2019 Hendrik Schreiber, Meinard Müller.","","Information retrieval; Supervised learning; Music information retrieval; Music tempos; Post processing; Post-processing procedure; State of the art; Tempo estimations; Machine learning","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Shi Z.; Arul K.; Smith J.O.","Shi, Zhengshan (56768547200); Arul, Kumaran (57210209840); Smith, Julius O. (7410167523)","56768547200; 57210209840; 7410167523","Modeling and digitizing reproducing piano rolls","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069821605&partnerID=40&md5=3a139b13ae7aecb4dcff6d998f782f05","CCRMA, Stanford University, United States; Department of Music, Stanford University, United States","Shi Z., CCRMA, Stanford University, United States; Arul K., Department of Music, Stanford University, United States; Smith J.O., CCRMA, Stanford University, United States","Reproducing piano rolls are among the early music storage mediums, preserving fine details of a piano or organ performance on a continuous roll of paper with holes punched onto them. While early acoustic recordings suffer from poor quality sound, reproducing piano rolls benefit from the fidelity of a live piano for playback, and capture all features of a performance in what amounts to an early digital data format. However, due to limited availability of well maintained playback instruments and the condition of fragile paper, rolls have remained elusive and generally inaccessible for study. This paper proposes methods for modeling and digitizing reproducing piano rolls. Starting with an optical scan, we convert the raw image data into the MIDI file format by applying histogram-based image processing and building computational models of the musical expressions encoded on the rolls. Our evaluations show that MIDI emulations from our computational models are accurate on note level and proximate the musical expressions when compared with original playback recordings. © 2019 Zhengshan Shi, Kumaran Arul, Julius O. Smith.","","Audio recordings; Computation theory; Computational methods; Digital storage; Image processing; Information retrieval; Optical data processing; Computational model; Digital data format; Early musics; MIDI files; Musical expression; Raw image data; Musical instruments","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Oramas S.; Nieto O.; Barbieri F.; Serra X.","Oramas, Sergio (55582112200); Nieto, Oriol (55583364500); Barbieri, Francesco (56828967300); Serra, Xavier (55892979900)","55582112200; 55583364500; 56828967300; 55892979900","Multi-label music genre classification from audio, text, and images using deep features","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069917282&partnerID=40&md5=6a745cc023df02a48901b1f2befd0319","Music Technology Group, Universitat Pompeu Fabra, Spain; Pandora Media Inc., United States; TALN Group, Universitat Pompeu Fabra, Spain","Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Nieto O., Pandora Media Inc., United States; Barbieri F., TALN Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Music genres allow to categorize musical items that share common characteristics. Although these categories are not mutually exclusive, most related research is traditionally focused on classifying tracks into a single class. Furthermore, these categories (e.g., Pop, Rock) tend to be too broad for certain applications. In this work we aim to expand this task by categorizing musical items into multiple and fine-grained labels, using three different data modalities: audio, text, and images. To this end we present MuMu, a new dataset of more than 31k albums classified into 250 genre classes. For every album we have collected the cover image, text reviews, and audio tracks. Additionally, we propose an approach for multi-label genre classification based on the combination of feature embeddings learned with state-of-the-art deep learning methodologies. Experiments show major differences between modalities, which not only introduce new baselines for multi-label genre classification, but also suggest that combining them yields improved results. © 2019 Sergio Oramas, Oriol Nieto, Francesco Barbieri, Xavier Serra.","","Deep learning; Image classification; Information retrieval; Text processing; Audio track; Cover-image; Fine grained; Genre classification; Multi-label; Music genre; Music genre classification; State of the art; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Humphrey E.J.; Durand S.; McFee B.","Humphrey, Eric J. (55060792500); Durand, Simon (56303161700); McFee, Brian (34875379700)","55060792500; 56303161700; 34875379700","OpenMIC-2018: An open dataset for multiple instrument recognition","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056621572&partnerID=40&md5=662a690a001f004f12487c05710a7d1e","Spotify, Sweden; New York University, United States","Humphrey E.J., Spotify, Sweden; Durand S., Spotify, Sweden; McFee B., New York University, United States","Identification of instruments in polyphonic recordings is a challenging, but fundamental problem in music information retrieval. While there has been significant progress in developing predictive models for this and related classification tasks, we as a community lack a common data-set which is large, freely available, diverse, and representative of naturally occurring recordings. This limits our ability to measure the efficacy of computational models. This article describes the construction of a new, open data-set for multi-instrument recognition. The dataset contains 20,000 examples of Creative Commons-licensed music available on the Free Music Archive. Each example is a 10-second excerpt which has been partially labeled for the presence or absence of 20 instrument classes by annotators on a crowd-sourcing platform. We describe in detail how the instrument taxonomy was constructed, how the dataset was sampled and annotated, and compare its characteristics to similar, previous data-sets. Finally, we present experimental results and baseline model performance to motivate future work. © Eric J. Humphrey, Simon Durand, Brian McFee.","","Audio recordings; Information retrieval; Open Data; Classification tasks; Computational model; Creative Commons; Instrument recognition; Multiple instruments; Music information retrieval; Naturally occurring; Predictive models; Classification (of information)","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Syue J.-L.; Su L.; Lin Y.-J.; Li P.-C.; Lu Y.-K.; Wang Y.-L.; Su A.W.Y.","Syue, Jia-Ling (57485311200); Su, Li (55966919100); Lin, Yi-Ju (56022124700); Li, Pei-Ching (57195930038); Lu, Yen-Kuang (57210203015); Wang, Yu-Lin (56286441900); Su, Alvin W. Y. (8433191700)","57485311200; 55966919100; 56022124700; 57195930038; 57210203015; 56286441900; 8433191700","Accurate audio-to-score alignment for expressive violin recordings","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069915547&partnerID=40&md5=3ece1309ceca3c84960011487932c2b4","SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Music and Culture Technology Lab., IIS, Academia Sinica, Taiwan","Syue J.-L., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Su L., Music and Culture Technology Lab., IIS, Academia Sinica, Taiwan; Lin Y.-J., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Li P.-C., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Lu Y.-K., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Wang Y.-L., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; Su A.W.Y., SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan","An audio-to-score alignment system adaptive to various playing styles and techniques, and also with high accuracy for onset/offset annotation is the key step toward advanced research on automatic music expression analysis. Technical barriers include the processing of overlapped notes, repeated note sequences, and silence. Most of these characteristics vary with expressions. In this paper, the audio-toscore alignment problem of expressive violin performance is addressed. We propose a two-stage alignment system composed of the dynamic time warping (DTW) algorithm, simulation of overlapped sustain notes, background noise model, silence detection, and refinement process, to better capture the onset. More importantly, we utilize the nonnegative matrix factorization (NMF) method for synthesis of the reference signal in order to deal with highly diverse timbre in real-world performance. A dataset of annotated expressive violin recordings in which each piece is played with various expressive musical terms is used. The optimal choice of basic parameters considered in conventional alignment systems, such as features, distance functions in DTW, synthesis methods for the reference signal, and energy ratios, is analyzed. Different settings on different expressions are compared and discussed. Results show that the proposed methods notably improve the conventional DTW-based alignment method. © 2019 Jia-Ling Syue, Li Su, Yi-Ju Lin, Pei-Ching Li, Yen-Kuang Lu, Yu-Lin Wang, Alvin W. Y. Su.","","Alignment; Audio recordings; Factorization; Information retrieval; Matrix algebra; Musical instruments; Signal processing; Advanced researches; Alignment Problems; Distance functions; Dynamic time warping algorithms; Expression analysis; Nonnegative matrix factorization method (NMF); Real-world performance; Technical barriers; Audio acoustics","J.-L. Syue; SCREAM Lab., Department of CSIE, National Cheng-Kung University, Taiwan; email: P76044457@mail.ncku.edu.tw","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Bittner R.M.; Gu M.; Hernandez G.; Humphrey E.J.; Jehan T.; McCurry P.H.; Montecchio N.","Bittner, Rachel M. (55659619600); Gu, Minwei (58437656000); Hernandez, Gandalf (57210213578); Humphrey, Eric J. (55060792500); Jehan, Tristan (6504476037); McCurry, P. Hunter (57210206758); Montecchio, Nicola (25929429200)","55659619600; 58437656000; 57210213578; 55060792500; 6504476037; 57210206758; 25929429200","Automatic playlist sequencing and transitions","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069859516&partnerID=40&md5=f8aeda058978dce63d9362aef4bceefc","Spotify Inc., United States","Bittner R.M., Spotify Inc., United States; Gu M., Spotify Inc., United States; Hernandez G., Spotify Inc., United States; Humphrey E.J., Spotify Inc., United States; Jehan T., Spotify Inc., United States; McCurry P.H., Spotify Inc., United States; Montecchio N., Spotify Inc., United States","Professional music curators and DJs artfully arrange and mix recordings together to create engaging, seamless, and cohesive listening experiences, a craft enjoyed by audiences around the world. The average listener, however, lacks both the time and the skill necessary to create comparable experiences, despite access to same source material. As a result, user-generated listening sessions often lack the sophistication popularized by modern artists, e.g. tracks are played in their entirety with little or no thought given to their ordering. To these ends, this paper presents methods for automatically sequencing existing playlists and adding DJ-style crossfade transitions between tracks: the former is modeled as a graph traversal problem, and the latter as an optimization problem. Our approach is motivated by an analysis of listener data on a large music catalog, and subjectively evaluated by professional curators. © 2019 Rachel M. Bittner, Minwei Gu, Gandalf Hernandez, Eric J. Humphrey.","","Graph traversals; Optimization problems; Source material; User-generated; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Ens J.; Riecke B.E.; Pasquier P.","Ens, Jeff (57210203647); Riecke, Bernhard E. (6603396361); Pasquier, Philippe (8850202000)","57210203647; 6603396361; 8850202000","The significance of the low complexity dimension in music similarity judgements","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942343&partnerID=40&md5=6f2a5922cdb00b2413c4a9296af08620","Simon Fraser University, Canada","Ens J., Simon Fraser University, Canada; Riecke B.E., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada","Previous research has demonstrated that similarity judgements are context specific, as they are shaped by cultural exposure, familiarity, and the musical aesthetic of the content being compared. Although such research suggests that the criterion for similarity judgement varies with respect to the musical style of the content being compared, the specific musical factors which shape this criterion are unknown. Since dimensional complexity differentiates musical genres, and has been shown to affect similarity judgements following lifelong exposure, this experiment investigates the short-term influence of dimensional complexity on similarity judgements. Rhythmic and pitch sequences with two levels of complexity were factorially combined to create four distinct types of prototype melodies. 51 participants rated the similarity of each type of prototype melody (M) to two variations, one in which the pitch content was modified (Mp), and another in which the rhythmic content was modified (Mr). The results indicate that rhythm and pitch complexity both play a significant role, influencing the perceived similarity of Mp, and Mr. The dimension bearing low complexity information was found to be the predominant factor in similarity judgements, as participants found modifications to this dimension to significantly decrease perceived similarity. © 2019 Jeff Ens, Bernhard E. Riecke, Philippe Pasquier.","","Dimensional complexity; Music similarity; Musical genre; Short term; Similarity judgements; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Deng J.; Kwok Y.-K.","Deng, Junqi (56379399000); Kwok, Yu-Kwong (7101857718)","56379399000; 7101857718","Large vocabulary automatic chord estimation with an even chance training scheme","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057936249&partnerID=40&md5=e25576f9b0458ad1fbc4ceb538c76990","Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, Hong Kong","Deng J., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, Hong Kong; Kwok Y.-K., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, Hong Kong","This paper presents a large vocabulary automatic chord estimation system implemented using a bidirectional long short-term memory recurrent neural network trained with a skewed-class-aware scheme. This scheme gives the uncommon chord types much more exposure during the training process. The evaluation results indicate that: compared with a normal training scheme, the proposed scheme can boost the weighted chord symbol recalls of some uncommon chords and significantly improve the average chord quality accuracy, at the expense of the overall weighted chord symbol recall. © 2019 Junqi Deng and Yu-Kwong Kwok.","","Information retrieval; Vocabulary control; Estimation systems; Evaluation results; Large vocabulary; Training process; Training schemes; Recurrent neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Pons J.; Gong R.; Serra X.","Pons, Jordi (57190284265); Gong, Rong (57200496600); Serra, Xavier (55892979900)","57190284265; 57200496600; 55892979900","Score-informed syllable segmentation for a cappella singing voice with convolutional neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054982924&partnerID=40&md5=9ea1d5a2be43efd7ce78670134968982","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper introduces a new score-informed method for the segmentation of jingju a cappella singing phrase into syllables. The proposed method estimates the most likely sequence of syllable boundaries given the estimated syllable onset detection function (ODF) and its score. Throughout the paper, we first examine the jingju syllables structure and propose a definition of the term ""syllable onset"". Then, we identify which are the challenges that jingju a cappella singing poses. Further, we investigate how to improve the syllable ODF estimation with convolutional neural networks (CNNs). We propose a novel CNN architecture that allows to efficiently capture different timefrequency scales for estimating syllable onsets. Besides, we propose using a score-informed Viterbi algorithm - instead of thresholding the onset function-, because the available musical knowledge we have (the score) can be used to inform the Viterbi algorithm to overcome the identified challenges. The proposed method outperforms the state-of-the-art in syllable segmentation for jingju a cappella singing. We further provide an analysis of the segmentation errors which points possible research directions. © 2019 Jordi Pons, Rong Gong and Xavier Serra.","","Convolution; Information retrieval; Neural networks; Convolutional neural network; Onset detection; Segmentation error; Singing voices; State of the art; Syllable boundaries; Syllable segmentation; Time frequency; Viterbi algorithm","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Viraraghavan V.S.; Aravind R.; Murthy H.A.","Viraraghavan, Venkata Subramanian (57200146255); Aravind, R. (55930722800); Murthy, Hema A. (57200197348)","57200146255; 55930722800; 57200197348","A statistical analysis of gamakas in Carnatic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064663524&partnerID=40&md5=696515bdd523802e790dd9c79f6f4dfb","TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India; Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India","Viraraghavan V.S., TCS Research and Innovation, Embedded Systems and Robotics, Bangalore, India, Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Aravind R., Department of Electrical Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Department of Computer Science and Engineering, Indian Institute of Technology, Madras, India","Carnatic Music, a form of classical music prevalent in South India, has a central concept called rāgas, defined as melodic scales and/or a set of characteristic melodic phrases. These definitions also account for the continuous pitch movement in gamakas and micro-tonal adjustments to pitch values. In this paper, we present several statistics of gamakas to arrive at a model of Carnatic music. We draw upon the two-component model of Carnatic Music, which splits it into a slowly varying 'stage' and a detail, called 'dance'. Based on the statistics, we propose slightly altered definitions of two similar components called constant-pitch notes and transients. An automated implementation of these definitions is used in collecting statistics from 84 concert renditions. We then suggest that the constant-pitch notes and transients can be considered as context and detail respectively of the rāga, but add that both are necessary for defining the rāga. This is verified by performing listening tests on only the constant-pitch notes and transients independently. © 2019 Venkata Subramanian Viraraghavan, R Aravind, Hema A Murthy.","","Information retrieval; Classical musics; Listening tests; Pitch values; South India; Two-component model; Plants (botany)","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Yang L.-C.; Chou S.-Y.; Yang Y.-H.","Yang, Li-Chia (57194868256); Chou, Szu-Yu (56939788900); Yang, Yi-Hsuan (55218558400)","57194868256; 56939788900; 55218558400","Midinet: A convolutional generative adversarial network for symbolic-domain music generation","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","128","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069920583&partnerID=40&md5=5df3b16fa43783161ba76ee75d1796da","Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","Yang L.-C., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Chou S.-Y., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting. © 2019 Li-Chia Yang, Szu-Yu Chou, Yi-Hsuan Yang.","","Audio acoustics; Convolution; Information retrieval; Adversarial networks; Chord sequence; Convolutional neural network; Neural network model; Prior knowledge; User study; Wave forms; Recurrent neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tsai T.J.; Tjoa S.K.; Müller M.","Tsai, T.J. (55752421100); Tjoa, Steven K. (18042682600); Müller, Meinard (7404689873)","55752421100; 18042682600; 7404689873","Make your own accompaniment: Adapting full-mix recordings to match solo-only user recordings","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069946795&partnerID=40&md5=e2b7d197bbe81d8de90d485dbd9c8f95","Harvey Mudd College, Claremont, CA, United States; Galvanize, Inc., San Francisco, CA, United States; International Audio Laboratories Erlangen, Erlangen, Germany","Tsai T.J., Harvey Mudd College, Claremont, CA, United States; Tjoa S.K., Galvanize, Inc., San Francisco, CA, United States; Müller M., International Audio Laboratories Erlangen, Erlangen, Germany","We explore the task of generating an accompaniment track for a musician playing the solo part of a known piece. Unlike previous work in real-time accompaniment, we focus on generating the accompaniment track in an off-line fashion by adapting a full-mix recording (e.g. a professional CD recording or Youtube video) to match the user's tempo preferences. The input to the system is a set of recorded passages of a solo part played by the user (e.g. solo part in a violin concerto). These recordings are contiguous segments of music where the soloist part is active. Based on this input, the system identifies the corresponding passages within a full-mix recording of the same piece (i.e. contains both solo and accompaniment parts), and these passages are temporally warped to run synchronously to the soloonly recordings. The warped passages can serve as accompaniment tracks for the user to play along with at a tempo that matches his or her ability or desired interpretation. As the main technical contribution, we introduce a segmental dynamic time warping algorithm that simultaneously solves both the passage identification and alignment problems. We demonstrate the effectiveness of the proposed system on a pilot data set for classical violin. © 2019 TJ Tsai, Steven K. Tjoa, Meinard Müller.","","Information retrieval; Alignment Problems; Data set; Real-time accompaniment; Segmental dynamic time warping; Technical contribution; YouTube; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Ranjani H.G.; Paramashivan D.; Sreenivas T.V.","Ranjani, H.G. (19933973300); Paramashivan, Deepak (57193580432); Sreenivas, Thippur V. (7003644773)","19933973300; 57193580432; 7003644773","Quantized melodic contours in indian art music perception: Application to transcription","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061030421&partnerID=40&md5=64f3ed6c637296b9329e3301d3b139ac","Dept of ECE, Indian Institute of Science, Bangalore, India; Dept of Music, University of Alberta, Canada","Ranjani H.G., Dept of ECE, Indian Institute of Science, Bangalore, India; Paramashivan D., Dept of Music, University of Alberta, Canada; Sreenivas T.V., Dept of ECE, Indian Institute of Science, Bangalore, India","Rāgas in Indian Art Music have a florid dynamism associated with them. Owing to their inherent structural intricacies, the endeavor of mapping melodic contours to musical notation becomes cumbersome. We explore the potential of mapping, through quantization of melodic contours and listening test of synthesized music, to capture the nuances of rāgas. We address both Hindustani and Carnatic music forms of Indian Art Music. Two quantization schemes are examined using stochastic models of melodic pitch. We attempt to quantify the salience of rāga perception from reconstructed melodic contours. Perception experiments verify that much of the rāga nuances inclusive of the gamaka (subtle ornamentation) structures can be retained by sampling and quantizing critical points of melodic contours. Further, we show application of this result to automatically transcribe melody of Indian Art Music. © 2019 Ranjani, H. G., Deepak Paramashivan, Thippur V. Sreenivas.","","Information retrieval; Mapping; Stochastic systems; Transcription; Listening tests; Music perception; Musical notation; Perception experiment; Quantization schemes; Stochastic models","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Schnell N.; Schwarz D.; Larralde J.; Borghesi R.","Schnell, Norbert (12344577800); Schwarz, Diemo (7102731246); Larralde, Joseph (57196008876); Borghesi, Riccardo (6603522186)","12344577800; 7102731246; 57196008876; 6603522186","PiPo, a plugin interface for afferent data stream processing modules","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057034057&partnerID=40&md5=5160611dd6cab35216d14fc8efaf06aa","UMR STMS, IRCAM-CNRS-UPMC, France","Schnell N., UMR STMS, IRCAM-CNRS-UPMC, France; Schwarz D., UMR STMS, IRCAM-CNRS-UPMC, France; Larralde J., UMR STMS, IRCAM-CNRS-UPMC, France; Borghesi R., UMR STMS, IRCAM-CNRS-UPMC, France","We present PiPo, a plugin API for data stream processing with applications in interactive audio processing and music information retrieval as well as potentially other domains of signal processing. The development of the API has been motivated by our recurrent need to use a set of signal processing modules that extract low-level descriptors from audio and motion data streams in the context of different authoring environments and end-user applications. The API is designed to facilitate both, the development of modules and the integration of modules or module graphs into applications. It formalizes the processing of streams of multidimensional data frames which may represent regularly sampled signals as well as time-tagged events or numeric annotations. As we found it sufficient for the processing of incoming (i.e. afferent) data streams, PiPo modules have a single input and output and can be connected to sequential and parallel processing paths. After laying out the context and motivations, we present the concept and implementation of the PiPo API with a set of modules that allow for extracting low-level descriptors from audio streams. In addition, we describe the integration of the API into host environments such as Max, Juce, and OpenFrameworks. © 2019 Norbert Schnell, Diemo Schwarz, Joseph Larralde, Riccardo Borghesi.","","Audio acoustics; Data handling; Information retrieval; Authoring environments; Data stream processing; End-user applications; Interactive audio; Low level descriptors; Multidimensional data; Music information retrieval; Parallel processing; Audio signal processing","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Pauwels J.; O'Hanlon K.; Fazekas G.; Sandler M.B.","Pauwels, Johan (35113648700); O'Hanlon, Ken (55311607600); Fazekas, György (37107520200); Sandler, Mark B. (7202740804)","35113648700; 55311607600; 37107520200; 7202740804","Confidence measures and their applications in music labelling systems based on hidden markov models","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060938609&partnerID=40&md5=ce677d3c7aed8f3da2c82933140e2017","Centre for Digital Music, Queen Mary University of London, United Kingdom","Pauwels J., Centre for Digital Music, Queen Mary University of London, United Kingdom; O'Hanlon K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Inspired by previous work on confidence measures for tempo estimation in loops, we explore ways to add confidence measures to other music labelling tasks. We start by reflecting on the reasons why the work on loops was successful and argue that it is an example of the ideal scenario in which it is possible to define a confidence measure independently of the estimation algorithm. This requires additional domain knowledge not used by the estimation algorithm, which is rarely available. Therefore we move our focus to defining confidence measures for hidden Markov models, a technique used in multiple music information retrieval systems and beyond. We propose two measures that are oblivious to the specific labelling task, trading off performance for computational requirements. They are experimentally validated by means of a chord estimation task. Finally, we have a look at alternative uses of confidence measures, besides those applications that require a high precision rather than a high recall, such as most query retrievals. © 2019 Johan Pauwels, Ken O'Hanlon, György Fazekas, Mark B. Sandler.","","Information retrieval systems; Labels; Search engines; Computational requirements; Confidence Measure; Domain knowledge; Estimation algorithm; Labelling systems; Music information retrieval; Query retrieval; Tempo estimations; Hidden Markov models","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Salamon J.; Bittner R.M.; Bonada J.; Bosch J.J.; Gómez E.; Bello J.P.","Salamon, Justin (55184866100); Bittner, Rachel M. (55659619600); Bonada, Jordi (16199826200); Bosch, Juan J. (36760347300); Gómez, Emilia (14015483200); Bello, Juan Pablo (7102889110)","55184866100; 55659619600; 16199826200; 36760347300; 14015483200; 7102889110","An analysis/synthesis framework for automatic f0 annotation of multitrack datasets","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054290573&partnerID=40&md5=dda0f1252a3f1530a84f32afd85b8c56","Music and Audio Research Laboratory, New York University, United States; Music Technology Group, Universitat Pompeu Fabra, Spain","Salamon J., Music and Audio Research Laboratory, New York University, United States; Bittner R.M., Music and Audio Research Laboratory, New York University, United States; Bonada J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bosch J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Generating continuous f0 annotations for tasks such as melody extraction and multiple f0 estimation typically involves running a monophonic pitch tracker on each track of a multitrack recording and manually correcting any estimation errors. This process is labor intensive and time consuming, and consequently existing annotated datasets are very limited in size. In this paper we propose a framework for automatically generating continuous f0 annotations without requiring manual refinement: the estimate of a pitch tracker is used to drive an analysis/synthesis pipeline which produces a synthesized version of the track. Any estimation errors are now reflected in the synthesized audio, meaning the tracker's output represents an accurate annotation. Analysis is performed using a wide-band harmonic sinusoidal modeling algorithm which estimates the frequency, amplitude and phase of every harmonic, meaning the synthesized track closely resembles the original in terms of timbre and dynamics. Finally the synthesized track is automatically mixed back into the multitrack. The framework can be used to annotate multitrack datasets for training learning-based algorithms. Furthermore, we show that algorithms evaluated on the automatically generated/annotated mixes produce results that are statistically indistinguishable from those they produce on the original, manually annotated, mixes. We release a software library implementing the proposed framework, along with new datasets for melody, bass and multiple f0 estimation. © 2019 Justin Salamon, Rachel M. Bittner, Jordi Bonada, Juan J. Bosch, Emilia Gómez.","","Analysis/synthesis; Annotated datasets; Automatically generated; Learning-based algorithms; Melody extractions; Multi-track recording; Multiple-F0 estimations; Software libraries; Information retrieval","J. Salamon; Music and Audio Research Laboratory, New York University, United States; email: justin.salamon@nyu.edu","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Losorelli S.; Nguyen D.T.; Dmochowski J.P.; Kaneshiro B.","Losorelli, Steven (56790026500); Nguyen, Duc T. (57204283611); Dmochowski, Jacek P. (35742934900); Kaneshiro, Blair (56884405500)","56790026500; 57204283611; 35742934900; 56884405500","NMED-T: A tempo-focused dataset of cortical and behavioral responses to naturalistic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059990722&partnerID=40&md5=3e0c9adba1ddd1da48c1069bfbc565a5","Center for the Study of Language and Information, Stanford University, United States; Center for Computer Research in Music and Acoustics, Stanford University, United States; Department of Biomedical Engineering, City College of New York, United States","Losorelli S., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States; Nguyen D.T., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States; Dmochowski J.P., Department of Biomedical Engineering, City College of New York, United States; Kaneshiro B., Center for the Study of Language and Information, Stanford University, United States, Center for Computer Research in Music and Acoustics, Stanford University, United States","Understanding human perception of music is foundational to many research topics in Music Information Retrieval (MIR). While the field of MIR has shown a rising interest in the study of brain responses, access to data remains an obstacle. Here we introduce the Naturalistic Music EEG Dataset-Tempo (NMED-T), an open dataset of electrophysiological and behavioral responses collected from 20 participants who heard a set of 10 commercially available musical works. Song stimuli span various genres and tempos, and all contain electronically produced beats in duple meter. Preprocessed and aggregated responses include dense-array EEG and sensorimotor synchronization (tapping) responses, behavioral ratings of the songs, and basic demographic information. These data, along with illustrative analysis code, are published in Matlab format. Raw EEG and tapping data are also made available. In this paper we describe the construction of the dataset, present results from illustrative analyses, and document the format and attributes of the published data. This dataset facilitates reproducible research in neuroscience and cognitive MIR, and points to several possible avenues for future studies on human processing of naturalistic music. © 2019 Steven Losorelli, Duc T. Nguyen, Jacek P. Dmochowski, and Blair Kaneshiro.","","Electrophysiology; Information retrieval; Behavioral response; Brain response; Demographic information; Dense arrays; Human perception; Music information retrieval; Reproducible research; Research topics; Behavioral research","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Arzt A.; Widmer G.","Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","36681791200; 7004342843","Piece identification in classical piano music without reference scores","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892150&partnerID=40&md5=85022d5c99f0d0b8a9508851115708f0","Department of Computational Perception, Johannes Kepler University, Linz, Austria Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In this paper we describe an approach to identify the name of a piece of piano music, based on a short audio excerpt of a performance. Given only a description of the pieces in text format (i.e. no score information is provided), a reference database is automatically compiled by acquiring a number of audio representations (performances of the pieces) from internet sources. These are transcribed, preprocessed, and used to build a reference database via a robust symbolic fingerprinting algorithm, which in turn is used to identify new, incoming queries. The main challenge is the amount of noise that is introduced into the identification process by the music transcription algorithm and the automatic (but possibly suboptimal) choice of performances to represent a piece in the reference database. In a number of experiments we show how to improve the identification performance by increasing redundancy in the reference database and by using a preprocessing step to rate the reference performances regarding their suitability as a representation of the pieces in question. As the results show this approach leads to a robust system that is able to identify piano music with high accuracy - without any need for data annotation or manual data preparation. © 2019 Andreas Arzt, Gerhard Widmer.","","Audio acoustics; Information retrieval; Musical instruments; Query languages; Transcription; Audio representation; Data preparation; Fingerprinting algorithm; Identification process; Internet sources; Music transcription; Pre-processing step; Reference database; Computer music","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Narang K.; Rao P.","Narang, Krish (57210208563); Rao, Preeti (35180193500)","57210208563; 35180193500","Acoustic features for determining goodness of tabla strokes","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055008895&partnerID=40&md5=dd81871d4a946df6a1cd25eadc8647a7","Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Narang K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","The tabla is an essential component of the Hindustani classical music ensemble and therefore a popular choice with musical instrument learners. Early lessons typically target the mastering of individual strokes from the inventory of bols (spoken syllables corresponding to the distinct strokes) via training in the required articulatory gestures on the right and left drums. Exploiting the close links between the articulation, acoustics and perception of tabla strokes, this paper presents a study of the different timbral qualities that correspond to the correct articulation and to identified common misarticulations of the different bols. We present a dataset created out of correctly articulated and distinct categories of misarticulated strokes, all perceptually verified by an expert. We obtain a system that automatically labels a recording as a good or bad sound, and additionally identifies the precise nature of the misarticulation with a view to providing corrective feedback to the player. We find that acoustic features that are sensitive to the relatively small deviations from the good sound due to poorly articulated strokes are not necessarily the features that have proved successful in the recognition of strokes corresponding to distinct tabla bols as required for music transcription. © 2019 Krish Narang and Preeti Rao.","","Acoustic features; Articulatory gestures; Classical musics; Corrective feedbacks; Music transcription; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Van Kranenburg P.; Maessen G.","Van Kranenburg, Peter (35108158000); Maessen, Geert (57210213826)","35108158000; 57210213826","Comparing offertory melodies of five medieval christian chant traditions","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069935360&partnerID=40&md5=9084ed2e4ccb6c2428048b125c40b4e6","Utrecht University, Meertens Institute, Netherlands","Van Kranenburg P., Utrecht University, Meertens Institute, Netherlands; Maessen G.","In this study, we compare the melodies of five medieval chant traditions: Gregorian, Old Roman, Milanese, Beneventan, and Mozarabic. We present a newly created dataset containing several hundreds of offertory melodies, which are the longest and most complex within the total body of chant melodies. For each tradition, we train n-gram language models on a representation of the chants as sequence of chromatic intervals. By computing perplexities of the melodies, we get an indication of the relations between the traditions, revealing the melodies of the Gregorian tradition as most diverse. Next, we perform a classification experiment using global features of the melodies. The choice of features is informed by expert knowledge. We use properties of the intervallic content of the melodies, and properties of the melismas, revealing that significant differences exist between the traditions. For example, the Gregorian melodies contain less step-wise intervals compared to the other repertoires. Finally, we train a classifier on the perplexities as computed with the n-gram models, resulting in a very reliable classifier. © 2019 Peter van Kranenburg, Geert Maessen.","","Information retrieval; Christians; Expert knowledge; Global feature; Gregorian; N-gram language models; N-gram models; Computational linguistics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Dorfer M.; Arzt A.; Widmer G.","Dorfer, Matthias (55516844500); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","55516844500; 36681791200; 7004342843","Learning audio - Sheet music correspondences for score identification and offline alignment","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056619422&partnerID=40&md5=9ed5fe5a5b04cf24255aca3e53f1cdf2","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Dorfer M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Austria","This work addresses the problem of matching short excerpts of audio with their respective counterparts in sheet music images. We show how to employ neural networkbased cross-modality embedding spaces for solving the following two sheet music-related tasks: retrieving the correct piece of sheet music from a database when given a music audio as a search query; and aligning an audio recording of a piece with the corresponding images of sheet music. We demonstrate the feasibility of this in experiments on classical piano music by five different composers (Bach, Haydn, Mozart, Beethoven and Chopin), and additionally provide a discussion on why we expect multi-modal neural networks to be a fruitful paradigm for dealing with sheet music and audio at the same time. © 2019 Matthias Dorfer, Andreas Arzt, Gerhard Widmer.","","Audio recordings; Information retrieval; Query processing; Cross modality; Multi-modal neural networks; Network-based; Offline; Piano music; Search queries; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Wilkins J.; Seetharaman P.; Wahl A.; Pardo B.","Wilkins, Julia (57210189471); Seetharaman, Prem (55391829300); Wahl, Alison (57210187846); Pardo, Bryan (10242155400)","57210189471; 55391829300; 57210187846; 10242155400","Vocalset: A singing voice dataset","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059773103&partnerID=40&md5=cb8373b6c8893782076b2d15a5da7ddf","Computer Science, Northwestern University, Evanston, IL, United States; School of Music, Northwestern University, Evanston, IL, United States; School of Music, Ithaca College, Ithaca, NY, United States","Wilkins J., Computer Science, Northwestern University, Evanston, IL, United States, School of Music, Northwestern University, Evanston, IL, United States; Seetharaman P., Computer Science, Northwestern University, Evanston, IL, United States; Wahl A., School of Music, Northwestern University, Evanston, IL, United States, School of Music, Ithaca College, Ithaca, NY, United States; Pardo B., Computer Science, Northwestern University, Evanston, IL, United States","We present VocalSet, a singing voice dataset of a capella singing. Existing singing voice datasets either do not capture a large range of vocal techniques, have very few singers, or are single-pitch and devoid of musical context. VocalSet captures not only a range of vowels, but also a diverse set of voices on many different vocal techniques, sung in contexts of scales, arpeggios, long tones, and excerpts. VocalSet has recordings of 10.1 hours of 20 professional singers (11 male, 9 female) performing 17 different different vocal techniques. This data will facilitate the development of new machine learning models for singer identification, vocal technique identification, singing generation and other related applications. To illustrate this, we establish baseline results on vocal technique classification and singer identification by training convolutional network classifiers on VocalSet to perform these tasks. © Julia Wilkins, Prem Seetharaman, Alison Wahl, Bryan Pardo.","","Information retrieval; Baseline results; Convolutional networks; In contexts; Machine learning models; Singing voices; Large dataset","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Fonseca E.; Pons J.; Favory X.; Font F.; Bogdanov D.; Ferraro A.; Oramas S.; Porter A.; Serra X.","Fonseca, Eduardo (57209890862); Pons, Jordi (57190284265); Favory, Xavier (56940764000); Font, Frederic (35182806000); Bogdanov, Dmitry (35748642000); Ferraro, Andres (57204663736); Oramas, Sergio (55582112200); Porter, Alastair (55582507300); Serra, Xavier (55892979900)","57209890862; 57190284265; 56940764000; 35182806000; 35748642000; 57204663736; 55582112200; 55582507300; 55892979900","Freesound datasets: A platform for the creation of open audio datasets","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","116","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059741914&partnerID=40&md5=8e1d8b0655e4094c812c10221e2b28f0","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Fonseca E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Favory X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Font F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Ferraro A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Oramas S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community. © 2019 Eduardo Fonseca, Jordi Pons, Xavier Favory, Frederic Font, Dmitry Bogdanov, Andres Ferraro, Sergio Oramas, Alastair Porter, Xavier Serra.","","Information retrieval; Transparency; Collaborative approach; Dynamic character; Online platforms; Proof of concept; Research approach; Research communities; Research impacts; Sound and music computing; Large dataset","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Gupta C.; Grunberg D.; Rao P.; Wang Y.","Gupta, Chitralekha (36023604600); Grunberg, David (35785584800); Rao, Preeti (35180193500); Wang, Ye (36103845200)","36023604600; 35785584800; 35180193500; 36103845200","Towards automatic mispronunciation detection in singing","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054967342&partnerID=40&md5=74ae712ec1f5028fd0c4c2b9c149e773","School of Computing, National University of Singapore, Singapore, Singapore; NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, Singapore; Department of Electrical Engineering, Indian Institute of Technology Bombay, India","Gupta C., School of Computing, National University of Singapore, Singapore, Singapore, NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore, Singapore; Grunberg D., School of Computing, National University of Singapore, Singapore, Singapore; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","A tool for automatic pronunciation evaluation of singing is desirable for those learning a second language. However, efforts to obtain pronunciation rules for such a tool have been hindered by a lack of data; while many spokenword datasets exist that can be used in developing the tool, there are relatively few sung-lyrics datasets for such a purpose. In this paper, we demonstrate a proof-of-principle for automatic pronunciation evaluation in singing using a knowledge-based approach with limited data in an automatic speech recognition (ASR) framework. To demonstrate our approach, we derive mispronunciation rules specific to South-East Asian English accents in singing based on a comparative study of the pronunciation error patterns in singing versus speech. Using training data restricted to American English speech, we evaluate different methods involving the deduced L1-specific (native language) rules for singing. In the absence of L1 phone models, we incorporate the derived pronunciation variations in the ASR framework via a novel approach that combines acoustic models for sub-phonetic segments to represent the missing L1 phones. The word-level assessment achieved by the system on singing and speech is similar, indicating that it is a promising scheme for realizing a full-fledged pronunciation evaluation system for singing in future. © 2019 Chitralekha Gupta, David Grunberg, Preeti Rao, Ye Wang.","","Information retrieval; Knowledge based systems; Telephone sets; American English; Automatic speech recognition; Comparative studies; Knowledge-based approach; Mispronunciation detections; Pronunciation evaluations; Pronunciation variation; Proof of principles; Speech recognition","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Li B.; Dinesh K.; Sharma G.; Duan Z.","Li, Bochen (57191924185); Dinesh, Karthik (56252200200); Sharma, Gaurav (7202756906); Duan, Zhiyao (24450312900)","57191924185; 56252200200; 7202756906; 24450312900","Video-based vibrato detection and analysis for polyphonic string music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059783148&partnerID=40&md5=71c777de0a17aae33b61bbf82eae610d","Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States","Li B., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Dinesh K., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Sharma G., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States; Duan Z., Dept. of Electrical and Computer Engineering, University of Rochester, Rochester, NY, United States","In music performance, vibrato is an important artistic effect, where slight variations in pitch are introduced to add expressiveness and warmth. Automatic vibrato detection and analysis, although well studied for monophonic music, has rarely been explored for polyphonic music, because of the challenge in multi-pitch analysis. We propose a video-based approach for detecting and analyzing vibrato in polyphonic string music. Specifically, we capture the fine motion of the left hand of string players through optical flow analysis of video frames. We explore two methods. The first uses a feature extraction and SVM classification pipeline, and the second is an unsupervised technique based on autocorrelation analysis of the principal motion component. The proposed methods are compared with audio-only methods applied to individual instrument tracks separated from original audio mixture using the score. Experiments show that the proposed video-based methods achieve a significantly higher vibrato detection accuracy than the audio-based methods especially in high polyphony cases. Further experiments also demonstrate the utility of the approach in vibrato rate and extent analysis. © 2019 Bochen Li, Karthik Dinesh, Gaurav Sharma, Zhiyao Duan.","","Information retrieval; Autocorrelation analysis; Detection accuracy; Motion components; Music performance; Optical flow analysis; SVM classification; Unsupervised techniques; Video-based approach; Support vector machines","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Maezawa A.","Maezawa, Akira (35753591100)","35753591100","Fast and accurate: Improving a simple beat tracker with a selectively-applied deep beat identification","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068999934&partnerID=40&md5=800d39b6d3160ae17f5ed628f18e1240","Yamaha Corporation, Japan","Maezawa A., Yamaha Corporation, Japan","In music applications, audio beat tracking is a central component that requires both speed and accuracy, but a fast beat tracker typically has many beat phase errors, while an accurate one typically requires more computation. This paper achieves a fast tracking speed and a low beat phase error by applying a slow but accurate beat phase detector at only the most informative spots in a given song, and interpolating the rest by a fast tatum-level tracker. We present (1) a framework for selecting a small subset of the tatum indices that information-theoretically best describes the beat phases of the song, (2) a fast HMM-based beat tracker for tatum tracking, and (3) an accurate but slow beat detector using a deep neural network (DNN). The evaluations demonstrate that the proposed DNN beat phase detection halves the beat phase error of the HMM-based tracker and enables a 98% decrease in the required number of DNN invocations without dropping the accuracy. © 2019 Akira Maezawa.","","Audio acoustics; Errors; Information retrieval; Information theory; Phase comparators; Signal detection; Audio beat tracking; Central component; Fast-Tracking; HMM-based; Music applications; Phase detection; Phase detectors; Phase error; Deep neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tuggener L.; Elezi I.; Schmidhuber J.; Stadelmann T.","Tuggener, Lukas (57203925441); Elezi, Ismail (57192200478); Schmidhuber, Jürgen (7003514621); Stadelmann, Thilo (16069451700)","57203925441; 57192200478; 7003514621; 16069451700","Deep watershed detector for music object recognition","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053595487&partnerID=40&md5=93bb00b1a0e946d0fa2f213b1132c173","ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland; Dept. of Environmental Sciences, Informatics and Statistics, Ca’Foscari University of Venice, Italy; Faculty of Informatics, Università della Svizzera Italiana, Lugano, Switzerland","Tuggener L., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland, Faculty of Informatics, Università della Svizzera Italiana, Lugano, Switzerland; Elezi I., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland, Dept. of Environmental Sciences, Informatics and Statistics, Ca’Foscari University of Venice, Italy; Schmidhuber J., Faculty of Informatics, Università della Svizzera Italiana, Lugano, Switzerland; Stadelmann T., ZHAW Datalab, Zurich University of Applied Sciences, Winterthur, Switzerland","Optical Music Recognition (OMR) is an important and challenging area within music information retrieval, the accurate detection of music symbols in digital images is a core functionality of any OMR pipeline. In this paper, we introduce a novel object detection method, based on synthetic energy maps and the watershed transform, called Deep Watershed Detector (DWD). Our method is specifically tailored to deal with high resolution images that contain a large number of very small objects and is therefore able to process full pages of written music. We present state-of-the-art detection results of common music symbols and show DWD’s ability to work with synthetic scores equally well as with handwritten music. © Lukas Tuggener, Ismail Elezi, Jürgen Schmidhuber, Thilo Stadelmann.","","Information retrieval; Object recognition; Watersheds; Core functionality; Digital image; High resolution image; Music information retrieval; Object detection method; Optical music recognition; State of the art; Watershed transform; Object detection","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Parada-Cabaleiro E.; Batliner A.; Baird A.; Schuller B.W.","Parada-Cabaleiro, Emilia (57195938576); Batliner, Anton (6602152015); Baird, Alice (57191860582); Schuller, Björn W. (6603767415)","57195938576; 6602152015; 57191860582; 6603767415","The SEILS dataset: Symbolically encoded scores in modern-early notation for computational musicology","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069876193&partnerID=40&md5=f75a55e317e04b29e7605884022b82f8","Depart of Complex and Intelligent Systems, University of Passau, Germany; Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; GLAM - Group on Language, Audio and Music, Imperial College London, United Kingdom","Parada-Cabaleiro E., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Batliner A., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Baird A., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany; Schuller B.W., Depart of Complex and Intelligent Systems, University of Passau, Germany, Depart of Embedded Intelligence for Health Care and Wellbeing, Augsburg University, Germany, GLAM - Group on Language, Audio and Music, Imperial College London, United Kingdom","The automatic analysis of notated Renaissance music is restricted by a shortfall in codified repertoire. Thousands of scores have been digitised by music libraries across the world, but the absence of symbolically codified information makes these inaccessible for computational evaluation. Optical Music Recognition (OMR) made great progress in addressing this issue, however, early notation is still an on-going challenge for OMR. To this end, we present the Symbolically Encoded Il Lauro Secco (SEILS) dataset, a new dataset of codified scores for use within computational musicology. We focus on a collection of Italian madrigals from the 16th century, a polyphonic secular a cappella composition characterised by strong musical-linguistic synergies. Thirty madrigals for five unaccompanied voices are presented in modern and early notation, considering a variety of digital formats: Lilypond, Music XML, MIDI, and Finale (a total of 150 symbolically codified scores). Given the musical and poetic value of the chosen repertoire, we aim to promote synergies between computational musicology and linguistics. © 2019 Emilia Parada-Cabaleiro, Anton Batliner, Alice Baird, Björn W. Schuller.","","Linguistics; Automatic analysis; Computational evaluation; Digital format; Music library; Optical music recognition; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Smith J.B.L.; Goto M.","Smith, Jordan B. L. (55582613300); Goto, Masataka (7403505330)","55582613300; 7403505330","Multi-part pattern analysis: Combining structure analysis and source separation to discover intra-part repeated sequences","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892661&partnerID=40&md5=ba40552dfbb04060b97c4ba83fba5ab0","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Structure is usually estimated as a single-level phenomenon with full-texture repeats and homogeneous sections. However, structure is actually multi-dimensional: in a typical piece of music, individual instrument parts can repeat themselves in independent ways, and sections can be homogeneous with respect to several parts or only one part. We propose a novel MIR task, multi-part pattern analysis, that requires the discovery of repeated patterns within instrument parts. To discover repeated patterns in individual voices, we propose an algorithm that applies source separation and then tailors the structure analysis to each estimated source, using a novel technique to resolve transitivity errors. Creating ground truth for this task by hand would be infeasible for a large corpus, so we generate a synthetic corpus from MIDI files. We synthesize audio and produce measure-by-measure descriptions of which instruments are active and which repeat themselves exactly. Lastly, we present a set of appropriate evaluation metrics, and use them to compare our approach to a set of baselines. © 2019 Jordan B. L. Smith, Masataka Goto.","","Acoustic measuring instruments; Information retrieval; Textures; Evaluation metrics; Large corpora; Multi dimensional; Novel techniques; Pattern analysis; Repeated patterns; Repeated sequences; Structure analysis; Source separation","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Medeot G.; Cherla S.; Kosta K.; McVicar M.; Abdallah S.; Selvi M.; Newton-Rex E.; Webster K.","Medeot, Gabriele (57215355631); Cherla, Srikanth (24824198800); Kosta, Katerina (55582193200); McVicar, Matt (36721968600); Abdallah, Samer (11540522000); Selvi, Marco (57215361465); Newton-Rex, Ed (57215346339); Webster, Kevin (9335490700)","57215355631; 24824198800; 55582193200; 36721968600; 11540522000; 57215361465; 57215346339; 9335490700","StructureNet: Inducing structure in generated melodies","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064131047&partnerID=40&md5=344b6d04b7d994cb24e239e13d6a1308","Jukedeck Ltd., London, United Kingdom; Imperial College London, London, United Kingdom","Medeot G., Jukedeck Ltd., London, United Kingdom; Cherla S., Jukedeck Ltd., London, United Kingdom; Kosta K., Jukedeck Ltd., London, United Kingdom; McVicar M., Jukedeck Ltd., London, United Kingdom; Abdallah S., Jukedeck Ltd., London, United Kingdom; Selvi M., Jukedeck Ltd., London, United Kingdom; Newton-Rex E., Jukedeck Ltd., London, United Kingdom; Webster K., Imperial College London, London, United Kingdom","We present the StructureNet - a recurrent neural network for inducing structure in machine-generated compositions. This model resides in a musical structure space and works in tandem with a probabilistic music generation model as a modifying agent. It favourably biases the probabilities of those notes that result in the occurrence of structural elements it has learnt from a dataset. It is extremely flexible in that it is able to work with any such probabilistic model, it works well when training data is limited, and the types of structure it can be made to induce are highly customisable. We demonstrate through our experiments on a subset of the Nottingham dataset that melodies generated by a recurrent neural network based melody model are indeed more structured in the presence of the StructureNet. © McVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster. McVicar, Samer Abdallah, Marco Selvi, Ed Newton-Rex, Kevin Webster.","","Information retrieval; Modifying agents; Musical structures; Probabilistic modeling; Structural elements; Training data; Recurrent neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Suzuki J.; Kitahara T.","Suzuki, Jun'ichi (58833059400); Kitahara, Tetsuro (7201371361)","58833059400; 7201371361","A music player with song selection function for a group of people","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069937952&partnerID=40&md5=8b6c9b2f15e00c0557287125e6ce9ca6","College of Humanities and Sciences, Nihon University, Tokyo, 156-8550, Japan","Suzuki J., College of Humanities and Sciences, Nihon University, Tokyo, 156-8550, Japan; Kitahara T., College of Humanities and Sciences, Nihon University, Tokyo, 156-8550, Japan","There are often situations in which a group of people gather and listen to the same songs. However, majority of existing studies related to music information retrieval (MIR) have focused on personalization for individual users, and there have been only a few studies related to MIR intended for a group of people. Here, we present an Android music player with a music selection function for people who are listening to the same songs in the same place. We assume that each user owns his/her favorite songs on his/her Android device. Once a group of users gathers each user can launch this player on his/her smart- phone. Then, the player running on each device starts to communicate with other devices via Bluetooth. Information about songs stored in every device, along with the playback history, is collected to a device referred to as the master device. Then, the master device estimates each user's preference for every song based on playback history and music similarity. The master device then extracts songs that are highly preferred and sends a command to start playback to the devices storing these songs. Our experimental results demonstrate the successful estimation of music preferences based on music similarity. © 2019 Jun'ichi Suzuki and Tetsuro Kitahara.","","Information retrieval; Smartphones; Music information retrieval; Music players; Music preferences; Music similarity; Personalizations; Selection function; User's preferences; Android (operating system)","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Nishikimi R.; Nakamura E.; Goto M.; Itoyama K.; Yoshii K.","Nishikimi, Ryo (57207989997); Nakamura, Eita (24587601200); Goto, Masataka (7403505330); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120)","57207989997; 24587601200; 7403505330; 18042499100; 7103400120","Scale- And rhythm-aware musical note estimation for vocal F0 trajectories based on a semi-tatum-synchronous hierarchical hidden semi-Markov model","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063509726&partnerID=40&md5=7da047090fed1b9eb692a6107f7bba13","Graduate School of Informatics, Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; RIKEN AIP, Japan","Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan, RIKEN AIP, Japan","This paper presents a statistical method that estimates a sequence of musical notes from a vocal F0 trajectory. Since the onset times and F0s of sung notes are considerably deviated from the discrete tatums and pitches indicated in a musical score, a score model is crucial for improving timefrequency quantization of the F0s. We thus propose a hierarchical hidden semi-Markov model (HHSMM) that combines a score model representing the rhythms and pitches of musical notes with musical scales with an F0 model representing time-frequency deviations from a note sequence specified by a score. In the score model, musical scales are generated stochastically. Note pitches are then generated according to the scales and note onsets are generated according to a Markov process defined on the tatum grid. In the F0 model, onset deviations, smooth note-to-note F0 transitions, and F0 deviations are generated stochastically and added to the note sequence. Given an F0 trajectory, our method estimates the most likely sequence of musical notes while giving more importance on the score model than the F0 model. Experimental results showed that the proposed method outperformed an HMM-based method having no models of scales and rhythms. © 2019 Ryo Nishikimi, Eita Nakamura, Masataka Goto, Kat-sutoshi Itoyama, Kazuyoshi Yoshii.","","Information retrieval; Trajectories; F0 model; Hidden semi-Markov modeling; HMM-based; Most likely; Musical notes; Musical scale; Musical score; Time frequency; Markov processes","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Smith J.B.L.; Chew E.","Smith, Jordan B. L. (55582613300); Chew, Elaine (8706714000)","55582613300; 8706714000","Automatic interpretation of music structure analyses: A validated technique for post-hoc estimation of the rationale for an annotation","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069907682&partnerID=40&md5=912dbf23bd2d27b7aa193b65feaa9577","National Institute of Advanced Industrial Science and Technology (AIST), Japan; Queen Mary University of London, United Kingdom","Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Chew E., Queen Mary University of London, United Kingdom","Annotations of musical structure usually provide a low level of detail: they include boundary locations and section labels, but do not indicate what makes the sections similar or distinct, or what changes in the music at each boundary. For those studying annotated corpora, it would be useful to know the rationale for each annotation, but collecting this information from listeners is burdensome and difficult. We propose a new algorithm for estimating which musical features formed the basis for each part of an annotation. To evaluate our approach, we use a synthetic dataset of music clips, all designed to have ambiguous structure, that was previously used and validated in a psychology experiment. We find that, compared to a previous optimization-based algorithm, our correlation-based approach is better able to predict the rationale for an analysis. Using the best version of our algorithm, we process examples from the SALAMI dataset and demonstrate how we can augment the structure annotation data with estimated rationales, inviting new ways to research and use the data. © 2019 Jordan B. L. Smith, Elaine Chew.","","Level of detail; Music clips; Music structure analysis; Musical features; Musical structures; Optimization-based algorithm; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hung Y.-N.; Yang Y.-H.","Hung, Yun-Ning (57209890181); Yang, Yi-Hsuan (55218558400)","57209890181; 55218558400","Frame-level instrument recognition by timbre and pitch","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062241227&partnerID=40&md5=79e566e865fe4b81becd5d2567e550d2","Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan","Hung Y.-N., Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT Innovation, Academia Sinica, Taipei, Taiwan","Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classification problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy.github.io/instrument-recognition/. © Yun-Ning Hung and Yi-Hsuan Yang.","","Classification (of information); Information retrieval; Neural networks; Automatic transcription; Baseline methods; Convolutional neural network; Instrument recognition; Multi label classification; Music information retrieval; Prediction tasks; Relative rates; Forecasting","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ren I.Y.; Koops H.V.; Volk A.; Swierstra W.","Ren, Iris Yuping (57210197214); Koops, Hendrik Vincent (55925589400); Volk, Anja (30567849900); Swierstra, Wouter (23486958400)","57210197214; 55925589400; 30567849900; 23486958400","In search of the consensus among musical pattern discovery algorithms","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069839829&partnerID=40&md5=d730d8c0d494e13fc0f9307eaa007281","University Utrecht, Netherlands","Ren I.Y., University Utrecht, Netherlands; Koops H.V., University Utrecht, Netherlands; Volk A., University Utrecht, Netherlands; Swierstra W., University Utrecht, Netherlands","Patterns are an essential part of music and there are many different algorithms that aim to discover them. Based on the improvements brought by using data fusion methods to find the consensus of algorithms on other MIR tasks, we hypothesize that fusing the output from musical pattern discovery algorithms will improve the pattern discovery results. In this paper, we explore two methods to combine the pattern output from ten state-of-the-art algorithms using two datasets. Both provide human-annotated patterns as ground truth. We show that finding the consensus among the output of different musical pattern discovery algorithms is challenging for two reasons: First, the number of patterns found by the algorithms exceeds patterns in human annotations by several orders of magnitude, with little agreement on what constitutes a pattern. Second, the algorithms perform inconsistently across different pieces. We show that algorithms lack a consensus with each other. Therefore, it is difficult to harness the collective wisdom of the algorithms to find ground truth patterns. The main contribution of this paper is a meta-analysis of the (dis)similarities among pattern discovery algorithms' output and using the output in two fusion methods. Furthermore, we discuss the implication of our results for the MIREX task. © 2019 Ris Yuping Ren, Hendrik Vincent Koops, Anja Volk, Wouter Swierstra.","","Data fusion; Data fusion methods; Fusion methods; Ground truth; Human annotations; Meta analysis; Orders of magnitude; Pattern discovery; State-of-the-art algorithms; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Laplante A.; Bowman T.D.; Aamar N.","Laplante, Audrey (37110930300); Bowman, Timothy D. (57189183302); Aamar, Nawel (57210206881)","37110930300; 57189183302; 57210206881","""I'm at #osheaga!"": Listening to the backchannel of a music festival on twitter","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055284501&partnerID=40&md5=a51abd74e4682ac675a8f3541e059507","Université de Montréal, Canada; Wayne State University, United States","Laplante A., Université de Montréal, Canada; Bowman T.D., Wayne State University, United States; Aamar N., Université de Montréal, Canada","It has become common practice for audience members to use social media to connect, share, and communicate with each other during events (e.g., sport events, elections, award ceremonies). But how is this backchannel used during a musical event and what does it say about how people engage with the music and the artists performing it? In this paper, we present the result of a study of a dataset composed of 31,140 tweets posted during and around the 10th edition of Osheaga, an important music festival held annually in Montreal. A combination of statistics and qualitative content analysis is used to examine the postings. This allows us to describe the content of these postings (i.e., topics, shared media), the type of message being shared (i.e., opinion, expression, information), and who the authors of these tweets are. © 2019 Audrey Laplante, Timothy D. Bowman, Nawel Aammar.","","Information retrieval; Back channels; Content analysis; Musical events; Social media; Sport events; Social networking (online)","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Wang C.-I.; Mysore G.J.; Dubnov S.","Wang, Cheng-I. (56668972700); Mysore, Gautham J. (24465525500); Dubnov, Shlomo (6602142600)","56668972700; 24465525500; 6602142600","Re-visiting the music segmentation problem with crowdsourcing","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068973259&partnerID=40&md5=6941d245f354ac3a1040d26888baec1d","UCSD, United States; Adobe Research, United States","Wang C.-I., UCSD, United States; Mysore G.J., Adobe Research, United States; Dubnov S., UCSD, United States","Identifying boundaries in music structural segmentation is a well studied music information retrieval problem. The goal is to develop algorithms that automatically identify segmenting time points in music that closely matches human annotated data. The annotation itself is challenging due to its subjective nature, such as the degree of change that constitutes a boundary, the location of such boundaries, and whether a boundary should be assigned to a single time frame or a range of frames. Existing datasets have been annotated by small number of experts and the annotators tend to be constrained to specific definitions of segmentation boundaries. In this paper, we re-examine the annotation problem. We crowdsource the problem to a large number of annotators and present an analysis of the results. Our preliminary study suggests that although there is a correlation to existing datasets, this form of annotations reveals additional information such as stronger vs. weaker boundaries, gradual vs. sudden boundaries, and the difference in perception of boundaries between musicians and non-musicians. The study suggests that it could be worth re-defining certain aspects of the boundary identification in music structural segmentation problem with a broader definition. © 2019 Cheng-I Wang, Gautham J. Mysore, Shlomo Dubnov.","","Information retrieval; Boundary identification; Degree of change; Music information retrieval; Music segmentations; Segmentation boundaries; Time frame; Time points; Crowdsourcing","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Meseguer-Brocal G.; Cohen-Hadria A.; Peeters G.","Meseguer-Brocal, Gabriel (57210191728); Cohen-Hadria, Alice (57195434186); Peeters, Geoffroy (22433836000)","57210191728; 57195434186; 22433836000","DALI: A large dataset of synchronized audio, lyrics and notes, automatically created using teacher-student machine learning paradigm","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064115409&partnerID=40&md5=a013152de6c6a5a93e8af19c843c97af","Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France","Meseguer-Brocal G., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Cohen-Hadria A., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France; Peeters G., Ircam Lab, CNRS, Sorbonne Université, Ministère de la Culture, Paris, F-75004, France","The goal of this paper is twofold. First, we introduce DALI, a large and rich multimodal dataset containing 5358 audio tracks with their time-aligned vocal melody notes and lyrics at four levels of granularity. The second goal is to explain our methodology where dataset creation and learning models interact using a teacher-student machine learning paradigm that benefits each other. We start with a set of manual annotations of draft time-aligned lyrics and notes made by non-expert users of Karaoke games. This set comes without audio. Therefore, we need to find the corresponding audio and adapt the annotations to it. To that end, we retrieve audio candidates from the Web. Each candidate is then turned into a singing-voice probability over time using a teacher, a deep convolutional neural network singing-voice detection system (SVD), trained on cleaned data. Comparing the time-aligned lyrics and the singing-voice probability, we detect matches and update the time-alignment lyrics accordingly. From this, we obtain new audio sets. They are then used to train new SVD students used to perform again the above comparison. The process could be repeated iteratively. We show that this allows to progressively improve the performances of our SVD and get better audio-matching and alignment. © Gabriel Meseguer-Brocal, Alice Cohen-Hadria, Geoffroy Peeters.","","Deep neural networks; Information retrieval; Iterative methods; Machine learning; Neural networks; Speech recognition; Students; Audio matching; Convolutional neural network; Learning models; Manual annotation; Multi-modal dataset; Singing voice detection; Singing voices; Time alignment; Large dataset","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Nakamura E.; Yoshii K.; Katayose H.","Nakamura, Eita (24587601200); Yoshii, Kazuyoshi (7103400120); Katayose, Haruhiro (8341539100)","24587601200; 7103400120; 8341539100","Performance error detection and post-processing for fast and accurate symbolic music alignment","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054228461&partnerID=40&md5=60c66c609d3d4bd7912509aad4d40323","Kyoto University, Japan; Kyoto University, RIKEN AIP, Japan; Kwansei Gakuin University, Japan","Nakamura E., Kyoto University, Japan; Yoshii K., Kyoto University, RIKEN AIP, Japan; Katayose H., Kwansei Gakuin University, Japan","This paper presents a fast and accurate alignment method for polyphonic symbolic music signals. It is known that to accurately align piano performances, methods using the voice structure are needed. However, such methods typically have high computational cost and they are applicable only when prior voice information is given. It is pointed out that alignment errors are typically accompanied by performance errors in the aligned signal. This suggests the possibility of correcting (or realigning) preliminary results by a fast (but not-so-accurate) alignment method with a refined method applied to limited segments of aligned signals, to save the computational cost. To realise this, we develop a method for detecting performance errors and a realignment method that works fast and accurately in local regions around performance errors. To remove the dependence on prior voice information, voice separation is performed to the reference signal in the local regions. By applying our method to results obtained by previously proposed hidden Markov models, the highest accuracies are achieved with short computation time. Our source code is published in the accompanying web page, together with a user interface to examine and correct alignment results. © 2019 Eita Nakamura, Kazuyoshi Yoshii, Haruhiro Katayose.","","Alignment; Hidden Markov models; Information retrieval; User interfaces; Websites; Alignment methods; Computation time; Computational costs; Performance error; Post processing; Reference signals; Voice informations; Voice separation; Errors","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Srinivasamurthy A.; Holzapfel A.; Serra X.","Srinivasamurthy, Ajay (55583336200); Holzapfel, Andre (18041818000); Serra, Xavier (55892979900)","55583336200; 18041818000; 55892979900","Informed automatic meter analysis of music recordings","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069915655&partnerID=40&md5=ece231d57c47d9c8e641ea1bf5fb3790","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Media Technology and Interaction Design Department, KTH Royal Institute of Technology, Stockholm, Sweden","Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Media Technology and Interaction Design Department, KTH Royal Institute of Technology, Stockholm, Sweden; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Automatic meter analysis aims to annotate a recording of a metered piece of music with its metrical structure. This analysis subsumes correct estimation of the type of meter, the tempo, and the alignment of the metrical structure with the music signal. Recently, Bayesian models have been successfully applied to several of meter analysis tasks, but depending on the musical context, meter analysis still poses significant challenges. In this paper, we investigate if there are benefits to automatic meter analysis from additional a priori information about the metrical structure of music. We explore informed automatic meter analysis, in which varying levels of prior information about the metrical structure of the music piece is available to analysis algorithms. We formulate different informed meter analysis tasks and discuss their practical applications, with a focus on Indian art music. We then adapt state of the art Bayesian meter analysis methods to these tasks and evaluate them on corpora of Indian art music. The experiments show that the use of additional information aids meter analysis and improves automatic meter analysis performance, with significant gains for analysis of downbeats. © 2019 Ajay Srinivasamurthy, Andre Holzapfel, Xavier Serra.","","Bayesian networks; Information retrieval; Analysis algorithms; Analysis method; Bayesian model; Music recording; Music signals; Prior information; Priori information; State of the art; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Waloschek S.; Hadjakos A.","Waloschek, Simon (57191268983); Hadjakos, Aristotelis (35113080700)","57191268983; 35113080700","Driftin’ down the scale: Dynamic time warping in the presence of pitch drift and transpositions","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060935316&partnerID=40&md5=c530b58b4c75b4efe0a1342e90031d57","Center of Music and Film Informatics, Detmold University of Music, Germany","Waloschek S., Center of Music and Film Informatics, Detmold University of Music, Germany; Hadjakos A., Center of Music and Film Informatics, Detmold University of Music, Germany","Recordings of a cappella music often exhibit significant pitch drift. This drift may accumulate over time to a total transposition of several semitones, which renders the canonical 2-dimensional Dynamic Time Warping (DTW) useless. We propose Transposition-Aware Dynamic Time Warping (TA-DTW), an approach that introduces a 3rd dimension to DTW. Steps in this dimension represent changes in transposition. Paired with suitable input features, TA-DTW computes an optimal alignment path between a symbolic score and a corresponding audio recording in the presence of pitch drift or arbitrary transpositions. © Simon Waloschek, Aristotelis Hadjakos.","","Information retrieval; Dynamic time warping; Input features; Optimal alignments; Audio recordings","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Park S.; Kim T.; Lee K.; Kwak N.","Park, Sungheon (57022528300); Kim, Taehoon (57199878191); Lee, Kyogu (8597995500); Kwak, Nojun (7005248772)","57022528300; 57199878191; 8597995500; 7005248772","Music source separation using stacked hourglass networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062230883&partnerID=40&md5=d0c1e1d786a2e509b5a00999fd3fb15b","Graduate School of Convergence Science and Technology, Seoul National University","Park S., Graduate School of Convergence Science and Technology, Seoul National University; Kim T., Graduate School of Convergence Science and Technology, Seoul National University; Lee K., Graduate School of Convergence Science and Technology, Seoul National University; Kwak N., Graduate School of Convergence Science and Technology, Seoul National University","In this paper, we propose a simple yet effective method for multiple music source separation using convolutional neural networks. Stacked hourglass network, which was originally designed for human pose estimation in natural images, is applied to a music source separation task. The network learns features from a spectrogram image across multiple scales and generates masks for each music source. The estimated mask is refined as it passes over stacked hourglass modules. The proposed framework is able to separate multiple music sources using a single network. Experimental results on MIR-1K and DSD100 datasets validate that the proposed method achieves competitive results comparable to the state-of-the-art methods in multiple music source separation and singing voice separation tasks. © Sungheon Park, Taehoon Kim, Kyogu Lee, Nojun Kwak.","","Information retrieval; Neural networks; Separation; Convolutional neural network; Human pose estimations; Multiple scale; Music source separations; Natural images; Singing voice separations; Single networks; State-of-the-art methods; Source separation","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ganguli K.K.; Rao P.","Ganguli, Kaustuv Kanti (56094514100); Rao, Preeti (35180193500)","56094514100; 35180193500","Towards computational modeling of the ungrammatical in a raga performance","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065210909&partnerID=40&md5=6d75df9c5217d063ddb8f0f090f00c91","Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Raga performance allows for considerable flexibility in interpretation of the raga grammar in order to incorporate elements of creativity via improvisation. It is therefore of much interest in pedagogy to understand what ungrammaticality might mean in the context of a given raga, and possibly develop means to detect this in an audio recording of the raga performance. One prominent notion is that ungrammaticality is considered to occur only when the performer ""treads"" on another, possibly allied, raga in a listener's perception. With this view, we consider modeling the technical boundary of a raga as that which separates it from another raga that is closest to it in its distinctive features. We wish to find computational models that can indicate ungrammaticality using a data-driven estimation of the model parameters; i.e. the raga performances of great artists are used to obtain representations that discriminate most between same and different raga performances. We choose a well-known pair of allied ragas (Deshkar and Bhupali in north Indian classical music) for an empirical study of computational representations for the distinctive attributes of tonal hierarchy and melodic shape of a chosen common descending phrase. © 2019 Kaustuv Kanti Ganguli and Preeti Rao.","","Computation theory; Information retrieval; Computational model; Data driven; Empirical studies; Indian classical music; Model parameters; Audio recordings","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Lu W.-T.; Su L.","Lu, Wei-Tsung (57207994024); Su, Li (55966919100)","57207994024; 55966919100","Vocal melody extraction with semantic segmentation and audio-symbolic domain transfer learning","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063526017&partnerID=40&md5=682a10ca9e511cef1676f3ffcdecf263","Institute of Information Science, Academia Sinica, Taiwan","Lu W.-T., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","The melody extraction problem is analogue to semantic segmentation on a time-frequency image, in which every pixel on the image is classified as a part of a melody object or not. Such an approach can benefit from a signal processing method that helps to enhance the true pitch contours on an image, and, a music language model with structural information on large-scale symbolic music data to be transfer into an audio-based model. In this paper, we propose a novel melody extraction system, using a deep convolutional neural network (DCNN) with dilated convolution as the semantic segmentation tool. The candidate pitch contours on the time-frequency image are enhanced by combining the spectrogram and cepstral-based features. Moreover, an adaptive progressive neural network is employed to transfer the semantic segmentation model in the symbolic domain to the one in the audio domain. This paper makes an attempt to bridge the semantic gaps between signal-level features and perceived melodies, and between symbolic data and audio data. Experiments show competitive accuracy of the proposed method on various datasets. © Wei-Tsung Lu and Li Su.","","Audio acoustics; Convolution; Deep neural networks; Extraction; Image enhancement; Information retrieval; Neural networks; Semantics; Convolutional neural network; Domain transfers; Language model; Melody extractions; Pitch contours; Semantic segmentation; Structural information; Time-frequency images; Image segmentation","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Sebastian J.; Murthy H.A.","Sebastian, Jilt (56637828700); Murthy, Hema A. (57200197348)","56637828700; 57200197348","Onset detection in composition items of Carnatic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069819332&partnerID=40&md5=0026186adf8b4d288a0374292fd5e6bf","Indian Institute of Technology, Madras, India","Sebastian J., Indian Institute of Technology, Madras, India; Murthy H.A., Indian Institute of Technology, Madras, India","Complex rhythmic patterns associated with Carnatic music are revealed from the stroke locations of percussion instruments. However, a comprehensive approach for the detection of these locations from composition items is lacking. This is a challenging problem since the melodic sounds (typically vocal and violin) generate soft-onset locations which result in a number of false alarms. In this work, a separation-driven onset detection approach is proposed. Percussive separation is performed using a Deep Recurrent Neural Network (DRNN) in the first stage. A single model is used to separate the percussive vs the non-percussive sounds using discriminative training and time-frequency masking. This is then followed by an onset detection stage based on group delay (GD) processing on the separated percussive track. The proposed approach is evaluated on a large dataset of live Carnatic music concert recordings and compared against percussive separation and onset detection baselines. The separation performance is significantly better than that of Harmonic- Percussive Separation (HPS) algorithm and onset detection performance is better than the state-of-the-art Convolutional Neural Network (CNN) based algorithm. The proposed approach has an absolute improvement of 18.4% compared with the detection algorithm applied directly on the composition items. © 2019 Jilt Sebastian, Hema A. Murthy.","","Group delay; Information retrieval; Large dataset; Location; Musical instruments; Recurrent neural networks; Convolutional neural network; Detection algorithm; Discriminative training; Number of false alarms; Percussion instruments; Rhythmic patterns; Separation performance; Time-Frequency Masking; Separation","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Langhabel J.; Lieck R.; Toussaint M.; Rohrmeier M.","Langhabel, Jonas (57210209636); Lieck, Robert (55308121500); Toussaint, Marc (7006246144); Rohrmeier, Martin (6507901506)","57210209636; 55308121500; 7006246144; 6507901506","Feature discovery for sequential prediction of monophonic music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069848051&partnerID=40&md5=e1f9d6f70082ef76cab942bb5e12d561","Department of Computer Science, TU Berlin, Germany; Machine Learning and Robotics Lab, University of Stuttgart, Germany; Systematic Musicology and Music Cognition, TU Dresden, Germany; Digital and Cognitive Musicology Lab, EPFL, Switzerland","Langhabel J., Department of Computer Science, TU Berlin, Germany; Lieck R., Machine Learning and Robotics Lab, University of Stuttgart, Germany, Systematic Musicology and Music Cognition, TU Dresden, Germany; Toussaint M., Machine Learning and Robotics Lab, University of Stuttgart, Germany; Rohrmeier M., Systematic Musicology and Music Cognition, TU Dresden, Germany, Digital and Cognitive Musicology Lab, EPFL, Switzerland","Learning a model for sequential prediction of symbolic music remains an open challenge. An important special case is the prediction of pitch sequences based on a corpus of monophonic music. We contribute to this line of research in two respects: (1) Our models improve the stateof- the-art performance. (2) Our method affords learning interpretable models by discovering an explicit set of relevant features. We discover features using the PULSE learning framework, which repetitively suggests new candidate features using a generative operation and selects features while optimizing the underlying model. Defining a domain-specific generative operation allows to combine multiple music-theoretically motivated features in a unified model and to control their interaction on a fine-grained level. We evaluate our models on a set of benchmark corpora of monophonic chorales and folk songs, outperforming previous work. Finally, we discuss the characteristics of the discovered features from a musicological perspective, giving concrete examples. © 2019 Jonas Langhabel, Robert Lieck, Marc Toussaint, Martin Rohrmeier.","","Forecasting; Information retrieval; Domain specific; Feature discovery; Learning frameworks; Monophonic music; Relevant features; Sequential prediction; State-of-the-art performance; Unified Modeling; Learning systems","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Tralie C.J.","Tralie, Christopher J. (55921094200)","55921094200","Early MFCC and HPCP fusion for robust cover song identification","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058527360&partnerID=40&md5=f91d16fbc397dc58204cbb4242c8c5eb","Duke University Department of Electrical and Computer Engineering, United States","Tralie C.J., Duke University Department of Electrical and Computer Engineering, United States","While most schemes for automatic cover song identification have focused on note-based features such as HPCP and chord profiles, a few recent papers surprisingly showed that local self-similarities of MFCC-based features also have classification power for this task. Since MFCC and HPCP capture complementary information, we design an unsupervised algorithm that combines normalized, beatsynchronous blocks of these features using cross-similarity fusion before attempting to locally align a pair of songs. As an added bonus, our scheme naturally incorporates structural information in each song to fill in alignment gaps where both feature sets fail. We show a striking jump in performance over MFCC and HPCP alone, achieving a state of the art mean reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new medium-sized hand designed benchmark dataset called ""Covers 1000,"" which consists of 395 cliques of cover songs for a total of 1000 songs, and we show that our algorithm achieves an MRR of 0.9 on this dataset for the first correctly identified song in a clique. We provide the precomputed HPCP and MFCC features, as well as beat intervals, for all songs in the Covers 1000 dataset for use in further research. © 2019 Christopher J. Tralie.","","Benchmark datasets; Classification power; Cover song identifications; Local selfsimilarity; Mean reciprocal ranks; State of the art; Structural information; Unsupervised algorithms; Information retrieval","C.J. Tralie; Duke University Department of Electrical and Computer Engineering, United States; email: ctralie@alumni.princeton.edu","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Gkiokas A.; Katsouros V.","Gkiokas, Aggelos (14035320300); Katsouros, Vassilis (6507518296)","14035320300; 6507518296","Convolutional neural networks for real-time beat tracking: A dancing robot application","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069811176&partnerID=40&md5=d6777e697f1645628d9924a4f04652cf","Institute for Language and Speech Processing, Athena Research and Innovation Center, Athens, Greece","Gkiokas A., Institute for Language and Speech Processing, Athena Research and Innovation Center, Athens, Greece; Katsouros V., Institute for Language and Speech Processing, Athena Research and Innovation Center, Athens, Greece","In this paper a novel approach that adopts Convolutional Neural Networks (CNN) for the Beat Tracking task is proposed. The proposed architecture involves 2 convolutional layers with the CNN filter dimensions corresponding to time and band frequencies, in order to learn a Beat Activation Function (BAF) from a time-frequency representation. The output of each convolutional layer is computed only over the past values of the previous layer, to enable the computation of the BAF in an online fashion. The output of the CNN is post-processed by a dynamic programming algorithm in combination with a bank of resonators for calculating the salient rhythmic periodicities. The proposed method has been designed to be computational efficient in order to be embedded on a dancing NAO robot application, where the dance moves of the choreography are synchronized with the beat tracking output. The proposed system was submitted to the Signal Processing Cup Challenge 2017 and ranked among the top third algorithms. © 2019 Aggelos Gkiokas and Vassilis Katsouros.","","Convolution; Information retrieval; Neural networks; Signal processing; Activation functions; Band frequencies; Beat tracking; Convolutional neural network; Dynamic programming algorithm; On-line fashion; Proposed architectures; Time-frequency representations; Dynamic programming","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Dong H.-W.; Yang Y.-H.","Dong, Hao-Wen (57205542731); Yang, Yi-Hsuan (55218558400)","57205542731; 55218558400","Convolutional generative adversarial networks with binary neurons for polyphonic music generation","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062221767&partnerID=40&md5=93ca92bd23325fb4bb16ad53b747e79f","Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","Dong H.-W., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for IT innovation, Academia Sinica, Taipei, Taiwan","It has been shown recently that deep convolutional generative adversarial networks (GANs) can learn to generate music in the form of piano-rolls, which represent music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing, such as hard thresholding (HT) or Bernoulli sampling (BS), to obtain the final binary-valued results. In this paper, we study whether we can have a convolutional GAN model that directly creates binary-valued piano-rolls by using binary neurons. Specifically, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network is trained in two stages. Firstly, the generator and the discriminator are pretrained. Then, the refiner network is trained along with the discriminator to learn to binarize the real-valued piano-rolls the pretrained generator creates. Experimental results show that using binary neurons instead of HT or BS indeed leads to better results in a number of objective measures. Moreover, deterministic binary neurons perform better than stochastic ones in both objective measures and a subjective test. The source code, training data and audio examples of the generated results can be found at https://salu133445.github.io/bmusegan/. © Hao-Wen Dong and Yi-Hsuan Yang.","","Convolution; Information retrieval; Musical instruments; Refining; Stochastic systems; Adversarial networks; Binary neurons; Hard thresholding; Objective measure; Output layer; Polyphonic music; Post processing; Training data; Neurons","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Teng Y.; Zhao A.; Goudeseune C.","Teng, Yifei (57210203046); Zhao, Anny (57210210096); Goudeseune, Camille (6504445678)","57210203046; 57210210096; 6504445678","Generating nontrivial melodies for music as a service","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906814&partnerID=40&md5=efb76f10627bc620d85e7e97c599944f","U. of Illinois, Dept. of ECE, United States; U. of Illinois, Beckman Inst., United States","Teng Y., U. of Illinois, Dept. of ECE, United States; Zhao A., U. of Illinois, Dept. of ECE, United States; Goudeseune C., U. of Illinois, Beckman Inst., United States","We present a hybrid neural network and rule-based system that generates pop music. Music produced by pure rule-based systems often sounds mechanical. Music produced by machine learning sounds better, but still lacks hierarchical temporal structure. We restore temporal hierarchy by augmenting machine learning with a temporal production grammar, which generates the music's overall structure and chord progressions. A compatible melody is then generated by a conditional variational recurrent autoencoder. The autoencoder is trained with eight-measure segments from a corpus of 10,000 MIDI files, each of which has had its melody track and chord progressions identified heuristically. The autoencoder maps melody into a multi-dimensional feature space, conditioned by the underlying chord progression. A melody is then generated by feeding a random sample from that space to the autoencoder's decoder, along with the chord progression generated by the grammar. The autoencoder can make musically plausible variations on an existing melody, suitable for recurring motifs. It can also reharmonize a melody to a new chord progression, keeping the rhythm and contour. The generated music compares favorably with that generated by other academic and commercial software designed for the music-as-a-service industry. © 2019 Yifei Teng, Anny Zhao, Camille Goudeseune.","","Information retrieval; Auto encoders; Commercial software; Feature space; Hybrid neural networks; Multi dimensional; Random sample; Service industries; Temporal structures; Machine learning","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Li S.; Dixon S.; Plumbley M.D.","Li, Shengchen (37039722900); Dixon, Simon (7201479437); Plumbley, Mark D. (6603774794)","37039722900; 7201479437; 6603774794","Clustering expressive timing with regressed polynomial coefficients demonstrated by a model selection test","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056144567&partnerID=40&md5=cdc8e3fde30ded30671464e84955e369","Beijing University of Posts and Telecommunications, China; Queen Mary University of London, United Kingdom; University of Surrey, United Kingdom","Li S., Beijing University of Posts and Telecommunications, China; Dixon S., Queen Mary University of London, United Kingdom; Plumbley M.D., University of Surrey, United Kingdom","Though many past works have tried to cluster expressive timing within a phrase, there have been few attempts to cluster features of expressive timing with constant dimensions regardless of phrase lengths. For example, used as a way to represent expressive timing, tempo curves can be regressed by a polynomial function such that the number of regressed polynomial coefficients remains constant with a given order regardless of phrase lengths. In this paper, clustering the regressed polynomial coefficients is proposed for expressive timing analysis. A model selection test is presented to compare Gaussian Mixture Models (GMMs) fitting regressed polynomial coefficients and fitting expressive timing directly. As there are no expected results of clustering expressive timing, the proposed method is demonstrated by how well the expressive timing are approximated by the centroids of GMMs. The results show that GMMs fitting the regressed polynomial coefficients outperform GMMs fitting expressive timing directly. This conclusion suggests that it is possible to use regressed polynomial coefficients to represent expressive timing within a phrase and cluster expressive timing within phrases of different lengths. © 2019 Shengchen Li, Simon Dixon, Mark D. Plumbley.","","Information retrieval; Timing circuits; Cluster feature; Gaussian mixture model (GMMs); Model Selection; Polynomial coefficients; Polynomial functions; Timing Analysis; Polynomials","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Crestel L.; Esling P.; Heng L.; McAdams S.","Crestel, Léopold (57210214369); Esling, Philippe (36052697700); Heng, Lena (57210205712); McAdams, Stephen (55763888700)","57210214369; 36052697700; 57210205712; 55763888700","A database linking piano and orchestral midi scores with application to automatic projective orchestration","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936840&partnerID=40&md5=640a5e75390a5f0747977b3e324f1577","Music Representations, IRCAM, Paris, France; Schulich School of Music, McGill University, Montréal, QC, Canada","Crestel L., Music Representations, IRCAM, Paris, France; Esling P., Music Representations, IRCAM, Paris, France; Heng L., Schulich School of Music, McGill University, Montréal, QC, Canada; McAdams S., Schulich School of Music, McGill University, Montréal, QC, Canada","This article introduces the Projective Orchestral Database (POD), a collection of MIDI scores composed of pairs linking piano scores to their corresponding orchestrations. To the best of our knowledge, this is the first database of its kind, which performs piano or orchestral prediction, but more importantly which tries to learn the correlations between piano and orchestral scores. Hence, we also introduce the projective orchestration task, which consists in learning how to perform the automatic orchestration of a piano score. We show how this task can be addressed using learning methods and also provide methodological guidelines in order to properly use this database. © 2019 Léopold Crestel, Philippe Esling, Lena Heng, Stephen McAdams.","","Database systems; Information retrieval; Learning systems; Learning methods; Methodological guidelines; Musical instruments","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Lim H.; Rhyu S.; Lee K.","Lim, Hyungui (57210208134); Rhyu, Seungyeon (57210211262); Lee, Kyogu (8597995500)","57210208134; 57210211262; 8597995500","Chord generation from symbolic melody using blstm networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062227482&partnerID=40&md5=e8f1136a5e56f618044ffab8e4d23305","Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea; Center for Super Intelligence, Seoul National University, South Korea","Lim H., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea, Center for Super Intelligence, Seoul National University, South Korea; Rhyu S., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, South Korea, Center for Super Intelligence, Seoul National University, South Korea","Generating a chord progression from a monophonic melody is a challenging problem because a chord progression requires a series of layered notes played simultaneously. This paper presents a novel method of generating chord sequences from a symbolic melody using bidirectional long short-term memory (BLSTM) networks trained on a lead sheet database. To this end, a group of feature vectors composed of 12 semitones is extracted from the notes in each bar of monophonic melodies. In order to ensure that the data shares uniform key and duration characteristics, the key and the time signatures of the vectors are normalized. The BLSTM networks then learn from the data to incorporate the temporal dependencies to produce a chord progression. Both quantitative and qualitative evaluations are conducted by comparing the proposed method with the conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8% and 11.4% performance increase from the other models, respectively. User studies further confirm that the chord sequences generated by the proposed method are preferred by listeners. © 2019 Hyungui Lim, Seungyeon Ryu and Kyogu Lee.","","Chord generation; Chord sequence; Data share; Feature vectors; HMM-based; Qualitative evaluations; User study; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Calvo-Zaragoza J.; Rizo D.","Calvo-Zaragoza, Jorge (55847598300); Rizo, David (14042169500)","55847598300; 14042169500","Camera-primus: Neural end-to-end optical music recognition on realistic monophonic scores","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052792251&partnerID=40&md5=961ee6797dd26984831c13c62328f514","PRHLT Research Center, Universitat Politècnica de València, Spain; Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana (ISEA.CV), Universidad de Alicante, Spain","Calvo-Zaragoza J., PRHLT Research Center, Universitat Politècnica de València, Spain; Rizo D., Instituto Superior de Enseñanzas Artísticas de la Comunidad Valenciana (ISEA.CV), Universidad de Alicante, Spain","The optical music recognition (OMR) field studies how to automate the process of reading the musical notation present in a given image. Among its many uses, an interesting scenario is that in which a score captured with a camera is to be automatically reproduced. Recent approaches to OMR have shown that the use of deep neural networks allows important advances in the field. However, these approaches have been evaluated on images with ideal conditions, which do not correspond to the previous scenario. In this work, we evaluate the performance of an end-to-end approach that uses a deep convolutional recurrent neural network (CRNN) over non-ideal image conditions of music scores. Consequently, our contribution also consists of Camera-PrIMuS, a corpus of printed monophonic scores of real music synthetically modified to resemble camera-based realistic scenarios, involving distortions such as irregular lighting, rotations, or blurring. Our results confirm that the CRNN is able to successfully solve the task under these conditions, obtaining an error around 2% at music-symbol level, thereby representing a groundbreaking piece of research towards useful OMR systems. © Jorge Calvo-Zaragoza, David Rizo.","","Cameras; Deep neural networks; Information retrieval; Camera-based; Field studies; Monophonic score; Music scores; Musical notation; Non ideals; Optical music recognition; Realistic scenario; Recurrent neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Fang J.; Grunberg D.; Litman D.; Wang Y.","Fang, Jiakun (57202504362); Grunberg, David (35785584800); Litman, Diane (7004334087); Wang, Ye (36103845200)","57202504362; 35785584800; 7004334087; 36103845200","Discourse analysis of lyric and lyric-based classification of music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064572335&partnerID=40&md5=347f757b17524cff597812105e807780","School of Computing, National University of Singapore, Singapore, Singapore; Department of Computer Science, University of Pittsburgh, United States","Fang J., School of Computing, National University of Singapore, Singapore, Singapore; Grunberg D., School of Computing, National University of Singapore, Singapore, Singapore; Litman D., Department of Computer Science, University of Pittsburgh, United States; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","Lyrics play an important role in the semantics and the structure of many pieces of music. However, while many existing lyric analysis systems consider each sentence of a given set of lyrics separately, lyrics are more naturally understood as multi-sentence units, where the relations between sentences is a key factor. Here we describe a series of experiments using discourse-based features, which describe the relations between different sentences within a set of lyrics, for several common Music Information Retrieval tasks. We first investigate genre recognition and present evidence that incorporating discourse features allow for more accurate genre classification than singlesentence lyric features do. Similarly, we examine the problem of release date estimation by passing features to classifiers to determine the release period of a particular song, and again determine that an assistance from discoursebased features allow for superior classification relative to single-sentence lyric features alone. These results suggest that discourse-based features are potentially useful for Music Information Retrieval tasks. © 2019 Jiakun Fang, David Grunberg, Diane Litman, Ye Wang.","","Information retrieval; Semantics; Analysis system; Discourse analysis; Genre classification; Key factors; Music information retrieval; Release date; Classification (of information)","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Liu M.; Hu X.; Schedl M.","Liu, Meijun (57194541576); Hu, Xiao (55496358400); Schedl, Markus (8684865900)","57194541576; 55496358400; 8684865900","Artist preferences and cultural, socio-economic distances across countries: A big data perspective","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058481801&partnerID=40&md5=d07d14f90eb3591577dcb61f1c76af99","University of Hong Kong, Hong Kong, Hong Kong; Johannes Kepler University Linz, Austria","Liu M., University of Hong Kong, Hong Kong, Hong Kong; Hu X., University of Hong Kong, Hong Kong, Hong Kong; Schedl M., Johannes Kepler University Linz, Austria","Users in different countries may have different music preferences, possibly due to geographical, economic, linguistic, and cultural factors. Revealing the relationship between music preference and cultural socio-economic differences across countries is of great importance for music information retrieval in a cross-country or cross-cultural context. Existing works are usually based on small samples in one or several countries or take only one or two socio-economic aspects into account. To bridge the gap, this study makes use of a large-scale music listening dataset, LFM-1b with more than one billion music listening logs, to explore possible associations between a variety of cultural and socio-economic measurements and artist preferences in 20 countries. From a big data perspective, the results reveal: 1) there is a highly uneven distribution of preferred artists across countries; 2) the linguistic differences among these countries are positively associated with the distances in artist preferences; 3) country differences in three of the six cultural dimensions considered in this study have positive influences on the difference of artist preferences among the countries; and 4) geographical and economic distances among the countries have no significant relationship with their artist preferences. © 2019 Meijun Liu, Xiao Hu, Markus Schedl.","","Information retrieval; Large dataset; Linguistics; Country differences; Cultural context; Cultural dimensions; Cultural factors; Linguistic differences; Music information retrieval; Music preferences; Socioeconomic aspects; Economic analysis","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Chen T.-P.; Su L.","Chen, Tsung-Ping (57207993823); Su, Li (55966919100)","57207993823; 55966919100","Functional harmony recognition of symbolic music data with multi-task recurrent neural networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064106041&partnerID=40&md5=4d0f25fa01bfdec99d55f509fdc23b96","Institute of Information Science, Academia Sinica, Taiwan","Chen T.-P., Institute of Information Science, Academia Sinica, Taiwan; Su L., Institute of Information Science, Academia Sinica, Taiwan","Previous works on chord recognition mainly focus on chord symbols but overlook other essential features that matter in musical harmony. To tackle the functional harmony recognition problem, we compile a new professionally annotated dataset of symbolic music encompassing not only chord symbols, but also various interrelated chord functions such as key modulation, chord inversion, secondary chords, and chord quality. We further present a novel holistic system in functional harmony recognition; a multi-task learning (MTL) architecture is implemented with the recurrent neural network (RNN) to jointly model chord functions in an end-to-end scenario. Experimental results highlight the capability of the proposed recognition system, and a promising improvement of the system by employing multi-task learning instead of single-task learning. This is one attempt to challenge the end-to-end chord recognition task from the perspective of functional harmony so as to uncover the grand structure ruling the flow of musical sound. The dataset and the source code of the proposed system is announced at https://github.com/Tsung-Ping/functional-harmony. © Tsung-Ping Chen and Li Su.","","Information retrieval; Learning systems; Chord recognition; Essential features; Multitask learning; Musical sounds; Recognition systems; Recurrent neural network (RNN); Single task learning; Source codes; Recurrent neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Ariga S.; Fukayama S.; Goto M.","Ariga, Shunya (57195936337); Fukayama, Satoru (56407004300); Goto, Masataka (7403505330)","57195936337; 56407004300; 7403505330","Song2Guitar: A difficulty-aware arrangement system for generating guitar solo covers from polyphonic audio of popular music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069872079&partnerID=40&md5=87fb63b36810c00aa026a2e6e682f63f","University of Tokyo, Japan; AIST, Japan","Ariga S., University of Tokyo, Japan; Fukayama S., AIST, Japan; Goto M., AIST, Japan","This paper describes Song2Guitar which automatically generates difficulty-aware guitar solo cover of popular music from its acoustic signals. Previous research has utilized hidden Markov models (HMMs) to generate playable guitar piece from music scores. Our Song2Guitar extends the framework by leveraging MIR technologies so that it can handle beats, chords and melodies extracted from polyphonic audio. Furthermore, since it is important to generate a guitar piece to meet the skill of a player, Song2Guitar generates guitar solo covers in consideration of playing difficulty. We conducted a data-driven investigation to find what factor makes a guitar piece difficult to play, and restricted Song2Guitar to use certain hand forms adaptively so that the player can play the piece without experiencing too much difficulty. The user interface of Song2Guitar is also implemented and is used to conduct user tests. The results indicated that Song2Guitar succeeded in generating guitar solo covers from polyphonic audio with various playing difficulties. © 2019 Shunya Ariga, Satoru Fukayama, Masataka Goto.","","Hidden Markov models; Information retrieval; Musical instruments; User interfaces; Acoustic signals; Data driven; Hidden markov models (HMMs); Music scores; Popular music; User tests; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Brunner G.; Konrad A.; Wang Y.; Wattenhofer R.","Brunner, Gino (57202464689); Konrad, Andres (57210183201); Wang, Yuyi (57194039783); Wattenhofer, Roger (6701529043)","57202464689; 57210183201; 57194039783; 6701529043","MiDI-VAE: Modeling dynamics and instrumentation of music with applications to style transfer","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","40","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062228576&partnerID=40&md5=7a67171304a500b145090efddd3cb82f","Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland","Brunner G., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Konrad A., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Wang Y., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland; Wattenhofer R., Department of Electrical Engineering and Information Technology, ETH Zurich, Switzerland","We introduce MIDI-VAE, a neural network model based on Variational Autoencoders that is capable of handling polyphonic music with multiple instrument tracks, as well as modeling the dynamics of music by incorporating note durations and velocities. We show that MIDI-VAE can perform style transfer on symbolic music by automatically changing pitches, dynamics and instruments of a music piece from, e.g., a Classical to a Jazz style. We evaluate the efficacy of the style transfer by training separate style validation classifiers. Our model can also interpolate between short pieces of music, produce medleys and create mixtures of entire songs. The interpolations smoothly change pitches, dynamics and instrumentation to create a harmonic bridge between two music pieces. To the best of our knowledge, this work represents the first successful attempt at applying neural style transfer to complete musical compositions. © Gino Brunner, Andres Konrad, Yuyi Wang, Roger Wattenhofer.","","Dynamics; Information retrieval; Autoencoders; Model dynamics; Multiple instruments; Musical composition; Neural network model; Polyphonic music; Computer music","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Sears D.R.W.; Arzt A.; Frostel H.; Sonnleitner R.; Widmer G.","Sears, David R.W. (55874470000); Arzt, Andreas (36681791200); Frostel, Harald (55760080400); Sonnleitner, Reinhard (55566668400); Widmer, Gerhard (7004342843)","55874470000; 36681791200; 55760080400; 55566668400; 7004342843","Modeling harmony with skip-grams","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069869482&partnerID=40&md5=ae9de89945ed6e185b9a9e6033c9a29b","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Sears D.R.W., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Frostel H., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","String-based (or viewpoint) models of tonal harmony often struggle with data sparsity in pattern discovery and prediction tasks, particularly when modeling composite events like triads and seventh chords, since the number of distinct n-note combinations in polyphonic textures is potentially enormous. To address this problem, this study examines the efficacy of skip-grams in music research, an alternative viewpoint method developed in corpus linguistics and natural language processing that includes sub-sequences of n events (or n-grams) in a frequency distribution if their constituent members occur within a certain number of skips. Using a corpus consisting of four datasets of Western classical music in symbolic form, we found that including skip-grams reduces data sparsity in n-gram distributions by (1) minimizing the proportion of n-grams with negligible counts, and (2) increasing the coverage of contiguous n-grams in a test corpus. What is more, skip-grams significantly outperformed contiguous n-grams in discovering conventional closing progressions (called cadences). © 2019 Sears, Arzt, Frostel, Sonnleitner, Widmer.","","Information retrieval; Natural language processing systems; Textures; Classical musics; Corpus linguistics; Frequency distributions; Model composites; NAtural language processing; Pattern discovery; Polyphonic texture; Prediction tasks; Computational linguistics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Gururani S.; Summers C.; Lerch A.","Gururani, Siddharth (57195432595); Summers, Cameron (57190489636); Lerch, Alexander (22034963000)","57195432595; 57190489636; 22034963000","Instrument activity detection in polyphonic music using deep neural networks","2018","Proceedings of the 19th International Society for Music Information Retrieval Conference, ISMIR 2018","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064129027&partnerID=40&md5=11a113040c15a61a3e54701af82f704d","Center for Music Technology, Georgia Institute of Technology, United States; Gracenote, Emeryville, United States","Gururani S., Center for Music Technology, Georgia Institute of Technology, United States; Summers C., Gracenote, Emeryville, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","Although instrument recognition has been thoroughly research, recognition in polyphonic music still faces challenges. While most research in polyphonic instrument recognition focuses on predicting the predominant instruments in a given audio recording, instrument activity detection represents a generalized problem of detecting the presence or activity of instruments in a track on a fine-grained temporal scale. We present an approach for instrument activity detection in polyphonic music with temporal resolution ranging from one second to the track level. This system allows, for instance, to retrieve specific areas of interest such as guitar solos. Three classes of deep neural networks are trained to detect up to 18 instruments. The architectures investigated in this paper are: multi-layer perceptrons, convolutional neural networks, and convolutional-recurrent neural networks. An in-depth evaluation on publicly available multi-track datasets using methods such as AUC-ROC and Label Ranking Average Precision highlights different aspects of the model performance and indicates the importance of using multiple evaluation metrics. Furthermore, we propose a new visualization to discuss instrument confusion in a multi-label scenario. © Siddharth Gururani, Cameron Summers, Alexander Lerch.","","Audio recordings; Convolution; Deep neural networks; Information retrieval; Recurrent neural networks; Activity detection; Convolutional neural network; Depth evaluations; Evaluation metrics; Instrument recognition; Model performance; Multi-layer perceptrons; Temporal resolution; Multilayer neural networks","","","19th International Society for Music Information Retrieval Conference, ISMIR 2018","23 September 2018 through 27 September 2018","Paris","149382"
"Masada K.; Bunescu R.","Masada, Kristen (57210208432); Bunescu, Razvan (8285251100)","57210208432; 8285251100","Chord recognition in symbolic music using semi-Markov conditional random fields","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069863654&partnerID=40&md5=a866a9326c092ab8fef393bebb57c600","School of EECS, Ohio University, Athens, OH, United States","Masada K., School of EECS, Ohio University, Athens, OH, United States; Bunescu R., School of EECS, Ohio University, Athens, OH, United States","Chord recognition is a fundamental task in the harmonic analysis of tonal music, in which music is processed into a sequence of segments such that the notes in each segment are consistent with a corresponding chord label. We propose a machine learning model for chord recognition that uses semi-Markov Conditional Random Fields (semi-CRFs) to perform a joint segmentation and labeling of symbolic music. One benefit of using a semi-Markov model is that it enables the utilization of segment-level features, such as segment purity and chord coverage, that capture the extent to which the events in an entire segment of music are compatible with a candidate chord label. Correspondingly, we develop a rich set of segment-level features for a semi-CRF model that also incorporates the likelihood of a large number of chord-to-chord transitions. Evaluations on a dataset of Bach chorales and a corpus of theme and variations for piano by Beethoven and Mozart show that the proposed semi-CRF model outperforms a discriminatively trained Hidden Markov Model (HMM) that does sequential labeling of sounding events, thus demonstrating the suitability of semi-Markov models for joint segmentation and labeling of music. © 2019 Kristen Masada, Razvan Bunescu.","","Information retrieval; Chord recognition; Conditional random field; Crf models; Joint segmentation; Machine learning models; Semi Markov model; Semi-Markov; Tonal music; Hidden Markov models","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Gang N.; Kaneshiro B.; Berger J.; Dmochowski J.P.","Gang, Nick (57202874053); Kaneshiro, Blair (56884405500); Berger, Jonathan (7403413229); Dmochowski, Jacek P. (35742934900)","57202874053; 56884405500; 7403413229; 35742934900","Decoding neurally relevant musical features using canonical correlation analysis","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069887845&partnerID=40&md5=12188daff92a0cc61288572253f24fc5","Center for Computer Research in Music and Acoustics, Stanford University, United States; Department of Biomedical Engineering, City College of New York, United States","Gang N., Center for Computer Research in Music and Acoustics, Stanford University, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States; Berger J., Center for Computer Research in Music and Acoustics, Stanford University, United States; Dmochowski J.P., Department of Biomedical Engineering, City College of New York, United States","Music Information Retrieval (MIR) has been dominated by computational approaches. The possibility of leveraging neural systems via brain-computer interfaces is an alternative approach to annotating music. Here we test this idea by measuring correlations between musical features and brain responses in a statistically optimal fashion. Using an extensive dataset of electroencephalographic (EEG) responses to a variety of natural music stimuli, we employed Canonical Correlation Analysis to identify spatial EEG components that track temporal stimulus components. We found multiple statistically significant dimensions of stimulus-response correlation (SRC) for all songs studied. The temporal filters that maximize correlation with the neural response highlight harmonics and subharmonics of that song's beat frequency, with different harmonics emphasized by different components. The most stimulus-driven component of the EEG has an anatomically plausible, symmetric frontocentral topography that is preserved across stimuli. Our results suggest that different neural circuits encode different temporal hierarchies of natural music. Moreover, as techniques for decoding EEG advance, it may be possible to automatically label music via brain-computer interfaces that capture neural responses that are then translated into stimulus annotations. © 2019 Nick Gang, Blair Kaneshiro, Jonathan Berger, and Jacek P. Dmochowski.","","Correlation methods; Decoding; Electroencephalography; Information retrieval; Topography; Canonical correlation analysis; Computational approach; Electroencephalographic (EEG); Music information retrieval; Musical features; Neural circuits; Stimulus response; Temporal filters; Brain computer interface","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Bigo L.; Giraud M.; Groult R.; Guiomard-Kagan N.; Levé F.","Bigo, Louis (37057150800); Giraud, Mathieu (8700367400); Groult, Richard (6507884031); Guiomard-Kagan, Nicolas (57210209791); Levé, Florence (55893852300)","37057150800; 8700367400; 6507884031; 57210209791; 55893852300","Sketching sonata form structure in selected classical string quartets","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069824769&partnerID=40&md5=d53f10b4c96a97fadff34ee29ae943fb","CRIStAL, UMR 9189, CNRS, Université de Lille, France; MIS, Université de Picardie Jules Verne, Amiens, France","Bigo L., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Giraud M., CRIStAL, UMR 9189, CNRS, Université de Lille, France; Groult R., MIS, Université de Picardie Jules Verne, Amiens, France; Guiomard-Kagan N., MIS, Université de Picardie Jules Verne, Amiens, France; Levé F., CRIStAL, UMR 9189, CNRS, Université de Lille, France, MIS, Université de Picardie Jules Verne, Amiens, France","Many classical works from 18th and 19th centuries are sonata forms, exhibiting a piece-level tonal path through an exposition, a development and a recapitulation and involving two thematic zones as well as other elements. The computational music analysis of scores with such a largescale structure is a challenge for the MIR community and should gather different analysis techniques. We propose first steps in that direction, combining analysis features on symbolic scores on patterns, harmony, and other elements into a structure estimated by a Viterbi algorithm on a Hidden Markov Model. We test this strategy on a set of first movements of Haydn and Mozart string quartets. The proposed computational analysis strategy finds some pertinent features and sketches the sonata form structure in some pieces that have a simple sonata form. © 2019 Louis Bigo, Mathieu Giraud, Richard Groult, Nicolas Guiomard-Kagan, Florence Levé.","","Hidden Markov models; Viterbi algorithm; 19th century; Computational analysis; Different analysis techniques; Large-scale structure; Music analysis; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Chen N.; Wang S.","Chen, Ning (57061407700); Wang, Shijun (57196046345)","57061407700; 57196046345","High-level music descriptor extraction algorithm based on combination of multi-channel CNNs and LSTM","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061061065&partnerID=40&md5=2e00b4dba9ae41486902f067b0ffebb4","East China University of Science and Technology, China","Chen N., East China University of Science and Technology, China; Wang S., East China University of Science and Technology, China","Although Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTM) have yielded impressive performances in a variety of Music Information Retrieval (MIR) tasks, the complementarity among the CNNs of different architectures and that between CNNs and LSTM are seldom considered. In this paper, multichannel CNNs with different architectures and LSTM are combined into one unified architecture (Multi-Channel Convolutional LSTM, MCCLSTM) to extract high-level music descriptors. First, three channels of CNNs with different shapes of filter are applied on each spectrogram image chunk to extract the pitch-, tempo-, and bassrelevant descriptors, respectively. Then, the outputs of each CNNs channel are concatenated and then passed through a fully connected layer to obtain the fused descriptor. Finally, LSTM is applied on the fused descriptor sequence of the whole track to extract its long-term structure property to obtain the high-level descriptor. To prove the efficiency of the MCCLSTM model, the obtained high-level music descriptor is applied to the music genre classification and emotion prediction task. Experimental results demonstrate that, when compared with the hand-crafted schemes or conventional deep learning (Multi Layer Perceptrons (MLP), CNNs, and LSTM) based ones, MCCLSTM achieves higher prediction accuracy on three music collections with different kinds of semantic tags. © 2019 Ning Chen, Shijun Wang.","","Convolution; Deep learning; Information retrieval; Network architecture; Semantics; Convolutional neural network; Descriptor extractions; High-level descriptor; Long-term structures; Multi-layer perceptrons; Music genre classification; Music information retrieval; Unified architecture; Long short-term memory","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Vogl R.; Dorfer M.; Widmer G.; Knees P.","Vogl, Richard (57190968947); Dorfer, Matthias (55516844500); Widmer, Gerhard (7004342843); Knees, Peter (8219023200)","57190968947; 55516844500; 7004342843; 8219023200","Drum transcription via joint beat and drum modeling using convolutional recurrent neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069918587&partnerID=40&md5=73e91ae536f07943fcf44c0319a040f0","Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Dept. of Computational Perception, Johannes Kepler University Linz, Austria","Vogl R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria, Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University Linz, Austria; Knees P., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Existing systems for automatic transcription of drum tracks from polyphonic music focus on detecting drum instrument onsets but lack consideration of additional meta information like bar boundaries, tempo, and meter. We address this limitation by proposing a system which has the capability to detect drum instrument onsets along with the corresponding beats and downbeats. In this design, the system has the means to utilize information on the rhythmical structure of a song which is closely related to the desired drum transcript. To this end, we introduce and compare different architectures for this task, i.e., recurrent, convolutional, and recurrent-convolutional neural networks. We evaluate our systems on two well-known data sets and an additional new data set containing both drum and beat annotations. We show that convolutional and recurrentconvolutional neural networks perform better than state-ofthe- art methods and that learning beats jointly with drums can be beneficial for the task of drum detection. © 2019 Richard Vogl, Matthias Dorfer, Gerhard Widmer, Peter Knees.","","Convolution; Information retrieval; Transcription; Art methods; Automatic transcription; Convolutional neural network; Data set; Existing systems; Meta information; Polyphonic music; Recurrent neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Laroche C.; Papadopoulos H.; Kowalski M.; Richard G.","Laroche, Clément (57188866032); Papadopoulos, Hélène (24462317300); Kowalski, Matthieu (55127695300); Richard, Gaël (57195915952)","57188866032; 24462317300; 55127695300; 57195915952","Genre specific dictionaries for harmonic/percussive source separation","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030224512&partnerID=40&md5=4ae57f001aa4c4d7dafe8d12a0a3fdb3","LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, Paris, France; Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France; Parietal project-team, INRIA, CEA-Saclay, France","Laroche C., LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, Paris, France, Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France; Papadopoulos H., Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France; Kowalski M., Univ Paris-Sud-CNRS-CentraleSupelec, L2S, Gif-sur-Yvette, France, Parietal project-team, INRIA, CEA-Saclay, France; Richard G., LTCI, CNRS, Télécom ParisTech, Univ Paris-Saclay, Paris, France","Blind source separation usually obtains limited performance on real and polyphonic music signals. To overcome these limitations, it is common to rely on prior knowledge under the form of side information as in Informed Source Separation or on machine learning paradigms applied on a training database. In the context of source separation based on factorization models such as the Non-negative Matrix Factorization, this supervision can be introduced by learning specific dictionaries. However, due to the large diversity of musical signals it is not easy to build sufficiently compact and precise dictionaries that will well characterize the large array of audio sources. In this paper, we argue that it is relevant to construct genre-specific dictionaries. Indeed, we show on a task of harmonic/percussive source separation that the dictionaries built on genre-specific training subsets yield better performances than cross-genre dictionaries. © Clément Laroche, Hélène Papadopoulos, Matthieu Kowalski, Gaël Richard.","","Audio acoustics; Factorization; Information retrieval; Learning systems; Factorization model; Musical signals; Nonnegative matrix factorization; Polyphonic music; Prior knowledge; Side information; Training database; Training subsets; Blind source separation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Repetto R.C.; Serra X.","Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57200495266; 55892979900","A collection of music scores for corpus based jingju singing research","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041393599&partnerID=40&md5=bd39ff80e9b15bf5f36cd41bc2ec940d","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The MIR research on jingju (also known as Beijing or Peking opera) music has taken audio as the main source of information. Music scores are an important resource for the musicological research of this tradition, but no machine readable ones have been available for computational analysis. In order to explore the potential of symbolic score data for jingju music research, we have expanded the Comp- Music Jingju Music Corpus, which contains mostly audio, with a collection of 92 machine readable scores, for a total of 897 melodic lines. Since our purpose is the study of jingju singing in terms of its musical system elements, we have selected the arias used as examples in reference jingju music textbooks. The collection is accompanied by scores metadata, curated annotations per score and melodic line, and a set of software tools for extracting statistical information from it. All the gathered data and developed software are available for research purposes. In this paper we first discuss the culture specific concepts that are needed for understanding the contents of the collection, followed by a detailed description of it. We then present a series of computational analyses performed on the scores and discuss some musicological findings. © 2019 Rafael Caro Repetto, Xavier Serra.","","Information retrieval; Computational analysis; Corpus-based; Music scores; Research purpose; Statistical information; System elements; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Ewert S.; Wang S.; Müller M.; Sandler M.","Ewert, Sebastian (32667575400); Wang, Siying (56939449600); Müller, Meinard (7404689873); Sandler, Mark (7202740804)","32667575400; 56939449600; 7404689873; 7202740804","Score-informed identification of missing and extra notes in piano recordings","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029561826&partnerID=40&md5=90170cc9a72954350067f47057f229ba","Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; International Audio Laboratories, Erlangen, Germany","Ewert S., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Wang S., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Müller M., International Audio Laboratories, Erlangen, Germany; Sandler M., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom","A main goal in music tuition is to enable a student to play a score without mistakes, where common mistakes include missing notes or playing additional extra ones. To automatically detect these mistakes, a first idea is to use a music transcription method to detect notes played in an audio recording and to compare the results with a corresponding score. However, as the number of transcription errors produced by standard methods is often considerably higher than the number of actual mistakes, the results are often of limited use. In contrast, our method exploits that the score already provides rough information about what we seek to detect in the audio, which allows us to construct a tailored transcription method. In particular, we employ score-informed source separation techniques to learn for each score pitch a set of templates capturing the spectral properties of that pitch. After extrapolating the resulting template dictionary to pitches not in the score, we estimate the activity of each MIDI pitch over time. Finally, making again use of the score, we choose for each pitch an individualized threshold to differentiate note onsets from spurious activity in an optimized way. We indicate the accuracy of our approach on a dataset of piano pieces commonly used in education. © Sebastian Ewert, Siying Wang, Meinard Müller and Mark Sandler.","","Audio acoustics; Information retrieval; Source separation; Transcription; Music transcription; Score-informed source separations; Spectral properties; Transcription methods; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Tanaka T.; Bemman B.; Meredith D.","Tanaka, Tsubasa (56300856400); Bemman, Brian (56989782300); Meredith, David (57125703100)","56300856400; 56989782300; 57125703100","Integer programming formulation of the problem of generating Milton Babbitt’s all-partition arrays","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052203043&partnerID=40&md5=9628f58e544418bc1f1f3ffb97ef39fe","STMS Lab, IRCAM, CNRS, UPMC, Paris, France; Aalborg University, Aalborg, Denmark","Tanaka T., STMS Lab, IRCAM, CNRS, UPMC, Paris, France; Bemman B., Aalborg University, Aalborg, Denmark; Meredith D., Aalborg University, Aalborg, Denmark","Milton Babbitt (1916–2011) was a composer of twelve-tone serial music noted for creating the all-partition array. The problem of generating an all-partition array involves finding a rectangular array of pitch-class integers that can be partitioned into regions, each of which represents a distinct integer partition of 12. Integer programming (IP) has proven to be effective for solving such combinatorial problems, however, it has never before been applied to the problem addressed in this paper. We introduce a new way of viewing this problem as one in which restricted overlaps between integer partition regions are allowed. This permits us to describe the problem using a set of linear constraints necessary for IP. In particular, we show that this problem can be defined as a special case of the well-known problem of set-covering (SCP), modified with additional constraints. Due to the difficulty of the problem, we have yet to discover a solution. However, we assess the potential practicality of our method by running it on smaller similar problems. © Tsubasa Tanaka, Brian Bemman, David Meredith.","","Information retrieval; Combinatorial problem; Integer partitions; Integer programming formulations; Linear constraints; Rectangular arrays; Set coverings; Integer programming","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Scholz R.; Ramalho G.; Cabral G.","Scholz, Ricardo (15770403100); Ramalho, Geber (6602152013); Cabral, Giordano (14041251800)","15770403100; 6602152013; 14041251800","Cross task study on mirex recent results: An index for evolution measurement and some stagnation hypotheses","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042800257&partnerID=40&md5=9ebf43182cbf962361d7118869a66559","Universidade Federal de Pernambuco, Recife, PE, Brazil","Scholz R.; Ramalho G., Universidade Federal de Pernambuco, Recife, PE, Brazil; Cabral G.","In the last 20 years, Music Information Retrieval (MIR) has been an expanding research field, and the MIREX competition has become the main evaluation venue in MIR field. Analyzing recent results for various tasks of MIREX (MIR Evaluation eXchange), we observed that the evolution of task solutions follows two different patterns: for some tasks, the results apparently hit stagnation, whereas for others, they seem getting better over time. In this paper, (a) we compile the MIREX results of the last 6 years, (b) we propose a configurable quantitative index for evolution trend measurement of MIREX tasks, and (c) we discuss possible explanations or hypotheses for the stagnation phenomena hitting some of them. This paper hopes to incite a debate in the MIR research community about the progress in the field and how to adequately measure evolution trends. © Ricardo Scholz, Geber Ramalho, Giordano Cabral.","","C (programming language); Information retrieval; Evolution trend; Music information retrieval; Quantitative indices; Research communities; Research fields; Insecticides","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Fields B.; Rhodes C.","Fields, Ben (35733701400); Rhodes, Christophe (57196565939)","35733701400; 57196565939","Listen to me – Don’t listen to me: What can communities of critics tell us about music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069988936&partnerID=40&md5=a799cb5e630077b37db21edc1df23ad5","Dept. of Computing Goldsmiths, University of London, New Cross London, SE14 6NW, United Kingdom","Fields B., Dept. of Computing Goldsmiths, University of London, New Cross London, SE14 6NW, United Kingdom; Rhodes C., Dept. of Computing Goldsmiths, University of London, New Cross London, SE14 6NW, United Kingdom","Social knowledge and data sharing on the Web takes many forms. So too do the ways people share ideas and opinions. In this paper we examine one such emerging form: the amateur critic. In particular, we examine genius.com, a website which allows its users to annotate and explain the meaning of segments of lyrics in music and other written works. We describe a novel dataset of approximately 700,000 users’ activity on genius.com, their social connections, and song annotation activity. The dataset encompasses over 120,000 songs, with more than 3 million unique annotations. Using this dataset, we model overlap in interest or expertise through the proxy of co-annotation. This is the basis for a complex network model of the activity on genius.com, which is then used for community detection. We introduce a new measure of network community activity: community skew. Through this analysis we draw a comparison of between co-annotation and notions of genre and categorisation in music. We show a new view on the social constructs of genre in music. © Ben Fields, Christophe Rhodes.","","Complex networks; Community detection; Complex network models; Data Sharing; Network communities; Social connection; Social Knowledge; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Schramm R.; McLeod A.; Steedman M.; Benetos E.","Schramm, Rodrigo (56259281600); McLeod, Andrew (57114559100); Steedman, Mark (6602901918); Benetos, Emmanouil (16067946900)","56259281600; 57114559100; 6602901918; 16067946900","Multi-pitch detection and voice assignment for a cappella recordings of multiple singers","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037744474&partnerID=40&md5=7d5949e95df1dce57b442efcc961b012","Department of Music, Universidade Federal do Rio Grande do Sul, Brazil; School of Informatics, University of Edinburgh, United Kingdom; Centre for Digital Music, Queen Mary University of London, United Kingdom","Schramm R., Department of Music, Universidade Federal do Rio Grande do Sul, Brazil, Centre for Digital Music, Queen Mary University of London, United Kingdom; McLeod A., School of Informatics, University of Edinburgh, United Kingdom; Steedman M., School of Informatics, University of Edinburgh, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","This paper presents a multi-pitch detection and voice assignment method applied to audio recordings containing a cappella performances with multiple singers. A novel approach combining an acoustic model for multi-pitch detection and a music language model for voice separation and assignment is proposed. The acoustic model is a spectrogram factorization process based on Probabilistic Latent Component Analysis (PLCA), driven by a 6-dimensional dictionary with pre-learned spectral templates. The voice separation component is based on hidden Markov models that use musicological assumptions. By integrating the models, the system can detect multiple concurrent pitches in vocal music and assign each detected pitch to a specific voice corresponding to a voice type such as soprano, alto, tenor or bass (SATB). This work focuses on four-part compositions, and evaluations on recordings of Bach Chorales and Barbershop quartets show that our integrated approach achieves an F-measure of over 70% for frame-based multipitch detection and over 45% for four-voice assignment. © 2019 Rodrigo Schramm, Andrew McLeod, Mark Steedman, Emmanouil Benetos.","","Audio recordings; Hidden Markov models; Information retrieval; Acoustic model; Integrated approach; Language model; Multi pitches; Probabilistic latent component analysis; Process-based; Spectrograms; Voice separation; Speech recognition","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Eghbal-Zadeh H.; Widmer G.","Eghbal-Zadeh, Hamid (37080509500); Widmer, Gerhard (7004342843)","37080509500; 7004342843","Noise robust music artist recognition using I-vector features","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057552407&partnerID=40&md5=4b1922d50d1d492b384f0f84eb486c5a","Department of Computational Perception, Johannes Kepler University of Linz, Austria","Eghbal-Zadeh H., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In music information retrieval (MIR), dealing with different types of noise is important and the MIR models are frequently used in noisy environments such as live performances. Recently, i-vector features have shown great promise for some major tasks in MIR, such as music similarity and artist recognition. In this paper, we introduce a novel noise-robust music artist recognition system using i-vector features. Our method uses a short sample of noise to learn the parameters of noise, then using a Maximum A Postriori (MAP) estimation it estimates clean i-vectors given noisy i-vectors. We examine the performance of multiple systems confronted with different kinds of additive noise in a clean training - noisy testing scenario. Using open-source tools, we have synthesized 12 different noisy versions from a standard 20-class music artist recognition dataset encountered with 4 different kinds of additive noise with 3 different Signal-to-Noise-Ratio (SNR). Using these datasets, we carried out music artist recognition experiments comparing the proposed method with the state-of-the-art. The results suggest that the proposed method outperforms the state-of-the-art. © Hamid Eghbal-zadeh Gerhard Widmer.","","Additive noise; Additives; Information retrieval; Vectors; Multiple systems; Music information retrieval; Music similarity; Noisy environment; Noisy versions; Open source tools; Recognition systems; State of the art; Signal to noise ratio","H. Eghbal-Zadeh; Department of Computational Perception, Johannes Kepler University of Linz, Austria; email: hamid.eghbal-zadeh@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Kirlin P.B.","Kirlin, Phillip B. (55582044600)","55582044600","Global properties of expert and algorithmic hierarchical music analyses","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069990178&partnerID=40&md5=4ce64e2912489fee953365ea867ded40","Department of Mathematics and Computer Science, Rhodes College, United States","Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States","In recent years, advances in machine learning and increases in data set sizes have produced a number of viable algorithms for analyzing music in a hierarchical fashion according to the guidelines of music theory. Many of these algorithms, however, are based on techniques that rely on a series of local decisions to construct a complete music analysis, resulting in analyses that are not guaranteed to resemble ground-truth analyses in their large-scale organizational shapes or structures. In this paper, we examine a number of hierarchical music analysis data sets — drawing from Schenkerian analysis and other analytical systems based on A Generative Theory of Tonal Music — to study three global properties calculated from the shapes of the analyses. The major finding presented in this work is that it is possible for an algorithm that only makes local decisions to produce analyses that resemble expert analyses with regards to the three global properties in question. We also illustrate specific similarities and differences in these properties across both ground-truth and algorithmically-produced analyses. © Phillip B. Kirlin.","","Hierarchical systems; Information retrieval; Analytical systems; Data set size; Expert analysis; Global properties; Ground truth; Local decisions; Music analysis; Music theory; Machine learning","P.B. Kirlin; Department of Mathematics and Computer Science, Rhodes College, United States; email: kirlinp@rhodes.edu","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bas de Haas W.; Volk A.","Bas de Haas, W. (51160955300); Volk, Anja (30567849900)","51160955300; 30567849900","Meter detection in symbolic music using inner metric analysis","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029331271&partnerID=40&md5=f759e40d6e9a6abf9ac12e8d29a4abc7","Utrecht University, Netherlands","Bas de Haas W., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands","In this paper we present PRIMA: a new model tailored to symbolic music that detects the meter and the first downbeat position of a piece. Given onset data, the metrical structure of a piece is interpreted using the Inner Metric Analysis (IMA) model. IMA identifies the strong and weak metrical positions in a piece by performing a periodicity analysis, resulting in a weight profile for the entire piece. Next, we reduce IMA to a feature vector and model the detection of the meter and its first downbeat position probabilistically. In order to solve the meter detection problem effectively, we explore various feature selection and parameter optimisation strategies, including Genetic, Maximum Likelihood, and Expectation-Maximisation algorithms. PRIMA is evaluated on two datasets of MIDI files: a corpus of ragtime pieces, and a newly assembled pop dataset. We show that PRIMA outperforms autocorrelation-based meter detection as implemented in the MIDItoolbox on these datasets. © W. Bas de Haas, Anja Volk.","","Data privacy; Information retrieval; Maximum likelihood; Network security; Detection problems; Expectation-maximisation; Feature vectors; Metric analysis; MIDI files; Parameter optimisation; Periodicity analysis; Weight profile; Feature extraction","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Panteli M.; Dixon S.","Panteli, Maria (55915922100); Dixon, Simon (7201479437)","55915922100; 7201479437","On the evaluation of rhythmic and melodic descriptors for music similarity","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038969135&partnerID=40&md5=69d5f7fed2d41dc3f3b2ad6fb711f47d","Centre for Digital Music, Queen Mary University of London, United Kingdom","Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In exploratory studies of large music collections where often no ground truth is available, it is essential to evaluate the suitability of the underlying methods prior to drawing any conclusions. In this study we focus on the evaluation of audio features that can be used for rhythmic and melodic content description and similarity estimation. We select a set of state-of-the-art rhythmic and melodic descriptors and assess their invariance with respect to transformations of timbre, recording quality, tempo and pitch. We create a dataset of synthesised audio and investigate which features are invariant to the aforementioned transformations and whether invariance is affected by characteristics of the music style and the monophonic or polyphonic character of the audio recording. From the descriptors tested, the scale transform performed best for rhythm classification and retrieval and pitch bihistogram performed best for melody. The proposed evaluation strategy can inform decisions in the feature design process leading to significant improvement in the reliability of the features. © Maria Panteli, Simon Dixon.","","Audio acoustics; Information retrieval; Content description; Evaluation strategies; Exploratory studies; Large music collections; Music similarity; Scale transform; Similarity estimation; State of the art; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Miron M.; Janer J.; Gómez E.","Miron, Marius (55585881400); Janer, Jordi (35068089300); Gómez, Emilia (14015483200)","55585881400; 35068089300; 14015483200","Monaural score-informed source separation for classical music using convolutional neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048592475&partnerID=40&md5=cec65d90e0571d4eb0eeee0f07876500","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Score information has been shown to improve music source separation when included into non-negative matrix factorization (NMF) frameworks. Recently, deep learning approaches have outperformed NMF methods in terms of separation quality and processing time, and there is scope to extend them with score information. In this paper, we propose a score-informed separation system for classical music that is based on deep learning. We propose a method to derive training features from audio files and the corresponding coarsely aligned scores for a set of classical music pieces. Additionally, we introduce a convolutional neural network architecture (CNN) with the goal of estimating time-frequency masks for source separation. Our system is trained with synthetic renditions derived from the original scores and can be used to separate real-life performances based on the same scores, provided a coarse audio-to-score alignment. The proposed system achieves better performance (SDR and SIR) and is less computationally intensive than a score-informed NMF system on a dataset comprising Bach chorales. © 2019 Marius Miron, Jordi Janer, Emilia Gómez.","","Audio acoustics; Convolution; Deep learning; Factorization; Information retrieval; Matrix algebra; Network architecture; Neural networks; Separation; Convolutional neural network; Learning approach; Music source separations; Nonnegative matrix factorization; Real-life performance; Score-informed source separations; Separation systems; Training features; Source separation","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hedges T.; Wiggins G.","Hedges, Thomas (41661532500); Wiggins, Geraint (14032393700)","41661532500; 14032393700","Improving predictions of derived viewpoints in multiple viewpoint systems","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070023128&partnerID=40&md5=c965997cd99b5ceaeb5b0dc3aa1ec5fc","Queen Mary University of London, United Kingdom","Hedges T., Queen Mary University of London, United Kingdom; Wiggins G., Queen Mary University of London, United Kingdom","This paper presents and tests a method for improving the predictive power of derived viewpoints in multiple viewpoints systems. Multiple viewpoint systems are a well established method for the statistical modelling of sequential symbolic musical data. A useful class of viewpoints known as derived viewpoints map symbols from a basic event space to a viewpoint-specific domain. Probability estimates are calculated in the derived viewpoint domain before an inverse function maps back to the basic event space to complete the model. Since an element in the derived viewpoint domain can potentially map onto multiple basic elements, probability mass is distributed between the basic elements with a uniform distribution. As an alternative, this paper proposes a distribution weighted by zero-order frequencies of the basic elements to inform this probability mapping. Results show this improves the predictive performance for certain derived viewpoints, allowing them to be selected in viewpoint selection. © Thomas Hedges, Geraint Wiggins.","","Information retrieval; Inverse functions; Multiple viewpoints; Predictive performance; Probability estimate; Probability mapping; Statistical modelling; Uniform distribution; Viewpoint selection; Probability distributions","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Hu X.; Choi K.; Lee J.H.; Laplante A.; Hao Y.; Cunningham S.J.; Downie J.S.","Hu, Xiao (55496358400); Choi, Kahyun (56452038000); Lee, Jin Ha (57190797465); Laplante, Audrey (37110930300); Hao, Yun (56147494900); Cunningham, Sally Jo (7201937110); Downie, J. Stephen (7102932568)","55496358400; 56452038000; 57190797465; 37110930300; 56147494900; 7201937110; 7102932568","WIMIR: An informetric study on women authors in ISMIR","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069898466&partnerID=40&md5=0191e6d8a4e09e8c82df4f2787a9f1bb","University of Hong Kong, China; University of Illinois, United States; Univeristy of Washington, United States; Université de Montréal, Canada; University of Waikato, New Zealand","Hu X., University of Hong Kong, China; Choi K., University of Illinois, United States; Lee J.H., Univeristy of Washington, United States; Laplante A., Université de Montréal, Canada; Hao Y., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Downie J.S., University of Illinois, United States","The Music Information Retrieval (MIR) community is becoming increasingly aware of a gender imbalance evident in ISMIR participation and publication. This paper reports upon a comprehensive informetric study of the publication, authorship and citation characteristics of female researchers in the context of the ISMIR conferences. All 1,610 papers in the ISMIR proceedings written by 1,910 unique authors from 2000 to 2015 were collected and analyzed. Only 14.1% of all papers were led by female researchers. Temporal analysis shows that the percentage of lead female authors has not improved over the years, but more papers have appeared with female coauthors in very recent years. Topics and citation numbers are also analyzed and compared between female and male authors to identify research emphasis and to measure impact. The results show that the most prolific authors of both genders published similar numbers of ISMIR papers and the citation counts of lead authors in both genders had no significant difference. We also analyzed the collaboration patterns to discover whether gender is related to the number of collaborators. Implications of these findings are discussed and suggestions are proposed on how to continue encouraging and supporting female participation in the MIR field. © Xiao Hu, Kahyun Choi, Jin Ha Lee, Audrey Laplante, Yun Hao, Sally Jo Cunningham and J. Stephen Downie.","","Collaboration patterns; Community IS; Informetric studies; Music information retrieval; Similar numbers; Temporal analysis; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Sonnleitner R.; Arzt A.; Widmer G.","Sonnleitner, Reinhard (55566668400); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","55566668400; 36681791200; 7004342843","Landmark-based audio fingerprinting for DJ mix monitoring","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030224868&partnerID=40&md5=9f44d3cde3307389a7cf5ff647b3300e","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in discotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifications are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data – DJ mixes that were performed in discotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annotations. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher performance on this task than the other methods. © Reinhard Sonnleitner, Andreas Arzt, Gerhard Widmer.","","Computer music; Fading (radio); Information retrieval; Navigation; Audio fingerprinting; Creative Commons; Media monitoring; Pitch shifting; Reference tracks; Research communities; Revenue distributions; Signal modification; Audio systems","R. Sonnleitner; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: reinhard.sonnleitner@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Valle R.; Fremont D.J.; Akkaya I.; Donze A.; Freed A.; Seshia S.S.","Valle, Rafael (56406952300); Fremont, Daniel J. (56383540400); Akkaya, Ilge (55211209400); Donze, Alexandre (22333270000); Freed, Adrian (7006838725); Seshia, Sanjit S. (56434045800)","56406952300; 56383540400; 55211209400; 22333270000; 7006838725; 56434045800","Learning and visualizing music specifications using pattern graphs","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998573&partnerID=40&md5=06fbde7a02a51a177f1825c5763dfe1d","UC Berkeley, CNMAT, United States; UC Berkeley, United States","Valle R., UC Berkeley, CNMAT, United States; Fremont D.J., UC Berkeley, United States; Akkaya I., UC Berkeley, United States; Donze A., UC Berkeley, United States; Freed A., UC Berkeley, CNMAT, United States; Seshia S.S., UC Berkeley, United States","We describe a system to learn and visualize specifications from song(s) in symbolic and audio formats. The core of our approach is based on a software engineering procedure called specification mining. Our procedure extracts patterns from feature vectors and uses them to build pattern graphs. The feature vectors are created by segmenting song(s) and extracting time and and frequency domain features from them, such as chromagrams, chord degree and interval classification. The pattern graphs built on these feature vectors provide the likelihood of a pattern between nodes, as well as start and ending nodes. The pattern graphs learned from a song(s) describe formal specifications that can be used for human interpretable quantitatively and qualitatively song comparison or to perform supervisory control in machine improvisation. We offer results in song summarization, song and style validation and machine improvisation with formal specifications. © Rafael Valle, Daniel J. Fremont, Ilge Akkaya, Alexandre Donze, Adrian Freed, Sanjit S. Seshia.","","Frequency domain analysis; Graphic methods; Information retrieval; Feature vectors; Frequency domains; Machine improvisations; Specification mining; Supervisory control; Formal specification","R. Valle; UC Berkeley, CNMAT, United States; email: rafaelvalle@berkeley.edu","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Michael Winters R.; Gururani S.; Lerch A.","Michael Winters, R. (57204922250); Gururani, Siddharth (57195432595); Lerch, Alexander (22034963000)","57204922250; 57195432595; 22034963000","Automatic practice logging: Introduction, dataset & preliminary study","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070013832&partnerID=40&md5=27084b9fdf0baa61047c95ba623e10ec","Georgia Tech Center for Music Technology (GTCMT), Georgia","Michael Winters R., Georgia Tech Center for Music Technology (GTCMT), Georgia; Gururani S., Georgia Tech Center for Music Technology (GTCMT), Georgia; Lerch A., Georgia Tech Center for Music Technology (GTCMT), Georgia","Musicians spend countless hours practicing their instruments. To document and organize this time, musicians commonly use practice charts to log their practice. However, manual techniques require time, dedication, and experience to master, are prone to fallacy and omission, and ultimately can not describe the subtle variations in each repetition. This paper presents an alternative: by analyzing and classifying the audio recorded while practicing, logging could occur automatically, with levels of detail, accuracy, and ease that would not be possible otherwise. Towards this goal, we introduce the problem of Automatic Practice Logging (APL), including a discussion of the benefits and unique challenges it raises. We then describe a new dataset of over 600 annotated recordings of solo piano practice, which can be used to design and evaluate APL systems. After framing our approach to the problem, we present an algorithm designed to align short segments of practice audio with reference recordings using pitch chroma and dynamic time warping. © R. Michael Winters, Siddharth Gururani, Alexander Lerch.","","Information retrieval; Dynamic time warping; Levels of detail; Manual techniques; Short segments; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Ibrahim K.M.; Grunberg D.; Agres K.; Gupta C.; Wang Y.","Ibrahim, Karim M. (57204045658); Grunberg, David (35785584800); Agres, Kat (25222806800); Gupta, Chitralekha (36023604600); Wang, Ye (36103845200)","57204045658; 35785584800; 25222806800; 36023604600; 36103845200","Intelligibility of sung lyrics: A pilot study","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048552209&partnerID=40&md5=f30f7afa8849eeb3c83e472bd63620eb","Department of Computer Science, National University of Singapore, Singapore, Singapore; Institute of High Performance Computing, ASTAR, Singapore, Singapore","Ibrahim K.M., Department of Computer Science, National University of Singapore, Singapore, Singapore; Grunberg D., Department of Computer Science, National University of Singapore, Singapore, Singapore; Agres K., Institute of High Performance Computing, ASTAR, Singapore, Singapore; Gupta C., Department of Computer Science, National University of Singapore, Singapore, Singapore; Wang Y., Department of Computer Science, National University of Singapore, Singapore, Singapore","We propose a system to automatically assess the intelligibility of sung lyrics. We are particularly interested in being able to identify songs which are intelligible to second language learners, as such individuals often sing along the song to help them learn their second language, but this is only helpful if the song is intelligible enough for them to understand. As no automatic system for identifying 'intelligible' songs currently exists, songs for second language learners are generally selected by hand, a timeconsuming and onerous process. We conducted an experiment in which test subjects, all of whom are learning English as a second language, were presented with 100 excerpts of songs drawn from five different genres. The test subjects listened to and transcribed the excerpts and the intelligibility of each excerpt was assessed based on average transcription accuracy across subjects. Excerpts that were more accurately transcribed on average were considered to be more intelligible than those less accurately transcribed on average. We then tested standard acoustic features to determine which were most strongly correlated with intelligibility. Our final system classifies the intelligibility of the excerpts and achieves 66% accuracy for 3 classes of intelligibility. © 2019 Karim M. Ibrahim, David Grunberg, Kat Agres, Chitralekha Gupta, Ye Wang.","","Information retrieval; Acoustic features; Automatic systems; Learning English; Pilot studies; Second language; Second language learners; Transcription","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Rodríguez-Algarra F.; Sturm B.L.; Maruri-Aguilar H.","Rodríguez-Algarra, Francisco (57210236249); Sturm, Bob L. (14014190500); Maruri-Aguilar, Hugo (23477749400)","57210236249; 14014190500; 23477749400","Analysing scattering-based music content analysis systems: Where’s the music?","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936094&partnerID=40&md5=f288f83d3c4164a42d3c1d378c37b77b","Centre for Digital Music, Queen Mary University of London, United Kingdom; School of Mathematical Sciences, Queen Mary University of London, United Kingdom","Rodríguez-Algarra F., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Maruri-Aguilar H., School of Mathematical Sciences, Queen Mary University of London, United Kingdom","Music content analysis (MCA) systems built using scattering transform features are reported quite successful in the GTZAN benchmark music dataset. In this paper, we seek to answer why. We first analyse the feature extraction and classification components of scattering-based MCA systems. This guides us to perform intervention experiments on three factors: train/test partition, classifier and recording spectrum. The partition intervention shows a decrease in the amount of reproduced ground truth by the resulting systems. We then replace the learning algorithm with a binary decision tree, and identify the impact of specific feature dimensions. We finally alter the spectral content related to such dimensions, which reveals that these scattering-based systems exploit acoustic information below 20 Hz to reproduce GTZAN ground truth. The source code to reproduce our experiments is available online. 1 © Francisco Rodríguez-Algarra, Bob L. Sturm, Hugo Maruri-Aguilar.","","Binary trees; Decision trees; Information retrieval; Acoustic information; Binary decision trees; Feature dimensions; Feature extraction and classification; Music content analysis; Scattering transforms; Source codes; Spectral content; Trees (mathematics)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Weigl D.M.; Page K.R.","Weigl, David M. (55359633400); Page, Kevin R. (8104972900)","55359633400; 8104972900","A framework for distributed semantic annotation of musical score: ""Take it to the bridge!""","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046934871&partnerID=40&md5=4f1e62012e0f4dd9ee68ed8da70a43c8","Oxford e-Research Centre, University of Oxford, United Kingdom","Weigl D.M., Oxford e-Research Centre, University of Oxford, United Kingdom; Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom","Music notation expresses performance instructions in a way commonly understood by musicians, but printed paper parts are limited to encodings of static, a priori knowledge. In this paper we present a platform for multi-way communication between collaborating musicians through the dynamic modification of digital parts: the Music Encoding and Linked Data (MELD) framework for distributed real-time annotation of digital music scores. MELD users and software agents create semantic annotations of music concepts and relationships, which are associated with musical structure specified by the Music Encoding Initiative schema (MEI). Annotations are expressed in RDF, allowing alternative music vocabularies (e.g., popular vs. classical music structures) to be applied. The same underlying framework retrieves, distributes, and processes information that addresses semantically distinguishable music elements. Further knowledge is incorporated from external sources through the use of Linked Data. The RDF is also used to match annotation types and contexts to rendering actions which display the annotations upon the digital score. Here, we present a MELD implementation and deployment which augments the digital music scores used by musicians in a group performance, collaboratively changing the sequence within and between pieces in a set list. © 2019 David M. Weigl and Kevin R. Page.","","Data handling; Encoding (symbols); Information retrieval; Linked data; Semantics; Signal encoding; Software agents; Classical musics; Dynamic modifications; External sources; Group performance; Music notation; Musical structures; Priori knowledge; Semantic annotations; Semantic Web","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Calvo-Zaragoza J.; Valero-Mas J.J.; Pertusa A.","Calvo-Zaragoza, Jorge (55847598300); Valero-Mas, Jose J. (56453252300); Pertusa, Antonio (8540257800)","55847598300; 56453252300; 8540257800","End-to-end Optical Music Recognition using neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045320607&partnerID=40&md5=e30bb7f46f89022d6c317f5e8616a0f2","Centre for Interdisciplinary Research in Music, Media and Technology, McGill University, Montreal, QC, Canada; Software and Computing Systems, University of Alicante, Alicante, Spain","Calvo-Zaragoza J., Centre for Interdisciplinary Research in Music, Media and Technology, McGill University, Montreal, QC, Canada; Valero-Mas J.J., Software and Computing Systems, University of Alicante, Alicante, Spain; Pertusa A., Software and Computing Systems, University of Alicante, Alicante, Spain","This work addresses the Optical Music Recognition (OMR) task in an end-to-end fashion using neural networks. The proposed architecture is based on a Recurrent Convolutional Neural Network topology that takes as input an image of a monophonic score and retrieves a sequence of music symbols as output. In the first stage, a series of convolutional filters are trained to extract meaningful features of the input image, and then a recurrent block models the sequential nature of music. The system is trained using a Connectionist Temporal Classification loss function, which avoids the need for a frame-by-frame alignment between the image and the ground-truth music symbols. Experimentation has been carried on a set of 90,000 synthetic monophonic music scores with more than 50 different possible labels. Results obtained depict classification error rates around 2 % at symbol level, thus proving the potential of the proposed end-to-end architecture for OMR. The source code, dataset, and trained models are publicly released for reproducible research and future comparison purposes. © 2019 Jorge Calvo-Zaragoza, Jose J. Valero-Mas, Antonio Pertusa.","","Convolution; Information retrieval; Network architecture; Classification error rate; Convolutional neural network; Frame alignments; Monophonic score; Optical music recognition; Proposed architectures; Reproducible research; Temporal classification; Recurrent neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Deng J.; Kwok Y.-K.","Deng, Junqi (56379399000); Kwok, Yu-Kwong (7101857718)","56379399000; 7101857718","A hybrid Gaussian-HMM-deep-learning approach for automatic chord estimation with very large vocabulary","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033574575&partnerID=40&md5=ae1d73a1d6ae663e7c25005e2334f19b","Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong","Deng J., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong; Kwok Y.-K., Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong","We propose a hybrid Gaussian-HMM-Deep-Learning approach for automatic chord estimation with very large chord vocabulary. The Gaussian-HMM part is similar to Chordino, which is used as a segmentation engine to divide input audio into note spectrogram segments. Two types of deep learning models are proposed to classify these segments into chord labels, which are then connected as chord sequences. Two sets of evaluations are conducted with two large chord vocabularies. The first evaluation is conducted in a recent MIREX standard way. Results show that our approach has obvious advantage over the state-of-the-art large-vocabulary-with-inversions supportable ACE system in terms of large vocabularies, although is outperformed by in small vocabularies. Through analyzing and deducing system behaviors behind the results, we see interesting chord confusion patterns made by different systems, which conceivably point to a demand of more balanced and consistent annotated datasets for training and testing. The second evaluation preliminarily demonstrates our approach’s superiority on a jazz chord vocabulary with 36 chord types, compared with a Chordino-like Gaussian-HMM baseline system with augmented vocabulary capacity. © Junqi Deng and Yu-Kwong Kwok.","","Gaussian distribution; Information retrieval; Vocabulary control; Annotated datasets; Baseline systems; Large vocabulary; Learning approach; Learning models; State of the art; System behaviors; Training and testing; Deep learning","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bandiera G.; Picas O.R.; Tokuda H.; Hariya W.; Oishi K.; Serra X.","Bandiera, Giuseppe (57210231447); Picas, Oriol Romani (56891419000); Tokuda, Hiroshi (56890530500); Hariya, Wataru (56891729000); Oishi, Koji (56890538400); Serra, Xavier (55892979900)","57210231447; 56891419000; 56890530500; 56891729000; 56890538400; 55892979900","Good-sounds.org: A framework to explore goodness in instrumental sounds","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046885523&partnerID=40&md5=9ec7970529a543d52c6bab9156e44778","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Technology Development Dept., KORG Inc., Tokyo, Japan","Bandiera G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Picas O.R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Tokuda H., Technology Development Dept., KORG Inc., Tokyo, Japan; Hariya W., Technology Development Dept., KORG Inc., Tokyo, Japan; Oishi K., Technology Development Dept., KORG Inc., Tokyo, Japan; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We introduce good-sounds.org, a community driven framework based on freesound.org to explore the concept of goodness in instrumental sounds. Goodness is considered here as the common agreed basic sound quality of an instrument without taking into consideration musical expressiveness. Musicians upload their sounds and vote on existing sounds, and from the collected data the system is able to develop sound goodness measures of relevance for music education applications. The core of the system is a database of sounds, together with audio features extracted from them using MTG’s Essentia library and user annotations related to the goodness of the sounds. The web front-end provides useful data visualizations of the sound attributes and tools to facilitate user interaction. To evaluate the framework, we carried out an experiment to rate sound goodness of single notes of nine orchestral instruments. In it, users rated the sounds using an AB vote over a set of sound attributes defined to be of relevance in the characterization of single notes of instrumental sounds. With the obtained votes we built a ranking of the sounds for each attribute and developed a model that rates the goodness for each of the selected sound attributes. Using this approach, we have succeeded in obtaining results comparable to a model that was built from expert generated evaluations. © Giuseppe Bandiera, Oriol Romani Picas, Hiroshi Tokuda, Wataru Hariya, Koji Oishi, Xavier Serra.","","Information retrieval; Sound reproduction; Audio features; Front end; Music education; Musical expressiveness; Sound Quality; User annotations; User interaction; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Raffel C.; Ellis D.P.W.","Raffel, Colin (55354986300); Ellis, Daniel P.W. (13609089200)","55354986300; 13609089200","Extracting ground truth information from MIDI files: A MIDIfesto","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069988770&partnerID=40&md5=2cf606f83c5c25ff20791200e202ed5b","LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States","Raffel C., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States","MIDI files abound and provide a bounty of information for music informatics. We enumerate the types of information available in MIDI files and describe the steps necessary for utilizing them. We also quantify the reliability of this data by comparing it to human-annotated ground truth. The results suggest that developing better methods to leverage information present in MIDI files will facilitate the creation of MIDI-derived ground truth for audio content-based MIR. © Colin Raffel and Daniel P. W. Ellis.","","Audio content; Ground truth; MIDI files; Music Informatics; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Holzapfel A.; Benetos E.","Holzapfel, Andre (18041818000); Benetos, Emmanouil (16067946900)","18041818000; 16067946900","The sousta corpus: Beat-informed automatic transcription of traditional dance tunes","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028026832&partnerID=40&md5=48483ce8cfb745b095dc3dd1c082fac9","Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Centre for Digital Music, Queen Mary University of London, United Kingdom","Holzapfel A., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper, we present a new corpus for research in computational ethnomusicology and automatic music transcription, consisting of traditional dance tunes from Crete. This rich dataset includes audio recordings, scores transcribed by ethnomusicologists and aligned to the audio performances, and meter annotations. A second contribution of this paper is the creation of an automatic music transcription system able to support the detection of multiple pitches produced by lyra (a bowed string instrument). Furthermore, the transcription system is able to cope with deviations from standard tuning, and provides temporally quantized notes by combining the output of the multi-pitch detection stage with a state-of-the-art meter tracking algorithm. Experiments carried out for note tracking using 25ms onset tolerance reach 41.1% using information from the multi-pitch detection stage only, 54.6% when integrating beat information, and 57.9% when also supporting tuning estimation. The produced meter aligned transcriptions can be used to generate staff notation, a fact that increases the value of the system for studies in ethnomusicology. © Andre Holzapfel, Emmanouil Benetos.","","Audio recordings; Information retrieval; Automatic music transcription; Automatic transcription; Bowed string; Multi pitches; State of the art; Tracking algorithm; Transcription","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Stasiak B.","Stasiak, Bartłomiej (14045944200)","14045944200","DTV-based melody cutting for DTW-based melody search and indexing in QBH systems","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995242&partnerID=40&md5=9ca698b5ac128b0f194c649980b6d064","Institute of Information Technology, Lodz University of Technology, Poland","Stasiak B., Institute of Information Technology, Lodz University of Technology, Poland","Melody analysis is an important processing step in several areas of Music Information Retrieval (MIR). Processing the pitch values extracted from raw input audio signal may be computationally complex as it requires substantial effort to reduce the uncertainty resulting i.a. from tempo variability and transpositions. A typical example is the melody matching problem in Query-by-Humming (QbH) systems, where Dynamic Time Warping (DTW) and note-based approaches are typically applied. In this work we present a new, simple and efficient method of investigating the melody content which may be used for approximate, preliminary matching of melodies irrespective of their tempo and length. The proposed solution is based on Discrete Total Variation (DTV) of the melody pitch vector, which may be computed in linear time. We demonstrate its practical application for finding the appropriate melody cutting points in the R*-tree-based DTW indexing framework. The experimental validation is based on a dataset of 4431 queries and over 4000 template melodies, constructed specially for testing Query-by-Humming systems. © Bartłomiej Stasiak.","","Indexing (of information); Information retrieval; Query processing; Statistical tests; Dynamic time warping; Experimental validations; Indexing framework; Matching problems; Music information retrieval; Processing steps; Query by humming systems; Total variation; Audio signal processing","B. Stasiak; Institute of Information Technology, Lodz University of Technology, Poland; email: bartlomiej.stasiak@p.lodz.pl","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Atherton J.; Kaneshiro B.","Atherton, Jack (57195221009); Kaneshiro, Blair (56884405500)","57195221009; 56884405500","I said it first: Topological analysis of lyrical influence networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069892722&partnerID=40&md5=273b7d5b98a8ab55fdf58909457e4fbe","Center for Computer Research in Music and Acoustics, Stanford University, United States","Atherton J., Center for Computer Research in Music and Acoustics, Stanford University, United States; Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, United States","We present an analysis of musical influence using intact lyrics of over 550,000 songs, extending existing research on lyrics through a novel approach using directed networks. We form networks of lyrical influence over time at the level of three-word phrases, weighted by tf-idf. An edge reduction analysis of strongly connected components suggests highly central artist, songwriter, and genre network topologies. Visualizations of the genre network based on multidimensional scaling confirm network centrality and provide insight into the most influential genres at the heart of the network. Next, we present metrics for influence and self-referential behavior, examining their interactions with network centrality and with the genre diversity of songwriters. Here, we uncover a negative correlation between songwriters’ genre diversity and the robustness of their connections. By examining trends among the data for top genres, songwriters, and artists, we address questions related to clustering, influence, and isolation of nodes in the networks. We conclude by discussing promising future applications of lyrical influence networks in music information retrieval research. The networks constructed in this study are made publicly available for research purposes. © Jack Atherton and Blair Kaneshiro.","","Information retrieval; Future applications; Influence networks; Multi-dimensional scaling; Music information retrieval; Negative correlation; Network centralities; Strongly connected component; Topological analysis; Topology","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Liebman E.; Stone P.; White C.N.","Liebman, Elad (55324775700); Stone, Peter (7203001213); White, Corey N. (23096672900)","55324775700; 7203001213; 23096672900","Impact of music on decision making in quantitative tasks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069862638&partnerID=40&md5=ea2e89d4329b12c86125617d57345d35","Department of Computer Science, University of Texas at Austin, Austria; Department of Psychology, Syracuse University, United States","Liebman E., Department of Computer Science, University of Texas at Austin, Austria; Stone P., Department of Computer Science, University of Texas at Austin, Austria; White C.N., Department of Psychology, Syracuse University, United States","The goal of this study is to explore which aspects of people’s analytical decision making are affected when exposed to music. To this end, we apply a stochastic sequential model of simple decisions, the drift-diffusion model (DDM), to understand risky decision behavior. Numerous studies have demonstrated that mood can affect emotional and cognitive processing, but the exact nature of the impact music has on decision making in quantitative tasks has not been sufficiently studied. In our experiment, participants decided whether to accept or reject multiple bets with different risk vs. reward ratios while listening to music that was chosen to induce positive or negative mood. Our results indicate that music indeed alters people’s behavior in a surprising way - happy music made people make better choices. In other words, it made people more likely to accept good bets and reject bad bets. The DDM decomposition indicated the effect focused primarily on both the caution and the information processing aspects of decision making. To further understand the correspondence between auditory features and decision making, we studied how individual aspects of music affect response patterns. Our results are particularly interesting when compared with recent results regarding the impact of music on emotional processing, as they illustrate that music affects analytical decision making in a fundamentally different way, hinting at a different psychological mechanism that music impacts. © Elad Liebman, Peter Stone, Corey N. White.","","Behavioral research; Information retrieval; Stochastic models; Stochastic systems; Analytical decisions; Auditory feature; Cognitive processing; Drift-diffusion model; Exposed to; Response patterns; Risky decisions; Sequential model; Decision making","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Besson V.; Gurrieri M.; Rigaux P.; Tacaille A.; Thion V.","Besson, Vincent (57225813990); Gurrieri, Marco (57210236466); Rigaux, Philippe (57204372163); Tacaille, Alice (55123176500); Thion, Virginie (55876776700)","57225813990; 57210236466; 57204372163; 55123176500; 55876776700","A methodology for quality assessment in collaborative score libraries","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028053774&partnerID=40&md5=2223293dfaa3cb66b6d012ca07cab58a","CESR, Univ. Tours, France; CEDRIC/CNAM, Paris, France; IReMus, Sorbonne Universités, Paris, France; IRISA, Univ. Rennes 1, Lannion, France","Besson V., CESR, Univ. Tours, France; Gurrieri M., CESR, Univ. Tours, France; Rigaux P., CEDRIC/CNAM, Paris, France; Tacaille A., IReMus, Sorbonne Universités, Paris, France; Thion V., IRISA, Univ. Rennes 1, Lannion, France","We examine quality issues raised by the development of XML-based Digital Score Libraries. Based on the authors’ practical experience, the paper exposes the quality shortcomings inherent to the complexity of music encoding, and the lack of support from state-of-the-art formats. We also identify the various facets of the “quality” concept with respect to usages and motivations. We finally propose a general methodology to introduce quality management as a first-level concern in the management of score collections. © Vincent Besson, Marco Gurrieri, Philippe Rigaux, Alice Tacaille, Virginie Thion.","","Digital libraries; Information retrieval; Libraries; Digital score; General methodologies; Practical experience; Quality assessment; Quality issues; State of the art; Quality management","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Dorfer M.; Arzt A.; Widmer G.","Dorfer, Matthias (55516844500); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","55516844500; 36681791200; 7004342843","Towards score following in sheet music images","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030557163&partnerID=40&md5=b4887c8ab38eabce742ae6d8ae4c91e4","Department of Computational Perception, Johannes Kepler University Linz, Austria","Dorfer M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","This paper addresses the matching of short music audio snippets to the corresponding pixel location in images of sheet music. A system is presented that simultaneously learns to read notes, listens to music and matches the currently played music to its corresponding notes in the sheet. It consists of an end-to-end multi-modal convolutional neural network that takes as input images of sheet music and spectrograms of the respective audio snippets. It learns to predict, for a given unseen audio snippet (covering approximately one bar of music), the corresponding position in the respective score line. Our results suggest that with the use of (deep) neural networks – which have proven to be powerful image processing models – working with sheet music becomes feasible and a promising future research direction. © Matthias Dorfer, Andreas Arzt, Gerhard Widmer.","","Image processing; Information retrieval; Neural networks; Convolutional neural network; End to end; Future research directions; Input image; Multi-modal; Pixel location; Score-following; Spectrograms; Audio acoustics","M. Dorfer; Department of Computational Perception, Johannes Kepler University Linz, Austria; email: matthias.dorfer@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Cardoso J.P.V.; Pontello L.F.; Holanda P.H.F.; Guilherme B.; Goussevskaia O.; da Silva A.P.C.","Cardoso, João Paulo V. (57188711448); Pontello, Luciana Fujii (17435906600); Holanda, Pedro H.F. (56728649600); Guilherme, Bruno (56728842200); Goussevskaia, Olga (10046405700); da Silva, Ana Paula C. (35182777600)","57188711448; 17435906600; 56728649600; 56728842200; 10046405700; 35182777600","Mixtape: Direction-based navigation in large media collections","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027164189&partnerID=40&md5=142c523b8e614c1982dd9255859b0b08","Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil","Cardoso J.P.V., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Pontello L.F., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Holanda P.H.F., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Guilherme B., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; Goussevskaia O., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil; da Silva A.P.C., Computer Science Department, Universidade Federal de Minas Gerais (UFMG), Brazil","In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a scalable data structure to store and retrieve similarity information and propose a novel navigation framework that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape 1 , a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns. © João Paulo V. Cardoso, Luciana Fujii Pontello, Pedro H. F. Holanda, Bruno Guilherme, Olga Goussevskaia, Ana Paula Couto da Silva.","","Information retrieval; User interfaces; Automatic evaluation; Navigation paths; Real-time feedback; Similarity informations; User feedback; User profile; Vector operations; WEB application; Navigation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Andrade N.; Figueiredo F.","Andrade, Nazareno (7003429497); Figueiredo, Flavio (35317374600)","7003429497; 35317374600","Exploring the latent structure of collaborations in music recordings: A case study in jazz","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998639&partnerID=40&md5=1e41d98f34feb87804f155799c196dc7","Universidade Federal de Campina Grande, Brazil; IBM Research, Brazil","Andrade N., Universidade Federal de Campina Grande, Brazil; Figueiredo F., IBM Research, Brazil","Music records are largely a byproduct of collaborative efforts. Understanding how musicians collaborate to create records provides a step to understand the social production of music. This work leverages recent methods from trajectory mining to investigate how musicians have collaborated over time to record albums. Our case study analyzes data from the Discogs.com database from the Jazz domain. Our analysis examines how to explore the latent structure of collaboration between leading artists or bands and instrumentists over time. Moreover, we leverage the latent structure of our dataset to perform large-scale quantitative analyses of typical collaboration dynamics in different artist communities. © Nazareno Andrade, Flavio Figueiredo.","","Information retrieval; Large dataset; Latent structures; Music recording; Social productions; Trajectory minings; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Liu I.-T.; Randall R.","Liu, I-Ting (58451120900); Randall, Richard (36697042100)","58451120900; 36697042100","Predicting missing music components with bidirectional long short-term memory neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995647&partnerID=40&md5=350cb6ccf0e5558b3ae2c93c1b3f30a2","Carnegie Mellon University, School of Music, United States; Carnegie Mellon University, School of Music, Center for the Neural Basis of Cognition, United States","Liu I.-T., Carnegie Mellon University, School of Music, United States; Randall R., Carnegie Mellon University, School of Music, Center for the Neural Basis of Cognition, United States","Successfully predicting missing components (entire parts or voices) from complex multipart musical textures has attracted researchers of music information retrieval and music theory. However, these applications were limited to either two-part melody and accompaniment (MA) textures or four-part Soprano-Alto-Tenor-Bass (SATB) textures. This paper proposes a robust framework applicable to both textures using a Bidirectional Long-Short Term Memory (BLSTM) recurrent neural network. The BLSTM system was evaluated using frame-wise accuracies on the Nottingham Folk Song dataset and J. S. Bach Chorales. Experimental results demonstrated that adding bidirectional links to the neural network improves prediction accuracy by 3% on average. Specifically, BLSTM outperforms other neural-network based methods by 4.6% on average for four-part SATB and two-part MA textures (employing a transition matrix). The high accuracies obtained with BLSTM on both two-part and four-part textures indicated that BLSTM is the most robust and applicable structure for predicting missing components from multi-part musical textures. © I-Ting Liu, Richard Randall.","","Brain; Forecasting; Information retrieval; Long short-term memory; Bi-directional links; Folk songs; Music information retrieval; Music theory; Prediction accuracy; Transition matrices; Textures","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Schedl M.; Eghbal-Zadeh H.; Gómez E.; Tkalčič M.","Schedl, Markus (8684865900); Eghbal-Zadeh, Hamid (37080509500); Gómez, Emilia (14015483200); Tkalčič, Marko (24438438300)","8684865900; 37080509500; 14015483200; 24438438300","An analysis of agreement in classical music perception and its relationship to listener characteristics","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066072685&partnerID=40&md5=d5c945c1b0914926d857410fc06bc5c6","Johannes Kepler University, Linz, Austria; Universitat Pompeu Fabra, Barcelona, Spain; Free University of Bozen–Bolzano, Italy","Schedl M., Johannes Kepler University, Linz, Austria; Eghbal-Zadeh H., Johannes Kepler University, Linz, Austria; Gómez E., Universitat Pompeu Fabra, Barcelona, Spain; Tkalčič M., Free University of Bozen–Bolzano, Italy","We present a study, carried out on 241 participants, which investigates on classical music material the agreement of listeners on perceptual music aspects (related to emotion, tempo, complexity, and instrumentation) and the relationship between listener characteristics and these aspects. For the currently popular task of music emotion recognition, the former question is particularly important when defining a ground truth of emotions perceived in a given music collection. We characterize listeners via a range of factors, including demographics, musical inclination, experience, and education, and personality traits. Participants rate the music material under investigation, i.e., 15 expert-defined segments of Beethoven’s 3rd symphony, “Eroica”, in terms of 10 emotions, perceived tempo, complexity, and number of instrument groups. Our study indicates only slight agreement on most perceptual aspects, but significant correlations between several listener characteristics and perceptual qualities. © Markus Schedl, Hamid Eghbal-Zadeh, Emilia Gómez, Marko Tkalčič.","","Information retrieval; Classical musics; Ground truth; Music collection; Music emotions; Music materials; Perceptual aspects; Perceptual quality; Personality traits; Behavioral research","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Sturm B.L.","Sturm, Bob L. (14014190500)","14014190500","Revisiting priorities: Improving MIR evaluation practices","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038946615&partnerID=40&md5=2ffb7fbe000fe85c83d5039e39baf20b","Centre for Digital Music, Queen Mary University of London, United Kingdom","Sturm B.L., Centre for Digital Music, Queen Mary University of London, United Kingdom","While there is a consensus that evaluation practices in music informatics (MIR) must be improved, there is no consensus about what should be prioritised in order to do so. Priorities include: 1) improving data; 2) improving figures of merit; 3) employing formal statistical testing; 4) employing cross-validation; and/or 5) implementing transparent, central and immediate evaluation. In this position paper, I argue how these priorities treat only the symptoms of the problem and not its cause: MIR lacks a formal evaluation framework relevant to its aims. I argue that the principal priority is to adapt and integrate the formal design of experiments (DOE) into the MIR research pipeline. Since the aim of DOE is to help one produce the most reliable evidence at the least cost, it stands to reason that DOE will make a significant contribution to MIR. Accomplishing this, however, will not be easy, and will require far more effort than is currently being devoted to it. © Bob L. Sturm.","","Design of experiments; Cross validation; Evaluation framework; Figures of merits; Least cost; Music Informatics; Position papers; Statistical testing; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Panteli M.; Benetos E.; Dixon S.","Panteli, Maria (55915922100); Benetos, Emmanouil (16067946900); Dixon, Simon (7201479437)","55915922100; 16067946900; 7201479437","Learning a feature space for similarity in world music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027104271&partnerID=40&md5=a877ba8582188006b323054f960fb110","Centre for Digital Music, Queen Mary University of London, United Kingdom","Panteli M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this study we investigate computational methods for assessing music similarity in world music. We use state-of-the-art audio features to describe musical content in world music recordings. Our music collection is a subset of the Smithsonian Folkways Recordings with audio examples from 31 countries from around the world. Using supervised and unsupervised dimensionality reduction techniques we learn feature representations for music similarity. We evaluate how well music styles separate in this learned space with a classification experiment. We obtained moderate performance classifying the recordings by country. Analysis of misclassifications revealed cases of geographical or cultural proximity. We further evaluate the learned space by detecting outliers, i.e. identifying recordings that stand out in the collection. We use a data mining technique based on Mahalanobis distances to detect outliers and perform a listening experiment in the ‘odd one out’ style to evaluate our findings. We are able to detect, amongst others, recordings of non-musical content as outliers as well as music with distinct timbral and harmonic content. The listening experiment reveals moderate agreement between subjects’ ratings and our outlier estimation. © Maria Panteli, Emmanouil Benetos, Simon Dixon.","","Audio acoustics; Data mining; Information retrieval; Statistics; Dimensionality reduction techniques; Feature representation; Harmonic contents; Mahalanobis distances; Misclassifications; Music collection; Music similarity; State of the art; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Southall C.; Stables R.; Hockman J.","Southall, Carl (57201882762); Stables, Ryan (55322057300); Hockman, Jason (36730968100)","57201882762; 55322057300; 36730968100","Automatic drum transcription for polyphonic recordings using soft attention mechanisms and convolutional neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047935133&partnerID=40&md5=1bff37502f4a0a6dbd40abd5ba7004ef","DMT Lab, Birmingham City University, Birmingham, United Kingdom","Southall C., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Stables R., DMT Lab, Birmingham City University, Birmingham, United Kingdom; Hockman J., DMT Lab, Birmingham City University, Birmingham, United Kingdom","Automatic drum transcription is the process of generating symbolic notation for percussion instruments within audio recordings. To date, recurrent neural network (RNN) systems have achieved the highest evaluation accuracies for both drum solo and polyphonic recordings, however the accuracies within a polyphonic context still remain relatively low. To improve accuracy for polyphonic recordings, we present two approaches to the ADT problem: First, to capture the dynamism of features in multiple time-step hidden layers, we propose the use of soft attention mechanisms (SA) and an alternative RNN configuration containing additional peripheral connections (PC). Second, to capture these same trends at the input level, we propose the use of a convolutional neural network (CNN), which uses a larger set of time-step features. In addition, we propose the use of a bidirectional recurrent neural network (BRNN) in the peak-picking stage. The proposed systems are evaluated along with two state-of-the-art ADT systems in five evaluation scenarios, including a newly-proposed evaluation methodology designed to assess the generalisability of ADT systems. The results indicate that all of the newly proposed systems achieve higher accuracies than the stateof- the-art RNN systems for polyphonic recordings and that the additional BRNN peak-picking stage offers slight improvement in certain contexts. © 2019 Carl Southall, Ryan Stables and Jason Hockman.","","Audio recordings; Convolution; Information retrieval; Musical instruments; Transcription; Attention mechanisms; Bidirectional recurrent neural networks; Convolutional neural network; Evaluation accuracy; Evaluation methodologies; Multiple time step; Percussion instruments; Recurrent neural network (RNN); Recurrent neural networks","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Lostanlen V.; Cella C.-E.","Lostanlen, Vincent (57170761700); Cella, Carmine-Emanuele (36614069100)","57170761700; 36614069100","Deep convolutional networks on the pitch spiral for music instrument recognition","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050035309&partnerID=40&md5=410f8b9f3cd57c797e7b2bded031f941","École Normale Supérieure, PSL Research University, CNRS, Paris, France","Lostanlen V., École Normale Supérieure, PSL Research University, CNRS, Paris, France; Cella C.-E., École Normale Supérieure, PSL Research University, CNRS, Paris, France","Musical performance combines a wide range of pitches, nuances, and expressive techniques. Audio-based classification of musical instruments thus requires to build signal representations that are invariant to such transformations. This article investigates the construction of learned convolutional architectures for instrument recognition, given a limited amount of annotated training data. In this context, we benchmark three different weight sharing strategies for deep convolutional networks in the time-frequency domain: temporal kernels; time-frequency kernels; and a linear combination of time-frequency kernels which are one octave apart, akin to a Shepard pitch spiral. We provide an acoustical interpretation of these strategies within the source-filter framework of quasi-harmonic sounds with a fixed spectral envelope, which are archetypal of musical notes. The best classification accuracy is obtained by hybridizing all three convolutional layers into a single deep learning architecture. © Vincent Lostanlen and Carmine-Emanuele Cella.","","Convolution; Deep learning; Frequency domain analysis; Information retrieval; Network architecture; Annotated training data; Classification accuracy; Convolutional networks; Instrument recognition; Learning architectures; Signal representations; Time frequency domain; Time frequency kernels; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bantula H.; Giraldo S.; Ramirez R.","Bantula, Helena (57210234836); Giraldo, Sergio (56950430600); Ramirez, Rafael (35280935600)","57210234836; 56950430600; 35280935600","Jazz ensemble expressive performance modeling","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069999043&partnerID=40&md5=845c7115930b6185bf90e986b7a6e588","Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain","Bantula H., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain; Giraldo S., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain; Ramirez R., Music Technology Group, Universitat . Pompeu Fabra, Barcelona, Spain","Computational expressive music performance studies the analysis and characterisation of the deviations that a musician introduces when performing a musical piece. It has been studied in a classical context where timing and dynamic deviations are modeled using machine learning techniques. In jazz music, work has been done previously on the study of ornament prediction in guitar performance, as well as in saxophone expressive modeling. However, little work has been done on expressive ensemble performance. In this work, we analysed the musical expressivity of jazz guitar and piano from two different perspectives: solo and ensemble performance. The aim of this paper is to study the influence of piano accompaniment into the performance of a guitar melody and vice versa. Based on a set of recordings made by professional musicians, we extracted descriptors from the score, we transcribed the guitar and the piano performances and calculated performance actions for both instruments. We applied machine learning techniques to train models for each performance action, taking into account both solo and ensemble descriptors. Finally, we compared the accuracy of the induced models. The accuracy of most models increased when ensemble information was considered, which can be explained by the interaction between musicians. © 2016 Bantula, Helena, Giraldo, Sergio, Ramirez, Rafael.","","Computer music; Information retrieval; Learning algorithms; Machine learning; Applied machine learning; Classical context; Dynamic deviations; Ensemble performance; Expressive music performance; Expressive performance; Machine learning techniques; Musical expressivity; Musical instruments","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Ycart A.; Benetos E.","Ycart, Adrien (57192301993); Benetos, Emmanouil (16067946900)","57192301993; 16067946900","A study on LSTM networks for polyphonic music sequence modelling","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044325645&partnerID=40&md5=2ee7901090da14dffc1d209988cec37f","Centre for Digital Music, Queen Mary University of London, United Kingdom","Ycart A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Neural networks, and especially long short-term memory networks (LSTM), have become increasingly popular for sequence modelling, be it in text, speech, or music. In this paper, we investigate the predictive power of simple LSTM networks for polyphonic MIDI sequences, using an empirical approach. Such systems can then be used as a music language model which, combined with an acoustic model, can improve automatic music transcription (AMT) performance. As a first step, we experiment with synthetic MIDI data, and we compare the results obtained in various settings, throughout the training process. In particular, we compare the use of a fixed sample rate against a musically-relevant sample rate. We test this system both on synthetic and real MIDI data. Results are compared in terms of note prediction accuracy. We show that the higher the sample rate is, the better the prediction is, because self transitions are more frequent. We suggest that for AMT, a musically-relevant sample rate is crucial in order to model note transitions, beyond a simple smoothing effect. © 2019 Adrien Ycart and Emmanouil Benetos.","","Computer music; Information retrieval; Automatic music transcription; Empirical approach; Polyphonic music; Prediction accuracy; Predictive power; Short term memory; Smoothing effects; Training process; Long short-term memory","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Calvo-Zaragoza J.; Vigliensoni G.; Fujinaga I.","Calvo-Zaragoza, Jorge (55847598300); Vigliensoni, Gabriel (55217696900); Fujinaga, Ichiro (9038140900)","55847598300; 55217696900; 9038140900","One-step detection of background, staff lines, and symbols in medieval music manuscripts with convolutional neural networks","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046125397&partnerID=40&md5=52460a0e0a2e606b51aabb02dec73b68","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada","Calvo-Zaragoza J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, QC, Canada","One of the most complex stages of optical music recognition workflows is the detection and isolation of musical symbols. Traditionally, this goal is achieved by performing preprocesses of binarization and staff-line removal. However, these are commonly performed using heuristics that do not generalize widely when applied to different types of documents such as medieval scores. In this paper we propose an effective and generalizable approach to address this problem in one step. Our proposal classifies each pixel of the image among background, staff lines, and symbols using supervised learning techniques, namely convolutional neural networks. Experiments on a set of medieval music pages proved that the proposed approach is very accurate, achieving a performance upwards of 90% and outperforming common ensembles of binarization and staffline removal algorithms. © 2019 Jorge Calvo-Zaragoza, Gabriel Vigliensoni, and Ichiro Fujinaga.","","Information retrieval; Neural networks; Supervised learning; Binarizations; Convolutional neural network; Line removal; Musical symbols; One-step detections; Optical music recognition; Removal algorithms; Work-flows; Convolution","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Kinnaird K.M.","Kinnaird, Katherine M. (57205699026)","57205699026","Aligned hierarchies: A multi-scale structure-based representation for music-based data streams","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069817882&partnerID=40&md5=081924e949af655380ba6104cc3bf1a9","Department of Mathematics, Statistics, and Computer Science, Macalester College, Saint Paul, MN, United States","Kinnaird K.M., Department of Mathematics, Statistics, and Computer Science, Macalester College, Saint Paul, MN, United States","We introduce aligned hierarchies, a low-dimensional representation for music-based data streams, such as recordings of songs or digitized representations of scores. The aligned hierarchies encode all hierarchical decompositions of repeated elements from a high-dimensional and noisy music-based data stream into one object. These aligned hierarchies can be embedded into a classification space with a natural notion of distance. We construct the aligned hierarchies by finding, encoding, and synthesizing all repeated structure present in a music-based data stream. For a data set of digitized scores, we conducted experiments addressing the fingerprint task that achieved perfect precision-recall values. These experiments provide an initial proof of concept for the aligned hierarchies addressing MIR tasks. © Katherine M. Kinnaird.","","Information retrieval; Data stream; Digitized representation; Hierarchical decompositions; High-dimensional; Low-dimensional representation; Multi-scale structures; Notion of distance; Proof of concept; Encoding (symbols)","K.M. Kinnaird; Department of Mathematics, Statistics, and Computer Science, Macalester College, Saint Paul, United States; email: kkinnair@macalester.edu","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Chung C.-H.; Chen Y.; Chen H.","Chung, Chia-Hao (57161731400); Chen, Yian (57191506636); Chen, Homer (8236841800)","57161731400; 57191506636; 8236841800","Exploiting playlists for representation of songs and words for text-based music retrieval","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044545821&partnerID=40&md5=c93a824634c8bcd2fb922ea46209addd","National Taiwan University, Taiwan; KKBOX Inc., Taiwan","Chung C.-H., National Taiwan University, Taiwan; Chen Y., KKBOX Inc., Taiwan; Chen H., National Taiwan University, Taiwan","As a result of the growth of online music streaming services, a large number of playlists have been created by users and service providers. The title of each playlist provides useful information, such as the theme and listening context, of the songs in the playlist. In this paper, we investigate how to exploit the words extracted from playlist titles for text-based music retrieval. The main idea is to represent songs and words in a common latent space so that the music retrieval is converted to the problem of selecting songs that are the nearest neighbors of the query word in the latent space. Specifically, an unsupervised learning method is proposed to generate a latent representation of songs and words, where the learning objects are the co-occurring songs and words in playlist titles. Five metrics (precision, recall, coherence, diversity, and popularity) are considered for performance evaluation of the proposed method. Qualitative results demonstrate that our method is able to capture the semantic meaning of songs and words, owning to the proximity property of related songs and words in the latent space. © 2019 Chia-Hao Chung, Yian Chen, Homer Chen.","","Semantics; Unsupervised learning; Learning objects; Music retrieval; Nearest neighbors; Online music; Proximity properties; Query words; Service provider; Unsupervised learning method; Information retrieval","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Balke S.; Driedger J.; Abeßer J.; Dittmar C.; Müller M.","Balke, Stefan (56940401000); Driedger, Jonathan (55582371700); Abeßer, Jakob (36607532300); Dittmar, Christian (15051598000); Müller, Meinard (7404689873)","56940401000; 55582371700; 36607532300; 15051598000; 7404689873","Towards evaluating multiple predominant melody annotations in jazz recordings","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066076805&partnerID=40&md5=4a28bce98c0ba461e00483d1253014ce","International Audio Laboratories, Erlangen, Germany; Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Balke S., International Audio Laboratories, Erlangen, Germany; Driedger J., International Audio Laboratories, Erlangen, Germany; Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Dittmar C., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Melody estimation algorithms are typically evaluated by separately assessing the task of voice activity detection and fundamental frequency estimation. For both subtasks, computed results are typically compared to a single human reference annotation. This is problematic since different human experts may differ in how they specify a predominant melody, thus leading to a pool of equally valid reference annotations. In this paper, we address the problem of evaluating melody extraction algorithms within a jazz music scenario. Using four human and two automatically computed annotations, we discuss the limitations of standard evaluation measures and introduce an adaptation of Fleiss’ kappa that can better account for multiple reference annotations. Our experiments not only highlight the behavior of the different evaluation measures, but also give deeper insights into the melody extraction task. © Stefan Balke, Jakob Abeßer, Jonathan Driedger, Christian Dittmar, Meinard Müller.","","Extraction; Information retrieval; Speech recognition; Estimation algorithm; Evaluation measures; Fundamental frequency estimation; Human expert; Melody extractions; Multiple references; Standard evaluations; Voice activity detection; Frequency estimation","S. Balke; International Audio Laboratories, Erlangen, Germany; email: stefan.balke@audiolabs-erlangen.de","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Fan J.; Tatar K.; Thorogood M.; Pasquier P.","Fan, Jianyu (57089446500); Tatar, Kivanç (57189068416); Thorogood, Miles (56301157400); Pasquier, Philippe (8850202000)","57089446500; 57189068416; 56301157400; 8850202000","Ranking-based emotion recognition for experimental music","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047331324&partnerID=40&md5=bdc2424c16a3c81c3ce0e31db11e51bf","Simon Fraser University, Vancouver, BC, Canada","Fan J., Simon Fraser University, Vancouver, BC, Canada; Tatar K., Simon Fraser University, Vancouver, BC, Canada; Thorogood M., Simon Fraser University, Vancouver, BC, Canada; Pasquier P., Simon Fraser University, Vancouver, BC, Canada","Emotion recognition is an open problem in Affective Computing the field. Music emotion recognition (MER) has challenges including variability of musical content across genres, the cultural background of listeners, reliability of ground truth data, and the modeling human hearing in computational domains. In this study, we focus on experimental music emotion recognition. First, we present a music corpus that contains 100 experimental music clips and 40 music clips from 8 musical genres. The dataset (the music clips and annotations) is publicly available at: http://metacreation.net/project/emusic/. Then, we present a crowdsourcing method that we use to collect ground truth via ranking the valence and arousal of music clips. Next, we propose a smoothed RankSVM (SRSVM) method. The evaluation has shown that the SRSVM outperforms four other ranking algorithms. Finally, we analyze the distribution of perceived emotion of experimental music against other genres to demonstrate the difference between genres. © 2019 Jianyu Fan, Kivanç Tatar, Miles Thorogood, Philippe Pasquier.","","Audition; Behavioral research; Information retrieval; Speech recognition; Affective Computing; Computational domains; Cultural backgrounds; Emotion recognition; Experimental musics; Ground truth data; Music emotions; Ranking algorithm; Computer music","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Cancino-Chacón C.; Grachten M.; Agres K.","Cancino-Chacón, Carlos (55968703500); Grachten, Maarten (8974600000); Agres, Kat (25222806800)","55968703500; 8974600000; 25222806800","From bach to the beatles: The simulation of human tonal expectation using ecologically-trained predictive models","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050636761&partnerID=40&md5=608e41877763431bd867c4560edb3317","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria; Institute of High Performance Computing, ASTAR, Singapore, Singapore","Cancino-Chacón C., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Grachten M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Agres K., Institute of High Performance Computing, ASTAR, Singapore, Singapore","Tonal structure is in part conveyed by statistical regularities between musical events, and research has shown that computational models reflect tonal structure in music by capturing these regularities in schematic constructs like pitch histograms. Of the few studies that model the acquisition of perceptual learning from musical data, most have employed self-organizing models that learn a topology of static descriptions of musical contexts. Also, the stimuli used to train these models are often symbolic rather than acoustically faithful representations of musical material. In this work we investigate whether sequential predictive models of musical memory (specifically, recurrent neural networks), trained on audio from commercial CD recordings, induce tonal knowledge in a similar manner to listeners (as shown in behavioral studies in music perception). Our experiments indicate that various types of recurrent neural networks produce musical expectations that clearly convey tonal structure. Furthermore, the results imply that although implicit knowledge of tonal structure is a necessary condition for accurate musical expectation, the most accurate predictive models also use other cues beyond the tonal structure of the musical context. © 2019 Carlos Cancino-Chacón, Maarten Grachten, Kat Agres.","","Information retrieval; Recurrent neural networks; Topology; Behavioral studies; Computational model; Implicit knowledge; Musical materials; Perceptual learning; Predictive models; Self-organizing model; Statistical regularity; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hori G.; Sagayama S.","Hori, Gen (7004703236); Sagayama, Shigeki (7004859104)","7004703236; 7004859104","Minimax viterbi algorithm for HMM-based guitar fingering decision","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069936599&partnerID=40&md5=09a9c80d2f036fd5932d3e7e917cd522","Asia University, RIKEN, Japan; Meiji University, Japan","Hori G., Asia University, RIKEN, Japan; Sagayama S., Meiji University, Japan","Previous works on automatic fingering decision for string instruments have been mainly based on path optimization by minimizing the difficulty of a whole phrase that is typically defined as the sum of the difficulties of moves required for playing the phrase. However, from a practical viewpoint of beginner players, it is more important to minimize the maximum difficulty of a move required for playing the phrase, that is, to make the most difficult move easier. To this end, we introduce a variant of the Viterbi algorithm (termed the “minimax Viterbi algorithm”) that finds the path of the hidden states that maximizes the minimum transition probability (not the product of the transition probabilities) and apply it to HMM-based guitar fingering decision. We compare the resulting fingerings by the conventional Viterbi algorithm and our proposed minimax Viterbi algorithm to show the appropriateness of our new method. © Gen Hori, Shigeki Sagayama.","","Information retrieval; Hidden state; HMM-based; Minimax; Path optimizations; String instruments; Transition probabilities; Viterbi algorithm","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Hajič J., jr.; Novotný J.; Pecina P.; Pokorný J.","Hajič, Jan (57193414196); Novotný, Jiří (57225711459); Pecina, Pavel (23393602100); Pokorný, Jaroslav (57206933518)","57193414196; 57225711459; 23393602100; 57206933518","Further steps towards a standard testbed for optical music recognition","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045196546&partnerID=40&md5=25c08e64173a7a53e49b49c9e71d8ed4","Charles University, Institute of Formal and Applied Linguistics, Czech Republic; Charles University, Department of Software Engineering, Czech Republic","Hajič J., jr., Charles University, Institute of Formal and Applied Linguistics, Czech Republic; Novotný J., Charles University, Department of Software Engineering, Czech Republic; Pecina P., Charles University, Institute of Formal and Applied Linguistics, Czech Republic; Pokorný J., Charles University, Department of Software Engineering, Czech Republic","Evaluating Optical Music Recognition (OMR) is notoriously difficult and automated end-to-end OMR evaluation metrics are not available to guide development. In “Towards a Standard Testbed for Optical Music Recognition: Definitions, Metrics, and Page Images”, Byrd and Simonsen recently stress that a benchmarking standard is needed in the OMR community, both with regards to data and evaluation metrics. We build on their analysis and definitions and present a prototype of an OMR benchmark. We do not, however, presume to present a complete solution to the complex problem of OMR benchmarking. Our contributions are: (a) an attempt to define a multilevel OMR benchmark dataset and a practical prototype implementation for both printed and handwritten scores, (b) a corpus-based methodology for assessing automated evaluation metrics, and an underlying corpus of over 1000 qualified relative cost-to-correct judgments. We then assess several straightforward automated MusicXML evaluation metrics against this corpus to establish a baseline over which further metrics can improve. © Jan Hajič jr., Jiří Novotný, Pavel Pecina, Jaroslav Pokorný.","","Benchmarking; Information retrieval; Testbeds; Automated evaluation; Benchmark datasets; Complete solutions; Complex problems; Evaluation metrics; Optical music recognition; Prototype implementations; Relative costs; Automation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Dzhambazov G.; Srinivasamurthy A.; Şentürk S.; Serra X.","Dzhambazov, Georgi (56271863800); Srinivasamurthy, Ajay (55583336200); Şentürk, Sertan (43461595100); Serra, Xavier (55892979900)","56271863800; 55583336200; 43461595100; 55892979900","On the use of note onsets for improved lyrics-to-audio alignment in Turkish Makam music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069940317&partnerID=40&md5=2133b87ad055e74d67a6ff4fd716acfe","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Lyrics-to-audio alignment aims to automatically match given lyrics and musical audio. In this work we extend a state of the art approach for lyrics-to-audio alignment with information about note onsets. In particular, we consider the fact that transition to next lyrics syllable usually implies transition to a new musical note. To this end we formulate rules that guide the transition between consecutive phonemes when a note onset is present. These rules are incorporated into the transition matrix of a variable-time hidden Markov model (VTHMM) phonetic recognizer based on MFCCs. An estimated melodic contour is input to an automatic note transcription algorithm, from which the note onsets are derived. The proposed approach is evaluated on 12 a cappella audio recordings of Turkish Makam music using a phrase-level accuracy measure. Evaluation of the alignment is also presented on a polyphonic version of the dataset in order to assess how degradation in the extracted onsets affects performance. Results show that the proposed model outperforms a baseline approach unaware of onset transition rules. To the best of our knowledge, this is the one of the first approaches tackling lyrics tracking, which combines timbral features with a melodic feature in the alignment process itself. © Georgi Dzhambazov, Ajay Srinivasamurthy, Sertan Şentürk, Xavier Serra .","","Alignment; Audio recordings; Hidden Markov models; Information retrieval; Accuracy measures; Audio alignments; Automatically match; Musical audio; Musical notes; State-of-the-art approach; Transition matrices; Transition rule; Audio acoustics","G. Dzhambazov; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: georgi.dzhambazov@upf.edu","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Wu C.-W.; Lerch A.","Wu, Chih-Wei (57188865070); Lerch, Alexander (22034963000)","57188865070; 22034963000","Automatic drum transcription using the student-teacher learning paradigm with unlabeled music data","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047918161&partnerID=40&md5=9dd53c41582b87036079647dcc8a3453","Georgia Institute of Technology, Center for Music Technology, United States","Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","Automatic drum transcription is a sub-task of automatic music transcription that converts drum-related audio events into musical notation. While noticeable progress has been made in the past by combining pattern recognition methods with audio signal processing techniques, the major limitation of many state-of-the-art systems still originates from the difficulty of obtaining a meaningful amount of annotated data to support the data-driven algorithms. In this work, we address the challenge of insufficiently labeled data by exploring the possibility of utilizing unlabeled music data from online resources. Specifically, a student neural network is trained using the labels generated from multiple teacher systems. The performance of the model is evaluated on a publicly available dataset. The results show the general viability of using unlabeled music data to improve the performance of drum transcription systems. © 2019 Chih-Wei Wu, Alexander Lerch.","","Audio signal processing; Audio systems; Information retrieval; Pattern recognition systems; Transcription; Audio events; Automatic music transcription; Data-driven algorithm; Musical notation; Online resources; Pattern recognition method; State-of-the-art system; Student teachers; Audio acoustics","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Kum S.; Oh C.; Nam J.","Kum, Sangeun (57208185300); Oh, Changheun (57224659763); Nam, Juhan (35812266500)","57208185300; 57224659763; 35812266500","Melody extraction on vocal segments using multi-column deep neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070016484&partnerID=40&md5=14cb76caf431c119af14d673c745d607","Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea","Kum S., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea; Oh C., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea; Nam J., Graduate School of Culture Technology, Korea Advanced Institute of Science and Technology, South Korea","Singing melody extraction is a task that tracks pitch contour of singing voice in polyphonic music. While the majority of melody extraction algorithms are based on computing a saliency function of pitch candidates or separating the melody source from the mixture, data-driven approaches based on classification have been rarely explored. In this paper, we present a classification-based approach for melody extraction on vocal segments using multi-column deep neural networks. In the proposed model, each of neural networks is trained to predict a pitch label of singing voice from spectrogram, but their outputs have different pitch resolutions. The final melody contour is inferred by combining the outputs of the networks and post-processing it with a hidden Markov model. In order to take advantage of the data-driven approach, we also augment training data by pitch-shifting the audio content and modifying the pitch label accordingly. We use the RWC dataset and vocal tracks of the MedleyDB dataset for training the model and evaluate it on the ADC 2004, MIREX 2005 and MIR-1k datasets. Through several settings of experiments, we show incremental improvements of the melody prediction. Lastly, we compare our best result to those of previous state-of-the-arts. © Sangeun Kum, Changheun Oh, Juhan Nam.","","Extraction; Hidden Markov models; Information retrieval; Data-driven approach; Incremental improvements; Melody contours; Melody extractions; Pitch contours; Polyphonic music; Post processing; State of the art; Deep neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Fournier-S’niehotta R.; Rigaux P.; Travers N.","Fournier-S’niehotta, Raphaël (57192992432); Rigaux, Philippe (57204372163); Travers, Nicolas (23391144800)","57192992432; 57204372163; 23391144800","Querying XML score databases: Xquery is not enough!","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038881602&partnerID=40&md5=7bfc87285504dd3b6f4780d1de133e30","CNAM, France","Fournier-S’niehotta R., CNAM, France; Rigaux P., CNAM, France; Travers N., CNAM, France","The paper addresses issues related to the design of query languages for searching and restructuring collections of XML-encoded music scores. We advocate against a direct approach based on XQuery, and propose a more powerful strategy that first extracts a structured representation of music notation from score encodings, and then manipulates this representation in closed form with dedicated operators. The paper exposes the content model, the resulting language, and describes our implementation on top of a large Digital Score Library (DSL). © Raphaël Fournier-S’niehotta, Philippe Rigaux, Nicolas Travers.","","Digital libraries; Information retrieval; Modems; Query languages; Closed form; Content model; Digital score; Direct approach; Encodings; Music notation; Music scores; XML","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Hennequin R.; Rigaud F.","Hennequin, Romain (36504173400); Rigaud, François (6506715928)","36504173400; 6506715928","Long-term reverberation modeling for under-determined audio source separation with application to vocal melody extraction","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046989676&partnerID=40&md5=dca5f48be8ccb5eb8ae18eaec1dbf0ae","Deezer R and D, 10 rue d’Athènes, Paris, 75009, France; Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","Hennequin R., Deezer R and D, 10 rue d’Athènes, Paris, 75009, France; Rigaud F., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","In this paper, we present a way to model long-term reverberation effects in under-determined source separation algorithms based on a non-negative decomposition framework. A general model for the sources affected by reverberation is introduced and update rules for the estimation of the parameters are presented. Combined with a well-known source-filter model for singing voice, an application to the extraction of reverberated vocal tracks from polyphonic music signals is proposed. Finally, an objective evaluation of this application is described. Performance improvements are obtained compared to the same model without reverberation modeling, in particular by significantly reducing the amount of interference between sources. © Romain Hennequin, François Rigaud.","","Extraction; Information retrieval; Reverberation; Sonar; Audio source separation; Melody extractions; Objective evaluation; Reverberation effects; Reverberation modeling; Separation algorithms; Source-filter models; Under-determined; Source separation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Font F.; Serra X.","Font, Frederic (35182806000); Serra, Xavier (55892979900)","35182806000; 55892979900","Tempo estimation for music loops and a simple confidence measure","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060925975&partnerID=40&md5=bca0555eaf9d785918b14868202908ab","Music Technology Group, Universitat Pompeu Fabra, Spain","Font F., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Tempo estimation is a common task within the music information retrieval community, but existing works are rarely evaluated with datasets of music loops and the algorithms are not tailored to this particular type of content. In addition to this, existing works on tempo estimation do not put an emphasis on providing a confidence value that indicates how reliable their tempo estimations are. In current music creation contexts, it is common for users to search for and use loops shared in online repositories. These loops are typically not produced by professionals and lack annotations. Hence, the existence of reliable tempo estimation algorithms becomes necessary to enhance the reusability of loops shared in such repositories. In this paper, we test six existing tempo estimation algorithms against four music loop datasets containing more than 35k loops. We also propose a simple and computationally cheap confidence measure that can be applied to any existing algorithm to estimate the reliability of their tempo predictions when applied to music loops. We analyse the accuracy of the algorithms in combination with our proposed confidence measure, and see that we can significantly improve the algorithms’ performance when only considering music loops with high estimated confidence. © Frederic Font and Xavier Serra.","","Reusability; Confidence Measure; Confidence values; Music creation; Music information retrieval; Online repositories; Tempo estimations; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Nakano T.; Mochihashi D.; Yoshii K.; Goto M.","Nakano, Tomoyasu (24344775400); Mochihashi, Daichi (15045398700); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","24344775400; 15045398700; 7103400120; 7403505330","Musical typicality: How many similar songs exist?","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035074897&partnerID=40&md5=b3b7496b30e484557b24c3cbb030c105","National Institute of Advanced Industrial Science and Technology (AIST), Japan; Institute of Statistical Mathematics, Japan; Kyoto University, Japan","Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Mochihashi D., Institute of Statistical Mathematics, Japan; Yoshii K., Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We propose a method for estimating the musical “typicality” of a song from an information theoretic perspective. While musical similarity compares just two songs, musical typicality quantifies how many of the songs in a set are similar. It can be used not only to express the uniqueness of a song but also to recommend one that is representative of a set. Building on the type theory in information theory (Cover & Thomas 2006), we use a Bayesian generative model of musical features and compute the typicality of a song as the sum of the probabilities of the songs that share the type of the given song. To evaluate estimated results, we focused on vocal timbre which can be evaluated quantitatively by using the singer’s gender. Estimated typicality is evaluated against the Pearson correlation coefficient between the computed typicality and the ratio of the number of male singers to female singers of a song-set. Our result shows that the proposed measure works more effectively to estimate musical typicality than the previous model based simply on generative probabilities. © Tomoyasu Nakano, Daichi Mochihashi, Kazuyoshi Yoshii, Masataka Goto.","","Correlation methods; Information retrieval; Bayesian generative models; Model-based OPC; Musical features; Musical similarity; Pearson correlation coefficients; Type theory; Information theory","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Su L.; Chuang T.-Y.; Yang Y.-H.","Su, Li (55966919100); Chuang, Tsung-Ying (57210231504); Yang, Yi-Hsuan (55218558400)","55966919100; 57210231504; 55218558400","Exploiting frequency, periodicity and harmonicity using advanced time-frequency concentration techniques for multipitch estimation of choir and symphony","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070011351&partnerID=40&md5=d7ed69ab00ba0b62515a6cfc1189e988","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Chuang T.-Y., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","To advance research on automatic music transcription (AMT), it is important to have labeled datasets with sufficient diversity and complexity that support the creation and evaluation of robust algorithms to deal with issues seen in real-world polyphonic music signals. In this paper, we propose new datasets and investigate signal processing algorithms for multipitch estimation (MPE) in choral and symphony music, which have been seldom considered in AMT research. We observe that MPE in these two types of music is challenging because of not only the high polyphony number, but also the possible imprecision in pitch for notes sung or played by multiple singers or musicians in unison. To improve the robustness of pitch estimation, experiments show that it is beneficial to measure pitch saliency by jointly considering frequency, periodicity and harmonicity information. Moreover, we can improve the localization and stability of pitch by the multi-taper methods and nonlinear time-frequency reassignment techniques such as the Concentration of Time and Frequency (ConceFT) transform. We show that the proposed unsupervised methods to MPE compare favorably with, if not superior to, state-of-the-art supervised methods in various types of music signals from both existing and the newly created datasets. © Li Su, Tsung-Ying Chuang and Yi-Hsuan Yang.","","Computer music; Information retrieval; Mathematical transformations; Signal processing; Automatic music transcription; Multi-pitch estimations; Signal processing algorithms; Supervised methods; Time and frequencies; Time-frequency concentrations; Time-frequency reassignments; Unsupervised method; Frequency estimation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Chen L.; Stolterman E.; Raphael C.","Chen, Liang (57192611138); Stolterman, Erik (22836641700); Raphael, Christopher (7004214964)","57192611138; 22836641700; 7004214964","Human-interactive optical music recognition","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041410266&partnerID=40&md5=19200f47abaaa99d5728f065db68d332","Indiana University Bloomington, United States","Chen L., Indiana University Bloomington, United States; Stolterman E., Indiana University Bloomington, United States; Raphael C., Indiana University Bloomington, United States","We propose a human-driven Optical Music Recognition (OMR) system that creates symbolic music data from common Western notation scores. Despite decades of development, OMR still remains largely unsolved as state-of-the-art automatic systems are unable to give reliable and useful results on a wide range of documents. For this reason our system, Ceres, combines human input and machine recognition to efficiently generate high-quality symbolic data. We propose a scheme for human-in-the-loop recognition allowing the user to constrain the recognition in two ways. The human actions allow the user to impose either a pixel labeling or model constraint, while the system re-recognizes subject to these constraints. We present evaluation based on different users’ log data using both Ceres and Sibelius software to produce the same music documents. We conclude that our system shows promise for transcribing complicated music scores with high accuracy. © Liang Chen, Erik Stolterman, Christopher Raphael.","","Automatic systems; High-accuracy; Human-in-the-loop; Machine recognition; Model constraints; Optical music recognition; Pixel labeling; State of the art; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Korzeniowski F.; Widmer G.","Korzeniowski, Filip (56328099700); Widmer, Gerhard (7004342843)","56328099700; 7004342843","Feature learning for chord recognition: The deep chroma extractor","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054251070&partnerID=40&md5=729ab5cf5412bfe2ff378ae994c3cba4","Department of Computational Perception, Johannes Kepler University Linz, Austria","Korzeniowski F., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","We explore frame-level audio feature learning for chord recognition using artificial neural networks. We present the argument that chroma vectors potentially hold enough information to model harmonic content of audio for chord recognition, but that standard chroma extractors compute too noisy features. This leads us to propose a learned chroma feature extractor based on artificial neural networks. It is trained to compute chroma features that encode harmonic information important for chord recognition, while being robust to irrelevant interferences. We achieve this by feeding the network an audio spectrum with context instead of a single frame as input. This way, the network can learn to selectively compensate noise and resolve harmonic ambiguities. We compare the resulting features to hand-crafted ones by using a simple linear frame-wise classifier for chord recognition on various data sets. The results show that the learned feature extractor produces superior chroma vectors for chord recognition. © Filip Korzeniowski and Gerhard Widmer.","","Deep learning; Harmonic analysis; Information retrieval; Machine learning; Neural networks; Audio features; Audio spectrum; Chord recognition; Chroma features; Feature extractor; Feature learning; Harmonic contents; Single frames; Classification (of information)","F. Korzeniowski; Department of Computational Perception, Johannes Kepler University Linz, Austria; email: filip.korzeniowski@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Guiomard-Kagan N.; Giraud M.; Groult R.; Levé F.","Guiomard-Kagan, Nicolas (57210209791); Giraud, Mathieu (8700367400); Groult, Richard (6507884031); Levé, Florence (55893852300)","57210209791; 8700367400; 6507884031; 55893852300","Improving voice separation by better connecting contigs","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063466257&partnerID=40&md5=7b0b659d18c05c4a879322ee7dbfc232","MIS, Univ. Picardie Jules Verne, Amiens, France; CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France","Guiomard-Kagan N., MIS, Univ. Picardie Jules Verne, Amiens, France; Giraud M., CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France; Groult R., MIS, Univ. Picardie Jules Verne, Amiens, France; Levé F., MIS, Univ. Picardie Jules Verne, Amiens, France, CRIStAL, UMR CNRS 9189, Univ. Lille, Lille, France","Separating a polyphonic symbolic score into monophonic voices or streams helps to understand the music and may simplify further pattern matching. One of the best ways to compute this separation, as proposed by Chew and Wu in 2005 [2], is to first identify contigs that are portions of the music score with a constant number of voices, then to progressively connect these contigs. This raises two questions: Which contigs should be connected first? And, how should these two contigs be connected? Here we propose to answer simultaneously these two questions by considering a set of musical features that measures the quality of any connection. The coefficients weighting the features are optimized through a genetic algorithm. We benchmark the resulting connection policy on corpora containing fugues of the Well-Tempered Clavier by J. S. Bach as well as on string quartets, and we compare it against previously proposed policies [2, 9]. The contig connection is improved, particularly when one takes into account the whole content of voice fragments to assess the quality of their possible connection. © Nicolas Guiomard-Kagan, Mathieu Giraud, Richard Groult, Florence Levé.","","Genetic algorithms; Information retrieval; Pattern matching; Contigs; Music scores; Musical features; Voice separation; Separation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Wu C.-W.; Lerch A.","Wu, Chih-Wei (57188865070); Lerch, Alexander (22034963000)","57188865070; 22034963000","On drum playing technique detection in polyphonic mixtures","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047948804&partnerID=40&md5=dd940b0faf169ae77c52270a917f0dc1","Georgia Institute of Technology, Center for Music Technology, United States","Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","In this paper, the problem of drum playing technique detection in polyphonic mixtures of music is addressed. We focus on the identification of 4 rudimentary techniques: strike, buzz roll, flam, and drag. The specifics and the challenges of this task are being discussed, and different sets of features are compared, including various features extracted from NMF-based activation functions, as well as baseline spectral features. We investigate the capabilities and limitations of the presented system in the case of real-world recordings and polyphonic mixtures. To design and evaluate the system, two datasets are introduced: a training dataset generated from individual drum hits, and additional annotations of the well-known ENST drum dataset minus one subset as test dataset. The results demonstrate issues with the traditionally used spectral features, and indicate the potential of using NMF activation functions for playing technique detection, however, the performance of polyphonic music still leaves room for future improvement. © Chih-Wei Wu, Alexander Lerch.","","Chemical activation; Information retrieval; Statistical tests; Activation functions; Future improvements; Playing techniques; Polyphonic music; Real-world; Sets of features; Spectral feature; Training dataset; Mixtures","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Buccoli M.; Zanoni M.; Fazekas G.; Sarti A.; Sandler M.","Buccoli, Michele (56004934400); Zanoni, Massimiliano (57192936338); Fazekas, György (37107520200); Sarti, Augusto (55074405100); Sandler, Mark (7202740804)","56004934400; 57192936338; 37107520200; 55074405100; 7202740804","A higher-dimensional expansion of affective norms for english terms for music tagging","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070017560&partnerID=40&md5=d09acb6afd7f8d2125a60ef108893364","Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Centre For Digital Music, Queen Mary, University of London, United Kingdom","Buccoli M., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Zanoni M., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Fazekas G., Centre For Digital Music, Queen Mary, University of London, United Kingdom; Sarti A., Dipartimento di Elettronica, Informatica e Bioingegneria, Politecnico di Milano, Italy; Sandler M., Centre For Digital Music, Queen Mary, University of London, United Kingdom","The Valence, Arousal and Dominance (VAD) model for emotion representation is widely used in music analysis. The ANEW dataset is composed of more than 2000 emotion related descriptors annotated in the VAD space. However, due to the low number of dimensions of the VAD model, the distribution of terms of the ANEW dataset tends to be compact and cluttered. In this work, we aim at finding a possibly higher-dimensional transformation of the VAD space, where the terms of the ANEW dataset are better organised conceptually and bear more relevance to music tagging. Our approach involves the use of a kernel expansion of the ANEW dataset to exploit a higher number of dimensions, and the application of distance learning techniques to find a distance metric that is consistent with the semantic similarity among terms. In order to train the distance learning algorithms, we collect information on the semantic similarity from human annotation and editorial tags. We evaluate the quality of the method by clustering the terms in the found high-dimensional domain. Our approach exhibits promising results with objective and subjective performance metrics, showing that a higher dimensional space could be useful to model semantic similarity among terms of the ANEW dataset. © Michele Buccoli, Massimiliano Zanoni, György Fazekas, Augusto Sarti, Mark Sandler.","","Distance education; Information retrieval; Learning algorithms; Semantics; Distance metrics; Emotion representation; High-dimensional; Higher-dimensional; Human annotations; Kernel expansion; Semantic similarity; Subjective performance; Quality control","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Demetriou A.; Larson M.; Liem C.C.S.","Demetriou, Andrew (57192921545); Larson, Martha (56574709600); Liem, Cynthia C.S. (21733247100)","57192921545; 56574709600; 21733247100","Go with the flow: When listeners use music as technology","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045467222&partnerID=40&md5=9e51a92df9f326b27eee48fe29380e62","Delft University of Technology, Delft, Netherlands; Radboud University, Nijmegen, Netherlands","Demetriou A., Delft University of Technology, Delft, Netherlands; Larson M., Delft University of Technology, Delft, Netherlands, Radboud University, Nijmegen, Netherlands; Liem C.C.S., Delft University of Technology, Delft, Netherlands","Music has been shown to have a profound effect on listeners’ internal states as evidenced by neuroscience research. Listeners report selecting and listening to music with specific intent, thereby using music as a tool to achieve desired psychological effects within a given context. In light of these observations, we argue that music information retrieval research must revisit the dominant assumption that listening to music is only an end unto itself. Instead, researchers should embrace the idea that music is also a technology used by listeners to achieve a specific desired internal state, given a particular set of circumstances and a desired goal. This paper focuses on listening to music in isolation (i.e., when the user listens to music by themselves with headphones) and surveys research from the fields of social psychology and neuroscience to build a case for a new line of research in music information retrieval on the ability of music to produce flow states in listeners. We argue that interdisciplinary collaboration is necessary in order to develop the understanding and techniques necessary to allow listeners to exploit the full potential of music as psychological technology. © Andrew Demetriou, Martha Larson, Cynthia C. S. Liem.","","Neurology; Flow state; Interdisciplinary collaborations; Internal state; Music information retrieval; Psychological effects; Social psychology; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Schreiber H.","Schreiber, Hendrik (55586286200)","55586286200","Genre ontology learning: Comparing curated with crowd-sourced ontologies","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069803074&partnerID=40&md5=9c606212d21409daaf427cdfbd95d166","Tagtraum Industries Incorporated, United States","Schreiber H., Tagtraum Industries Incorporated, United States","The Semantic Web has made it possible to automatically find meaningful connections between musical pieces which can be used to infer their degree of similarity. Similarity in turn, can be used by recommender systems driving music discovery or playlist generation. One useful facet of knowledge for this purpose are fine-grained genres and their inter-relationships. In this paper we present a method for learning genre ontologies from crowd-sourced genre labels, exploiting genre co-occurrence rates. Using both lexical and conceptual similarity measures, we show that the quality of such learned ontologies is comparable with manually created ones. In the process, we document properties of current reference genre ontologies, in particular a high degree of disconnectivity. Further, motivated by shortcomings of the established taxonomic precision measure, we define a novel measure for highly disconnected ontologies. © Hendrik Schreiber.","","Crowdsourcing; Recommender systems; Co-occurrence; Current reference; Degree of similarity; Disconnectivity; Inter-relationships; Musical pieces; Ontology learning; Similarity measure; Ontology","H. Schreiber; Tagtraum Industries Incorporated, United States; email: hs@tagtraum.com","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Pachet F.; Papadopoulos A.; Roy P.","Pachet, François (6701441655); Papadopoulos, Alexandre (56393920000); Roy, Pierre (55506419000)","6701441655; 56393920000; 55506419000","Sampling variations of sequences for structured music generation","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051464943&partnerID=40&md5=2b5ad1d5d2f03b1ee5164376e686aae5","Sony CSL Paris, France; UPMC Univ Paris 06, UMR 7606, LIP6, France","Pachet F., Sony CSL Paris, France; Papadopoulos A., UPMC Univ Paris 06, UMR 7606, LIP6, France; Roy P., Sony CSL Paris, France","Recently, machine-learning techniques have been successfully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly structured. In particular, musical sequences do not exhibit pattern structure, as typically found in human composed music. We present an approach to generate structured sequences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propagation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles. © 2019 François Pachet, Alexandre Papadopoulos, Pierre Roy.","","Information retrieval; Arbitrary structures; Belief propagation; Machine learning techniques; Musical similarity; Pattern structure; Sampled sequences; Statistical modeling; Structured sequence; Learning systems","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Silva D.F.; Yeh C.-C.M.; Batista G.E.A.P.A.; Keogh E.","Silva, Diego F. (55585876200); Yeh, Chin-Chia M. (57193525608); Batista, Gustavo E.A.P.A. (55062789000); Keogh, Eamonn (7006166198)","55585876200; 57193525608; 55062789000; 7006166198","SiMPle: Assessing music similarity using subsequences joins","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038877908&partnerID=40&md5=1d84c7b57299a5295a85bca200a9cbad","Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Department of Computer Science and Engineering, University of California, Riverside, United States","Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil, Department of Computer Science and Engineering, University of California, Riverside, United States; Yeh C.-C.M., Department of Computer Science and Engineering, University of California, Riverside, United States; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Keogh E., Department of Computer Science and Engineering, University of California, Riverside, United States","Most algorithms for music information retrieval are based on the analysis of the similarity between feature sets extracted from the raw audio. A common approach to assessing similarities within or between recordings is by creating similarity matrices. However, this approach requires quadratic space for each comparison and typically requires a costly post-processing of the matrix. In this work, we propose a simple and efficient representation based on a subsequence similarity join, which may be used in several music information retrieval tasks. We apply our method to the cover song recognition problem and demonstrate that it is superior to state-of-the-art algorithms. In addition, we demonstrate how the proposed representation can be exploited for multiple applications in music processing. © Diego F. Silva, Chin-Chia M. Yeh, Gustavo E. A. P. A. Batista, Eamonn Keogh.","","Information retrieval; Cover songs; Feature sets; Multiple applications; Music information retrieval; Music similarity; Post processing; Similarity join; State-of-the-art algorithms; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Driedger J.; Balke S.; Ewert S.; Müller M.","Driedger, Jonathan (55582371700); Balke, Stefan (56940401000); Ewert, Sebastian (32667575400); Müller, Meinard (7404689873)","55582371700; 56940401000; 32667575400; 7404689873","Template-based vibrato analysis of music signals","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028042493&partnerID=40&md5=d7e926624fb2b3273d92860ebcee69de","International Audio Laboratories, Erlangen, Germany; Queen Mary University of London, United Kingdom","Driedger J., International Audio Laboratories, Erlangen, Germany; Balke S., International Audio Laboratories, Erlangen, Germany; Ewert S., Queen Mary University of London, United Kingdom; Müller M., International Audio Laboratories, Erlangen, Germany","The automated analysis of vibrato in complex music signals is a highly challenging task. A common strategy is to proceed in a two-step fashion. First, a fundamental frequency (F0) trajectory for the musical voice that is likely to exhibit vibrato is estimated. In a second step, the trajectory is then analyzed with respect to periodic frequency modulations. As a major drawback, however, such a method cannot recover from errors made in the inherently difficult first step, which severely limits the performance during the second step. In this work, we present a novel vibrato analysis approach that avoids the first error-prone F0-estimation step. Our core idea is to perform the analysis directly on a signal’s spectrogram representation where vibrato is evident in the form of characteristic spectro-temporal patterns. We detect and parameterize these patterns by locally comparing the spectrogram with a predefined set of vibrato templates. Our systematic experiments indicate that this approach is more robust than F0-based strategies. © Jonathan Driedger, Stefan Balke, Sebastian Ewert, Meinard Müller.","","Information retrieval; Spectrographs; Analysis approach; Automated analysis; Common strategy; F0 estimations; Fundamental frequencies; Systematic experiment; Template-based; Temporal pattern; Signal analysis","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Velarde G.; Weyde T.; Chacón C.C.; Meredith D.; Grachten M.","Velarde, Gissel (58436115300); Weyde, Tillman (24476899500); Chacón, Carlos Cancino (55968703500); Meredith, David (57125703100); Grachten, Maarten (8974600000)","58436115300; 24476899500; 55968703500; 57125703100; 8974600000","Composer recognition based on 2D-filtered piano-rolls","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037338117&partnerID=40&md5=32669c6d7ebafd83c1ed24991c4d1650","Department of Architecture Design and Media Technology, Aalborg University, Denmark; Department of Computer Science, City University London, United Kingdom; Austrian Research Institute for Artificial Intelligence, Austria","Velarde G., Department of Architecture Design and Media Technology, Aalborg University, Denmark; Weyde T., Department of Computer Science, City University London, United Kingdom; Chacón C.C., Austrian Research Institute for Artificial Intelligence, Austria; Meredith D., Department of Architecture Design and Media Technology, Aalborg University, Denmark; Grachten M., Austrian Research Institute for Artificial Intelligence, Austria","We propose a method for music classification based on the use of convolutional models on symbolic pitch–time representations (i.e. piano-rolls) which we apply to composer recognition. An excerpt of a piece to be classified is first sampled to a 2D pitch–time representation which is then subjected to various transformations, including convolution with predefined filters (Morlet or Gaussian) and classified by means of support vector machines. We combine classifiers based on different pitch representations (MIDI and morphetic pitch) and different filter types and configurations. The method does not require parsing of the music into separate voices, or extraction of any other predefined features prior to processing; instead it is based on the analysis of texture in a 2D pitch–time representation. We show that filtering significantly improves recognition and that the method proves robust to encoding, transposition and amount of information. On discriminating between Haydn and Mozart string quartet movements, our best classifier reaches state-of-the-art performance in leave-one-out cross validation. © Gissel Velarde, Carlos Cancino Chacón, Tillman Weyde, David Meredith, Maarten Grachten.","","Convolution; Information retrieval; Statistical methods; Support vector machines; Textures; Amount of information; Convolutional model; Gaussians; Leave-one-out cross validations; Music classification; State-of-the-art performance; Information filtering","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Yang L.; Rajab K.Z.; Chew E.","Yang, Luwei (56987962600); Rajab, Khalid Z. (6603711813); Chew, Elaine (8706714000)","56987962600; 6603711813; 8706714000","AVA: An interactive system for visual and quantitative analyses of vibrato and portamento performance styles","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065063893&partnerID=40&md5=3d9524b37b598b845eefe352beae4445","Centre for Digital Music, Queen Mary University of London, United Kingdom; Antennas and Electromagnetics Group, Queen Mary University of London, United Kingdom","Yang L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Rajab K.Z., Antennas and Electromagnetics Group, Queen Mary University of London, United Kingdom; Chew E., Centre for Digital Music, Queen Mary University of London, United Kingdom","Vibratos and portamenti are important expressive features for characterizing performance style on instruments capable of continuous pitch variation such as strings and voice. Accurate study of these features is impeded by time consuming manual annotations. We present AVA, an interactive tool for automated detection, analysis, and visualization of vibratos and portamenti. The system implements a Filter Diagonalization Method (FDM)-based and a Hidden Markov Model-based method for vibrato and portamento detection. Vibrato parameters are reported directly from the FDM, and portamento parameters are given by the best fit Logistic Model. The graphical user interface (GUI) allows the user to edit the detection results, to view each vibrato or portamento, and to read the output parameters. The entire set of results can also be written to a text file for further statistical analysis. Applications of AVA include music summarization, similarity assessment, music learning, and musicological analysis. We demonstrate AVA’s utility by using it to analyze vibratos and portamenti in solo performances of two Beijing opera roles and two string instruments, erhu and violin. © Luwei Yang, Khalid Z. Rajab and Elaine Chew.","","Graphical user interfaces; Hidden Markov models; Automated detection; Filter diagonalization methods; Graphical user interfaces (GUI); Interactive system; Music summarization; Output parameters; Similarity assessment; String instruments; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Koops H.V.; Bas de Haas W.; Bountouridis D.; Volk A.","Koops, Hendrik Vincent (55925589400); Bas de Haas, W. (51160955300); Bountouridis, Dimitrios (36607463900); Volk, Anja (30567849900)","55925589400; 51160955300; 36607463900; 30567849900","Integration and quality assessment of heterogeneous chord sequences using data fusion","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069003712&partnerID=40&md5=3120e3369143d07e6124d76fab5c2113","Department of Information and Computing Sciences, Utrecht University, Netherlands; Chordify, Netherlands","Koops H.V., Department of Information and Computing Sciences, Utrecht University, Netherlands; Bas de Haas W., Chordify, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands","Two heads are better than one, and the many are smarter than the few. Integrating knowledge from multiple sources has shown to increase retrieval and classification accuracy in many domains. The recent explosion of crowd-sourced information, such as on websites hosting chords and tabs for popular songs, calls for sophisticated algorithms for data-driven quality assessment and data integration to create better, and more reliable data. In this paper, we propose to integrate the heterogeneous output of multiple automatic chord extraction algorithms using data fusion. First we show that data fusion creates significantly better chord label sequences from multiple sources, outperforming its source material, majority voting and random source integration. Second, we show that data fusion is capable of assessing the quality of sources with high precision from source agreement, without any ground-truth knowledge. Our study contributes to a growing body of work showing the benefits of integrating knowledge from multiple sources in an advanced way. © Hendrik Vincent Koops, W. Bas de Haas, Dimitrios Bountouridis, Anja Volk.","","Data fusion; Information retrieval; Chord sequence; Classification accuracy; Extraction algorithms; High-precision; Multiple source; Quality assessment; Random sources; Source material; Data integration","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Andersen K.; Knees P.","Andersen, Kristina (24079506700); Knees, Peter (8219023200)","24079506700; 8219023200","Conversations with expert users in music retrieval and research challenges for creative MIR","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027199576&partnerID=40&md5=6be4653e8e0d91848b0ec351c83e5654","Studio for Electro Instrumental Music (STEIM), Amsterdam, Netherlands; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Andersen K., Studio for Electro Instrumental Music (STEIM), Amsterdam, Netherlands; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Sample retrieval remains a central problem in the creative process of making electronic dance music. This paper describes the findings from a series of interview sessions involving users working creatively with electronic music. We conducted in-depth interviews with expert users on location at the Red Bull Music Academies in 2014 and 2015. When asked about their wishes and expectations for future technological developments in interfaces, most participants mentioned very practical requirements of storing and retrieving files. A central aspect of the desired systems is the need to provide increased flow and unbroken periods of concentration and creativity. From the interviews, it becomes clear that for Creative MIR, and in particular, for music interfaces for creative expression, traditional requirements and paradigms for music and audio retrieval differ to those from consumer-centered MIR tasks such as playlist generation and recommendation and that new paradigms need to be considered. Despite all technical aspects being controllable by the experts themselves, searching for sounds to use in composition remains a largely semantic process. From the outcomes of the interviews, we outline a series of possible conclusions and areas and pose two research challenges for future developments of sample retrieval interfaces in the creative domain. © Kristina Andersen, Peter Knees.","","Computer music; Electronic musical instruments; Information retrieval; Semantics; Surveys; Central problems; Creative process; In-depth interviews; Music interfaces; Practical requirements; Research challenges; Technical aspects; Technological development; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Tian M.; Sandler M.B.","Tian, Mi (55977248100); Sandler, Mark B. (7202740804)","55977248100; 7202740804","Music structural segmentation across genres with Gammatone features","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001383&partnerID=40&md5=324c63ddce05fa3518d262bfb023f51a","Centre for Digital Music, Queen Mary University of London, United Kingdom","Tian M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Music structural segmentation (MSS) studies to date mainly employ audio features describing the timbral, harmonic or rhythmic aspects of the music and are evaluated using datasets consisting primarily of Western music. A new dataset of Chinese traditional Jingju music with structural annotations is introduced in this paper to complement the existing evaluation framework. We discuss some statistics of the annotations analysing the inter-annotator agreements. We present two auditory features derived from the Gammatone filters based respectively on the cepstral analysis and the spectral contrast description. The Gammatone features and two commonly used features, Mel-frequency cepstral coefficients (MFCCs) and chromagram, are evaluated on the Jingju dataset as well as two existing used ones using several state-of-the-art algorithms. The investigated Gammatone features outperform MFCCs and chromagram when evaluated on the Jingju dataset and show similar performance with the Western datasets. We identify the presented Gammatone features as effective structure descriptors, especially for music lacking notable timbral or harmonic sectional variations. Results also indicate that the design of audio features and segmentation algorithms should be adapted to specific music genres to interpret individual structural patterns. © Mi Tian, Mark B. Sandler.","","Information retrieval; Auditory feature; Cepstral analysis; Evaluation framework; Gammatone filters; Mel-frequency cepstral coefficients; Segmentation algorithms; State-of-the-art algorithms; Structural pattern; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Chung C.-H.; Lou J.-K.; Chen H.","Chung, Chia-Hao (57161731400); Lou, Jing-Kai (58363676300); Chen, Homer (8236841800)","57161731400; 58363676300; 8236841800","A latent representation of users, sessions, and songs for listening behavior analysis","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027147737&partnerID=40&md5=0d34afa36d3d3a8c8e5c1ab7c8605165","National Taiwan University, Taiwan; KKBOX Inc, Taiwan","Chung C.-H., National Taiwan University, Taiwan; Lou J.-K., KKBOX Inc, Taiwan; Chen H., National Taiwan University, Taiwan","Understanding user listening behaviors is important to the personalization of music recommendation. In this paper, we present an approach that discovers user behavior from a large-scale, real-world listening record. The proposed approach generates a latent representation of users, listening sessions, and songs, where each of these objects is represented as a point in the multi-dimensional latent space. Since the distance between two points is an indication of the similarity of the two corresponding objects, it becomes extremely simple to evaluate the similarity between songs or the matching of songs with the user preference. By exploiting this feature, we provide a two-dimensional user behavior analysis framework for music recommendation. Exploring the relationships between user preference and the contextual or temporal information in the session data through this framework significantly facilitates personalized music recommendation. We provide experimental results to illustrate the strengths of the proposed approach for user behavior analysis. © Chia-Hao Chung, Jing-Kai Lou, Homer Chen.","","Information retrieval; Behavior analysis; Multi dimensional; Music recommendation; Personalizations; Real-world; Temporal information; User behavior analysis; User behaviors; Behavioral research","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Gray P.; Bunescu R.","Gray, Patrick (57195739590); Bunescu, Razvan (8285251100)","57195739590; 8285251100","A neural greedy model for voice separation in symbolic music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037737122&partnerID=40&md5=05b01292792a1dacc6ad52ec939ae421","School of EECS, Ohio University, Athens, OH, United States","Gray P., School of EECS, Ohio University, Athens, OH, United States; Bunescu R., School of EECS, Ohio University, Athens, OH, United States","Music is often experienced as a simultaneous progression of multiple streams of notes, or voices. The automatic separation of music into voices is complicated by the fact that music spans a voice-leading continuum ranging from monophonic, to homophonic, to polyphonic, often within the same work. We address this diversity by defining voice separation as the task of partitioning music into streams that exhibit both a high degree of external perceptual separation from the other streams and a high degree of internal perceptual consistency, to the maximum degree that is possible in the given musical input. Equipped with this task definition, we manually annotated a corpus of popular music and used it to train a neural network with one hidden layer that is connected to a diverse set of perceptually informed input features. The trained neural model greedily assigns notes to voices in a left to right traversal of the input chord sequence. When evaluated on the extraction of consecutive within voice note pairs, the model obtains over 91% F-measure, surpassing a strong baseline based on an iterative application of an envelope extraction function. © Patrick Gray, Razvan Bunescu.","","Extraction; Information retrieval; Automatic separations; Chord sequence; Envelope extraction; Input features; Maximum degree; Multiple streams; Neural modeling; Voice separation; Multilayer neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Creager E.; Stein N.D.; Badeau R.; Depalle P.","Creager, Elliot (57204806664); Stein, Noah D. (23570211200); Badeau, Roland (11939840000); Depalle, Philippe (35577100200)","57204806664; 23570211200; 11939840000; 35577100200","Nonnegative tensor factorization with frequency modulation cues for blind audio source separation","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061218406&partnerID=40&md5=36a0456f4f6dce8d512df53ca69e07ab","Analog Devices Lyric Labs, Cambridge, MA, United States; LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, Paris, France; CIRMMT, McGill University, Montréal, Canada","Creager E., Analog Devices Lyric Labs, Cambridge, MA, United States, CIRMMT, McGill University, Montréal, Canada; Stein N.D., Analog Devices Lyric Labs, Cambridge, MA, United States; Badeau R., LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay, Paris, France, CIRMMT, McGill University, Montréal, Canada; Depalle P., CIRMMT, McGill University, Montréal, Canada","We present Vibrato Nonnegative Tensor Factorization, an algorithm for single-channel unsupervised audio source separation with an application to separating instrumental or vocal sources with nonstationary pitch from music recordings. Our approach extends Nonnegative Matrix Factorization for audio modeling by including local estimates of frequency modulation as cues in the separation. This permits the modeling and unsupervised separation of vibrato or glissando musical sources, which is not possible with the basic matrix factorization formulation. The algorithm factorizes a sparse nonnegative tensor comprising the audio spectrogram and local frequency-slope-to-frequency ratios, which are estimated at each time-frequency bin using the Distributed Derivative Method. The use of local frequency modulations as separation cues is motivated by the principle of common fate partial grouping from Auditory Scene Analysis, which hypothesizes that each latent source in a mixture is characterized perceptually by coherent frequency and amplitude modulations shared by its component partials. We derive multiplicative factor updates by Minorization-Maximization, which guarantees convergence to a local optimum by iteration. We then compare our method to the baseline on two separation tasks: one considers synthetic vibrato notes, while the other considers vibrato string instrument recordings. © Elliot Creager, Noah D. Stein, Roland Badeau, Philippe Depalle.","","Audio acoustics; Audio recordings; Blind source separation; Factorization; Information retrieval; Iterative methods; Matrix algebra; Separation; Tensors; Audio source separation; Auditory Scene Analysis; Coherent frequency; Derivative method; Multiplicative factors; Nonnegative matrix factorization; Nonnegative tensor factorizations; String instruments; Frequency modulation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Kelz R.; Dorfer M.; Korzeniowski F.; Böck S.; Arzt A.; Widmer G.","Kelz, Rainer (57195436721); Dorfer, Matthias (55516844500); Korzeniowski, Filip (56328099700); Böck, Sebastian (55413719000); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","57195436721; 55516844500; 56328099700; 55413719000; 36681791200; 7004342843","On the potential of simple framewise approaches to piano transcription","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","62","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983069&partnerID=40&md5=b76eabad7894eeb717e52cc82a4bce82","Department of Computational Perception, Johannes Kepler University Linz, Austria","Kelz R., Department of Computational Perception, Johannes Kepler University Linz, Austria; Dorfer M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Korzeniowski F., Department of Computational Perception, Johannes Kepler University Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","In an attempt at exploring the limitations of simple approaches to the task of piano transcription (as usually defined in MIR), we conduct an in-depth analysis of neural network-based framewise transcription. We systematically compare different popular input representations for transcription systems to determine the ones most suitable for use with neural networks. Exploiting recent advances in training techniques and new regularizers, and taking into account hyper-parameter tuning, we show that it is possible, by simple bottom-up frame-wise processing, to obtain a piano transcriber that outperforms the current published state of the art on the publicly available MAPS dataset – without any complex post-processing steps. Thus, we propose this simple approach as a new baseline for this dataset, for future transcription research to build on and improve. © Rainer Kelz, Matthias Dorfer, Filip Korzeniowski, Sebastian Böck, Andreas Arzt, Gerhard Widmer.","","Information retrieval; Musical instruments; Bottom up; Hyper-parameter; In-depth analysis; Post processing; Simple approach; State of the art; Training techniques; Transcription","R. Kelz; Department of Computational Perception, Johannes Kepler University Linz, Austria; email: rainer.kelz@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Holzapfel A.; Grill T.","Holzapfel, Andre (18041818000); Grill, Thomas (6506275367)","18041818000; 6506275367","Bayesian meter tracking on learned signal representations","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069900283&partnerID=40&md5=f22dfcb22728a02e4e76a93aa11d29e1","Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Holzapfel A., Austrian Research Institute for Artificial Intelligence (OFAI), Austria; Grill T., Austrian Research Institute for Artificial Intelligence (OFAI), Austria","Most music exhibits a pulsating temporal structure, known as meter. Consequently, the task of meter tracking is of great importance for the domain of Music Information Retrieval. In our contribution, we specifically focus on Indian art musics, where meter is conceptualized at several hierarchical levels, and a diverse variety of metrical hierarchies exist, which poses a challenge for state of the art analysis methods. To this end, for the first time, we combine Convolutional Neural Networks (CNN), allowing to transcend manually tailored signal representations, with subsequent Dynamic Bayesian Tracking (BT), modeling the recurrent metrical structure in music. Our approach estimates meter structures simultaneously at two metrical levels. The results constitute a clear advance in meter tracking performance for Indian art music, and we also demonstrate that these results generalize to a set of Ballroom dances. Furthermore, the incorporation of neural network output allows a computationally efficient inference. We expect the combination of learned signal representations through CNNs and higher-level temporal modeling to be applicable to all styles of metered music, provided the availability of sufficient training data. © Andre Holzapfel, Thomas Grill.","","Bayesian networks; Information retrieval; Knowledge based systems; Recurrent neural networks; Computationally efficient; Convolutional neural network; Hierarchical level; Music information retrieval; Signal representations; Temporal modeling; Temporal structures; Tracking performance; Computer music","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Gregorio J.; Kim Y.E.","Gregorio, Jeff (55858427500); Kim, Youngmoo E. (24724623000)","55858427500; 24724623000","Phrase-level audio segmentation of jazz improvisations informed by symbolic data","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064122871&partnerID=40&md5=549a3db6029d9e29471bfbb62cd7c8e7","Drexel University, Dept. of Electrical and Computer Engineering, United States","Gregorio J., Drexel University, Dept. of Electrical and Computer Engineering, United States; Kim Y.E., Drexel University, Dept. of Electrical and Computer Engineering, United States","Computational music structure analysis encompasses any model attempting to organize music into qualitatively salient structural units, which can include anything in the heirarchy of large scale form, down to individual phrases and notes. While much existing audio-based segmentation work attempts to capture repetition and homogeneity cues useful at the form and thematic level, the time scales involved in phrase-level segmenation and the avoidance of repetition in improvised music necessitate alternate approaches in approaching jazz structure analysis. Recently, the Weimar Jazz Database has provided transcriptions of solos by a variety of eminent jazz performers. Utilizing a subset of these transcriptions aligned to their associated audio sources, we propose a model based on supervised training of a Hidden Markov Model with ground-truth state sequences designed to encode melodic contours appearing frequently in jazz improvisations. Results indicate that representing likely melodic contours in this way allows a low-level audio feature set containing primarily timbral and harmonic information to more accurately predict phrase boundaries. © Jeff Gregorio and Youngmoo E. Kim.","","Hidden Markov models; Information retrieval; Transcription; Alternate approaches; Audio segmentation; Model-based OPC; Music structure analysis; Phrase boundary; State sequences; Structure analysis; Supervised trainings; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Figueiredo F.; Ribeiro B.; Faloutsos C.; Andrade N.; Almeida J.M.","Figueiredo, Flavio (35317374600); Ribeiro, Bruno (8710903100); Faloutsos, Christos (7006005166); Andrade, Nazareno (7003429497); Almeida, Jussara M. (35586158500)","35317374600; 8710903100; 7006005166; 7003429497; 35586158500","Mining online music listening trajectories","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047727481&partnerID=40&md5=dd34c240b533b3a7f89f2c7ca2202f0a","IBM Research, Brazil; Purdue University, India; Carnegie Mellon University, United States; Universidade Federal de Campina Grande, Brazil; Universidade Federal de Minas Gerais, Brazil","Figueiredo F., IBM Research, Brazil; Ribeiro B., Purdue University, India; Faloutsos C., Carnegie Mellon University, United States; Andrade N., Universidade Federal de Campina Grande, Brazil; Almeida J.M., Universidade Federal de Minas Gerais, Brazil","Understanding the listening habits of users is a valuable undertaking for musicology researchers, artists, consumers and online businesses alike. With the rise of Online Music Streaming Services (OMSSs), large amounts of user behavioral data can be exploited for this task. In this paper, we present SWIFT-FLOWS, an approach that models user listening habits in regards to how user attention transitions between artists. SWIFT-FLOWS combines recent advances in trajectory mining, coupled with modulated Markov models as a means to capture both how users switch attention from one artist to another, as well as how users fixate their attention in a single artist over short or large periods of time. We employ SWIFT-FLOWS on OMSSs datasets showing that it provides: (1) semantically meaningful representation of habits; (2) accurately models the attention span of users. © Figueiredo, Ribeiro, Faloutsos, Andrade, Almeida.","","Information retrieval; Markov processes; Behavioral data; Large amounts; Markov model; Online business; Online music; Trajectory minings; User attention; Behavioral research","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Lambert A.J.; Weyde T.; Armstrong N.","Lambert, Andrew J. (57198302612); Weyde, Tillman (24476899500); Armstrong, Newton (55863565400)","57198302612; 24476899500; 55863565400","Adaptive frequency neural networks for dynamic pulse and metre perception","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069605671&partnerID=40&md5=fa161a71a0da450a4cfc190f07192301","City University London, United Kingdom","Lambert A.J., City University London, United Kingdom; Weyde T., City University London, United Kingdom; Armstrong N., City University London, United Kingdom","Beat induction, the means by which humans listen to music and perceive a steady pulse, is achieved via a perceptual and cognitive process. Computationally modelling this phenomenon is an open problem, especially when processing expressive shaping of the music such as tempo change. To meet this challenge we propose Adaptive Frequency Neural Networks (AFNNs), an extension of Gradient Frequency Neural Networks (GFNNs). GFNNs are based on neurodynamic models and have been applied successfully to a range of difficult music perception problems including those with syncopated and polyrhythmic stimuli. AFNNs extend GFNNs by applying a Hebbian learning rule to the oscillator frequencies. Thus the frequencies in an AFNN adapt to the stimulus through an attraction to local areas of resonance, and allow for a great dimensionality reduction in the network. Where previous work with GFNNs has focused on frequency and amplitude responses, we also consider phase information as critical for pulse perception. Evaluating the time-based output, we find significantly improved responses of AFNNs compared to GFNNs to stimuli with both steady and varying pulse frequencies. This leads us to believe that AFNNs could replace the linear filtering methods commonly used in beat tracking and tempo estimation systems, and lead to more accurate methods. © Andrew J. Lambert, Tillman Weyde, and Newton Armstrong.","","Information retrieval; Adaptive frequency; Amplitude response; Cognitive process; Dimensionality reduction; Linear filtering method; Neurodynamic models; Oscillator frequency; Tempo estimations; Computer music","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Nieto O.; Bello J.P.","Nieto, Oriol (55583364500); Bello, Juan Pablo (7102889110)","55583364500; 7102889110","Systematic exploration of computational music structure research","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070008001&partnerID=40&md5=78d35f0a7d87a874a5d25de30c8f71d4","Pandora Media, Inc., United States; Music and Audio Research Laboratory, New York University, United States","Nieto O., Pandora Media, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","In this work we present a framework containing open source implementations of multiple music structural segmentation algorithms and employ it to explore the hyper parameters of features, algorithms, evaluation metrics, datasets, and annotations of this MIR task. Besides testing and discussing the relative importance of the moving parts of the computational music structure eco-system, we also shed light on its current major challenges. Additionally, a new dataset containing multiple structural annotations for tracks that are particularly ambiguous to analyze is introduced, and used to quantify the impact of specific annotators when assessing automatic approaches to this task. Results suggest that more than one annotation per track is necessary to fully address the problem of ambiguity in music structure research. © Oriol Nieto, Juan Pablo Bello.","","Automatic approaches; Evaluation metrics; Hyper-parameter; Moving parts; Music structures; Open source implementation; Segmentation algorithms; Systematic exploration; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Southall C.; Stables R.; Hockman J.","Southall, Carl (57201882762); Stables, Ryan (55322057300); Hockman, Jason (36730968100)","57201882762; 55322057300; 36730968100","Automatic drum transcription using bi-directional recurrent neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044052239&partnerID=40&md5=8c3f8aac96e563f8fc3e7a61da9d2a27","Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom","Southall C., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom; Stables R., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom; Hockman J., Digital Media Technology Laboratory (DMT Lab), Birmingham City University, Birmingham, United Kingdom","Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive instruments in audio recordings. Neural networks have already been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We propose the use of neural networks for ADT in order to exploit their ability to capture a complex configuration of features associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neural network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suitable for online operation. In both systems, a separate network is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilising the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respectively. The results demonstrate the effectiveness of the presented methods for solo percussion and a capacity for identifying snare drums, which are historically the most difficult drum class to detect. © Carl Southall, Ryan Stables, Jason Hockman.","","Audio acoustics; Audio recordings; Audio systems; Information retrieval; Source separation; Transcription; Bi-directional; Complex configuration; In-field; Music notation; Offline; Online operations; Onset detection; Time-series data; Recurrent neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Tse T.; Salamon J.; Williams A.; Jiang H.; Law E.","Tse, Tim (57210235360); Salamon, Justin (55184866100); Williams, Alex (55752661600); Jiang, Helga (57210235823); Law, Edith (35173013200)","57210235360; 55184866100; 55752661600; 57210235823; 35173013200","Ensemble: A hybrid human-machine system for generating melody scores from audio","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001294&partnerID=40&md5=f7cbe786b0f15f9e2f20b7e547f011eb","University of Waterloo, Canada; New York University, United States","Tse T., University of Waterloo, Canada; Salamon J., New York University, United States; Williams A., University of Waterloo, Canada; Jiang H., University of Waterloo, Canada; Law E., University of Waterloo, Canada","Music transcription is a highly complex task that is difficult for automated algorithms, and equally challenging to people, even those with many years of musical training. Furthermore, there is a shortage of high-quality datasets for training automated transcription algorithms. In this research, we explore a semi-automated, crowdsourced approach to generate music transcriptions, by first running an automatic melody transcription algorithm on a (polyphonic) song to produce a series of discrete notes representing the melody, and then soliciting the crowd to correct this melody. We present a novel web-based interface that enables the crowd to correct transcriptions, report results from an experiment to understand the capabilities of non-experts to perform this challenging task, and characterize the characteristics and actions of workers and how they correlate with transcription performance. © Tim Tse, Justin Salamon, Alex Williams, Helga Jiang and Edith Law.","","Automation; Information retrieval; Multimedia systems; Transcription; Automated algorithms; Complex task; High quality; Music transcription; Web-based interface; Computer music","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Groves R.","Groves, Ryan (56120491300)","56120491300","Automatic melodic reduction using a supervised probabilistic context-free grammar","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064348639&partnerID=40&md5=d2193efca7deec1969459261fd766bdb","","","This research explores a Natural Language Processing technique utilized for the automatic reduction of melodies: the Probabilistic Context-Free Grammar (PCFG). Automatic melodic reduction was previously explored by means of a probabilistic grammar [11] [1]. However, each of these methods used unsupervised learning to estimate the probabilities for the grammar rules, and thus a corpus-based evaluation was not performed. A dataset of analyses using the Generative Theory of Tonal Music (GTTM) exists [13], which contains 300 Western tonal melodies and their corresponding melodic reductions in tree format. In this work, supervised learning is used to train a PCFG for the task of melodic reduction, using the tree analyses provided by the GTTM dataset. The resulting model is evaluated on its ability to create accurate reduction trees, based on a node-by-node comparison with ground-truth trees. Multiple data representations are explored, and example output reductions are shown. Motivations for performing melodic reduction include melodic identification and similarity, efficient storage of melodies, automatic composition, variation matching, and automatic harmonic analysis. © Ryan Groves.","","Context free grammars; Digital storage; Forestry; Information retrieval; Machine learning; Natural language processing systems; Automatic composition; Grammar rules; Ground truth; Multiple data; NAtural language processing; Probabilistic context free grammars; Probabilistic grammars; Reduction trees; Context free languages","R. Groves; email: groves.ryan@gmail.com","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Nishikimi R.; Nakamura E.; Itoyama K.; Yoshii K.","Nishikimi, Ryo (57207989997); Nakamura, Eita (24587601200); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120)","57207989997; 24587601200; 18042499100; 7103400120","Musical note estimation for F0 trajectories of singing voices based on a Bayesian semi-beat-synchronous HMM","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063527235&partnerID=40&md5=65be7635bb30aace1f9cac219ccc0d2a","Graduate School of Informatics, Kyoto University, Japan","Nishikimi R., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper presents a statistical method that estimates a sequence of discrete musical notes from a temporal trajectory of vocal F0s. Since considerable effort has been devoted to estimate the frame-level F0s of singing voices from music audio signals, we tackle musical note estimation for those F0s to obtain a symbolic musical score. A naïve approach to musical note estimation is to quantize the vocal F0s at a semitone level in every time unit (e.g., half beat). This approach, however, fails when the vocal F0s are significantly deviated from those specified by a musical score. The onsets of musical notes are often delayed or advanced from beat times and the vocal F0s fluctuate according to singing expressions. To deal with these deviations, we propose a Bayesian hidden Markov model that allows musical notes to change in semi-synchronization with beat times. Both the semitone-level F0s and onset deviations of musical notes are regarded as latent variables and the frequency deviations are modeled by an emission distribution. The musical notes and their onset and frequency deviations are jointly estimated by using Gibbs sampling. Experimental results showed that the proposed method improved the accuracy of musical note estimation against baseline methods. © Ryo Nishikimi, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii.","","Hidden Markov models; Information retrieval; Baseline methods; Emission distribution; Frequency deviation; Gibbs sampling; Latent variable; Musical score; Singing voices; Temporal trajectories; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Ojima Y.; Nakamura E.; Itoyama K.; Yoshii K.","Ojima, Yuta (57204770280); Nakamura, Eita (24587601200); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120)","57204770280; 24587601200; 18042499100; 7103400120","A hierarchical bayesian model of chords, pitches, and spectrograms for multipitch analysis","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038407976&partnerID=40&md5=e1bbcea5d8cf9383237c26b601f069d1","Graduate School of Informatics, Kyoto University, Japan","Ojima Y., Graduate School of Informatics, Kyoto University, Japan; Nakamura E., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan","This paper presents a statistical multipitch analyzer that can simultaneously estimate pitches and chords (typical pitch combinations) from music audio signals in an unsupervised manner. A popular approach to multipitch analysis is to perform nonnegative matrix factorization (NMF) for estimating the temporal activations of semitone-level pitches and then execute thresholding for making a piano-roll representation. The major problems of this cascading approach are that an optimal threshold is hard to determine for each musical piece and that musically inappropriate pitch combinations are allowed to appear. To solve these problems, we propose a probabilistic generative model that fuses an acoustic model (NMF) for a music spectrogram with a language model (hidden Markov model; HMM) for pitch locations in a hierarchical Bayesian manner. More specifically, binary variables indicating the existences of pitches are introduced into the framework of NMF. The latent grammatical structures of those variables are regulated by an HMM that encodes chord progressions and pitch co-occurrences (chord components). Given a music spectrogram, all the latent variables (pitches and chords) are estimated jointly by using Gibbs sampling. The experimental results showed the great potential of the proposed method for unified music transcription and grammar induction. © Yuta Ojima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii.","","Bayesian networks; Factorization; Hidden Markov models; Information retrieval; Matrix algebra; Spectrographs; Binary variables; Grammar induction; Grammatical structure; Hierarchical bayesian; Hierarchical Bayesian modeling; Music transcription; Nonnegative matrix factorization; Optimal threshold; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Stoller D.; Dixon S.","Stoller, Daniel (56928086600); Dixon, Simon (7201479437)","56928086600; 7201479437","Analysis and classification of phonation modes in singing","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021047&partnerID=40&md5=7b813e6dd44000f7e88da3dfb4a7f9a5","Queen Mary University of London, United Kingdom","Stoller D., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom","Phonation mode is an expressive aspect of the singing voice and can be described using the four categories neutral, breathy, pressed and flow. Previous attempts at automatically classifying the phonation mode on a dataset containing vowels sung by a female professional have been lacking in accuracy or have not sufficiently investigated the characteristic features of the different phonation modes which enable successful classification. In this paper, we extract a large range of features from this dataset, including specialised descriptors of pressedness and breathiness, to analyse their explanatory power and robustness against changes of pitch and vowel. We train and optimise a feed-forward neural network (NN) with one hidden layer on all features using cross validation to achieve a mean F-measure above 0.85 and an improved performance compared to previous work. Applying feature selection based on mutual information and retaining the nine highest ranked features as input to a NN results in a mean F-measure of 0.78, demonstrating the suitability of these features to discriminate between phonation modes. Training and pruning a decision tree yields a simple rule set based only on cepstral peak prominence (CPP), temporal flatness and average energy that correctly categorises 78% of the recordings. © Daniel Stoller, Simon Dixon.","","Decision trees; Feedforward neural networks; Information retrieval; Large dataset; Linguistics; Multilayer neural networks; Speech; Average energy; Cross validation; Descriptors; Explanatory power; F measure; Hidden layers; Mutual informations; Singing voices; Classification (of information)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Seetharaman P.; Pardo B.","Seetharaman, Prem (55391829300); Pardo, Bryan (10242155400)","55391829300; 10242155400","Simultaneous separation and segmentation in layered music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054225782&partnerID=40&md5=f82bff8340695ff281677d00d2685afa","Northwestern University, United States","Seetharaman P., Northwestern University, United States; Pardo B., Northwestern University, United States","In many pieces of music, the composer signals how individual sonic elements (samples, loops, the trumpet section) should be grouped by introducing sources or groups in a layered manner. We propose to discover and leverage the layering structure and use it for both structural segmentation and source separation. We use reconstruction error from non-negative matrix factorization (NMF) to guide structure discovery. Reconstruction error spikes at moments of significant sonic change. This guides segmentation and also lets us group basis sets for NMF. The number of sources, the types of sources, and when the sources are active are not known in advance. The only information is a specific type of layering structure. There is no separate training phase to learn a good basis set. No prior seeding of the NMF matrices is required. Unlike standard approaches to NMF there is no need for a post-processor to partition the learned basis functions by group. Source groups are learned automatically from the data. We evaluate our method on mixtures consisting of looping source groups. This separation approach outperforms a standard clustering NMF source separation approach on such mixtures. We find our segmentation approach is competitive with state-of-the-art segmentation methods on this dataset. © Prem Seetharaman, Bryan Pardo.","","Factorization; Information retrieval; Matrix algebra; Mixtures; Separation; Basis functions; Nonnegative matrix factorization; Number of sources; Post-processor; Reconstruction error; Segmentation methods; Simultaneous separation; State of the art; Source separation","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Gong R.; Pons J.; Serra X.","Gong, Rong (57200496600); Pons, Jordi (57190284265); Serra, Xavier (55892979900)","57200496600; 57190284265; 55892979900","Audio to score matching by combining phonetic and duration information","2017","Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041405755&partnerID=40&md5=d796cd5b3a2dbca343f2f389f06f7eca","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Pons J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We approach the singing phrase audio to score matching problem by using phonetic and duration information - with a focus on studying the jingju a cappella singing case. We argue that, due to the existence of a basic melodic contour for each mode in jingju music, only using melodic information (such as pitch contour) will result in an ambiguous matching. This leads us to propose a matching approach based on the use of phonetic and duration information. Phonetic information is extracted with an acoustic model shaped with our data, and duration information is considered with the Hidden Markov Models (HMMs) variants we investigate. We build a model for each lyric path in our scores and we achieve the matching by ranking the posterior probabilities of the decoded most likely state sequences. Three acoustic models are investigated: (i) convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and (iii) Gaussian mixture models (GMMs). Also, two duration models are compared: (i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model. Results show that CNNs perform better in our (small) audio dataset and also that HSMM outperforms the post-processor duration model. © 2019 Rong Gong, Jordi Pons and Xavier Serra.","","Deep neural networks; Hidden Markov models; Information retrieval; Linguistics; Neural networks; Trellis codes; Convolutional neural network; Duration modeling; Gaussian mixture model (GMMs); Hidden markov models (HMMs); Hidden semi-Markov models; Melodic information; Phonetic information; Posterior probability; Information use","","","18th International Society for Music Information Retrieval Conference, ISMIR 2017","23 October 2017 through 27 October 2017","Suzhou","149381"
"Hsu K.-C.; Lin C.-S.; Chi T.-S.","Hsu, Kai-Chun (57210232901); Lin, Chih-Shan (57210235175); Chi, Tai-Shih (7005693586)","57210232901; 57210235175; 7005693586","Sparse coding based music genre classification using spectro-temporal modulations","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048391041&partnerID=40&md5=fa52dbc1256373ba84dd0d063c8d006c","Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Hsu K.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Lin C.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Chi T.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Spectro-temporal modulations (STMs) of the sound convey timbre and rhythm information so that they are intuitively useful for automatic music genre classification. The STMs are usually extracted from a time-frequency representation of the acoustic signal. In this paper, we investigate the efficacy of two kinds of STM features, the Gabor features and the rate-scale (RS) features, selectively extracted from various time-frequency representations, including the short-time Fourier transform (STFT) spectrogram, the constant-Q transform (CQT) spectrogram and the auditory (AUD) spectrogram, in recognizing the music genre. In our system, the dictionary learning and sparse coding techniques are adopted for training the support vector machine (SVM) classifier. Both spectral-type features and modulation-type features are used to test the system. Experiment results show that the RS features extracted from the log. magnituded CQT spectrogram produce the highest recognition rate in classifying the music genre. © Kai-Chun Hsu, Chih-Shan Lin, Tai-Shih Chi.","","Codes (symbols); Information retrieval; Modulation; Spectrographs; Support vector machines; Acoustic signals; Constant q transforms; Dictionary learning; Modulation types; Music genre classification; Short time Fourier transforms; Spectro-temporal modulations; Time-frequency representations; Classification (of information)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Calvo-Zaragoza J.; Rizo D.; Iñesta J.M.","Calvo-Zaragoza, Jorge (55847598300); Rizo, David (14042169500); Iñesta, Jose M. (6701387099)","55847598300; 14042169500; 6701387099","Two (note) heads are better than one: Pen-based multimodal interaction with music scores","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045189375&partnerID=40&md5=b6f4e2800c3be5a5f219f73a7545002e","Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain","Calvo-Zaragoza J., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain; Rizo D., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain; Iñesta J.M., Pattern Recognition and Artificial Intelligence Group, Department of Software and Computing Systems, University of Alicante, Spain","Digitizing early music sources requires new ways of dealing with musical documents. Assuming that current technologies cannot guarantee a perfect automatic transcription, our intention is to develop an interactive system in which user and software collaborate to complete the task. Since conventional score post-editing might be tedious, the user is allowed to interact using an electronic pen. Although this provides a more ergonomic interface, this interaction must be decoded as well. In our framework, the user traces the symbols using the electronic pen over a digital surface, which provides both the underlying image (offline data) and the drawing made by the e-pen (online data) to improve classification. Applying this methodology over 70 scores of the target musical archive, a dataset of 10 230 bimodal samples of 30 different symbols was obtained and made available for research purposes. This paper presents experimental results on classification over this dataset, in which symbols are recognized by combining the two modalities. This combination of modes has demonstrated its good performance, decreasing the error rate of using each modality separately and achieving an almost error-free performance. © Jorge Calvo-Zaragoza, David Rizo, Jose M. Iñesta.","","Electronic musical instruments; Image enhancement; Information retrieval; Automatic transcription; Combination of modes; Current technology; Digital surfaces; Electronic pen; Interactive system; Multi-Modal Interactions; Research purpose; Classification (of information)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Lewis D.; Crawford T.; Müllensiefen D.","Lewis, David (55742498400); Crawford, Tim (15054056900); Müllensiefen, Daniel (23019169400)","55742498400; 15054056900; 23019169400","Instrumental idiom in the 16th century: Embellishment patterns in arrangements of vocal music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069810926&partnerID=40&md5=ebf544073735aa07561c8e726dcf51a4","Goldsmiths, University of London, United Kingdom","Lewis D., Goldsmiths, University of London, United Kingdom; Crawford T., Goldsmiths, University of London, United Kingdom; Müllensiefen D., Goldsmiths, University of London, United Kingdom","Much surviving 16th-century instrumental music consists of arrangements (‘intabulations’) of vocal music, in tablature for solo lute. Intabulating involved deciding what to omit from a score to fit the instrument, and making it fit under the hand. Notes were usually added as embellishments to the original plain score, using idiomatic patterns, typically at cadences, but often filling simple intervals in the vocal parts with faster notes. Here we test whether such patterns are both characteristic of lute intabulations as a class (vs original lute music) and of different genres within that class. We use patterns identified in the musicological literature to search two annotated corpora of encoded lute music using the SIA(M)ESE algorithm. Diatonic patterns occur in many chromatic forms, accidentals being added depending how the arranger applied the conventions of musica ficta. Rhythms must be applied at three different scales as notation is inconsistent across the repertory. This produced over 88,000 short melodic queries to search in two corpora totalling just over 6,000 encodings of lute pieces. We show that our method clearly discriminates between intabulations and original music for the lute (p < .001); it also can distinguish sacred and secular genres within the vocal models (p < .001). © David Lewis, Tim Crawford, Daniel Müllensiefen.","","Encodings; Vocal music; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"López-Serrano P.; Dittmar C.; Driedger J.; Müller M.","López-Serrano, Patricio (57195436618); Dittmar, Christian (15051598000); Driedger, Jonathan (55582371700); Müller, Meinard (7404689873)","57195436618; 15051598000; 55582371700; 7404689873","Towards modeling and decomposing loop-based electronic music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028067276&partnerID=40&md5=af70192bade8a2f68137b13028e08a19","International Audio Laboratories Erlangen, Germany","López-Serrano P., International Audio Laboratories Erlangen, Germany; Dittmar C., International Audio Laboratories Erlangen, Germany; Driedger J., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the field of MIR. A fundamental structural unit in EM are loops—audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a musical structure in which loops are repeatedly triggered and overlaid. This particular structure allows new perspectives on well-known MIR tasks. In this paper we first review a prototypical production technique for EM from which we derive a simplified model. We then use our model to illustrate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by finding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as fingerprinting and non-negative matrix factor deconvolution. © Patricio López-Serrano, Christian Dittmar, Jonathan Driedger, Meinard Müller.","","Audio acoustics; Digital devices; Electronic musical instruments; Information retrieval; Digital audio; Electronic music; Musical structures; Non-negative matrix; Production techniques; Research subjects; Structural unit; Computer music","P. López-Serrano; International Audio Laboratories Erlangen, Germany; email: patricio.lopez.serrano@audiolabs-erlangen.de","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Fuller J.; Hubener L.; Kim Y.-S.; Lee J.H.","Fuller, John (57210232975); Hubener, Lauren (57210231378); Kim, Yea-Seul (56157789900); Lee, Jin Ha (57190797465)","57210232975; 57210231378; 56157789900; 57190797465","Elucidating user behavior in music services through persona and gender","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061411382&partnerID=40&md5=dbd3d0b2a33ed4bc0f20cbb8d393a447","University of Washington, United States","Fuller J., University of Washington, United States; Hubener L., University of Washington, United States; Kim Y.-S., University of Washington, United States; Lee J.H., University of Washington, United States","Prior user studies in the music information retrieval field have identified different personas representing the needs, goals, and characteristics of specific user groups for a user-centered design of music services. However, these personas were derived from a qualitative study involving a small number of participants and their generalizability has not been tested. The objectives of this study are to explore the applicability of seven user personas, developed in prior research, with a larger group of users and to identify the correlation between personas and the use of different types of music services. In total, 962 individuals were surveyed in order to understand their behaviors and preferences when interacting with music streaming services. Using a stratified sampling framework, key characteristics of each persona were extracted to classify users into specific persona groups. Responses were also analyzed in relation to gender, which yielded significant differences. Our findings support the development of more targeted approaches in music services rather than a universal service model. © John Fuller, Lauren Hubener, Yea-Seul Kim, Jin Ha Lee.","","Information retrieval; Key characteristics; Music information retrieval; Music streaming; Qualitative study; Stratified sampling; Universal service; User behaviors; User groups; Behavioral research","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Choi K.; Fazekas G.; Sandler M.","Choi, Keunwoo (57190944352); Fazekas, György (37107520200); Sandler, Mark (7202740804)","57190944352; 37107520200; 7202740804","Automatic tagging using deep convolutional neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","125","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069993895&partnerID=40&md5=a04f30f5a33ff5beff624d3978e643a5","Queen Mary University of London, United Kingdom","Choi K., Queen Mary University of London, United Kingdom; Fazekas G., Queen Mary University of London, United Kingdom; Sandler M., Queen Mary University of London, United Kingdom","We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data. © Keunwoo Choi, György Fazekas, Mark Sandler.","","Convolution; Information retrieval; Network architecture; Neural networks; Spectrographs; Automatic Music Tagging; Automatic tagging; Content-based; Convolutional neural network; Effective time; Number of layers; State-of-the-art performance; Subsampling layers; Deep neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Lu Y.-C.; Wu C.-W.; Lu C.-T.; Lerch A.","Lu, Yen-Cheng (56024608900); Wu, Chih-Wei (57188865070); Lu, Chang-Tien (7404804653); Lerch, Alexander (22034963000)","56024608900; 57188865070; 7404804653; 22034963000","Automatic outlier detection in music genre datasets","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038962698&partnerID=40&md5=2b0fa8b48b6e0c2918fb1d70031207d8","Department of Computer Science, Virginia Tech, United States; Center for Music Technology, Georgia Institute of Technology, United States","Lu Y.-C., Department of Computer Science, Virginia Tech, United States; Wu C.-W., Center for Music Technology, Georgia Institute of Technology, United States; Lu C.-T., Department of Computer Science, Virginia Tech, United States; Lerch A., Center for Music Technology, Georgia Institute of Technology, United States","Outlier detection, also known as anomaly detection, is an important topic that has been studied for decades. An outlier detection system is able to identify anomalies in a dataset and thus improve data integrity by removing the detected outliers. It has been successfully applied to different types of data in various fields such as cyber-security, finance, and transportation. In the field of Music Information Retrieval (MIR), however, the number of related studies is small. In this paper, we introduce different state-of-the-art outlier detection techniques and evaluate their viability in the context of music datasets. More specifically, we present a comparative study of 6 outlier detection algorithms applied to a Music Genre Recognition (MGR) dataset. It is determined how well algorithms can identify mislabeled or corrupted files, and how much the quality of the dataset can be improved. Results indicate that state-of-the-art anomaly detection systems have problems identifying anomalies in MGR datasets reliably. © Yen-Cheng Lu, Chih-Wei Wu, Chang-Tien Lu, Alexander Lerch.","","Data handling; Information retrieval; Statistics; Anomaly detection systems; Comparative studies; Cyber security; Data integrity; Music information retrieval; Outlier detection algorithm; Outlier detection systems; State of the art; Anomaly detection","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Jeong I.-Y.; Lee K.","Jeong, Il-Young (56226863100); Lee, Kyogu (8597995500)","56226863100; 8597995500","Learning temporal features using a deep neural network and its application to music genre classification","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028393149&partnerID=40&md5=30f563318989fb30ba78478958fdae7d","Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","Jeong I.-Y., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Lee K., Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, South Korea","In this paper, we describe a framework for temporal feature learning from audio with a deep neural network, and apply it to music genre classification. To this end, we revisit the conventional spectral feature learning framework, and reformulate it in the cepstral modulation spectrum domain, which has been successfully used in many speech and music-related applications for temporal feature extraction. Experimental results using the GTZAN dataset show that the temporal features learned from the proposed method are able to obtain classification accuracy comparable to that of the learned spectral features. © Il-Young Jeong and Kyogu Lee.","","Audio acoustics; Classification (of information); Information retrieval; Machine learning; Cepstral; Classification accuracy; ITS applications; Modulation spectrum; Music genre classification; Spectral feature; Temporal features; Deep neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Waloschek S.; Berndt A.; Bohl B.W.; Hadjakos A.","Waloschek, Simon (57191268983); Berndt, Axel (25960650300); Bohl, Benjamin W. (55920317700); Hadjakos, Aristotelis (35113080700)","57191268983; 25960650300; 55920317700; 35113080700","Interactive scores in classical music production","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060930767&partnerID=40&md5=082aa24f57331470658e960f5cf05cf4","Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany","Waloschek S., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Berndt A., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Bohl B.W., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany; Hadjakos A., Center of Music And Film Informatics (CeMFI), University of Music Detmold, Germany","The recording of classical music is mostly centered around the score of a composition. During editing of these recordings, however, further technical visualizations are used. Introducing digital interactive scores to the recording and editing process can enhance the workflow significantly and speed up the production process. This paper gives a short introduction to the recording process and outlines possibilities that arise with interactive scores. Current related music information retrieval research is discussed, showing a potential path to score-based editing. © Simon Waloschek, Axel Berndt, Benjamin W. Bohl, Aristotelis Hadjakos.","","Information retrieval; Classical musics; Music information retrieval; Production process; Recording process; Speed up; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Shanahan D.; Neubarth K.; Conklin D.","Shanahan, Daniel (55693677900); Neubarth, Kerstin (25925322600); Conklin, Darrell (57220096325)","55693677900; 25925322600; 57220096325","Mining musical traits of social functions in native American music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039416796&partnerID=40&md5=ff5af630087d59b31bc8a187a4fb7f3e","Louisiana State University, Baton Rouge, LA, United States; Canterbury Christ Church University, United Kingdom; University of the Basque Country UPV/EHU, San Sebastian, Spain; IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","Shanahan D., Louisiana State University, Baton Rouge, LA, United States; Neubarth K., Canterbury Christ Church University, United Kingdom; Conklin D., University of the Basque Country UPV/EHU, San Sebastian, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","Native American music is perhaps one of the most documented repertoires of indigenous folk music, being the subject of empirical ethnomusicological analyses for significant portions of the early 20th century. However, it has been largely neglected in more recent computational research, partly due to a lack of encoded data. In this paper we use the symbolic encoding of Frances Densmore’s collection of over 2000 songs, digitized between 1998 and 2014, to examine the relationship between internal musical features and social function. More specifically, this paper applies contrast data mining to discover global feature patterns that describe generalized social functions. Extracted patterns are discussed with reference to early ethnomusicological work and recent approaches to music, emotion, and ethology. A more general aim of this paper is to provide a methodology in which contrast data mining can be used to further examine the interactions between musical features and external factors such as social function, geography, language, and emotion. © Daniel Shanahan, Kerstin Neubarth, Darrell Conklin.","","Biology; Information retrieval; 20th century; Computational researches; External factors; Global feature; Musical features; Native Americans; Social function; Data mining","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Hyrkas J.; Howe B.","Hyrkas, Jeremy (56497022000); Howe, Bill (57203255530)","56497022000; 57203255530","MusicDB: A platform for longitudinal music analytics","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070007303&partnerID=40&md5=8db7049dff0909075761355c3cb35159","University of Washington, United States","Hyrkas J., University of Washington, United States; Howe B., University of Washington, United States","With public data sources such as Million Song dataset, researchers can now study longitudinal questions about the patterns of popular music, but the scale and complexity of the data complicate analysis. We propose MusicDB, a new approach for longitudinal music analytics that adapts techniques from relational databases to the music setting. By representing song timeseries data relationally, we aim to dramatically decrease the programming effort required for complex analytics while significantly improving scalability. We show how our platform can improve performance by reducing the amount of data accessed for many common analytics tasks, and how such tasks can be implemented quickly in relational languages — variants of SQL. We further show that expressing music analytics tasks over relational representations allows the system to automatically parallelize and optimize the resulting programs to improve performance. We evaluate our system by expressing complex analytics tasks including calculating song density and beat-aligning features and showing significant performance improvements over previous results. Finally, we evaluate expressiveness by reproducing the results from a recent analysis of longitudinal music trends using the Million Song dataset. © Jeremy Hyrkas, Bill Howe.","","Improve performance; New approaches; Popular music; Public data source; Relational Database; Relational languages; Relational representations; Time-series data; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Beauguitte P.; Duggan B.; Kelleher J.","Beauguitte, Pierre (57215142210); Duggan, Bryan (24823868500); Kelleher, John (14035902400)","57215142210; 24823868500; 14035902400","A corpus of annotated Irish traditional dance music recordings: Design and benchmark evaluations","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056639280&partnerID=40&md5=fcbcb5f6e1bc24b287fe472923be65de","School of Computing, Dublin Institute of Technology, Ireland","Beauguitte P., School of Computing, Dublin Institute of Technology, Ireland; Duggan B., School of Computing, Dublin Institute of Technology, Ireland; Kelleher J., School of Computing, Dublin Institute of Technology, Ireland","An emerging trend in music information retrieval (MIR) is the use of supervised machine learning to train automatic music transcription models. A prerequisite of adopting a machine learning methodology is the availability of annotated corpora. However, different genres of music have different characteristics and modelling these characteristics is an important part of creating state of the art MIR systems. Consequently, although some music corpora are available the use of these corpora is tied to the specific music genre, instrument type and recording context the corpus covers. This paper introduces the first corpus of annotations of audio recordings of Irish traditional dance music that covers multiple instrument types and both solo studio and live session recordings. We first discuss the considerations that motivated our design choices in developing the corpus. We then benchmark a number of automatic music transcription algorithms against the corpus. © Pierre Beauguitte, Bryan Duggan and John Kelleher.","","Audio acoustics; Audio recordings; Information retrieval; Machine learning; Supervised learning; Transcription; Automatic music transcription; Benchmark evaluation; Emerging trends; Multiple instruments; Music information retrieval; Music recording; State of the art; Supervised machine learning; Computer music","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bogdanov D.; Porter A.; Herrera P.; Serra X.","Bogdanov, Dmitry (35748642000); Porter, Alastair (55582507300); Herrera, Perfecto (24824250300); Serra, Xavier (55892979900)","35748642000; 55582507300; 24824250300; 55892979900","Cross-collection evaluation for music classification tasks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020298303&partnerID=40&md5=d2501a0704d71226ca108a4b03c2e7c1","Music Technology Group, Universitat Pompeu Fabra, Spain","Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Spain; Porter A., Music Technology Group, Universitat Pompeu Fabra, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Many studies in music classification are concerned with obtaining the highest possible cross-validation result. However, some studies have noted that cross-validation may be prone to biases and that additional evaluations based on independent out-of-sample data are desirable. In this paper we present a methodology and software tools for cross-collection evaluation for music classification tasks. The tools allow users to conduct large-scale evaluations of classifier models trained within the AcousticBrainz platform, given an independent source of ground-truth annotations, and its mapping with the classes used for model training. To demonstrate the application of this methodology we evaluate five models trained on genre datasets commonly used by researchers for genre classification, and use collaborative tags from Last.fm as an independent source of ground truth. We study a number of evaluation strategies using our tools on validation sets from 240,000 to 1,740,000 music recordings and discuss the results. © Dmitry Bogdanov, Alastair Porter, Perfecto Herrera, Xavier Serra.","","Audio recordings; Information retrieval; Classifier models; Cross validation; Evaluation strategies; Genre classification; Independent sources; Music classification; Music recording; Validation sets; Classification (of information)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Osmalskyj J.; Foster P.; Dixon S.; Embrechts J.-J.","Osmalskyj, Julien (55967414200); Foster, Peter (55328977600); Dixon, Simon (7201479437); Embrechts, Jean-Jacques (6603811516)","55967414200; 55328977600; 7201479437; 6603811516","Combining features for cover song identification","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069981861&partnerID=40&md5=4f9f7466843a406fa51ecce8b0d0f25f","University of Liège, Belgium; Queen Mary University of London, United Kingdom","Osmalskyj J., University of Liège, Belgium; Foster P., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom; Embrechts J.-J., University of Liège, Belgium","In this paper, we evaluate a set of methods for combining features for cover song identification. We first create multiple classifiers based on global tempo, duration, loudness, beats and chroma average features, training a random forest for each feature. Subsequently, we evaluate standard combination rules for merging these single classifiers into a composite classifier based on global features. We further obtain two higher level classifiers based on chroma features: one based on comparing histograms of quantized chroma features, and a second one based on computing cross-correlations between sequences of chroma features, to account for temporal information. For combining the latter chroma-based classifiers with the composite classifier based on global features, we use standard rank aggregation methods adapted from the information retrieval literature. We evaluate performance with the Second Hand Song dataset, where we quantify performance using multiple statistics. We observe that each combination rule outperforms single methods in terms of the total number of identified queries. Experiments with rank aggregation methods show an increase of up to 23.5 % of the number of identified queries, compared to single classifiers. © Julien Osmalskyj, Peter Foster, Simon Dixon, Jean-Jacques Embrechts.","","Decision trees; Information retrieval; Chroma features; Combination rules; Cover song identifications; Cross correlations; Multiple classifiers; Random forests; Rank aggregation; Temporal information; Classification (of information)","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Park J.; Lee K.","Park, Jeongsoo (57194133607); Lee, Kyogu (8597995500)","57194133607; 8597995500","Harmonic-percussive source separation using harmonicity and sparsity constraints","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013834233&partnerID=40&md5=a09d867bedf3a54e8f4f07bccb150276","Music and Audio Research Group, Seoul National University, Seoul, South Korea","Park J., Music and Audio Research Group, Seoul National University, Seoul, South Korea; Lee K., Music and Audio Research Group, Seoul National University, Seoul, South Korea","In this paper, we propose a novel approach to harmonic-percussive sound separation (HPSS) using Non-negative Matrix Factorization (NMF) with sparsity and harmonicity constraints. Conventional HPSS methods have focused on temporal continuity of harmonic components and spectral continuity of percussive components. However, it may not be appropriate to use them to separate time-varying harmonic signals such as vocals, vibratos, and glissandos, as they lack in temporal continuity. Based on the observation that the spectral distributions of harmonic and percussive signals differ – i.e., harmonic components have harmonic and sparse structure while percussive components are broadband – we propose an algorithm that successfully separates the rapidly time-varying harmonic signals from the percussive ones by imposing different constraints on the two groups of spectral bases. Experiments with real recordings as well as synthesized sounds show that the proposed method outperforms the conventional methods. © Jeongsoo Park, Kyogu Lee.","","Factorization; Harmonic analysis; Harmonic distortion; Information retrieval; Matrix algebra; Separation; Conventional methods; Harmonic components; Nonnegative matrix factorization; Sparsity constraints; Spectral distribution; Synthesized sounds; Temporal continuity; Time-varying harmonics; Source separation","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Otsuka M.; Kitahara T.","Otsuka, Masaki (57191245128); Kitahara, Tetsuro (7201371361)","57191245128; 7201371361","Improving MIDI guitar’s accuracy with NMF and neural net","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983120&partnerID=40&md5=091d6db78e5620a3abb9d98234189b1d","Graduate School of Integrated Basic Sciences, Nihon University, Japan","Otsuka M., Graduate School of Integrated Basic Sciences, Nihon University, Japan; Kitahara T., Graduate School of Integrated Basic Sciences, Nihon University, Japan","In this paper, we propose a method for improving the accuracy of MIDI guitars. MIDI guitars are useful tools for various purposes from inputting MIDI data to enjoying a jam session system, but existing MIDI guitars do not have sufficient accuracy in converting the performance to an MIDI form. In this paper, we make an attempt on improving the accuracy of a MIDI guitar by integrating it with an audio transcription method based on non-negative matrix factorization (NMF). First, we investigate an NMF-based algorithm for transcribing guitar performances. Although the NMF is a promising method, an effective post-process (i.e., converting the NMF’s output to an MIDI form) is a non-trivial problem. We propose use of a neural network for this conversion. Next, we investigate a method for integrating the outputs of the MIDI guitar and NMF. Because they have different tendencies in wrong outputs, we take an policy of outputting only common parts in the two outputs. Experimental results showed that the F-score of our method was 0.626 whereas those of the MIDI-guitar-only and NMF-and-neural-network-only methods were 0.347 and 0.526, respectively. © Masaki Otsuka and Tetsuro Kitahara.","","Factorization; Information retrieval; Musical instruments; Transcription; F-score; Non trivial problems; Nonnegative matrix factorization; Post process; Transcription methods; Matrix algebra","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Ellis R.J.; Xing Z.; Fang J.; Wang Y.","Ellis, Robert J. (57089639000); Xing, Zhe (57210204745); Fang, Jiakun (57202504362); Wang, Ye (36103845200)","57089639000; 57210204745; 57202504362; 36103845200","Quantifying lexical novelty in song lyrics","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048580257&partnerID=40&md5=99e60122b96313b88b8754717080f0e2","School of Computing, National University of Singapore, Singapore","Ellis R.J., School of Computing, National University of Singapore, Singapore; Xing Z., School of Computing, National University of Singapore, Singapore; Fang J., School of Computing, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Novelty is an important psychological construct that affects both perceptual and behavioral processes. Here, we propose a lexical novelty score (LNS) for a song’s lyric, based on the statistical properties of a corpus of 275,905 lyrics (available at www.smcnus.org/lyrics/). A lyric-level LNS was derived as a function of the inverse document frequencies of its unique words. An artist-level LNS was then computed using the LNSs of lyrics uniquely associated with each artist. Statistical tests were performed to determine whether lyrics and artists on Billboard Magazine’s lists of “All-Time Top 100” songs and artists had significantly lower LNSs than “non-top” songs and artists. An affirmative and highly consistent answer was found in both cases. These results highlight the potential utility of the LNS as a feature for MIR. © Robert J Ellis, Zhe Xing, Jiakun Fang, and Ye Wang.","","Information retrieval; Behavioral process; Inverse Document Frequency; Potential utility; Statistical properties; Unique word; Number theory","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Dittmar C.; Lehner B.; Prätzlich T.; Müller M.; Widmer G.","Dittmar, Christian (15051598000); Lehner, Bernhard (7003282869); Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873); Widmer, Gerhard (7004342843)","15051598000; 7003282869; 55582692100; 7404689873; 7004342843","Cross-version singing voice detection in classical opera recordings","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028076913&partnerID=40&md5=4e5f407e0be1b87a10e7cc97913aa719","International Audio Laboratories, Erlangen, Germany; Johannes Kepler University, Linz, Austria","Dittmar C., International Audio Laboratories, Erlangen, Germany; Lehner B., Johannes Kepler University, Linz, Austria; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany; Widmer G., Johannes Kepler University, Linz, Austria","In the field of Music Information Retrieval (MIR), the automated detection of the singing voice within a given music recording constitutes a challenging and important research problem. In this study, our goal is to find those segments within a classical opera recording, where one or several singers are active. As our main contributions, we first propose a novel audio feature that extends a state-of-the-art feature set that has previously been applied to singing voice detection in popular music recordings. Second, we describe a simple bootstrapping procedure that helps to improve the results in the case that the test data is not reflected well by the training data. Third, we show that a cross-version approach can help to stabilize the results even further. © Christian Dittmar, Bernhard Lehner, Thomas Prätzlich, Meinard Müller, Gerhard Widmer.","","Audio acoustics; Information retrieval; Speech recognition; Automated detection; Music information retrieval; Music recording; Popular music recordings; Research problems; Singing voice detection; Singing voices; State of the art; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Vinutha T.P.; Sankagiri S.; Ganguli K.K.; Rao P.","Vinutha, T.P. (56628761100); Sankagiri, Suryanarayana (57191328774); Ganguli, Kaustuv Kanti (56094514100); Rao, Preeti (35180193500)","56628761100; 57191328774; 56094514100; 35180193500","Structural segmentation and visualization of sitar and sarod concert audio","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021954900&partnerID=40&md5=e207ac0a02f0a738a64c040ee77eb1b4","Department of Electrical Engineering, IIT Bombay, India","Vinutha T.P., Department of Electrical Engineering, IIT Bombay, India; Sankagiri S., Department of Electrical Engineering, IIT Bombay, India; Ganguli K.K., Department of Electrical Engineering, IIT Bombay, India; Rao P., Department of Electrical Engineering, IIT Bombay, India","Hindustani classical instrumental concerts follow an episodic development that, musicologically, is described via changes in the rhythmic structure. Uncovering this structure in a musically relevant form can provide for powerful visual representations of the concert audio that is of potential value in music appreciation and pedagogy. We investigate the structural analysis of the metered section (gat) of concerts of two plucked string instruments, the sitar and sarod. A prominent aspect of the gat is the interplay between the melody soloist and the accompanying drummer (tabla). The tempo as provided by the tabla together with the rhythmic density of the sitar/sarod plucks serve as the main dimensions that predict the transition between concert sections. We present methods to access the stream of tabla onsets separately from the sitar/sarod onsets, addressing challenges that arise in the instrument separation. Further, the robust detection of tempo and the estimation of rhythmic density of sitar/sarod plucks are discussed. A case study of a fully annotated concert is presented, and is followed by results of achieved segmentation accuracy on a database of sitar and sarod gats across artists. © Vinutha T.P., Suryanarayana Sankagiri, Kaustuv Kanti Ganguli, Preeti Rao.","","Information retrieval; Plucked-string instruments; Potential values; Rhythmic structures; Robust detection; Segmentation accuracy; Visual representations; Audio acoustics","P. Rao; Department of Electrical Engineering, IIT Bombay, India; email: prao@ee.iitb.ac.in","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Wolff D.; MacFarlane A.; Weyde T.","Wolff, Daniel (35091498700); MacFarlane, Andrew (7102685938); Weyde, Tillman (24476899500)","35091498700; 7102685938; 24476899500","Comparative music similarity modelling using transfer learning across user groups","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023772102&partnerID=40&md5=5cec125cb4e7d3be1a411c9245f9c97b","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","Wolff D., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; MacFarlane A., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","We introduce a new application of transfer learning for training and comparing music similarity models based on relative user data: The proposed Relative Information-Theoretic Metric Learning (RITML) algorithm adapts a Mahalanobis distance using an iterative application of the ITML algorithm, thereby extending it to relative similarity data. RITML supports transfer learning by training models with respect to a given template model that can provide prior information for regularisation. With this feature we use information from larger datasets to build better models for more specific datasets, such as user groups from different cultures or of different age. We then evaluate what model parameters, in this case acoustic features, are relevant for the specific models when compared to the general user data. We to this end introduce the new CASimIR dataset, the first openly available relative similarity dataset with user attributes. With two age-related subsets, we show that transfer learning with RITML leads to better age-specific models. RITML here improves learning on small datasets. Using the larger MagnaTagATune dataset, we show that RITML performs as well as state-of-the-art algorithms in terms of general similarity estimation. © Daniel Wolff, Andrew MacFarlane and Tillman Weyde.","","Computer music; Information retrieval; Information theory; Iterative methods; Acoustic features; Mahalanobis distances; Music similarity; Prior information; Relative information; Similarity estimation; State-of-the-art algorithms; Transfer learning; Learning systems","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Vatolkin I.; Rudolph G.; Weihs C.","Vatolkin, Igor (25652236100); Rudolph, Günter (15021491400); Weihs, Claus (6602344572)","25652236100; 15021491400; 6602344572","Evaluation of album effect for feature selection in music genre recognition","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017563262&partnerID=40&md5=c77b2f46dad9895439ffb9519acfe7dc","TU Dortmund, Department of Computer Science, Germany; TU Dortmund, Faculty of Statistics, Germany","Vatolkin I., TU Dortmund, Department of Computer Science, Germany; Rudolph G., TU Dortmund, Department of Computer Science, Germany; Weihs C., TU Dortmund, Faculty of Statistics, Germany","With an increasing number of available music characteristics, feature selection becomes more important for various categorisation tasks, helping to identify relevant features and remove irrelevant and redundant ones. Another advantage is the decrease of runtime and storage demands. However, sometimes feature selection may lead to “over-optimisation” when data in the optimisation set is too different from data in the independent validation set. In this paper, we extend our previous work on feature selection for music genre recognition and focus on so-called “album effect” meaning that optimised classification models may overemphasize relevant characteristics of particular artists and albums rather than learning relevant properties of genres. For that case we examine the performance of classification models on two validation sets after the optimisation with feature selection: the first set with tracks not used for training and feature selection but randomly selected from the same albums, and the second set with tracks selected from other albums. As it can be expected, the classification performance on the second set decreases. Nevertheless, in almost all cases the feature selection remains beneficial compared to complete feature sets and a baseline using MFCCs, if applied for an ensemble of classifiers, proving robust generalisation performance. © Igor Vatolkin, Günter Rudolph, Claus Weihs.","","Digital storage; Information retrieval; Classification models; Classification performance; Ensemble of classifiers; Feature sets; Generalisation; Optimisations; Relevant features; Validation sets; Feature extraction","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Schlüter J.","Schlüter, Jan (55063593300)","55063593300","Learning to pinpoint singing voice from weakly labeled examples","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016639226&partnerID=40&md5=0aa22bfd458650cf02218e0dc9a64e89","Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Building an instrument detector usually requires temporally accurate ground truth that is expensive to create. However, song-wise information on the presence of instruments is often easily available. In this work, we investigate how well we can train a singing voice detection system merely from song-wise annotations of vocal presence. Using convolutional neural networks, multiple-instance learning and saliency maps, we can not only detect singing voice in a test signal with a temporal accuracy close to the state-of-the-art, but also localize the spectral bins with precision and recall close to a recent source separation method. Our recipe may provide a basis for other sequence labeling tasks, for improving source separation or for inspecting neural networks trained on auditory spectrograms. © Jan Schlüter.","","Information retrieval; Learning systems; Neural networks; Source separation; Convolutional neural network; Ground truth; Multiple instance learning; Precision and recall; Sequence Labeling; Singing voice detection; Singing voices; State of the art; Speech recognition","J. Schlüter; Austrian Research Institute for Artificial Intelligence, Vienna, Austria; email: jan.schlueter@ofai.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Percival G.; Fukayama S.; Goto M.","Percival, Graham (23135676800); Fukayama, Satoru (56407004300); Goto, Masataka (7403505330)","23135676800; 56407004300; 7403505330","SONG2QUARTET: A system for generating string quartet cover songs from polyphonic audio of popular music","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029925880&partnerID=40&md5=c88bc89774b89f989cc13c87c091b4c3","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Percival G., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We present Song2Quartet, a system for generating string quartet versions of popular songs by combining probabilistic models estimated from a corpus of symbolic classical music with the target audio file of any song. Song2Quartet allows users to add novelty to listening experience of their favorite songs and gain familiarity with string quartets. Previous work in automatic arrangement of music only used symbolic scores to achieve a particular musical style; our challenge is to also consider audio features of the target popular song. In addition to typical audio music content analysis such as beat and chord estimation, we also use time-frequency spectral analysis in order to better reflect partial phrases of the song in its cover version. Song2Quartet produces a probabilistic network of possible musical notes at every sixteenth note for each accompanying instrument of the quartet by combining beats, chords, and spectrogram from the target song with Markov chains estimated from our corpora of quartet music. As a result, the musical score of the cover version can be generated by finding the optimal paths through these networks. We show that the generated results follow the conventions of classical string quartet music while retaining some partial phrases and chord voicings from the target audio. © Graham Percival, Satoru Fukayama, Masataka Goto.","","Frequency estimation; Information retrieval; Markov processes; Spectrum analysis; Audio features; Classical musics; Musical notes; Musical score; Popular music; Probabilistic models; Probabilistic network; Time frequency; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Durand S.; Essid S.","Durand, Simon (56303161700); Essid, Slim (16033218700)","56303161700; 16033218700","Downbeat detection with conditional random fields and deep learned features","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013873952&partnerID=40&md5=164da3952003b41df67291a78127a269","LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France","Durand S., LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France; Essid S., LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France","In this paper, we introduce a novel Conditional Random Field (CRF) system that detects the downbeat sequence of musical audio signals. Feature functions are computed from four deep learned representations based on harmony, rhythm, melody and bass content to take advantage of the high-level and multi-faceted aspect of this task. Downbeats being dynamic, the powerful CRF classification system allows us to combine our features with an adapted temporal model in a fully data-driven fashion. Some meters being under-represented in our training set, we show that data augmentation enables a statistically significant improvement of the results by taking into account class imbalance. An evaluation of different configurations of our system on nine datasets shows its efficiency and potential over a heuristic based approach and four downbeat tracking algorithms. © Simon Durand, Slim Essid.","","Audio acoustics; Heuristic algorithms; Image segmentation; Information retrieval; Classification system; Conditional random field; Data augmentation; Its efficiencies; Musical audio signal; Temporal modeling; Tracking algorithm; Under-represented; Random processes","S. Durand; LTCI, CNRS, Télécom ParisTech Université Paris-Saclay, Paris, 75013, France; email: simon.durand@telecom-paristech.fr","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Janssen B.; van Kranenburg P.; Volk A.","Janssen, Berit (56448253700); van Kranenburg, Peter (35108158000); Volk, Anja (30567849900)","56448253700; 35108158000; 30567849900","A comparison of symbolic similarity measures for finding occurrences of melodic segments","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019356178&partnerID=40&md5=dff8351b3be2ad3241181338d17dce44","Meertens Institute, Amsterdam, Netherlands; Utrecht University, Netherlands","Janssen B., Meertens Institute, Amsterdam, Netherlands; van Kranenburg P., Meertens Institute, Amsterdam, Netherlands; Volk A., Utrecht University, Netherlands","To find occurrences of melodic segments, such as themes, phrases and motifs, in musical works, a well-performing similarity measure is needed to support human analysis of large music corpora. We evaluate the performance of a range of melodic similarity measures to find occurrences of phrases in folk song melodies. We compare the similarity measures correlation distance, city-block distance, Euclidean distance and alignment, proposed for melody comparison in computational ethnomusicology; furthermore Implication-Realization structure alignment and B-spline alignment, forming successful approaches in symbolic melodic similarity; moreover, wavelet transform and the geometric approach Structure Induction, having performed well in musical pattern discovery. We evaluate the success of the different similarity measures through observing retrieval success in relation to human annotations. Our results show that local alignment and SIAM perform on an almost equal level to human annotators. © Berit Janssen, Peter van Kranenburg, Anja Volk.","","Information retrieval; Wavelet transforms; City-block distances; Correlation distance; Euclidean distance; Geometric approaches; Melodic similarity; Similarity measure; Structure alignment; Symbolic similarity; Alignment","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Padilla V.; McLean A.; Marsden A.; Ng K.","Padilla, Victor (55516671100); McLean, Alex (24476549200); Marsden, Alan (15054644600); Ng, Kia (15848728000)","55516671100; 24476549200; 15054644600; 15848728000","Improving optical music recognition by combining outputs from multiple sources","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069947431&partnerID=40&md5=1fd611726ce7e48ca5497bf9307a9af9","Lancaster University, United Kingdom; University of Leeds, United Kingdom","Padilla V., Lancaster University, United Kingdom; McLean A., University of Leeds, United Kingdom; Marsden A., Lancaster University, United Kingdom; Ng K., University of Leeds, United Kingdom","Current software for Optical Music Recognition (OMR) produces outputs with too many errors that render it an unrealistic option for the production of a large corpus of symbolic music files. In this paper, we propose a system which applies image pre-processing techniques to scans of scores and combines the outputs of different commercial OMR programs when applied to images of different scores of the same piece of music. As a result of this procedure, the combined output has around 50% fewer errors when compared to the output of any one OMR program. Image pre-processing splits scores into separate movements and sections and removes ossia staves which confuse OMR software. Post-processing aligns the outputs from different OMR programs and from different sources, rejecting outputs with the most errors and using majority voting to determine the likely correct details. Our software produces output in MusicXML, concentrating on accurate pitch and rhythm and ignoring grace notes. Results of tests on the six string quartets by Mozart dedicated to Joseph Haydn and the first six piano sonatas by Mozart are presented, showing an average recognition rate of around 95%. © Victor Padilla, Alex McLean, Alan Marsden & Kia Ng.","","Errors; Information retrieval; Image preprocessing; Large corpora; Multiple source; Music files; Optical music recognition; Post processing; Image processing","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Gasser M.; Arzt A.; Gadermaier T.; Grachten M.; Widmer G.","Gasser, Martin (18037419600); Arzt, Andreas (36681791200); Gadermaier, Thassilo (57003475800); Grachten, Maarten (8974600000); Widmer, Gerhard (7004342843)","18037419600; 36681791200; 57003475800; 8974600000; 7004342843","Classical music on the web – User interfaces and data representations","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056638022&partnerID=40&md5=094acbf767e1faa7458206f14b4f7953","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Arzt A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria; Gadermaier T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Grachten M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","We present a set of web-based user interfaces for explorative analysis and visualization of classical orchestral music and a web API that serves as a backend to those applications; we describe use cases that motivated our developments within the PHENICX project, which promotes a vital interaction between Music Information Retrieval research groups and a world-renowned symphony orchestra. Furthermore, we describe two real-world applications that involve the work presented here. Firstly, our web applications are used in the editorial stage of a periodically released subscription-based mobile app by the Royal Concertgebouw Orchestra (RCO) 1 , which serves as a content-distribution channel for multi-modally enhanced recordings of classical concerts. Secondly, our web API and user interfaces have been successfully used to provide real-time information (such as the score, and explanatory comments from musicologists) to the audience during a live concert of the RCO. © Martin Gasser, Andreas Arzt, Thassilo Gadermaier, Maarten Grachten, Gerhard Widmer.","","Application programming interfaces (API); Information retrieval; Classical musics; Content distribution; Data representations; Mobile app; Music information retrieval; Real-time information; Symphony orchestras; WEB application; User interfaces","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kruspe A.M.","Kruspe, Anna M. (54971261400)","54971261400","Training phoneme models for singing with “songified” speech data","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054284624&partnerID=40&md5=4b433df6952625f646ede2eb348928b9","Fraunhofer IDMT, Ilmenau, Germany","Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany","Speech recognition in singing is a task that has not been widely researched so far. Singing possesses several characteristics that differentiate it from speech. Therefore, algorithms and models that were developed for speech usually perform worse on singing. One of the bottlenecks in many algorithms is the recognition of phonemes in singing. We noticed that this recognition step can be improved when using singing data in model training, but to our knowledge, there are no large datasets of singing data annotated with phonemes. However, such data does exist for speech. We therefore propose to make phoneme recognition models more robust for singing by training them on speech data that has artificially been made more “song-like”. We test two main modifications on speech data: Time stretching and pitch shifting. Artificial vibrato is also tested. We then evaluate models trained on different combinations of these modified speech recordings. The utilized modeling algorithms are Neural Networks and Deep Belief Networks. © Anna M. Kruspe.","","Audio acoustics; Information retrieval; Large dataset; Speech; Deep belief networks; Large datasets; Model algorithms; Phoneme models; Phoneme recognition; Pitch shifting; Speech recording; Time stretching; Continuous speech recognition","A.M. Kruspe; Fraunhofer IDMT, Ilmenau, Germany; email: kpe@idmt.fraunhofer.de","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Dai J.; Mauch M.; Dixon S.","Dai, Jiajie (57205391597); Mauch, Matthias (36461512900); Dixon, Simon (7201479437)","57205391597; 36461512900; 7201479437","Analysis of intonation trajectories in solo singing","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030988394&partnerID=40&md5=0c9ee521b06c0ea2b02625f0cbd9e544","Centre for Digital Music, Queen Mary University of London, United Kingdom","Dai J., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present a new dataset for singing analysis and modelling, and an exploratory analysis of pitch accuracy and pitch trajectories. Shortened versions of three pieces from The Sound of Music were selected: “Edelweiss”, “Do-Re-Mi” and “My Favourite Things”. 39 participants sang three repetitions of each excerpt without accompaniment, resulting in a dataset of 21762 notes in 117 recordings. To obtain pitch estimates we used the Tony software’s automatic transcription and manual correction tools. Pitch accuracy was measured in terms of pitch error and interval error. We show that singers’ pitch accuracy correlates significantly with self-reported singing skill and musical training. Larger intervals led to larger errors, and the tritone interval in particular led to average errors of one third of a semitone. Note duration (or inter-onset interval) had a significant effect on pitch accuracy, with greater accuracy on longer notes. To model drift in the tonal centre over time, we present a sliding window model which reveals patterns in the pitch errors of some singers. Based on the trajectory, we propose a measure for the magnitude of drift: tonal reference deviation (TRD). The data and software are freely available. 1. © Jiajie Dai, Matthias Mauch, Simon Dixon.","","Information retrieval; Trajectories; Analysis and modelling; Automatic transcription; Average errors; Exploratory analysis; Inter onset intervals; Interval errors; Pitch errors; Sliding window models; Errors","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Ringwalt D.; Dannenberg R.B.","Ringwalt, Dan (57210234242); Dannenberg, Roger B. (7003266250)","57210234242; 7003266250","Image quality estimation for multi-score OMR","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070005553&partnerID=40&md5=333110eabd6982005d838cc07cd8ce04","Carnegie Mellon University, School of Computer Science, United States","Ringwalt D., Carnegie Mellon University, School of Computer Science, United States; Dannenberg R.B., Carnegie Mellon University, School of Computer Science, United States","Optical music recognition (OMR) is the recognition of images of musical scores. Recent research has suggested aligning the results of OMR from multiple scores of the same work (multi-score OMR, MS-OMR) to improve accuracy. As a simpler alternative, we have developed features which predict the quality of a given score, allowing us to select the highest-quality score to use for OMR. Furthermore, quality may be used to weight each score in an alignment, which should improve existing systems’ robustness. Using commercial OMR software on a test set of MIDI recordings and multiple corresponding scores, our predicted OMR accuracy is weakly but significantly correlated with the true accuracy. Improved features should be able to produce highly consistent results. © Dan Ringwalt, Roger B. Dannenberg.","","Information retrieval; Existing systems; Image quality estimation; Musical score; Optical music recognition; Recent researches; Test sets; Software testing","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Vall A.; Skowron M.; Knees P.; Schedl M.","Vall, Andreu (56286378600); Skowron, Marcin (8533461300); Knees, Peter (8219023200); Schedl, Markus (8684865900)","56286378600; 8533461300; 8219023200; 8684865900","Improving music recommendations with a weighted factorization of the tagging activity","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058982056&partnerID=40&md5=94907ed0dce04059083d898e393b8fb2","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Vall A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Skowron M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Collaborative filtering systems for music recommendations are often based on implicit feedback derived from listening activity. Hybrid approaches further incorporate additional sources of information in order to improve the quality of the recommendations. In the context of a music streaming service, we present a hybrid model based on matrix factorization techniques that fuses the implicit feedback derived from the users’ listening activity with the tags that users have given to musical items. In contrast to existing work, we introduce a novel approach to exploit tags by performing a weighted factorization of the tagging activity. We evaluate the model for the task of artist recommendation, using the expected percentile rank as metric, extended with confidence intervals to enable the comparison between models. Thus, our contribution is twofold: (1) we introduce a novel model that uses tags to improve music recommendations and (2) we extend the evaluation methodology to compare the performance of different recommender systems. © Andreu Vall, Marcin Skowron, Peter Knees, Markus Schedl.","","Collaborative filtering; Factorization; Recommender systems; Collaborative filtering systems; Confidence interval; Evaluation methodologies; Implicit feedback; Matrix factorizations; Music recommendation; Music streaming; Sources of informations; User interfaces","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Miron M.; Carabias-Orti J.J.; Janer J.","Miron, Marius (55585881400); Carabias-Orti, Julio José (25926912600); Janer, Jordi (35068089300)","55585881400; 25926912600; 35068089300","Improving score-informed source separation for classical music through note refinement","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057593513&partnerID=40&md5=f83953b89a1276da98058beb4457003f","Music Technology Group, Universitat Pompeu Fabra, Spain","Miron M., Music Technology Group, Universitat Pompeu Fabra, Spain; Carabias-Orti J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Spain","Signal decomposition methods such as Non-negative Matrix Factorization (NMF) demonstrated to be a suitable approach for music signal processing applications, including sound source separation. To better control this decomposition, NMF has been extended using prior knowledge and parametric models. In fact, using score information considerably improved separation results. Nevertheless, one of the main problems of using score information is the misalignment between the score and the actual performance. A potential solution to this problem is the use of audio to score alignment systems. However, most of them rely on a tolerance window that clearly affects the separation results. To overcome this problem, we propose a novel method to refine the aligned score at note level by detecting both, onset and offset for each note present in the score. Note refinement is achieved by detecting shapes and contours in the estimated instrument-wise time activation (gains) matrix. Decomposition is performed in a supervised way, using training instrument models and coarsely-aligned score information. The detected contours define time-frequency note boundaries, and they increase the sparsity. Finally, we have evaluated our method for informed source separation using a dataset of Bach chorales obtaining satisfactory results, especially in terms of SIR. © Marius Miron, Julio José Carabias-Orti, Jordi Janer.","","Audio systems; Factorization; Information retrieval; Information use; Matrix algebra; Separation; Alignment system; Instrument models; Music signal processing; Nonnegative matrix factorization; Parametric models; Score-informed source separations; Signal decomposition; Sound source separation; Source separation","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Koops H.V.; Volk A.; Bas de Haas W.","Koops, Hendrik Vincent (55925589400); Volk, Anja (30567849900); Bas de Haas, W. (51160955300)","55925589400; 30567849900; 51160955300","Corpus-based rhythmic pattern analysis of ragtime syncopation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022071068&partnerID=40&md5=ca211c15f944184ba3d7470c3ecdbe4c","Utrecht University, Netherlands","Koops H.V., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Bas de Haas W., Utrecht University, Netherlands","This paper presents a corpus-based study on rhythmic patterns in the RAG-collection of approximately 11.000 symbolically encoded ragtime pieces. While characteristic musical features that define ragtime as a genre have been debated since its inception, musicologists argue that specific syncopation patterns are most typical for this genre. Therefore, we investigate the use of syncopation patterns in the RAG-collection from its beginnings until the present time in this paper. Using computational methods, this paper provides an overview on the use of rhythmical patterns of the ragtime genre, thereby offering valuable new insights that complement musicological hypotheses about this genre. Specifically, we measure the amount of syncopation for each bar using Longuet-Higgins and Lee’s model of syncopation, determine the most frequent rhythmic patterns, and discuss the role of a specific short-long-short syncopation pattern that musicologists argue is characteristic for ragtime. A comparison between the ragtime (pre-1920) and modern (post-1920) era shows that the two eras differ in syncopation pattern use. Onset density and amount of syncopation increase after 1920. Moreover, our study confirms the musicological hypothesis on the important role of the short-long-short syncopation pattern in ragtime. These findings are pivotal in developing ragtime genre-specific features. © Hendrik Vincent Koops, Anja Volk, W. Bas de Haas.","","Corpus-based; Musical features; Rhythmic patterns; S models; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Nakamura E.; Cuvillier P.; Cont A.; Ono N.; Sagayama S.","Nakamura, Eita (24587601200); Cuvillier, Philippe (56431078400); Cont, Arshia (12344985300); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","24587601200; 56431078400; 12344985300; 7202472899; 7004859104","Autoregressive hidden semi-Markov model of symbolic music performance for score following","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064341619&partnerID=40&md5=9c6f69a155dc4179d52199033da8cf0c","National Institute of Informatics, Tokyo, 101-8430, Japan; Inria MuTant Project-Team, Ircam/UPMC, CNRS UMR STMS, Paris, 75004, France; Meiji University, Tokyo, 164-8525, Japan","Nakamura E., National Institute of Informatics, Tokyo, 101-8430, Japan; Cuvillier P., Inria MuTant Project-Team, Ircam/UPMC, CNRS UMR STMS, Paris, 75004, France; Cont A., Inria MuTant Project-Team, Ircam/UPMC, CNRS UMR STMS, Paris, 75004, France; Ono N., National Institute of Informatics, Tokyo, 101-8430, Japan; Sagayama S., Meiji University, Tokyo, 164-8525, Japan","A stochastic model of symbolic (MIDI) performance of polyphonic scores is presented and applied to score following. Stochastic modelling has been one of the most successful strategies in this field. We describe the performance as a hierarchical process of performer’s progression in the score and the production of performed notes, and represent the process as an extension of the hidden semi-Markov model. The model is compared with a previously studied model based on hidden Markov model (HMM), and reasons are given that the present model is advantageous for score following especially for scores with trills, tremolos, and arpeggios. This is also confirmed empirically by comparing the accuracy of score following and analysing the errors. We also provide a hybrid of this model and the HMM-based model which is computationally more efficient and retains the advantages of the former model. The present model yields one of the state-of-the-art score following algorithms for symbolic performance and can possibly be applicable for other music recognition problems. © Eita Nakamura, Philippe Cuvillier, Arshia Cont, Nobutaka Ono, Shigeki Sagayama.","","Hidden Markov models; Information retrieval; Stochastic systems; Hidden semi-Markov modeling; Hierarchical process; Model-based OPC; Music performance; Music recognition; Polyphonic scores; Score-following; State of the art; Stochastic models","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Zacharakis A.; Kaliakatsos-Papakostas M.; Cambouropoulos E.","Zacharakis, Asterios (35182350900); Kaliakatsos-Papakostas, Maximos (36092739500); Cambouropoulos, Emilios (9632551100)","35182350900; 36092739500; 9632551100","Conceptual blending in music cadences: A formal model and subjective evaluation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045905435&partnerID=40&md5=a883bd6891b97a1c89409906b53be498","School of Music Studies, Aristotle University of Thessaloniki, Greece","Zacharakis A., School of Music Studies, Aristotle University of Thessaloniki, Greece; Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","Conceptual blending is a cognitive theory whereby elements from diverse, but structurally-related, mental spaces are ‘blended’ giving rise to new conceptual spaces. This study focuses on structural blending utilising an algorithmic formalisation for conceptual blending applied to harmonic concepts. More specifically, it investigates the ability of the system to produce meaningful blends between harmonic cadences, which arguably constitute the most fundamental harmonic concept. The system creates a variety of blends combining elements of the penultimate chords of two input cadences and it further estimates the expected relationships between the produced blends. Then, a preliminary subjective evaluation of the proposed blending system is presented. A pairwise dissimilarity listening test was conducted using original and blended cadences as stimuli. Subsequent multidimensional scaling analysis produced spatial configurations for both behavioural data and dissimilarity estimations by the algorithm. Comparison of the two configurations showed that the system is capable of making fair predictions of the perceived dissimilarities between the blended cadences. This implies that this conceptual blending approach is able to create perceptually meaningful blends based on self-evaluation of its outcome. © Asterios Zacharakis, Maximos Kaliakatsos-Papakostas, Emilios Cambouropoulos.","","Harmonic analysis; Information retrieval; Cognitive theory; Conceptual spaces; Fundamental harmonic; Listening tests; Multidimensional scaling analysis; Self evaluation; Spatial configuration; Subjective evaluations; Blending","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Sigler A.; Wild J.; Handelman E.","Sigler, Andie (55648170500); Wild, Jon (57204258099); Handelman, Eliot (6602354866)","55648170500; 57204258099; 6602354866","Schematizing the treatment of dissonance in 16th-century counterpoint","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881724&partnerID=40&md5=01632c4a9f20890f5cb45d087db7c760","School of Computer Science, McGill University, Computing Music, Canada; Schulich School of Music, McGill University, Canada; Computing Music, Canada","Sigler A., School of Computer Science, McGill University, Computing Music, Canada; Wild J., Schulich School of Music, McGill University, Canada; Handelman E., Computing Music, Canada","We describe a computational project concerning labeling of dissonance treatments – schematic descriptions of the uses of dissonances. We use automatic score annotation and database methods to develop schemata for a large corpus of 16th-century polyphonic music. We then apply structural techniques to investigate coincidence of schemata, and to extrapolate from found structures to unused possibilities. © Andie Sigler, Jon Wild, Eliot Handelman.","","Large corpora; Polyphonic music; Schematic description; Structural techniques; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Matz D.; Cano E.; Abeßer J.","Matz, Daniel (57210234482); Cano, Estefanía (51161075800); Abeßer, Jakob (36607532300)","57210234482; 51161075800; 36607532300","New sonorities for early jazz recordings using sound source separation and automatic mixing tools","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028051686&partnerID=40&md5=6347908a0601c0132787ee9d5ce37364","University of Applied Sciences, Düsseldorf, Germany; Fraunhofer IDMT, Ilmenau, Germany","Matz D., University of Applied Sciences, Düsseldorf, Germany; Cano E., Fraunhofer IDMT, Ilmenau, Germany; Abeßer J., Fraunhofer IDMT, Ilmenau, Germany","In this paper, a framework for automatic mixing of early jazz recordings is presented. In particular, we propose the use of sound source separation techniques as a pre-processing step of the mixing process. In addition to an initial solo and accompaniment separation step, the proposed mixing framework is composed of six processing blocks: harmonic-percussive separation (HPS), cross-adaptive multi-track scaling (CAMTS), cross-adaptive equalizer (CAEQ), cross-adaptive dynamic spectral panning (CADSP), automatic excitation (AE), and time-frequency selective panning (TFSP). The effects of the different processing steps in the final quality of the mix are evaluated through a listening test procedure. The results show that the desired quality improvements in terms of sound balance, transparency, stereo impression, timbre, and overall impression can be achieved with the proposed framework. © Daniel Matz, Estefanía Cano, Jakob Abeßer.","","Acoustic generators; Audio recordings; Information retrieval; Mixing; Separation; Testing; Adaptive dynamics; Adaptive equalizer; Listening tests; Pre-processing step; Processing steps; Quality improvement; Sound source separation; Time frequency; Source separation","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kruspe A.M.","Kruspe, Anna M. (54971261400)","54971261400","Bootstrapping a system for phoneme recognition and keyword spotting in unaccompanied singing","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994195825&partnerID=40&md5=53784a3b9757e8446f4326f4675e64b5","Fraunhofer IDMT, Ilmenau, Germany","Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany","Speech recognition in singing is still a largely unsolved problem. Acoustic models trained on speech usually produce unsatisfactory results when used for phoneme recognition in singing. On the flipside, there is no phonetically annotated singing data set that could be used to train more accurate acoustic models for this task. In this paper, we attempt to solve this problem using the DAMP data set which contains a large number of recordings of amateur singing in good quality. We first align them to the matching textual lyrics using an acoustic model trained on speech. We then use the resulting phoneme alignment to train new acoustic models using only subsets of the DAMP singing data. These models are then tested for phoneme recognition and, on top of that, keyword spotting. Evaluation is performed for different subsets of DAMP and for an unrelated set of the vocal tracks of commercial pop songs. Results are compared to those obtained with acoustic models trained on the TIMIT speech data set and on a version of TIMIT augmented for singing. Our new approach shows significant improvements over both. © Anna M. Kruspe.","","Information retrieval; Acoustic model; Data set; Keyword spotting; New approaches; Phoneme alignments; Phoneme recognition; Speech data; Unsolved problems; Speech recognition","A.M. Kruspe; Fraunhofer IDMT, Ilmenau, Germany; email: kpe@idmt.fraunhofer.de","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Gulati S.; Serrà J.; Ganguli K.K.; Şentürk S.; Serra X.","Gulati, Sankalp (37087243200); Serrà, Joan (35749172500); Ganguli, Kaustuv K. (56094514100); Şentürk, Sertan (43461595100); Serra, Xavier (55892979900)","37087243200; 35749172500; 56094514100; 43461595100; 55892979900","Time-delayed melody surfaces for rāga recognition","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985928576&partnerID=40&md5=f08f4061995c33ed10a51d07ee725653","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Telefonica Research, Barcelona, Spain; Dept. of Electrical Engg., Indian Institute of Technology Bombay, Mumbai, India","Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Telefonica Research, Barcelona, Spain; Ganguli K.K., Dept. of Electrical Engg., Indian Institute of Technology Bombay, Mumbai, India; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Rāga is the melodic framework of Indian art music. It is a core concept used in composition, performance, organization, and pedagogy. Automatic rāga recognition is thus a fundamental information retrieval task in Indian art music. In this paper, we propose the time-delayed melody surface (TDMS), a novel feature based on delay coordinates that captures the melodic outline of a rāga. A TDMS describes both the tonal and the temporal characteristics of a melody, using only an estimation of the predominant pitch. Considering a simple k-nearest neighbor classifier, TDMSs outperform the state-of-the-art for rāga recognition by a large margin. We obtain 98% accuracy on a Hindustani music dataset of 300 recordings and 30 rāgas, and 87% accuracy on a Carnatic music dataset of 480 recordings and 40 rāgas. TDMSs are simple to implement, fast to compute, and have a musically meaningful interpretation. Since the concepts and formulation behind the TDMS are generic and widely applicable, we envision its usage in other music traditions beyond Indian art music. © Sankalp Gulati, Joan Serrà, Kaustuv K Ganguli, Sertan Şentürk and Xavier Serra.","","Information retrieval; Nearest neighbor search; Time delay; Delay coordinate; Feature-based; In compositions; K-nearest neighbor classifier; Large margins; State of the art; Temporal characteristics; Time delayed; Audio recordings","S. Gulati; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: sankalp.gulati@upf.edu","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Jančovič P.; Köküer M.; Baptiste W.","Jančovič, Peter (7801590038); Köküer, Münevver (6508139174); Baptiste, Wrena (57210231571)","7801590038; 6508139174; 57210231571","Automatic transcription of ornamented Irish traditional flute music using hidden Markov models","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062000686&partnerID=40&md5=782733db2a3efe7a558e592ebbed23ae","School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; School of Digital Media Technology, Birmingham City University, United Kingdom","Jančovič P., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Köküer M., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom, School of Digital Media Technology, Birmingham City University, United Kingdom; Baptiste W., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom, School of Digital Media Technology, Birmingham City University, United Kingdom","This paper presents an automatic system for note transcription of Irish traditional flute music containing ornamentation. This is a challenging problem due to the soft nature of onsets and short durations of ornaments. The proposed automatic transcription system is based on hidden Markov models, with separate models being built for notes and for single-note ornaments. Mel-frequency cepstral coefficients are employed to represent the acoustic signal. Different setups of parameters in feature extraction and acoustic modelling are explored. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD. The performance of the system is evaluated in terms of the transcription of notes as well as detection of onsets. It is demonstrated that the proposed system can achieve a very good note transcription and onset detection performance. Over 28% relative improvement in terms of the F-measure is achieved for onset detection in comparison to conventional onset detection methods based on signal energy and fundamental frequency. © Peter Jančovič, Münevver Köküer, Wrena Baptiste.","","Information retrieval; Transcription; Acoustic modelling; Acoustic signals; Automatic systems; Automatic transcription; Experimental evaluation; Fundamental frequencies; Mel frequency cepstral co-efficient; Short durations; Hidden Markov models","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Manaris B.; Stoudenmier S.","Manaris, Bill (6602079213); Stoudenmier, Seth (56407187100)","6602079213; 56407187100","Specter: Combining music information retrieval with sound spatialization","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070006592&partnerID=40&md5=0a904a1ba47e53e1aed3909d27681401","Computer Science Dept., College of Charleston, United States","Manaris B., Computer Science Dept., College of Charleston, United States; Stoudenmier S., Computer Science Dept., College of Charleston, United States","Specter combines music information retrieval (MIR) with sound spatialization to provide a simple, yet versatile environment to experiment with sound spatialization for music composition and live performance. Through various interfaces and sensors, users may position sounds at arbitrary locations and trajectories in a three-dimensional plane. The system utilizes the JythonMusic environment for symbolic music processing, music information retrieval, and live audio manipulation. It also incorporates Iannix, a 3D graphical, open-source sequencer, for real-time generation, manipulation, and storing of sound trajectory scores. Finally, through Glaser, a sound manipulation instrument, Specter renders the various sounds in space. The system architecture supports different sound spatialization techniques including Ambisonics and Vector Based Amplitude Panning. Various interfaces are discussed, including a Kinect-based sensor system, a LeapMotion-based hand-tracking interface, and a smartphone-based OSC controller. Finally, we present Migrant, a music composition, which utilizes and demonstrates Specter’s ability to combine MIR techniques with sound spatialization through inexpensive, minimal hardware. © Bill Manaris and Seth Stoudenmier.","","Information retrieval; Search engines; Smartphones; Sound reproduction; Amplitude panning; Hand tracking; Music composition; Music information retrieval; Sensor systems; Sound manipulations; Sound spatialization; System architectures; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Page K.R.; Nurmikko-Fuller T.; Rindfleisch C.; Weigl D.M.; Lewis R.; Dreyfus L.; De Roure D.","Page, Kevin R. (8104972900); Nurmikko-Fuller, Terhi (56811634500); Rindfleisch, Carolin (57190980464); Weigl, David M. (55359633400); Lewis, Richard (55457221800); Dreyfus, Laurence (57214037794); De Roure, David (6701509117)","8104972900; 56811634500; 57190980464; 55359633400; 55457221800; 57214037794; 6701509117","A toolkit for live annotation of opera performance: Experiences capturing wagner’s ring cycle","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041057870&partnerID=40&md5=5c58749dfebd505b5431f3bf65b5a05c","Oxford e-Research Centre, University of Oxford, United Kingdom; Faculty of Music, University of Oxford, United Kingdom; Department of Computing Goldsmiths, University of London, United Kingdom","Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom; Nurmikko-Fuller T., Oxford e-Research Centre, University of Oxford, United Kingdom; Rindfleisch C., Faculty of Music, University of Oxford, United Kingdom; Weigl D.M., Oxford e-Research Centre, University of Oxford, United Kingdom; Lewis R., Department of Computing Goldsmiths, University of London, United Kingdom; Dreyfus L., Faculty of Music, University of Oxford, United Kingdom; De Roure D., Oxford e-Research Centre, University of Oxford, United Kingdom","Performance of a musical work potentially provides a rich source of multimedia material for future investigation, both for musicologists’ study of reception and perception, and in improvement of computational methods applied to its analysis. This is particularly true of music theatre, where a traditional recording cannot sufficiently capture the ephemeral phenomena unique to each staging. In this paper we introduce a toolkit developed with, and used by, a musicologist throughout a complete multi-day production of Richard Wagner’s Der Ring des Nibelungen. The toolkit is centred on a tablet-based score interface through which the scholar makes notes on the scenic setting of the performance as it unfolds, supplemented by a variety of digital data gathered to structure and index the annotations. We report on our experience developing a system suitable for real-time use by the musicologist, structuring the data for reuse and further investigation using semantic web technologies, and of the practical challenges and compromises of fieldwork within a working theatre. Finally we consider the utility of our tooling from both a user perspective and through an initial quantitative investigation of the data gathered. © K. R. Page, T. Nurmikko-Fuller, C. Rindfleisch, D. M. Weigl, R. Lewis, L. Dreyfus, D. De Roure.","","Digital datas; Multimedia materials; Quantitative investigation; Real time; Semantic Web technology; User perspectives; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Devaney J.; Arthur C.; Condit-Schultz N.; Nisula K.","Devaney, Johanna (35766484200); Arthur, Claire (57191728327); Condit-Schultz, Nathaniel (56681201600); Nisula, Kirsten (56030646300)","35766484200; 57191728327; 56681201600; 56030646300","Theme and variation encodings with roman numerals (TaVERn): A new data set for symbolic music analysis","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025465927&partnerID=40&md5=cdf6631744730f6500c55627d7c57427","School of Music, Ohio State University, United States","Devaney J., School of Music, Ohio State University, United States; Arthur C., School of Music, Ohio State University, United States; Condit-Schultz N., School of Music, Ohio State University, United States; Nisula K., School of Music, Ohio State University, United States","The Theme And Variation Encodings with Roman Numerals (TAVERN) dataset consists of 27 complete sets of theme and variations for piano composed between 1765 and 1810 by Mozart and Beethoven. In these theme and variation sets, comparable harmonic structures are realized in different ways. This facilitates an evaluation of the effectiveness of automatic analysis algorithms in generalizing across different musical textures. The pieces are encoded in standard **kern format, with analyses jointly encoded using an extension to **kern. The harmonic content of the music was analyzed with both Roman numerals and function labels in duplicate by two different expert analyzers. The pieces are divided into musical phrases, allowing for multiple-levels of automatic analysis, including chord labeling and phrase parsing. This paper describes the content of the dataset in detail, including the types of chords represented, and discusses the ways in which the analyzers sometimes disagreed on the lower-level harmonic content (the Roman numerals) while converging at similar high-level structures (the function of the chords within the phrase). © Johanna Devaney, Claire Arthur, Nathaniel Condit-Schultz, and Kirsten Nisula.","","Encoding (symbols); Harmonic analysis; Information retrieval; Textures; Automatic analysis; Harmonic contents; Harmonic structures; High-level structure; Multiple levels; Music analysis; Musical phrase; Phrase parsing; Harmonic functions","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Rigaud F.; Radenen M.","Rigaud, François (6506715928); Radenen, Mathieu (55413231700)","6506715928; 55413231700","Singing voice melody transcription using deep neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008470713&partnerID=40&md5=454f25e5abb25152c80261764522b751","Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","Rigaud F., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France; Radenen M., Audionamix R and D, 171 quai de Valmy, Paris, 75010, France","This paper presents a system for the transcription of singing voice melodies in polyphonic music signals based on Deep Neural Network (DNN) models. In particular, a new DNN system is introduced for performing the f0 estimation of the melody, and another DNN, inspired from recent studies, is learned for segmenting vocal sequences. Preparation of the data and learning configurations related to the specificity of both tasks are described. The performance of the melody f0 estimation system is compared with a state-of-the-art method and exhibits highest accuracy through a better generalization on two different music databases. Insights into the global functioning of this DNN are proposed. Finally, an evaluation of the global system combining the two DNNs for singing voice melody transcription is presented. © François Rigaud and Mathieu Radenen.","","Information retrieval; Transcription; F0 estimations; Global systems; Music database; Polyphonic music; Singing voices; State-of-the-art methods; Deep neural networks","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Eghbal-zadeh H.; Lehner B.; Schedl M.; Widmer G.","Eghbal-zadeh, Hamid (37080509500); Lehner, Bernhard (7003282869); Schedl, Markus (8684865900); Widmer, Gerhard (7004342843)","37080509500; 7003282869; 8684865900; 7004342843","I-vectors for timbre-based music similarity and music artist classification","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050540904&partnerID=40&md5=c241a0547ba0f1f9bc02175180b17a45","Department of Computational Perception, Johannes Kepler University of Linz, Austria","Eghbal-zadeh H., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In this paper, we present a novel approach to extract song-level descriptors built from frame-level timbral features such as Mel-frequency cepstral coefficient (MFCC). These descriptors are called identity vectors or i-vectors and are the results of a factor analysis procedure applied on frame-level features. The i-vectors provide a low-dimensional and fixed-length representation for each song and can be used in a supervised and unsupervised manner. First, we use the i-vectors for an unsupervised music similarity estimation, where we calculate the distance between i-vectors in order to predict the genre of songs. Second, for a supervised artist classification task we report the performance measures using multiple classifiers trained on the i-vectors. Standard datasets for each task are used to evaluate our method and the results are compared with the state of the art. By only using timbral information, we already achieved the state of the art performance in music similarity (which uses extra information such as rhythm). In artist classification using timbre descriptors, our method outperformed the state of the art. © Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl, Gerhard Widmer.","","Information retrieval; Classification tasks; Low dimensional; Mel-frequency cepstral coefficients; Multiple classifiers; Music similarity; Performance measure; State of the art; State-of-the-art performance; Vectors","H. Eghbal-zadeh; Department of Computational Perception, Johannes Kepler University of Linz, Austria; email: hamid.eghbal-zadeh@jku.at","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Tralie C.J.; Bendich P.","Tralie, Christopher J. (55921094200); Bendich, Paul (36615092600)","55921094200; 36615092600","Cover song identification with timbral shape sequences","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757346&partnerID=40&md5=3b60e074e2af1564d612d6c6ce5cb3f3","Duke University, Department of Electrical and Computer Engineering, United States; Duke University, Department of Mathematics, United States","Tralie C.J., Duke University, Department of Electrical and Computer Engineering, United States; Bendich P., Duke University, Department of Mathematics, United States","We introduce a novel low level feature for identifying cover songs which quantifies the relative changes in the smoothed frequency spectrum of a song. Our key insight is that a sliding window representation of a chunk of audio can be viewed as a time-ordered point cloud in high dimensions. For corresponding chunks of audio between different versions of the same song, these point clouds are approximately rotated, translated, and scaled copies of each other. If we treat MFCC embeddings as point clouds and cast the problem as a relative shape sequence, we are able to correctly identify 42/80 cover songs in the “Covers 80” dataset. By contrast, all other work to date on cover songs exclusively relies on matching note sequences from Chroma derived features. © Christopher J. Tralie, Paul Bendich.","","Cover song identifications; Cover songs; Derived features; High dimensions; Low-level features; Point cloud; Sliding Window; Smoothed frequencies; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Liang D.; Zhan M.; Ellis D.P.W.","Liang, Dawen (55586031200); Zhan, Minshu (57210233577); Ellis, Daniel P.W. (13609089200)","55586031200; 57210233577; 13609089200","Content-aware collaborative music recommendation using pre-trained neural networks","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","53","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070002034&partnerID=40&md5=307511cccf0368671db312f3579ae81a","LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States","Liang D., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Zhan M., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States","Although content is fundamental to our music listening preferences, the leading performance in music recommendation is achieved by collaborative-filtering-based methods which exploit the similarity patterns in user’s listening history rather than the audio content of songs. Meanwhile, collaborative filtering has the well-known “cold-start” problem, i.e., it is unable to work with new songs that no one has listened to. Efforts on incorporating content information into collaborative filtering methods have shown success in many non-musical applications, such as scientific article recommendation. Inspired by the related work, we train a neural network on semantic tagging information as a content model and use it as a prior in a collaborative filtering model. Such a system still allows the user listening data to “speak for itself”. The proposed system is evaluated on the Million Song Dataset and shows comparably better result than the collaborative filtering approaches, in addition to the favorable performance in the cold-start case. © Dawen Liang, Minshu Zhan, Daniel P. W. Ellis.","","Audio acoustics; Information retrieval; Semantics; Collaborative filtering methods; Content information; Listening history; Music recommendation; Scientific articles; Semantic tagging; Similarity patterns; Trained neural networks; Collaborative filtering","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Makris D.; Kaliakatsos-Papakostas M.; Cambouropoulos E.","Makris, Dimos (56487313100); Kaliakatsos-Papakostas, Maximos (36092739500); Cambouropoulos, Emilios (9632551100)","56487313100; 36092739500; 9632551100","Probabilistic modular bass voice leading in melodic harmonisation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047461620&partnerID=40&md5=34cfd942165b96441adb3425c8e8be16","Department of Informatics, Ionian University, Corfu, Greece; School of Music Studies, Aristotle University of Thessaloniki, Greece","Makris D., Department of Informatics, Ionian University, Corfu, Greece; Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","Probabilistic methodologies provide successful tools for automated music composition, such as melodic harmonisation, since they capture statistical rules of the music idioms they are trained with. Proposed methodologies focus either on specific aspects of harmony (e.g., generating abstract chord symbols) or incorporate the determination of many harmonic characteristics in a single probabilistic generative scheme. This paper addresses the problem of assigning voice leading focussing on the bass voice, i.e., the realisation of the actual bass pitches of an abstract chord sequence, under the scope of a modular melodic harmonisation system where different aspects of the generative process are arranged by different modules. The proposed technique defines the motion of the bass voice according to several statistical aspects: melody voice contour, previous bass line motion, bass-to-melody distances and statistics regarding inversions and note doublings in chords. The aforementioned aspects of voicing are modular, i.e., each criterion is defined by independent statistical learning tools. Experimental results on diverse music idioms indicate that the proposed methodology captures efficiently the voice layout characteristics of each idiom, whilst additional analyses on separate statistically trained modules reveal distinctive aspects of each idiom. The proposed system is designed to be flexible and adaptable (for instance, for the generation of novel blended melodic harmonisations). © Dimos Makris, Maximos Kaliakatsos-Papakostas, Emilios Cambouropoulos.","","Computer music; Automated music composition; Chord sequence; Generative process; Harmonic characteristics; Harmonisation; Probabilistic methodology; Statistical learning; Statistical rules; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kaliakatsos-Papakostas M.; Zacharakis A.; Tsougras C.; Cambouropoulos E.","Kaliakatsos-Papakostas, Maximos (36092739500); Zacharakis, Asterios (35182350900); Tsougras, Costas (56407161500); Cambouropoulos, Emilios (9632551100)","36092739500; 35182350900; 56407161500; 9632551100","Evaluating the general chord type representation in tonal music and organising GCT chord labels in functional chord categories","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042477544&partnerID=40&md5=743d24571f868bab9f84e8c8724b878e","School of Music Studies, Aristotle University of Thessaloniki, Greece","Kaliakatsos-Papakostas M., School of Music Studies, Aristotle University of Thessaloniki, Greece; Zacharakis A., School of Music Studies, Aristotle University of Thessaloniki, Greece; Tsougras C., School of Music Studies, Aristotle University of Thessaloniki, Greece; Cambouropoulos E., School of Music Studies, Aristotle University of Thessaloniki, Greece","The General Chord Type (GCT) representation is appropriate for encoding tone simultaneities in any harmonic context (such as tonal, modal, jazz, octatonic, atonal). The GCT allows the re-arrangement of the notes of a harmonic sonority such that abstract idiom-specific types of chords may be derived. This encoding is inspired by the standard roman numeral chord type labelling and is, therefore, ideal for hierarchic harmonic systems such as the tonal system and its many variations; at the same time, it adjusts to any other harmonic system such as post-tonal, atonal music, or traditional polyphonic systems. In this paper the descriptive potential of the GCT is assessed in the tonal idiom by comparing GCT harmonic labels with human expert annotations (Kostka & Payne harmonic dataset). Additionally, novel methods for grouping and clustering chords, according to their GCT encoding and their functional role in chord sequences, are introduced. The results of both harmonic labelling and functional clustering indicate that the GCT representation constitutes a suitable scheme for representing effectively harmony in computational systems. © Maximos Kaliakatsos-Papakostas, Asterios Zacharakis, Costas Tsougras, Emilios Cambouropoulos.","","Encoding (symbols); Information retrieval; Signal encoding; Chord sequence; Computational system; Harmonic system; Human expert; Novel methods; Polyphonic system; Re-arrangement; Roman numerals; Harmonic analysis","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Elowsson A.","Elowsson, Anders (56381573300)","56381573300","Beat tracking with a cepstroid invariant neural network","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016607210&partnerID=40&md5=0d1575b17ef00ae0e5be25aad149919a","KTH Royal Institute of Technology, Sweden","Elowsson A., KTH Royal Institute of Technology, Sweden","We present a novel rhythm tracking architecture that learns how to track tempo and beats through layered learning. A basic assumption of the system is that humans understand rhythm by letting salient periodicities in the music act as a framework, upon which the rhythmical structure is interpreted. Therefore, the system estimates the cepstroid (the most salient periodicity of the music), and uses a neural network that is invariant with regards to the cepstroid length. The input of the network consists mainly of features that capture onset characteristics along time, such as spectral differences. The invariant properties of the network are achieved by subsampling the input vectors with a hop size derived from a musically relevant subdivision of the computed cepstroid of each song. The output is filtered to detect relevant periodicities and then used in conjunction with two additional networks, which estimates the speed and tempo of the music, to predict the final beat positions. We show that the architecture has a high performance on music with public annotations. © Anders Elowsson.","","Information retrieval; Beat tracking; Input vector; Invariant properties; Layered learning; Spectral differences; Network architecture","A. Elowsson; KTH Royal Institute of Technology, Sweden; email: elov@kth.se","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Dittmar C.; Pfleiderer M.; Müller M.","Dittmar, Christian (15051598000); Pfleiderer, Martin (57094855700); Müller, Meinard (7404689873)","15051598000; 57094855700; 7404689873","Automated estimation of ride cymbal swing ratios in jazz recordings","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020404260&partnerID=40&md5=110c3786b410b8b93854cbc4ca91b7f7","International Audio Laboratories Erlangen, Germany; Liszt University of Music Weimar, Germany","Dittmar C., International Audio Laboratories Erlangen, Germany; Pfleiderer M., Liszt University of Music Weimar, Germany; Müller M., International Audio Laboratories Erlangen, Germany","In this paper, we propose a new method suitable for the automatic analysis of microtiming played by drummers in jazz recordings. Specifically, we aim to estimate the drummers’ swing ratio in excerpts of jazz recordings taken from the Weimar Jazz Database. A first approach is based on automatic detection of ride cymbal (RC) onsets and evaluation of relative time intervals between them. However, small errors in the onset detection propagate considerably into the swing ratio estimates. As our main technical contribution, we propose to use the log-lag autocorrelation function (LLACF) as a mid-level representation for estimating swing ratios, circumventing the error-prone detection of RC onsets. In our experiments, the LLACF-based swing ratio estimates prove to be more reliable than the ones based on RC onset detection. Therefore, the LLACF seems to be the method of choice to process large amounts of jazz recordings. Finally, we indicate some implications of our method for microtiming studies in jazz research. © Christian Dittmar, Martin Pfleiderer, Meinard Müller.","","Information retrieval; Autocorrelation functions; Automated estimation; Automatic analysis; Automatic Detection; Mid-level representation; Onset detection; Relative time; Technical contribution; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Cherla S.; Tran S.N.; Weyde T.; d’Avila Garcez A.","Cherla, Srikanth (24824198800); Tran, Son N. (56272651200); Weyde, Tillman (24476899500); d’Avila Garcez, Artur (57188767693)","24824198800; 56272651200; 24476899500; 57188767693","Hybrid long- and short-term models of folk melodies","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069910924&partnerID=40&md5=6383bfd7ae3920afe46e11f8e3961dba","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Machine Learning Group, Department of Computer Science, City University London, United Kingdom","Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Tran S.N., Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom","In this paper, we present the results of a study on dynamic models for predicting sequences of musical pitch in melodies. Such models predict a probability distribution over the possible values of the next pitch in a sequence, which is obtained by combining the prediction of two components (1) a long-term model (LTM) learned offline on a corpus of melodies, as well as (2) a short-term model (STM) which incorporates context-specific information available during prediction. Both the LTM and the STM learn regularities in pitch sequences solely from data. The models are combined in an ensemble, wherein they are weighted by the relative entropies of their respective predictions. Going by previous work that demonstrates the success of Connectionist LTMs, we employ the recently proposed Recurrent Temporal Discriminative Restricted Boltzmann Machine (RTDRBM) as the LTM here. While it is indeed possible for the same model to also serve as an STM, our experiments showed that n-gram models tended to learn faster than the RTDRBM in an online setting and that the hybrid of an RTDRBM LTM and an n-gram STM gives us the best predictive performance yet on a corpus of monophonic chorale and folk melodies. © Srikanth Cherla, Son N. Tran, Tillman Weyde, Artur d’Avila Garcez.","","Information retrieval; Probability distributions; Long-term models; Musical pitch; On-line setting; Predictive performance; Relative entropy; Restricted boltzmann machine; Specific information; Two-component; Forecasting","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Savage P.E.; Atkinson Q.D.","Savage, Patrick E. (7202875108); Atkinson, Quentin D. (22633898400)","7202875108; 22633898400","Automatic tune family identification by musical sequence alignment","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045054433&partnerID=40&md5=10c51c61a65df7055ad8e7c67cd6f2a4","Tokyo University of the Arts, Dept. of Musicology, Japan; Auckland University, Dept. of Psychology, New Zealand","Savage P.E., Tokyo University of the Arts, Dept. of Musicology, Japan; Atkinson Q.D., Auckland University, Dept. of Psychology, New Zealand","Musics, like languages and genes, evolve through a process of transmission, variation, and selection. Evolution of musical tune families has been studied qualitatively for over a century, but quantitative analysis has been hampered by an inability to objectively distinguish between musical similarities that are due to chance and those that are due to descent from a common ancestor. Here we propose an automated method to identify tune families by adapting genetic sequence alignment algorithms designed for automatic identification and alignment of protein families. We tested the effectiveness of our method against a high-quality ground-truth dataset of 26 folk tunes from four diverse tune families (two English, two Japanese) that had previously been identified and aligned manually by expert musicologists. We tested different combinations of parameters related to sequence alignment and to modeling of pitch, rhythm, and text to find the combination that best matched the ground-truth classifications. The best-performing automated model correctly grouped 100% (26/26) of the tunes in terms of overall similarity to other tunes, identifying 85% (22/26) of these tunes as forming distinct tune families. The success of our approach on a diverse, cross-cultural ground-truth dataset suggests promise for future automated reconstruction of musical evolution on a wide scale. © Patrick E. Savage, Quentin D. Atkinson.","","Automation; Information retrieval; Automated methods; Automated modeling; Automatic identification; Genetic sequence; Ground-truth dataset; Musical similarity; Protein family; Sequence alignments; Alignment","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Ganguli K.K.; Gulati S.; Serra X.; Rao P.","Ganguli, Kaustuv Kanti (56094514100); Gulati, Sankalp (37087243200); Serra, Xavier (55892979900); Rao, Preeti (35180193500)","56094514100; 37087243200; 55892979900; 35180193500","Data-driven exploration of melodic structures in hindustani music","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022075371&partnerID=40&md5=476635bfbb0be4d91966e20dff1c312f","Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","Indian art music is quintessentially an improvisatory music form in which the line between ‘fixed’ and ‘free’ is extremely subtle. In a rāga performance, the melody is loosely constrained by the chosen composition but otherwise improvised in accordance with the rāga grammar. One of the melodic aspects that is governed by this grammar is the manner in which a melody evolves in time in the course of a performance. In this work, we aim to discover such implicit patterns or regularities present in the temporal evolution of vocal melodies of Hindustani music. We start by applying existing tools and techniques used in music information retrieval to a collection of concerts recordings of ālāp performances by renowned khayal vocal artists. We use svara-based and svara duration-based melodic features to study and quantify the manifestation of concepts such as vādi, samvādi, nyās and graha svara in the vocal performances. We show that the discovered patterns corroborate the musicological findings that describe the “unfolding” of a rāga in vocal performances of Hindustani music. The patterns discovered from the vocal melodies might help music students to learn improvisation and can complement the oral music pedagogy followed in this music tradition. © Kaustuv Kanti Ganguli, Sankalp Gulati, Xavier Serra, Preeti Rao.","","Data driven; Melodic structure; Music information retrieval; Temporal evolution; Tools and techniques; Vocal performance; Information retrieval","K.K. Ganguli; Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; email: kaustuvkanti@ee.iitb.ac.in","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Dzhambazov G.; Şentürk S.; Serra X.","Dzhambazov, Georgi (56271863800); Şentürk, Sertan (43461595100); Serra, Xavier (55892979900)","56271863800; 43461595100; 55892979900","Searching lyrical phrases in a-capella turkish makam recordings","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069989891&partnerID=40&md5=493354f73ca8ab6a9bca5baa7a7daa77","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Dzhambazov G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Search by lyrics, the problem of locating the exact occurrences of a phrase from lyrics in musical audio, is a recently emerging research topic. Unlike key-phrases in speech, lyrical key-phrases have durations that bear important relation to other musical aspects like the structure of a composition. In this work we propose an approach that address the differences of syllable durations, specific for singing. First a phrase is expanded to MFCC-based phoneme models, trained on speech. Then, we apply dynamic time warping between the phrase and audio to estimate candidate audio segments in the given audio recording. Next, the retrieved audio segments are ranked by means of a novel score-informed hidden Markov model, in which durations of the syllables within a phrase are explicitly modeled. The proposed approach is evaluated on 12 a-capella audio recordings of Turkish Makam music. Relying on standard speech phonetic models, we arrive at promising results that outperform a baseline approach unaware of lyrics durations. To the best of our knowledge, this is the first work tackling the problem of search by lyrical key-phrases. We expect that it can serve as a baseline for further research on singing material with similar musical characteristics. © Georgi Dzhambazov, Sertan Şentürk, Xavier Serra.","","Audio acoustics; Hidden Markov models; Information retrieval; Dynamic time warping; Key-phrase; Musical audio; Phoneme models; Research topics; Turkishs; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Sordo M.; Ogihara M.; Wuchty S.","Sordo, Mohamed (43462170300); Ogihara, Mitsunori (54420747900); Wuchty, Stefan (6602576457)","43462170300; 54420747900; 6602576457","Analysis of the evolution of research groups and topics in the ISMIR conference","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043707399&partnerID=40&md5=186fcb0bbb4e2cb1f031fdacc793b2eb","Center for Computational Science, University of Miami, United States; Dept. of Computer Science, University of Miami, United States","Sordo M., Center for Computational Science, University of Miami, United States; Ogihara M., Dept. of Computer Science, University of Miami, United States; Wuchty S., Dept. of Computer Science, University of Miami, United States","We present an analysis of the topics and research groups that participated in the ISMIR conference over the last 15 years, based on its proceedings. While we first investigate the topological changes of the co-authorship network as well as topics over time, we also identify groups of researchers, allowing us to investigate their evolution and topic dependence. Notably, we find that large groups last longer if they actively alter their membership. Furthermore, such groups tend to cover a wider selection of topics, suggesting that a change of members as well as of research topics increases their adaptability. In turn, smaller groups show the opposite behavior, persisting longer if their membership is altered minimally and focus on a smaller set of topics. Finally, by analyzing the effect of group size and lifespan on research impact, we observed that papers penned by medium sized and long lasting groups tend to have a citation advantage. © Mohamed Sordo, Mitsunori Ogihara, Stefan Wuchty.","","Co-authorship networks; Group size; Large groups; Long lasting; Research groups; Research impacts; Research topics; Topological changes; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Smith J.B.L.; Goto M.","Smith, Jordan B.L. (55582613300); Goto, Masataka (7403505330)","55582613300; 7403505330","Using priors to improve estimates of music structure","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009854686&partnerID=40&md5=606741b8131adff841f4c4ed599062a8","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Smith J.B.L., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","Existing collections of annotations of musical structure possess many strong regularities: for example, the lengths of segments are approximately log-normally distributed, as is the number of segments per annotation; and the lengths of two adjacent segments are highly likely to have an integer ratio. Since many aspects of structural annotations are highly regular, but few of these regularities are taken into account by current algorithms, we propose several methods of improving predictions of musical structure by using their likelihood according to prior distributions. We test the use of priors to improve a committee of basic segmentation algorithms, and to improve a committee of cutting-edge approaches submitted to MIREX. In both cases, we are unable to improve on the best committee member, meaning that our proposed approach is outperformed by simple parameter tuning. The same negative result was found despite incorporating the priors in multiple ways. To explain the result, we show that although there is a correlation overall between output accuracy and prior likelihood, the weakness of the correlation in the high-likelihood region makes the proposed method infeasible. We suggest that to improve on the state of the art using prior likelihoods, these ought to be incorporated at a deeper level of the algorithm. © Jordan B. L. Smith, Masataka Goto.","","Normal distribution; Committee members; Music structures; Musical structures; Parameter-tuning; Prior distribution; Segmentation algorithms; State of the art; Strong regularities; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Nuanáin C.Ó.; Herrera P.; Jordà S.","Nuanáin, Cárthach Ó. (56732945500); Herrera, Perfecto (24824250300); Jordà, Sergi (8228030900)","56732945500; 24824250300; 8228030900","An evaluation framework and case study for rhythmic concatenative synthesis","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019968153&partnerID=40&md5=a59c0dd7f3f7860cb74d595d40970995","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Nuanáin C.Ó., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Jordà S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present and report on a methodology for evaluating a creative MIR-based application of concatenative synthesis. After reviewing many existing applications of concatenative synthesis we have developed an application that specifically addresses loop-based rhythmic pattern generation. We describe how such a system could be evaluated with respect to its its objective retrieval performance and subjective responses of humans in a listener survey. Applying this evaluation strategy produced positive findings to help verify and validate the objectives of our system. We discuss the results of the evaluation and draw conclusions by contrasting the objective analysis with the subjective impressions of the users. © Cárthach Ó Nuanáin, Perfecto Herrera, Sergi Jordà.","","Evaluation framework; Evaluation strategies; Objective analysis; Retrieval performance; Rhythmic patterns; Subjective impressions; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Luo Y.-J.; Su L.; Yang Y.-H.; Chi T.-S.","Luo, Yin-Jyun (57226047667); Su, Li (55966919100); Yang, Yi-Hsuan (55218558400); Chi, Tai-Shih (7005693586)","57226047667; 55966919100; 55218558400; 7005693586","Detection of common mistakes in novice violin playing","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044638302&partnerID=40&md5=4f567b580d4e4f29cdb88467cb705652","Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taiwan","Luo Y.-J., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan, Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Su L., Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taiwan; Chi T.-S., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Analyzing and modeling playing mistakes are essential parts of computer-aided education tools in learning musical instruments. In this paper, we present a system for identifying four types of mistakes commonly made by novice violin players. We construct a new dataset comprising of 981 legato notes played by 10 players across different skill levels, and have violin experts annotate all possible mistakes associated with each note by listening to the recordings. Five feature representations are generated from the same feature set with different scales, including two note-level representations and three segment-level representations of the onset, sustain and offset, and are tested for automatically identifying playing mistakes. Performance is evaluated under the framework of using the Fisher score for feature selection and the support vector machine for classification. Results show that the F-measures using different feature representations can vary up to 20% for two types of playing mistakes. It demonstrates the different sensitivities of each feature representation to different mistakes. Moreover, our results suggest that the standard audio features such as MFCCs are not good enough and more advanced feature design may be needed. © Yin-Jyun Luo Li Su Yi-Hsuan Yang and Tai-Shih Chi.","","Computer aided instruction; Information retrieval; Support vector machines; Audio features; Computer-aided education; F measure; Feature representation; Feature sets; Fisher score; Skill levels; Musical instruments","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Cogliati A.; Temperley D.; Duan Z.","Cogliati, Andrea (56940539000); Temperley, David (6602761758); Duan, Zhiyao (24450312900)","56940539000; 6602761758; 24450312900","Transcribing human piano performances into music notation","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017519434&partnerID=40&md5=7a747ffa7f0903679a4a554c115ed932","Electrical and Computer Engineering, University of Rochester, United States; Eastman School of Music, University of Rochester, United States","Cogliati A., Electrical and Computer Engineering, University of Rochester, United States; Temperley D., Eastman School of Music, University of Rochester, United States; Duan Z., Electrical and Computer Engineering, University of Rochester, United States","Automatic music transcription aims to transcribe musical performances into music notation. However, existing transcription systems that have been described in research papers typically focus on multi-F0 estimation from audio and only output notes in absolute terms, showing frequency and absolute time (a piano-roll representation), but not in musical terms, with spelling distinctions (e.g., Ab versus G#) and quantized meter. To complete the transcription process, one would need to convert the piano-roll representation into a properly formatted and musically meaningful musical score. This process is non-trivial and largely unre-searched. In this paper we present a system that generates music notation output from human-recorded MIDI performances of piano music. We show that the correct estimation of the meter, harmony and streams in a piano performance provides a solid foundation to produce a properly formatted score. In a blind evaluation by professional music theorists, the proposed method outperforms two commercial programs and an open source program in terms of pitch notation and rhythmic notation, and ties for the top in terms of overall voicing and staff placement. © Andrea Cogliati, David Temperley, Zhiyao Duan.","","Audio systems; Frequency estimation; Information retrieval; Musical instruments; Transcription; Automatic music transcription; Blind evaluations; Commercial projects; F0 estimations; Musical performance; Open-source program; Research papers; Transcription process; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Jin R.; Raphael C.","Jin, Rong (55582166300); Raphael, Christopher (7004214964)","55582166300; 7004214964","Graph-based rhythm interpretation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069995407&partnerID=40&md5=5dfe2c8f9f8ec272ae24ade2bd9bf98e","Indiana University, School of Informatics and Computing, United States","Jin R., Indiana University, School of Informatics and Computing, United States; Raphael C., Indiana University, School of Informatics and Computing, United States","We present a system that interprets the notated rhythm obtained from optical music recognition (OMR). Our approach represents the notes and rests in a system measure as the vertices of a graph. We connect the graph by adding voice edges and coincidence edges between pairs of vertices, while the rhythmic interpretation follows simply from the connected graph. The graph identification problem is cast as an optimization where each potential edge is scored according to its plausibility. We seek the optimally scoring graph where the score is represented as a sum of edge scores. Experiments were performed on about 60 score pages showing that our system can handle difficult rhythmic situations including multiple voices, voices that merge and split, voices spanning two staves, and missing tuplets. © Rong Jin, Christopher Raphael.","","Graphic methods; Information retrieval; AND splits; Connected graph; Graph-based; Identification problem; Optical music recognition; Graph theory","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Zhang S.; Repetto R.C.; Serra X.","Zhang, Shuo (57195678445); Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57195678445; 57200495266; 55892979900","Predicting pairwise pitch contour relations based on linguistic tone information in Beijing opera singing","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069938667&partnerID=40&md5=e2bdfee592fae56294de8a6a0a13f41e","Music Technology Group, Universitat Pompeu Fabra, Spain","Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","The similarity between linguistic tones and melodic pitch contours in Beijing Opera can be captured either by the contour shape of single syllable units, or by the pairwise pitch height relations in adjacent syllable units. In this paper, we investigate the latter problem with a novel machine learning approach, using techniques from time series data mining. Approximately 1300 pairwise contour segments are extracted from a selection of 20 arias. We then formulate the problem as a supervised machine-learning task of predicting types of pairwise melodic relations based on linguistic tone information. The results give a comparative view of fixed and mixed-effects models that achieved around 70% of maximum accuracy. We discuss the superiority of the current method to that of the unsupervised learning in single-syllable-unit contour analysis of similarity in Beijing Opera. © Shuo Zhang, Rafael Caro Repetto, Xavier Serra.","","Information retrieval; Linguistics; Machine learning; Supervised learning; Contour analysis; Contour segments; Machine learning approaches; Maximum accuracies; Mixed effects models; Predicting types; Supervised machine learning; Time series data mining; Data mining","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Van Balen J.; Burgoyne J.A.; Bountouridis D.; Müllensiefen D.; Veltkamp R.C.","Van Balen, Jan (55874307000); Burgoyne, John Ashley (23007865600); Bountouridis, Dimitrios (36607463900); Müllensiefen, Daniel (23019169400); Veltkamp, Remco C. (7003421646)","55874307000; 23007865600; 36607463900; 23019169400; 7003421646","Corpus analysis tools for computational hook discovery","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066095143&partnerID=40&md5=a684729ec22c83848b81b8fd25830c71","Department of Information and Computing Sciences, Utrecht University, Netherlands; Music Cognition Group, University of Amsterdam, Netherlands; Department of Psychology, Goldsmiths, University of London, United Kingdom","Van Balen J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Burgoyne J.A., Music Cognition Group, University of Amsterdam, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Müllensiefen D., Department of Psychology, Goldsmiths, University of London, United Kingdom; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands","Compared to studies with symbolic music data, advances in music description from audio have overwhelmingly focused on ground truth reconstruction and maximizing prediction accuracy, with only a small fraction of studies using audio description to gain insight into musical data. We present a strategy for the corpus analysis of audio data that is optimized for interpretable results. The approach brings two previously unexplored concepts to the audio domain: audio bigram distributions, and the use of corpus-relative or “second-order” descriptors. To test the real-world applicability of our method, we present an experiment in which we model song recognition data collected in a widely-played music game. By using the proposed corpus analysis pipeline we are able to present a cognitively adequate analysis that allows a model interpretation in terms of the listening history and experience of our participants. We find that our corpus-based audio features are able to explain a comparable amount of variance to symbolic features for this task when used alone and that they can supplement symbolic features profitably when the two types of features are used in tandem. Finally, we highlight new insights into what makes music recognizable. © Jan Van Balen, John Ashley Burgoyne, Dimitrios Bountouridis, Daniel Müllensiefen, Remco C. Veltkamp.","","Information retrieval; Audio description; Audio features; Corpus analysis; Listening history; Model interpretations; Prediction accuracy; Second orders; Symbolic features; Audio acoustics","J. Van Balen; Department of Information and Computing Sciences, Utrecht University, Netherlands; email: J.M.H.VanBalen@uu.nl","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Weiß C.; Schaab M.","Weiß, Christof (56273046500); Schaab, Maximilian (57210232065)","56273046500; 57210232065","On the impact of key detection performance for identifying classical music styles","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069983258&partnerID=40&md5=8a7032e61465a68c40a92863461e2497","Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany","Weiß C., Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany; Schaab M., Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany","We study the automatic identification of Western classical music styles by directly using chroma histograms as classification features. Thereby, we evaluate the benefits of knowing a piece’s global key for estimating key-related pitch classes. First, we present four automatic key detection systems. We compare their performance on suitable datasets of classical music and optimize the algorithms’ free parameters. Using a second dataset, we evaluate automatic classification into the four style periods Baroque, Classical, Romantic, and Modern. To that end, we calculate global chroma statistics of each audio track. We then split up the tracks according to major and minor keys and circularly shift the chroma histograms with respect to the tonic note. Based on these features, we train two individual classifier models for major and minor keys. We test the efficiency of four chroma extraction algorithms for classification. Furthermore, we evaluate the impact of key detection performance on the classification results. Additionally, we compare the key-related chroma features to other chroma-based features. We obtain improved performance when using an efficient key detection method for shifting the chroma histograms. © Christof Weiß, Maximilian Schaab.","","Automation; Graphic methods; Information retrieval; Automatic classification; Automatic identification; Chroma extractions; Classification features; Classification results; Detection methods; Detection performance; Individual classifiers; Classification (of information)","C. Weiß; Fraunhofer Institute for Digital Media Technology, Ilmenau, Germany; email: christof.weiss@idmt.fraunhofer.de","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Liebman E.; Stone P.; White C.N.","Liebman, Elad (55324775700); Stone, Peter (7203001213); White, Corey N. (23096672900)","55324775700; 7203001213; 23096672900","How music alters decision making - Impact of music stimuli on emotional classification","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069837109&partnerID=40&md5=076e67e20fb02b27ed717edbf136cab2","Computer Science Department, University of Texas, Austin, United States; Department of Psychology, Syracuse University, United States","Liebman E., Computer Science Department, University of Texas, Austin, United States; Stone P., Computer Science Department, University of Texas, Austin, United States; White C.N., Department of Psychology, Syracuse University, United States","Numerous studies have demonstrated that mood can affect emotional processing. The goal of this study was to explore which components of the decision process are affected when exposed to music; we do so within the context of a stochastic sequential model of simple decisions, the drift-diffusion model (DDM). In our experiment, participants decided whether words were emotionally positive or negative while listening to music that was chosen to induce positive or negative mood. The behavioral results show that the music manipulation was effective, as participants were biased to label words positive in the positive music condition. The DDM shows that this bias was driven by a change in the starting point of evidence accumulation, which indicates an a priori response bias. In contrast, there was no evidence that music affected how participants evaluated the emotional content of the stimuli. To better understand the correspondence between auditory features and decision-making, we proceeded to study how individual aspects of music affect response patterns. Our results have implications for future studies of the connection between music and mood. © Elad Liebman, Peter Stone, Corey N. White.","","Information retrieval; Stochastic models; Stochastic systems; Auditory feature; Decision process; Drift-diffusion model; Emotional classification; Evidence accumulation; Implications for futures; Response patterns; Sequential model; Decision making","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Xia G.; Wang Y.; Dannenberg R.; Gordon G.","Xia, Guangyu (55586682600); Wang, Yun (56414303500); Dannenberg, Roger (7003266250); Gordon, Geoffrey (57203070987)","55586682600; 56414303500; 7003266250; 57203070987","Spectral learning for expressive interactive ensemble music performance","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044844984&partnerID=40&md5=430990a40484b7c8c0c2a91c69ad2550","School of Computer Science, Carnegie Mellon University, United States","Xia G., School of Computer Science, Carnegie Mellon University, United States; Wang Y., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, United States; Gordon G., School of Computer Science, Carnegie Mellon University, United States","We apply machine learning to a database of recorded ensemble performances to build an artificial performer that can perform music expressively in concert with human musicians. We consider the piano duet scenario and focus on the interaction of expressive timing and dynamics. We model different performers’ musical expression as co-evolving time series and learn their interactive relationship from multiple rehearsals. In particular, we use a spectral method, which is able to learn the correspondence not only between different performers but also between the performance past and future by reduced-rank partial regressions. We describe our model that captures the intrinsic interactive relationship between different performers, present the spectral learning procedure, and show that the spectral learning algorithm is able to generate a more human-like interaction. © Guangyu Xia, Yun Wang, Roger Dannenberg, Geoffrey Gordon.","","Information retrieval; Learning algorithms; Ensemble performance; Interactive relationships; Music performance; Musical expression; Partial regression; Spectral learning; Spectral learning algorithms; Spectral methods; Learning systems","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Oramas S.; Sordo M.; Espinosa-Anke L.; Serra X.","Oramas, Sergio (55582112200); Sordo, Mohamed (43462170300); Espinosa-Anke, Luis (56242866900); Serra, Xavier (55892979900)","55582112200; 43462170300; 56242866900; 55892979900","A semantic-based approach for artist similarity","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030750396&partnerID=40&md5=0be5c4795e0db187586203a7cb8c65a2","Music Technology Group, Universitat Pompeu Fabra, Spain; Center for Computational Science, University of Miami, United States; TALN Group, Universitat Pompeu Fabra, Spain","Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Sordo M., Center for Computational Science, University of Miami, United States; Espinosa-Anke L., TALN Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","This paper describes and evaluates a method for computing artist similarity from a set of artist biographies. The proposed method aims at leveraging semantic information present in these biographies, and can be divided in three main steps, namely: (1) entity linking, i.e. detecting mentions to named entities in the text and linking them to an external knowledge base; (2) deriving a knowledge representation from these mentions in the form of a semantic graph or a mapping to a vector-space model; and (3) computing semantic similarity between documents. We test this approach on a corpus of 188 artist biographies and a slightly larger dataset of 2,336 artists, both gathered from Last.fm. The former is mapped to the MIREX Audio and Music Similarity evaluation dataset, so that its similarity judgments can be used as ground truth. For the latter dataset we use the similarity between artists as provided by the Last.fm API. Our evaluation results show that an approach that computes similarity over a graph of entities and semantic categories clearly outperforms a baseline that exploits word co-occurrences and latent factors. © Sergio Oramas, Mohamed Sordo, Luis Espinosa-Anke, Xavier Serra.","","Audio acoustics; Information retrieval; Knowledge based systems; Knowledge management; Knowledge representation; Natural language processing systems; Statistical tests; Vector spaces; Artist similarities; Evaluation results; External knowledge; Semantic category; Semantic information; Semantic similarity; Vector space models; Word co-occurrence; Semantics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Weiß C.; Arifi-Müller V.; Prätzlich T.; Kleinertz R.; Müller M.","Weiß, Christof (56273046500); Arifi-Müller, Vlora (57189590835); Prätzlich, Thomas (55582692100); Kleinertz, Rainer (55553090300); Müller, Meinard (7404689873)","56273046500; 57189590835; 55582692100; 55553090300; 7404689873","Analyzing measure annotations for western classical music recordings","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026841879&partnerID=40&md5=a02732cdd121e1f7d91fe1e05440d870","International Audio Laboratories, Erlangen, Germany; Institut für Musikwissenschaft, Saarland University, Germany","Weiß C., International Audio Laboratories, Erlangen, Germany; Arifi-Müller V., International Audio Laboratories, Erlangen, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Kleinertz R., Institut für Musikwissenschaft, Saarland University, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","This paper approaches the problem of annotating measure positions in Western classical music recordings. Such annotations can be useful for navigation, segmentation, and cross-version analysis of music in different types of representations. In a case study based on Wagner’s opera “Die Walküre”, we analyze two types of annotations. First, we report on an experiment where several human listeners generated annotations in a manual fashion. Second, we examine computer-generated annotations which were obtained by using score-to-audio alignment techniques. As one main contribution of this paper, we discuss the inconsistencies of the different annotations and study possible musical reasons for deviations. As another contribution, we propose a kernel-based method for automatically estimating confidences of the computed annotations which may serve as a first step towards improving the quality of this automatic method. © Christof Weiß, Vlora Arifi-Müller, Thomas Prätzlich, Rainer Kleinertz, Meinard Müller.","","Information retrieval; Audio alignments; Automatic method; Classical musics; Computer generated; Human listeners; Kernel based methods; Manual fashion; Audio recordings","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bosch J.J.; Bittner R.M.; Salamon J.; Gómez E.","Bosch, Juan J. (36760347300); Bittner, Rachel M. (55659619600); Salamon, Justin (55184866100); Gómez, Emilia (14015483200)","36760347300; 55659619600; 55184866100; 14015483200","A comparison of melody extraction methods based on source-filter modelling","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023743808&partnerID=40&md5=8232c0c50e0e1ae57635845be7f88291","Music Technology Group, Universitat Pompeu Fabra, Spain; Music and Audio Research Laboratory, New York University, United States","Bosch J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bittner R.M., Music and Audio Research Laboratory, New York University, United States; Salamon J., Music and Audio Research Laboratory, New York University, United States; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain","This work explores the use of source-filter models for pitch salience estimation and their combination with different pitch tracking and voicing estimation methods for automatic melody extraction. Source-filter models are used to create a mid-level representation of pitch that implicitly incorporates timbre information. The spectrogram of a musical audio signal is modelled as the sum of the leading voice (produced by human voice or pitched musical instruments) and accompaniment. The leading voice is then modelled with a Smoothed Instantaneous Mixture Model (SIMM) based on a source-filter model. The main advantage of such a pitch salience function is that it enhances the leading voice even without explicitly separating it from the rest of the signal. We show that this is beneficial for melody extraction, increasing pitch estimation accuracy and reducing octave errors in comparison with simpler pitch salience functions. The adequate combination with voicing detection techniques based on pitch contour characterisation leads to significant improvements over state-of-the-art methods, for both vocal and instrumental music. © First author, Second author, Third author, Fourth author, Fifth author, Sixth author.","","Extraction; Information retrieval; Estimation methods; Instantaneous mixtures; Melody extractions; Mid-level representation; Musical audio signal; Pitch estimation; Source-filter models; State-of-the-art methods; Audio acoustics","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"McFee B.; Humphrey E.J.; Urbano J.","McFee, Brian (34875379700); Humphrey, Eric J. (55060792500); Urbano, Julián (36118414700)","34875379700; 55060792500; 36118414700","A plan for sustainable MIR evaluation","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013331154&partnerID=40&md5=48203c5ba896ab616933c771aebeaacf","Center for Data Science, MARL, New York University, United States; Spotify, Ltd, Sweden; Music Technology Group, Universitat Pompeu Fabra, Spain","McFee B., Center for Data Science, MARL, New York University, United States; Humphrey E.J., Spotify, Ltd, Sweden; Urbano J., Music Technology Group, Universitat Pompeu Fabra, Spain","The Music Information Retrieval Evaluation eXchange (MIREX) is a valuable community service, having established standard datasets, metrics, baselines, methodologies, and infrastructure for comparing MIR methods. While MIREX has managed to successfully maintain operations for over a decade, its long-term sustainability is at risk. The imposed constraint that input data cannot be made freely available to participants necessitates that all algorithms run on centralized computational resources, which are administered by a limited number of people. This incurs an approximately linear cost with the number of submissions, exacting significant tolls on both human and financial resources, such that the current paradigm becomes less tenable as participation increases. To alleviate the recurring costs of future evaluation campaigns, we propose a distributed, community-centric paradigm for system evaluation, built upon the principles of openness, transparency, reproducibility, and incremental evaluation. We argue that this proposal has the potential to reduce operating costs to sustainable levels. Moreover, the proposed paradigm would improve scalability, and eventually result in the release of large, open datasets for improving both MIR techniques and evaluation methods. © Brian McFee, Eric J. Humphrey, Julián Urbano.","","Information retrieval; Insecticides; Operating costs; Community services; Computational resources; Evaluation methods; Financial resources; Incremental evaluation; Long-term sustainability; Music information retrieval; Reproducibilities; Large dataset","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Chen N.; Downie J.S.; Xiao H.; Zhu Y.; Zhu J.","Chen, Ning (57061407700); Downie, J. Stephen (7102932568); Xiao, Haidong (8698019000); Zhu, Yu (56716924600); Zhu, Jie (57087544600)","57061407700; 7102932568; 8698019000; 56716924600; 57087544600","Modified perceptual linear prediction liftered cepstrum (MPLPLC) model for pop cover song recognition","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019656044&partnerID=40&md5=3a704b55d6f2ce0284b3fdffd3bec89b","Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; Graduate School of Library and Information Science, UIUC, United States; Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China; Dept. of Electronic Engineering, Shanghai Jiao Tong University, China","Chen N., Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; Downie J.S., Graduate School of Library and Information Science, UIUC, United States; Xiao H., Shanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai, China; Zhu Y., Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; Zhu J., Dept. of Electronic Engineering, Shanghai Jiao Tong University, China","Most of the features of Cover Song Identification (CSI), for example, Pitch Class Profile (PCP) related features, are based on the musical facets shared among cover versions: melody evolution and harmonic progression. In this work, the perceptual feature was studied for CSI. Our idea was to modify the Perceptual Linear Prediction (PLP) model in the field of Automatic Speech Recognition (ASR) by (a) introducing new research achievements in psychophysics, and (b) considering the difference between speech and music signals to make it consistent with human hearing and more suitable for music signal analysis. Furthermore, the obtained Linear Prediction Coefficients (LPCs) were mapped to LPC cepstrum coefficients, on which liftering was applied, to boost the timbre invariance of the resultant feature: Modified Perceptual Linear Prediction Liftered Cepstrum (MPLPLC). Experimental results showed that both LPC cepstrum coefficients mapping and cepstrum liftering were crucial in ensuring the identification power of the MPLPLC feature. The MPLPLC feature outperformed state-of-the-art features in the context of CSI and in resisting instrumental accompaniment variation. This study verifies that the mature techniques in the ASR or Computational Auditory Scene Analysis (CASA) fields may be modified and included to enhance the performance of the Music Information Retrieval (MIR) scheme. © Ning Chen, J. Stephen Downie, Haidong Xiao, Yu Zhu, Jie Zhu.","","Audition; Forecasting; Information retrieval; Automatic speech recognition; Computational auditory scene analysis; Cover song identifications; Linear prediction coefficients; Music information retrieval; Music signal analysis; Perceptual linear predictions; Speech and music signals; Speech recognition","N. Chen; Dept. of Elec. and Comm. Eng., East China Univ. of Sci. and Tech, China; email: nchen@ecust.edu.cn","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Guiomard-Kagan N.; Giraud M.; Groult R.; Levé F.","Guiomard-Kagan, Nicolas (57210209791); Giraud, Mathieu (8700367400); Groult, Richard (6507884031); Levé, Florence (55893852300)","57210209791; 8700367400; 6507884031; 55893852300","Comparing voice and stream segmentation algorithms","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063425793&partnerID=40&md5=19582ebbd7660bb954005ac1ab246807","MIS, U. Picardie Jules Verne (UPJV), Amiens, France; CRIStAL, CNRS, U. Lille, Lille, France","Guiomard-Kagan N., MIS, U. Picardie Jules Verne (UPJV), Amiens, France; Giraud M., CRIStAL, CNRS, U. Lille, Lille, France; Groult R., MIS, U. Picardie Jules Verne (UPJV), Amiens, France; Levé F., MIS, U. Picardie Jules Verne (UPJV), Amiens, France","Voice and stream segmentation algorithms group notes from polyphonic data into relevant units, providing a better understanding of a musical score. Voice segmentation algorithms usually extract voices from the beginning to the end of the piece, whereas stream segmentation algorithms identify smaller segments. In both cases, the goal can be to obtain mostly monophonic units, but streams with polyphonic data are also relevant. These algorithms usually cluster contiguous notes with close pitches. We propose an independent evaluation of four of these algorithms (Temperley, Chew and Wu, Ishigaki et al., and Rafailidis et al.) using several evaluation metrics. We benchmark the algorithms on a corpus containing the 48 fugues of Well-Tempered Clavier by J. S. Bach as well as 97 files of popular music containing actual polyphonic information. We discuss how to compare together voice and stream segmentation algorithms, and discuss their strengths and weaknesses. © Nicolas Guiomard-Kagan, Mathieu Giraud, Richard Groult, Florence Levé.","","Information retrieval; Evaluation metrics; Musical score; Popular music; Segmentation algorithms; Clustering algorithms","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Rodríguez-López M.E.; Volk A.","Rodríguez-López, Marcelo E. (56448500300); Volk, Anja (30567849900)","56448500300; 30567849900","Selective acquisition techniques for enculturation-based melodic phrase segmentation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063015268&partnerID=40&md5=b32895ae4940458b4a601aa885f87f9f","Utrecht University, Netherlands","Rodríguez-López M.E., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands","Automatic melody segmentation is an important yet unsolved problem in Music Information Retrieval. Research in the field of Music Cognition suggests that previous listening experience plays a considerable role in the perception of melodic segment structure. At present automatic melody segmenters that model listening experience commonly do so using unsupervised statistical learning with ‘non-selective’ information acquisition techniques, i.e. the learners gather and store information indiscriminately into memory. In this paper we investigate techniques for ‘selective’ information acquisition, i.e. our learning model uses a goal-oriented approach to select what to store in memory. We test the usefulness of the segmentations produced using selective acquisition learning in a melody classification experiment involving melodies of different cultures. Our results show that the segments produced by our selective learner segmenters substantially improve classification accuracy when compared to segments produced by a non-selective learner segmenter, two local segmentation methods, and two naïve baselines. © Marcelo E. Rodríguez-López, Anja Volk.","","Information retrieval; Mergers and acquisitions; Classification accuracy; Goal-oriented approach; Information acquisitions; Music information retrieval; Phrase segmentations; Segmentation methods; Statistical learning; Unsolved problems; Learning systems","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Huang Y.-H.; Chen X.; Beck S.; Burn D.; Van Gool L.","Huang, Yu-Hui (57203224778); Chen, Xuanli (57210231738); Beck, Serafina (57210234829); Burn, David (37118450200); Van Gool, Luc (22735702300)","57203224778; 57210231738; 57210234829; 37118450200; 22735702300","Automatic handwritten mensural notation interpreter: From manuscript to MIDI performance","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012864133&partnerID=40&md5=7bf9364a6e8ad469c070904fa27c0509","ESAT-PSI, iMinds, KU Leuven, Germany; Department of Musicology, KU Leuven, Germany; D-ITET, ETH Zürich, Germany","Huang Y.-H., ESAT-PSI, iMinds, KU Leuven, Germany; Chen X., ESAT-PSI, iMinds, KU Leuven, Germany; Beck S., Department of Musicology, KU Leuven, Germany; Burn D., Department of Musicology, KU Leuven, Germany; Van Gool L., D-ITET, ETH Zürich, Germany","This paper presents a novel automatic recognition framework for hand-written mensural music. It takes a scanned manuscript as input and yields as output modern music scores. Compared to the previous mensural Optical Music Recognition (OMR) systems, ours shows not only promising performance in music recognition, but also works as a complete pipeline which integrates both recognition and transcription. There are three main parts in this pipeline: i) region-of-interest detection, ii) music symbol detection and classification, and iii) transcription to modern music. In addition to the output in modern notation, our system can generate a MIDI file as well. It provides an easy platform for the musicologists to analyze old manuscripts. Moreover, it renders these valuable cultural heritage resources available to non-specialists as well, as they can now access such ancient music in a better understandable form. c Yu-Hui Huang?, Xuanli Chen?, Serafina Beck†, David Burn†, and Luc Van Gool.","","Image segmentation; Information retrieval; Transcription; Automatic recognition; Cultural heritages; Music recognition; Music scores; Old manuscripts; Optical music recognition; Region of interest; Symbol detection; Pipelines","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Risk L.; Mok L.; Hankinson A.; Cumming J.","Risk, Laura (55925323600); Mok, Lillio (57209301190); Hankinson, Andrew (54970482500); Cumming, Julie (53463213000)","55925323600; 57209301190; 54970482500; 53463213000","Melodic similarity in traditional French-Canadian instrumental dance tunes","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069998541&partnerID=40&md5=daa788e61fdb22c070f22bfd63beaec0","Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","Risk L., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Mok L., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Hankinson A., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Cumming J., Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","Commercial recordings of French-Canadian instrumental dance tunes represent a varied and complex corpus of study. This was a primarily aural tradition, transmitted from performer to performer with few notated sources until the late 20th century. Practitioners routinely combined tune segments to create new tunes and personalized settings of existing tunes. This has resulted in a corpus that exhibits an extreme amount of variation, even among tunes with the same name. In addition, the same tune or tune segment may appear under several different names. Previous attempts at building systems for automated retrieval and ranking of instrumental dance tunes perform well for near-exact matching of tunes, but do not work as well in retrieving and ranking, in order of most to least similar, variants of a tune; especially those with variations as extreme as this particular corpus. In this paper we will describe a new approach capable of ranked retrieval of variant tunes, and demonstrate its effectiveness on a transcribed corpus of incipits. © Laura Risk, Lillio Mok, Andrew Hankinson, Julie Cumming.","","20th century; Building systems; Exact matching; Melodic similarity; New approaches; Ranked retrieval; Information retrieval","L. Risk; Schulich School of Music, Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; email: laura.risk@mail.mcgill.ca","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Humphrey E.J.; Bello J.P.","Humphrey, Eric J. (55060792500); Bello, Juan P. (7102889110)","55060792500; 7102889110","Four timely insights on automatic chord estimation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066087545&partnerID=40&md5=e6f0b5211ce4308bf7715b714c5d1a76","Music and Audio Research Laboratory, New York University, United States; MuseAmi, Inc., United States","Humphrey E.J., Music and Audio Research Laboratory, New York University, United States, MuseAmi, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Automatic chord estimation (ACE) is a hallmark research topic in content-based music informatics, but like many other tasks, system performance appears to be converging to yet another glass ceiling. Looking toward trends in other machine perception domains, one might conclude that complex, data-driven methods have the potential to significantly advance the state of the art. Two recent efforts did exactly this for large-vocabulary ACE, but despite arguably achieving some of the highest results to date, both approaches plateau well short of having solved the problem. Therefore, this work explores the behavior of these two high performing, systems as a means of understanding obstacles and limitations in chord estimation, arriving at four critical observations: one, music recordings that invalidate tacit assumptions about harmony and tonality result in erroneous and even misleading performance; two, standard lexicons and comparison methods struggle to reflect the natural relationships between chords; three, conventional approaches conflate the competing goals of recognition and transcription to some undefined degree; and four, the perception of chords in real music can be highly subjective, making the very notion of “ground truth” annotations tenuous. Synthesizing these observations, this paper offers possible remedies going forward, and concludes with some perspectives on the future of both ACE research and the field at large. © Eric J. Humphrey, Juan P. Bello.","","Information retrieval; Comparison methods; Conventional approach; Data-driven methods; Large vocabulary; Machine perception; Music Informatics; Music recording; State of the art; Audio recordings","E.J. Humphrey; Music and Audio Research Laboratory, New York University, United States; email: eric@museami.com","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Cho H.-S.; Lee J.-Y.; Kim H.-G.","Cho, Hye-Seung (57031020600); Lee, Jun-Yong (56278630600); Kim, Hyoung-Gook (7410124081)","57031020600; 56278630600; 7410124081","Singing voice separation from monaural music based on kernel back-fitting using beta-order spectral amplitude estimation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046986831&partnerID=40&md5=f10f64620217eb803dc6d78338cace30","Kwangwoon University, Seoul, South Korea","Cho H.-S., Kwangwoon University, Seoul, South Korea; Lee J.-Y., Kwangwoon University, Seoul, South Korea; Kim H.-G., Kwangwoon University, Seoul, South Korea","Separating the leading singing voice from the musical background from a monaural recording is a challenging task that appears naturally in several music processing applications. Recently, kernel additive modeling with generalized spatial Wiener filtering (GW) was presented for music/voice separation. In this paper, an adaptive auditory filtering based on β-order minimum mean-square error spectral amplitude estimation (bSA) is applied to the kernel additive modeling for improving the singing voice separation performance from monaural music signal. The proposed algorithm is composed of five modules: short time Fourier transform, music/voice separation based on bSA, determination of back-fitting, back-fitting, and inverse short time Fourier transform. In the proposed method, the Singular Value Decomposition (SVD)-based factorized spectral amplitude exponent β for each kernel component is adaptively calculated for effective bSA-based auditory filtering performance during kernel back-fitting. Using a back-fitting threshold, the kernel back-fitting process can automatically be iteratively performed until convergence. Experimental results show that the proposed method achieves better separation performance than GW based on kernel additive modeling. © Hye-Seung Cho, Jun-Yong Lee, Hyoung-Gook Kim.","","Additives; Audio recordings; Information retrieval; Inverse problems; Iterative methods; Mean square error; Separation; Signal receivers; Singular value decomposition; Auditory filtering; Minimum mean square errors; Music/voice separations; Processing applications; Separation performance; Short time Fourier transforms; Singing voice separations; Spectral amplitude; Computer music","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Liang C.-Y.; Su L.; Yang Y.-H.; Lin H.-M.","Liang, Che-Yuan (56797197900); Su, Li (55966919100); Yang, Yi-Hsuan (55218558400); Lin, Hsin-Ming (56493443900)","56797197900; 55966919100; 55218558400; 56493443900","Musical offset detection of pitched instruments: The case of violin","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070003057&partnerID=40&md5=eb79253f54ec57c15ee39cc8429c5759","Academia Sinica, Taiwan; University of California, San Diego, United States","Liang C.-Y., Academia Sinica, Taiwan; Su L., Academia Sinica, Taiwan; Yang Y.-H., Academia Sinica, Taiwan; Lin H.-M., University of California, San Diego, United States","Musical offset detection is an integral part of a music signal processing system that requires complete characterization of note events. However, unlike onset detection, offset detection has seldom been the subject of an in-depth study in the music information retrieval community, possibly because of the ambiguity involved in the determination of offset times in music. This paper presents a preliminary study aiming at discussing ways to annotate and to evaluate offset times for pitched non-percussive instruments. Moreover, we conduct a case study of offset detection in violin recordings by evaluating a number of energy, spectral flux, and pitch based methods using a new dataset covering 6 different violin playing techniques. The new dataset, which is going to be shared with the research community, consists of 63 violin recordings that are thoroughly annotated based on perceptual loudness and note transition. The offset detection methods, which are adapted from well-known methods for onset detection, are evaluated using an onset-aware method we propose for this task. Result shows that the accuracy of offset detection is highly dependent on the playing techniques involved. Moreover, pitch-based methods can better get rid of the soft-decaying behavior of offsets and achieve the best result among others. © Che-Yuan Liang, Li Su, Yi-Hsuan Yang, Hsin-Ming Lin.","","Information retrieval; Musical instruments; Signal processing; In-depth study; Integral part; Music information retrieval; Music signal processing; Offset detection; Onset detection; Playing techniques; Research communities; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lee J.H.; Hu X.; Choi K.; Downie J.S.","Lee, Jin Ha (57190797465); Hu, Xiao (55496358400); Choi, Kahyun (56452038000); Downie, J. Stephen (7102932568)","57190797465; 55496358400; 56452038000; 7102932568","Mirex grand challenge 2014 user experience: Qualitative analysis of user feedback","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059545495&partnerID=40&md5=e234ddd75aaa1881354907f5e549dcdf","University of Washington, United States; University of Hong Kong, Hong Kong; University of Illinois, United States","Lee J.H., University of Washington, United States; Hu X., University of Hong Kong, Hong Kong; Choi K., University of Illinois, United States; Downie J.S., University of Illinois, United States","Evaluation has always been fundamental to the Music Information Retrieval (MIR) community, as evidenced by the popularity of the Music Information Retrieval Evaluation eXchange (MIREX). However, prior MIREX tasks have primarily focused on testing specialized MIR algorithms that sit on the back end of systems. Not until the Grand Challenge 2014 User Experience (GC14UX) task had the users’ overall interaction and experience with complete systems been formally evaluated. Three systems were evaluated based on five criteria. This paper reports the results of GC14UX, with a special focus on the qualitative analysis of 99 free text responses collected from evaluators. The analysis revealed additional user opinions, not fully captured by score ratings on the given criteria, and demonstrated the challenge of evaluating a variety of systems with different user goals. We conclude with a discussion on the implications of findings and recommendations for future UX evaluation tasks, including adding new criteria: Aesthetics, Performance, and Utility. © Jin Ha Lee, Xiao Hu, Kahyun Choi, J. Stephen Downie.","","Information retrieval; Complete system; Free texts; Grand Challenge; Music information retrieval; Qualitative analysis; User experience; User feedback; User goals; Insecticides","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Schreiber H.","Schreiber, Hendrik (55586286200)","55586286200","Improving genre annotations for the million song dataset","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021704&partnerID=40&md5=0ee4870f78349133cd89b7e462258bf3","Tagtraum Industries Incorporated, United States","Schreiber H., Tagtraum Industries Incorporated, United States","Any automatic music genre recognition (MGR) system must show its value in tests against a ground truth dataset. Recently, the public dataset most often used for this purpose has been proven problematic, because of mislabeling, duplications, and its relatively small size. Another dataset, the Million Song Dataset (MSD), a collection of features and metadata for one million tracks, unfortunately does not contain readily accessible genre labels. Therefore, multiple attempts have been made to add song-level genre annotations, which are required for supervised machine learning tasks. Thus far, the quality of these annotations has not been evaluated. In this paper we present a method for creating additional genre annotations for the MSD from databases, which contain multiple, crowd-sourced genre labels per song (Last.fm, beaTunes). Based on label co-occurrence rates, we derive taxonomies, which allow inference of top-level genres. These are most often used in MGR systems. We then combine multiple datasets using majority voting. This both promises a more reliable ground truth and allows the evaluation of the newly generated and preexisting datasets. To facilitate further research, all derived genre annotations are publicly available on our website. © Hendrik Schreiber.","","Information retrieval; Quality control; Supervised learning; Co-occurrence; Ground truth; Ground-truth dataset; Last.fm; Multiple data sets; Music genre; Public dataset; Supervised machine learning; Statistical tests","H. Schreiber; Tagtraum Industries Incorporated, United States; email: hs@tagtraum.com","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Tsai T.J.; Prätzlich T.; Müller M.","Tsai, T.J. (55752421100); Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873)","55752421100; 55582692100; 7404689873","Known-artist live song ID: A hashprint approach","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021309426&partnerID=40&md5=9aed1942b20bd5e739661c64b6542971","University of California Berkeley, Berkeley, CA, United States; International Audio Laboratories Erlangen, Erlangen, Germany","Tsai T.J., University of California Berkeley, Berkeley, CA, United States; Prätzlich T., International Audio Laboratories Erlangen, Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Erlangen, Germany","The goal of live song identification is to recognize a song based on a short, noisy cell phone recording of a live performance. We propose a system for known-artist live song identification and provide empirical evidence of its feasibility. The proposed system represents audio as a sequence of hashprints, which are binary fingerprints that are derived from applying a set of spectro-temporal filters to a spectrogram representation. The spectro-temporal filters can be learned in an unsupervised manner on a small amount of data, and can thus tailor its representation to each artist. Matching is performed using a cross-correlation approach with downsampling and rescoring. We evaluate our approach on the Gracenote live song identification benchmark data set, and compare our results to five other baseline systems. Compared to the previous state-of-the-art, the proposed system improves the mean reciprocal rank from .68 to .79, while simultaneously reducing the average runtime per query from 10 seconds down to 0.9 seconds. © TJ Tsai, Thomas Prätzlich, Meinard Müller.","","Baseline systems; Benchmark data; Binary fingerprints; Cell phone recordings; Cross correlations; Mean reciprocal ranks; State of the art; Temporal filters; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Dutta S.; Krishnaraj Sekhar P.V.; Murthy H.A.","Dutta, Shrey (55002622000); Krishnaraj Sekhar, P.V. (57210234394); Murthy, Hema A. (57200197348)","55002622000; 57210234394; 57200197348","Raga verification in carnatic music using longest common segment set","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039958134&partnerID=40&md5=84565ec51d738d5019204103167fcc06","Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","Dutta S., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Krishnaraj Sekhar P.V., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","There are at least 100 rāgas that are regularly performed in Carnatic music concerts. The audience determines the identity of rāgas within a few seconds of listening to an item. Most of the audience consists of people who are only avid listeners and not performers. In this paper, an attempt is made to mimic the listener. A rāga verification framework is therefore suggested. The rāga verification system assumes that a specific rāga is claimed based on similarity of movements and motivic patterns. The system then checks whether this claimed rāga is correct. For every rāga, a set of cohorts are chosen. A rāga and its cohorts are represented using pallavi lines of compositions. A novel approach for matching, called Longest Common Segment Set (LCSS), is introduced. The LCSS scores for a rāga are then normalized with respect to its cohorts in two different ways. The resulting systems and a baseline system are compared for two partitionings of a dataset. A dataset of 30 rāgas from Charsur Foundation 1 is used for analysis. An equal error rate (EER) of 12% is obtained. © Shrey Dutta, Krishnaraj Sekhar PV, Hema A. Murthy.","","Baseline systems; Equal error rate; Verification framework; Verification systems; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Szeto W.M.; Wong K.H.","Szeto, Wai Man (14521584800); Wong, Kin Hong (7404758551)","14521584800; 7404758551","A hierarchical Bayesian framework for score-informed source separation of piano music signals","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023761083&partnerID=40&md5=70991459d3809c447b6090472b1fc2f7","Office of University General Education, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","Szeto W.M., Office of University General Education, Chinese University of Hong Kong, Hong Kong; Wong K.H., Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong","Here we propose a score-informed monaural source separation system to extract every tone from a mixture of piano tone signals. Two sinusoidal models in our earlier work are employed in the above-mentioned system to represent piano tones: the General Model and the Piano Model. The General Model, a variant of sinusoidal modeling, can represent a single tone with high modeling quality, yet it fails to separate mixtures of tones due to the overlapping partials. The Piano Model, on the other hand, is an instrument-specific model tailored for piano. Its modeling quality is lower but it can learn from training data (consisting entirely of isolated tones), resolve the overlapping partials and thus separate the mixtures. We formulate a new hierarchical Bayesian framework to run both Models in the source separation process so that the mixtures with overlapping partials can be separated with high quality. The results show that our proposed system gives robust and accurate separation of piano tone signal mixtures (including octaves) while achieving significantly better quality than those reported in related work done previously. © Wai Man SZETO, Kin Hong WONG.","","Information retrieval; Mixtures; Musical instruments; Separation; Hierarchical bayesian; Model qualities; Monaural source; Related works; Score-informed source separations; Separation process; Sinusoidal model; Training data; Source separation","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Knees P.; Faraldo Á.; Herrera P.; Vogl R.; Böck S.; Hörschläger F.; Le Goff M.","Knees, Peter (8219023200); Faraldo, Ángel (57188729561); Herrera, Perfecto (24824250300); Vogl, Richard (57190968947); Böck, Sebastian (55413719000); Hörschläger, Florian (56534222200); Le Goff, Mickael (57210235455)","8219023200; 57188729561; 24824250300; 57190968947; 55413719000; 56534222200; 57210235455","Two data sets for tempo estimation and key detection in electronic dance music annotated from user corrections","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023766135&partnerID=40&md5=60f22a23a7f178f17bd30a35788fb3e7","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Native Instruments GmbH, Berlin, Germany","Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Faraldo Á., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Vogl R., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Hörschläger F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Le Goff M., Native Instruments GmbH, Berlin, Germany","We present two new data sets for automatic evaluation of tempo estimation and key detection algorithms. In contrast to existing collections, both released data sets focus on electronic dance music (EDM). The data sets have been automatically created from user feedback and annotations extracted from web sources. More precisely, we utilize user corrections submitted to an online forum to report wrong tempo and key annotations on the Beatport website. Beatport is a digital record store targeted at DJs and focusing on EDM genres. For all annotated tracks in the data sets, samples of at least one-minute-length can be freely downloaded. For key detection, further ground truth is extracted from expert annotations manually assigned to Beatport tracks for benchmarking purposes. The set for tempo estimation comprises 664 tracks and the set for key detection 604 tracks. We detail the creation process of both data sets and perform extensive benchmarks using state-of-the-art algorithms from both academic research and commercial products. © Peter Knees, Ángel Faraldo, Perfecto Herrera, Richard Vogl, Sebastian Böck, Florian Hörschläger, Mickael Le Goff.","","Information retrieval; Websites; Academic research; Automatic evaluation; Commercial products; Creation process; Detection algorithm; Expert annotations; State-of-the-art algorithms; Tempo estimations; Electronic musical instruments","P. Knees; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: peter.knees@jku.at","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lee J.H.; Kim Y.-S.; Hubbles C.","Lee, Jin Ha (57190797465); Kim, Yea-Seul (56157789900); Hubbles, Chris (57193931567)","57190797465; 56157789900; 57193931567","A look at the cloud from both sides now: An analysis of cloud music service usage","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021801859&partnerID=40&md5=b1a1f26b5c616fb0f77c92e727a04a1b","University of Washington, United States","Lee J.H., University of Washington, United States; Kim Y.-S., University of Washington, United States; Hubbles C., University of Washington, United States","Despite the increasing popularity of cloud-based music services, few studies have examined how users select and utilize these services, how they manage and access their music collections in the cloud, and the issues or challenges they are facing within these services. In this paper, we present findings from an online survey with 198 responses collected from users of commercial cloud music services, exploring their selection criteria, use patterns, perceived limitations, and future predictions. We also investigate differences in these aspects by age and gender. Our results elucidate previously under-studied changes in music consumption, music listening behaviors, and music technology adoption. The findings also provide insights into how to improve the future design of cloud-based music services, and have broader implications for any cloud-based services designed for managing and accessing personal media collections. © Jin Ha Lee, Yea-Seul Kim, Chris Hubbles.","","Future designs; Future predictions; Music collection; Music technologies; Online surveys; Personal media; Selection criteria; Service usage; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Inskip C.; Wiering F.","Inskip, Charles (23570463400); Wiering, Frans (8976178100)","23570463400; 8976178100","In their own words: Using text analysis to identify musicologists' attitudes towards technology","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041402342&partnerID=40&md5=b6c222d88d9ea0b56f41321c95808516","Department of Information Studies, University College London, United Kingdom; Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands","Inskip C., Department of Information Studies, University College London, United Kingdom; Wiering F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands","A widely distributed online survey gathered quantitative and qualitative data relating to the use of technology in the research practices of musicologists. This survey builds on existing work in the digital humanities and provides insights into the specific nature of musicology in relation to use and perceptions of technology. Analysis of the data (n=621) notes the preferences in resource format and the digital skills of the survey participants. The themes of comments on rewards, benefits, frustrations, risks, and limitations are explored using an h-point approach derived from applied linguistics. It is suggested that the research practices of musicologists reflect wider existing research into the digital humanities, and that efforts should be made into supporting development of their digital skills and providing usable, useful and reliable software created with a ‘musicology-centred’ design approach. This software should support online access to high quality digital resources (image, text, sound) which are comprehensive and discoverable, and can be shared, reused and manipulated at a micro- and macro level. © Charles Inskip, Frans Wiering.","","Information retrieval; Linguistics; Design approaches; Digital humanities; Digital resources; Micro and macro levels; On-line access; Online surveys; Qualitative data; Specific nature; Surveys","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Vigliensoni G.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); Fujinaga, Ichiro (9038140900)","55217696900; 9038140900","Automatic music recommendation systems: Do demographic, profiling, and contextual features improve their performance?","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026763411&partnerID=40&md5=31518a6a4ee3ed9fa93a8a84d46a329c","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Traditional automatic music recommendation systems’ performance typically rely on the accuracy of statistical models learned from past preferences of users on music items. However, additional sources of data such as demographic attributes of listeners, their listening behaviour, and their listening contexts encode information about listeners, and their listening habits, that may be used to improve the accuracy of music recommendation models. In this paper we introduce a large dataset of music listening histories with listeners’ demographic information, and a set of features to characterize aspects of people’s listening behaviour. The longevity of the collected listening histories, covering over two years, allows the retrieval of basic forms of listening context. We use this dataset in the evaluation of accuracy of a music artist recommendation model learned from past preferences of listeners on music items and their interaction with several combinations of people’s demographic, profiling, and contextual features. Our results indicate that using listeners’ self-declared age, country, and gender improve the recommendation accuracy by 8 percent. When a new profiling feature termed exploratoryness was added, the accuracy of the model increased by 12 percent. © Gabriel Vigliensoni and Ichiro Fujinaga.","","Large dataset; Population statistics; Contextual feature; Demographic information; Listening history; Music recommendation; Music Recommendation System; Recommendation accuracy; Recommender systems","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Vieira F.; Andrade N.","Vieira, Felipe (57210190038); Andrade, Nazareno (7003429497)","57210190038; 7003429497","Evaluating conflict management mechanisms for online social jukeboxes","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069986457&partnerID=40&md5=cefb6e298bf9e979071a09f228b2f9f4","Department of Systems and Computing, Universidade Federal de Campina, Grande, Brazil","Vieira F., Department of Systems and Computing, Universidade Federal de Campina, Grande, Brazil; Andrade N., Department of Systems and Computing, Universidade Federal de Campina, Grande, Brazil","Social music listening is a prevalent and often fruitful experience. Social jukeboxes are systems that enable social music listening with listeners collaboratively choosing the music to be played. Naturally, because music tastes are diverse, using social jukeboxes often involves conflicting interests. Because of that, virtually all social jukeboxes incorporate conflict management mechanisms. In contrast with their widespread use, however, little attention has been given to evaluating how different conflict management mechanisms function to preserve the positive experience of music listeners. This paper presents an experiment with three conflict management mechanisms and three groups of listeners. The mechanisms were chosen to represent those most commonly used in the state of the practice. Our study employs a mixed-methods approach to quantitatively analyze listeners’ satisfaction and to examine their impressions and views on conflict, conflict management mechanisms, and social jukeboxing. © Felipe Vieira, Nazareno Andrade.","","Conflict management; Mixed method; Positive experiences; State of the practice; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Barbancho I.; Tardón L.J.; Barbancho A.M.; Sbert M.","Barbancho, Isabel (6602638932); Tardón, Lorenzo J. (6602405058); Barbancho, Ana M. (6602362530); Sbert, Mateu (8427067100)","6602638932; 6602405058; 6602362530; 8427067100","Benford’s law for music analysis","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070001844&partnerID=40&md5=9bfc6bd94e11e5e611a916197e59b73e","Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; University of Girona, Institute of Informatics and Applications, Campus Montilivi, Girona, 17003, Spain","Barbancho I., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Campus Teatinos, Málaga, 29071, Spain; Sbert M., University of Girona, Institute of Informatics and Applications, Campus Montilivi, Girona, 17003, Spain","Benford’s law defines a peculiar distribution of the leading digits of a set of numbers. The behavior is logarithmic, with the leading digit 1 reflecting largest probability of occurrence and the remaining ones showing decreasing probabilities of appearance following a logarithmic trend. Many discussions have been carried out about the application of Benford’s law to many different fields. In this paper, a novel exploitation of Benford’s law for the analysis of audio signals is proposed. Three new audio features based on the evaluation of the degree of agreement of a certain audio dataset to Benford’s law are presented. These new proposed features are succesfully tested in two concrete audio tasks: the detection of artificially assembled chords and the estimation of the quality of the MIDI conversions. © Isabel Barbancho, Lorenzo J. Tardón, Ana M. Barbancho, Mateu Sbert.","","Information retrieval; Audio features; Audio signal; Music analysis; Probability of occurrence; Signal analysis","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Krebs F.; Böck S.; Dorfer M.; Widmer G.","Krebs, Florian (7006192702); Böck, Sebastian (55413719000); Dorfer, Matthias (55516844500); Widmer, Gerhard (7004342843)","7006192702; 55413719000; 55516844500; 7004342843","Downbeat tracking using beat-synchronous features and recurrent neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013992385&partnerID=40&md5=fe179b9b7d4b5226703edcc9835b8ce8","Department of Computational, Perception Johannes Kepler University, Linz, Austria","Krebs F., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Böck S., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Dorfer M., Department of Computational, Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational, Perception Johannes Kepler University, Linz, Austria","In this paper, we propose a system that extracts the downbeat times from a beat-synchronous audio feature stream of a music piece. Two recurrent neural networks are used as a front-end: the first one models rhythmic content on multiple frequency bands, while the second one models the harmonic content of the signal. The output activations are then combined and fed into a dynamic Bayesian network which acts as a rhythmical language model. We show on seven commonly used datasets of Western music that the system is able to achieve state-of-the-art results. © Florian Krebs, Sebastian Böck, Matthias Dorfer, Gerhard Widmer.","","Audio acoustics; Bayesian networks; Information retrieval; Audio features; Dynamic Bayesian networks; Front end; Harmonic contents; Language model; Multiple frequency; State of the art; Recurrent neural networks","F. Krebs; Department of Computational, Perception Johannes Kepler University, Linz, Austria; email: Florian.Krebs@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Stober S.; Sternin A.; Owen A.M.; Grahn J.A.","Stober, Sebastian (14027561800); Sternin, Avital (56655625000); Owen, Adrian M. (7202052668); Grahn, Jessica A. (13407386100)","14027561800; 56655625000; 7202052668; 13407386100","Towards music imagery information retrieval: Introducing the OpenMIIR dataset of EEG recordings from music perception and imagination","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023777656&partnerID=40&md5=9aaf552fc70b525a2ecfd693d1155cb6","Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Stober S., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Sternin A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Owen A.M., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Grahn J.A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Music imagery information retrieval (MIIR) systems may one day be able to recognize a song from only our thoughts. As a step towards such technology, we are presenting a public domain dataset of electroencephalography (EEG) recordings taken during music perception and imagination. We acquired this data during an ongoing study that so far comprises 10 subjects listening to and imagining 12 short music fragments – each 7–16s long – taken from well-known pieces. These stimuli were selected from different genres and systematically vary along musical dimensions such as meter, tempo and the presence of lyrics. This way, various retrieval scenarios can be addressed and the success of classifying based on specific dimensions can be tested. The dataset is aimed to enable music information retrieval researchers interested in these new MIIR challenges to easily test and adapt their existing approaches for music analysis like fingerprinting, beat tracking, or tempo estimation on EEG data. © Sebastian Stober, Avital Sternin, Adrian M. Owen and Jessica A. Grahn.","","Audio recordings; Electroencephalography; Electrophysiology; Information retrieval; Statistical tests; Beat tracking; Eeg datum; EEG recording; Music analysis; Music information retrieval; Music perception; Public domains; Tempo estimations; Search engines","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Böck S.; Krebs F.; Widmer G.","Böck, Sebastian (55413719000); Krebs, Florian (7006192702); Widmer, Gerhard (7004342843)","55413719000; 7006192702; 7004342843","Joint beat and downbeat tracking with recurrent neural networks","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","77","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013934982&partnerID=40&md5=dd15d9a9efd525c318f3371962379c4c","Department of Computational Perception Johannes Kepler University, Linz, Austria","Böck S., Department of Computational Perception Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception Johannes Kepler University, Linz, Austria","In this paper we present a novel method for jointly extracting beats and downbeats from audio signals. A recurrent neural network operating directly on magnitude spectrograms is used to model the metrical structure of the audio signals at multiple levels and provides an output feature that clearly distinguishes between beats and downbeats. A dynamic Bayesian network is then used to model bars of variable length and align the predicted beat and downbeat positions to the global best solution. We find that the proposed model achieves state-of-the-art performance on a wide range of different musical genres and styles. © Sebastian Böck, Florian Krebs, and Gerhard Widmer.","","Bayesian networks; Information retrieval; Audio signal; Dynamic Bayesian networks; Multiple levels; Musical genre; Spectrograms; State-of-the-art performance; Variable length; Recurrent neural networks","S. Böck; Department of Computational Perception Johannes Kepler University, Linz, Austria; email: sebastian.boeck@jku.at","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Yoshii K.; Itoyama K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Itoyama, Katsutoshi (18042499100); Goto, Masataka (7403505330)","7103400120; 18042499100; 7403505330","Infinite superimposed discrete all-pole modeling for multipitch analysis of wavelet spectrograms","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045460223&partnerID=40&md5=8db5269edd46c1ae72b4a72b664b402c","Graduate School of Informatics, Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a statistical multipich analyzer based on a source-filter model that decomposes a target music audio signal in terms of three major kinds of sound quantities: pitch (fundamental frequency: F0), timbre (spectral envelope), and intensity (amplitude). If the spectral envelope of an isolated sound is represented by an all-pole filter, linear predictive coding (LPC) can be used for filter estimation in the linear-frequency domain. The main problem of LPC is that although only the amplitudes of harmonic partials are reliable samples drawn from the spectral envelope, the whole spectrum is used for filter estimation. To solve this problem, we propose an infinite superimposed discrete all-pole (iSDAP) model that, given a music signal, can estimate an appropriate number of superimposed harmonic structures whose harmonic partials are drawn from a limited number of spectral envelopes. Our nonparametric Bayesian source-filter model is formulated in the log-frequency domain that better suits the frequency characteristics of human audition. Experimental results showed that the proposed model outperformed the counterpart model formulated in the linear frequency domain. © Kazuyoshi Yoshii, Katsutoshi Itoyama, Masataka Goto.","","Audio acoustics; Frequency domain analysis; Frequency estimation; Harmonic analysis; Information retrieval; Poles; Filter estimation; Frequency characteristic; Fundamental frequencies; Harmonic structures; Linear Predictive Coding; Non-parametric Bayesian; Source-filter models; Spectral envelopes; Wavelet analysis","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Ganguli K.K.; Rastogi A.; Pandit V.; Kantan P.; Rao P.","Ganguli, Kaustuv Kanti (56094514100); Rastogi, Abhinav (57193220666); Pandit, Vedhas (57191922969); Kantan, Prithvi (57210232372); Rao, Preeti (35180193500)","56094514100; 57193220666; 57191922969; 57210232372; 35180193500","Efficient melodic query based audio search for Hindustani vocal compositions","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039962722&partnerID=40&md5=134b1ff59448827eda7e64875086fbcc","Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Electrical Engineering, Stanford University, India","Ganguli K.K., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Rastogi A., Electrical Engineering, Stanford University, India; Pandit V., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Kantan P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, India","Time-series pattern matching methods that incorporate time warping have recently been used with varying degrees of success on tasks of search and discovery of melodic phrases from audio for Indian classical vocal music. While these methods perform effectively due to the minimal assumptions they place on the nature of the sampled pitch temporal trajectories, their practical applicability to retrieval tasks on real-world databases is seriously limited by their prohibitively large computational complexity. While dimensionality reduction of the time-series to discrete symbol strings is a standard approach that can exploit computational gains from the data compression as well as the availability of efficient string matching algorithms, the compressed representation of the pitch time series itself is not well understood given the pervasiveness of pitch inflections in the melodic shape of the raga phrases. We propose methods that are informed by domain knowledge to design the representation and to optimize parameter settings for the subsequent string matching algorithm. The methods are evaluated in the context of an audio query based search for Hindustani vocal compositions in audio recordings via the mukhda (refrain of the song). We present results that demonstrate performance close to that achieved by time-series matching but at orders of magnitude reduction in complexity. © Kaustuv Kanti Ganguli, Abhinav Rastogi, Vedhas Pandit, Prithvi Kantan, Preeti Rao.","","Audio recordings; Computational efficiency; Information retrieval; Pattern matching; Time series; Computational gains; Dimensionality reduction; Domain knowledge; Orders of magnitude; Parameter setting; Real-world database; Temporal trajectories; Time series patterns; Audio acoustics","K.K. Ganguli; Department of Electrical Engineering, Indian Institute of Technology Bombay, India; email: kaustuvkanti@ee.iitb.ac.in","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lykartsis A.; Wu C.-W.; Lerch A.","Lykartsis, Athanasios (57140810900); Wu, Chih-Wei (57188865070); Lerch, Alexander (22034963000)","57140810900; 57188865070; 22034963000","Beat histogram features from NMF-based novelty functions for music classification","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059461998&partnerID=40&md5=01ddce35b01dbd64e2388e6fc62a6f0d","Technische Universität Berlin, Audio Communication Group, Germany; Georgia Institute of Technology, Center for Music Technology, United States","Lykartsis A., Technische Universität Berlin, Audio Communication Group, Germany; Wu C.-W., Georgia Institute of Technology, Center for Music Technology, United States; Lerch A., Georgia Institute of Technology, Center for Music Technology, United States","In this paper we present novel rhythm features derived from drum tracks extracted from polyphonic music and evaluate them in a genre classification task. Musical excerpts are analyzed using an optimized, partially fixed Non-Negative Matrix Factorization (NMF) method and beat histogram features are calculated on basis of the resulting activation functions for each one out of three drum tracks extracted (Hi-Hat, Snare Drum and Bass Drum). The features are evaluated on two widely used genre datasets (GTZAN and Ballroom) using standard classification methods, concerning the achieved overall classification accuracy. Furthermore, their suitability in distinguishing between rhythmically similar genres and the performance of the features resulting from individual activation functions is discussed. Results show that the presented NMF-based beat histogram features can provide comparable performance to other classification systems, while considering strictly drum patterns. © Athanasios Lykartsis, Chih-Wei Wu, Alexander Lerch.","","Chemical activation; Factorization; Graphic methods; Information retrieval; Matrix algebra; Activation functions; Classification accuracy; Classification methods; Classification system; Genre classification; Histogram features; Music classification; Nonnegative matrix factorization; Classification (of information)","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Allik A.; Fazekas G.; Sandler M.","Allik, Alo (55546600300); Fazekas, György (37107520200); Sandler, Mark (7202740804)","55546600300; 37107520200; 7202740804","An ontology for audio features","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000779715&partnerID=40&md5=ef0f8d54b40ede353832243f63b14312","Queen Mary University of London, United Kingdom","Allik A., Queen Mary University of London, United Kingdom; Fazekas G., Queen Mary University of London, United Kingdom; Sandler M., Queen Mary University of London, United Kingdom","A plurality of audio feature extraction toolsets and feature datasets are used by the MIR community. Their different conceptual organisation of features and output formats however present difficulties in exchanging or comparing data, while very limited means are provided to link features with content and provenance. These issues are hindering research reproducibility and the use of multiple tools in combination. We propose novel Semantic Web ontologies (1) to provide a common structure for feature data formats and (2) to represent computational workflows of audio features facilitating their comparison. The Audio Feature Ontology provides a descriptive framework for expressing different conceptualisations of and designing linked data formats for content-based audio features. To accommodate different views in organising features, the ontology does not impose a strict hierarchical structure, leaving this open to task and tool specific ontologies that derive from a common vocabulary. The ontologies are based on the analysis of existing feature extraction tools and the MIR literature, which was instrumental in guiding the design process. They are harmonised into a library of modular interlinked ontologies that describe the different entities and activities involved in music creation, production and consumption. © Alo Allik, György Fazekas, Mark Sandler.","","Extraction; Feature extraction; Information retrieval; Audio feature extraction; Common structures; Computational workflows; Hierarchical structures; Music creation; Production and consumption; Reproducibilities; Semantic Web ontologies; Ontology","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Osmalskyj J.; Van Droogenbroeck M.; Embrechts J.-J.","Osmalskyj, Julien (55967414200); Van Droogenbroeck, Marc (6603144384); Embrechts, Jean-Jaques (6603811516)","55967414200; 6603144384; 6603811516","Enhancing cover song identification with hierarchical rank aggregation","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026665573&partnerID=40&md5=ddbfa346945bb1e28252fc1676f35ccd","INTELSIG Laboratory, University of Liège, Belgium","Osmalskyj J., INTELSIG Laboratory, University of Liège, Belgium; Van Droogenbroeck M., INTELSIG Laboratory, University of Liège, Belgium; Embrechts J.-J., INTELSIG Laboratory, University of Liège, Belgium","Cover song identification involves calculating pairwise similarities between a query audio track and a database of reference tracks. While most authors make exclusively use of chroma features, recent work tends to demonstrate that combining similarity estimators based on multiple audio features increases the performance. We improve this approach by using a hierarchical rank aggregation method for combining estimators based on different features. More precisely, we first aggregate estimators based on global features such as the tempo, the duration, the overall loudness, the number of beats, and the average chroma vector. Then, we aggregate the resulting composite estimator with four popular state-of-the-art methods based on chromas as well as timbre sequences. We further introduce a refinement step for the rank aggregation called “local Kemenization” and quantify its benefit for cover song identification. The performance of our method is evaluated on the Second Hand Song dataset. Our experiments show a significant improvement of the performance, up to an increase of more than 200 % of the number of queries identified in the Top-1, compared to previous results. © Julien Osmalskyj, Marc Van Droogenbroeck, Jean-Jaques Embrechts.","","Aggregates; Query processing; Audio features; Chroma features; Cover song identifications; Global feature; Rank aggregation; Reference tracks; Refinement step; State-of-the-art methods; Information retrieval","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Jiang Y.; Raphael C.","Jiang, Yucong (57210231595); Raphael, Christopher (7004214964)","57210231595; 7004214964","Instrument identification in optical music recognition","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069994755&partnerID=40&md5=839d8eb818df2f2432c6204cbf4688ad","School of Informatics and Computing, Indiana University, Bloomington, United States","Jiang Y., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","We present a method for recognizing and interpreting the text labels for the instruments in an orchestra score, thereby associating staves with instruments. This task is one of many necessary in optical music recognition. Our approach treats the score system as the basic unit of processing. A graph structure describes the possible orderings of instruments in the system. Each instrument may apply to several staves, may be represented with several possible text strings, and may appear at several possible positions relative to the staves. We find the optimal labeling of staves using a globally optimal dynamic programming approach that embeds simple template-based optical character reconition within the overall recognition scheme. When given an entire score, we simultaneously optimize on the text labeling for each system, as well as the character template models, thus adapting to the font at hand. Our implementation alternately optimizes over the text label identification and re-estimates the character templates. Experiments are presented on 10 different scores showing a significant improvement due to adaptation. © Yucong Jiang, Christopher Raphael.","","Dynamic programming; Information retrieval; Basic units; Character template; Graph structures; Instrument identification; Optical music recognition; Optimal labeling; Template-based; Text string; Character recognition","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Stober S.; Prätzlich T.; Müller M.","Stober, Sebastian (14027561800); Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873)","14027561800; 55582692100; 7404689873","Brain beats: Tempo extraction from EEG data","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026847895&partnerID=40&md5=24169d84dca09a2626f634b3a7f15f53","Research Focus Cognititive Sciences, University of Potsdam, Germany; International Audio Laboratories, Erlangen, Germany","Stober S., Research Focus Cognititive Sciences, University of Potsdam, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","This paper addresses the question how music information retrieval techniques originally developed to process audio recordings can be adapted for the analysis of corresponding brain activity data. In particular, we conducted a case study applying beat tracking techniques to extract the tempo from electroencephalography (EEG) recordings obtained from people listening to music stimuli. We point out similarities and differences in processing audio and EEG data and show to which extent the tempo can be successfully extracted from EEG signals. Furthermore, we demonstrate how the tempo extraction from EEG signals can be stabilized by applying different fusion approaches on the mid-level tempogram features. © Sebastian Stober, Thomas Prätzlich, Meinard Müller.","","Audio acoustics; Audio recordings; Brain; Electroencephalography; Electrophysiology; Extraction; Information retrieval; Beat tracking; Brain activity; Eeg datum; EEG signals; Music information retrieval; Data mining","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Bansal J.; Woolhouse M.","Bansal, Jotthi (57194707711); Woolhouse, Matthew (36024506900)","57194707711; 36024506900","Predictive power of personality on music-genre exclusivity","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016312813&partnerID=40&md5=4db0668d76a32ccceedc017495fd1a27","McMaster University, Canada","Bansal J., McMaster University, Canada; Woolhouse M., McMaster University, Canada","Studies reveal a strong relationship between personality and preferred musical genre. Our study explored this relationship using a new methodology: genre dispersion among people’s mobile-phone music collections. By analyzing the download behaviours of genre-defined user subgroups, we investigated the following questions: (1) do genre-preferring subgroups show distinct patterns of genre consumption and genre exclusivity; (2) does genre exclusivity relate to Big Five personality factors? We hypothesized that genre-preferring subgroups would vary in genre exclusivity, and that their degree of exclusivity would be linearly associated with the openness personality factor (if people have open personalities, they should be “open” to different musical styles). Consistent with our hypothesis, results showed that greater genre inclusivity, i.e. many genres in people’s music collections, positively associated with openness and (unexpectedly) agreeableness, suggesting that individuals with high openness and agreeableness have wider musical tastes than those with low openness and agreeableness. Our study corroborated previous research linking genre preference and personality, and revealed, in a novel way, the predictive power of personality on music-consumption. © Jotthi Bansal, Matthew Woolhouse.","","Audio recordings; Information retrieval; Big five; Music collection; Music genre; Musical genre; Predictive power; Behavioral research","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Cheng T.; Mauch M.; Benetos E.; Dixon S.","Cheng, Tian (56939676900); Mauch, Matthias (36461512900); Benetos, Emmanouil (16067946900); Dixon, Simon (7201479437)","56939676900; 36461512900; 16067946900; 7201479437","An attack/decay model for piano transcription","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017522507&partnerID=40&md5=fa1298a2ae508c51b4df50fbb4e805e6","Centre for Digital Music, Queen Mary University of London, United Kingdom","Cheng T., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We demonstrate that piano transcription performance for a known piano can be improved by explicitly modelling piano acoustical features. The proposed method is based on non-negative matrix factorisation, with the following three refinements: (1) introduction of attack and harmonic decay components; (2) use of a spike-shaped note activation that is shared by these components; (3) modelling the harmonic decay with an exponential function. Transcription is performed in a supervised way, with the training and test datasets produced by the same piano. First we train parameters for the attack and decay components on isolated notes, then update only the note activations for transcription. Experiments show that the proposed model achieves 82% on note-wise and 79% on frame-wise F-measures on the ‘ENSTDkCl’ subset of the MAPS database, outperforming the current published state of the art. © Tian Cheng, Matthias Mauch, Emmanouil Benetos and Simon Dixon.","","Chemical activation; Exponential functions; Information retrieval; Transcription; Decay components; F measure; Non-negative matrix factorisation; State of the art; Musical instruments","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Fu M.; Xia G.; Dannenberg R.; Wasserman L.","Fu, Mutian (57210232959); Xia, Guangyu (55586682600); Dannenberg, Roger (7003266250); Wasserman, Larry (57191231290)","57210232959; 55586682600; 7003266250; 57191231290","A statistical view on the expressive timing of piano rolled chords","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070021284&partnerID=40&md5=28edff1c3bd6eca05be5869cb5658196","School of Music, Carnegie Mellon University, United States; School of Computer Science, Carnegie Mellon University, United States","Fu M., School of Music, Carnegie Mellon University, United States; Xia G., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, United States; Wasserman L., School of Computer Science, Carnegie Mellon University, United States","Rolled or arpeggiated chords are notated chords performed by playing the notes sequentially, usually from lowest to highest in pitch. Arpeggiation is a characteristic of musical expression, or expressive timing, in piano performance. However, very few studies have investigated rolled chord performance. In this paper, we investigate two expressive timing properties of piano rolled chords: equivalent onset and onset span. Equivalent onset refers to the hidden onset that can functionally replace the onsets of the notes in a chord; onset span refers to the time interval from the first note onset to the last note onset. We ask two research questions. First, what is the equivalent onset of a rolled chord? Second, are the onset spans of different chords interpreted in the same way? The first question is answered by local tempo estimation while the second question is answered by Analysis of Variance. Also, we contribute a piano duet dataset for rolled chords analysis and other studies on expressive music performance. The dataset contains three pieces of music, each performed multiple times by different pairs of musicians. © Mutian Fu, Guangyu Xia, Roger Dannenberg, Larry Wasserman.","","Information retrieval; Timing circuits; Expressive music performance; Musical expression; Research questions; Tempo estimations; Time interval; Timing properties; Musical instruments","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Summers C.; Popp P.","Summers, Cameron (57190489636); Popp, Phillip (36626277000)","57190489636; 36626277000","Temporal music context identification with user listening data","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063021698&partnerID=40&md5=d7ec25e1ded6f02063364a5405a4e6eb","Gracenote, India","Summers C., Gracenote, India; Popp P., Gracenote, India","The times when music is played can indicate context for listeners. From the peaceful song for waking up each morning to the traditional song for celebrating a holiday to an up-beat song for enjoying the summer, the relationship between the music and the temporal context is clearly important. For music search and recommendation systems, an understanding of these relationships provides a richer environment to discover and listen. But with the large number of tracks available in music catalogues today, manually labeling track-temporal context associations is difficult, time consuming, and costly. This paper examines track-day contexts with the purpose of identifying relationships with specific music tracks. Improvements are made to an existing method for classifying Christmas tracks and a generalization to the approach is shown that allows automated discovery of music for any day of the year. Analyzing the top 50 tracks obtained from this method for three well-known holidays, Halloween, Saint Patrick’s Day, and July 4th, precision@50 was 95%, 99%, and 73%, respectively. © Cameron Summers, Phillip Popp.","","Automated discovery; Christmas; Context identification; Recommender systems","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kuribayashi T.; Asano Y.; Yoshikawa M.","Kuribayashi, Taku (56018106800); Asano, Yasuhito (7401729031); Yoshikawa, Masatoshi (24726417800)","56018106800; 7401729031; 24726417800","Towards support for understanding classical music: Alignment of content descriptions on the web","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062743588&partnerID=40&md5=5ff82315785813346a2ddf3071172996","Graduate School of Informatics, Kyoto University, Japan","Kuribayashi T., Graduate School of Informatics, Kyoto University, Japan; Asano Y., Graduate School of Informatics, Kyoto University, Japan; Yoshikawa M., Graduate School of Informatics, Kyoto University, Japan","Supporting the understanding of classical music is an important topic that involves various research fields such as text analysis and acoustics analysis. Content descriptions are explanations of classical music compositions that help a person to understand technical aspects of the music. Recently, Kuribayashi et al. proposed a method for obtaining content descriptions from the web. However, the content descriptions on a single page frequently explain a specific part of a composition only. Therefore, a person who wants to fully understand the composition suffers from a time-consuming task, which seems almost impossible for a novice of classical music. To integrate the content descriptions obtained from multiple pages, we propose a method for aligning each pair of paragraphs of such descriptions. Using dynamic time warping-based method along with our new ideas, (a) a distribution-based distance measure named w2DD, and (b) the concept of passage expressions, it is possible to align content descriptions of classical music better than when using cutting-edge text analysis methods. Our method can be extended in future studies to create applications systems to integrate descriptions with musical scores and performances. © Taku Kuribayashi, Yasuhito Asano, Masatoshi Yoshikawa.","","Information retrieval; Classical musics; Content description; Distance measure; Dynamic time warping; Research fields; Technical aspects; Text-analysis methods; Time-consuming tasks; Text processing","T. Kuribayashi; Accenture Japan Ltd, Japan; email: choco.ms@gmail.com","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Zangerle E.; Pichl M.; Hupfauf B.; Specht G.","Zangerle, Eva (36186499400); Pichl, Martin (56453385900); Hupfauf, Benedikt (6504590472); Specht, Günther (26665771200)","36186499400; 56453385900; 6504590472; 26665771200","Can microblogs predict music charts? An analysis of the relationship between #nowplaying tweets and music charts","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016319689&partnerID=40&md5=97e132a401b9768ad97d1666ee4d6704","Department of Computer Science, University of Innsbruck, Austria","Zangerle E., Department of Computer Science, University of Innsbruck, Austria; Pichl M., Department of Computer Science, University of Innsbruck, Austria; Hupfauf B., Department of Computer Science, University of Innsbruck, Austria; Specht G., Department of Computer Science, University of Innsbruck, Austria","Twitter is one of the leading social media platforms, where hundreds of millions of tweets cover a wide range of topics, including the music a user is listening to. Such #nowplaying tweets may serve as an indicator for future charts, however, this has not been thoroughly studied yet. Therefore, we investigate to which extent such tweets correlate with the Billboard Hot 100 charts and whether they allow for music charts prediction. The analysis is based on #nowplaying tweets and the Billboard charts of the years 2014 and 2015. We analyze three different aspects in regards to the time series representing #nowplaying tweets and the Billboard charts: (i) the correlation of Twitter and the Billboard charts, (ii) the temporal relation between those two and (iii) the prediction performance in regards to charts positions of tracks. We find that while there is a mild correlation between tweets and the charts, there is a temporal lag between these two time series for 90% of all tracks. As for the predictive power of Twitter, we find that incorporating Twitter information in a multivariate model results in a significant decrease of both the mean RMSE as well as the variance of rank predictions. © Eva Zangerle, Martin Pichl, Benedikt Hupfauf, Günther Specht.","","Forecasting; Information retrieval; Social networking (online); Time series; Microblogs; Multivariate modeling; Prediction performance; Predictive power; Social media platforms; Temporal relation; Graphic methods","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Kirlin P.B.; Thomas D.L.","Kirlin, Phillip B. (55582044600); Thomas, David L. (57210233856)","55582044600; 57210233856","Extending a model of monophonic hierarchical music analysis to homophony","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070012603&partnerID=40&md5=6fb419cf600883933e4f4125160066b9","Department of Mathematics and Computer Science, Rhodes College, United States","Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States; Thomas D.L., Department of Mathematics and Computer Science, Rhodes College, United States","Computers are now powerful enough and data sets large enough to enable completely data-driven studies of Schenkerian analysis, the most well-established variety of hierarchical music analysis. In particular, we now have probabilistic models that can be trained via machine learning algorithms to analyze music in a hierarchical fashion as a music theorist would. Most of these models, however, only analyze the monophonic melodic content of the music, as opposed to taking all of the musical voices into account. In this paper, we explore the feasibility of extending a probabilistic model developed for analyzing monophonic music to function with homophonic music. We present details of the new model, an algorithm for determining the most probable analysis of the music, and a number of experiments evaluating the quality of the analyses predicted by the model. We also describe how varying the way the model interprets rests in the input music affects the resulting analyses produced. © Phillip B. Kirlin and David L. Thomas.","","Information retrieval; Learning algorithms; Machine learning; Data driven; Monophonic music; Music analysis; Probabilistic modeling; Probabilistic models; Quality control","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lee J.H.; Price R.","Lee, Jin Ha (57190797465); Price, Rachel (57188594005)","57190797465; 57188594005","Understanding users of commercial music services through personas: Design implications","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044233522&partnerID=40&md5=6ad5f56841daf23d86ff235e08557e39","University of Washington, United States","Lee J.H., University of Washington, United States; Price R., University of Washington, United States","Most of the previous literature on music users’ needs, habits, and interactions with music information retrieval (MIR) systems focuses on investigating user groups of particular demographics or testing the usability of specific interfaces/systems. In order to improve our understanding of how users’ personalities and characteristics affect their needs and interactions with MIR systems, we conducted a qualitative user study across multiple commercial music services, utilizing interviews and think-aloud sessions. Based on the empirical user data, we have developed seven personas. These personas offer a deeper understanding of the different types of MIR system users and the relative importance of various design implications for each user type. Implications for system design include a renegotiation of our understanding of desired user engagement, especially with the habit of context-switching, designing systems for specialized uses, and addressing user concerns around privacy, transparency, and control. © Jin Ha Lee, Rachel Price.","","Design; Information retrieval; Context switching; Design implications; Designing systems; Music information retrieval; Specific interface; Think aloud; User engagement; User groups; Search engines","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Liem C.C.S.; Hanjalic A.","Liem, Cynthia C.S. (21733247100); Hanjalic, Alan (6701775211)","21733247100; 6701775211","Comparative analysis of orchestral performance recordings: An image-based approach","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057958699&partnerID=40&md5=2f79861325d91508624e254dc293df00","Delft University of Technology, Multimedia Computing Group, Netherlands","Liem C.C.S., Delft University of Technology, Multimedia Computing Group, Netherlands; Hanjalic A., Delft University of Technology, Multimedia Computing Group, Netherlands","Traditionally, the computer-assisted comparison of multiple performances of the same piece focused on performances on single instruments. Due to data availability, there also has been a strong bias towards analyzing piano performances, in which local timing, dynamics and articulation are important expressive performance features. In this paper, we consider the problem of analyzing multiple performances of the same symphonic piece, performed by different orchestras and different conductors. While differences between interpretations in this genre may include commonly studied features on timing, dynamics and articulation, the timbre of the orchestra and choices of balance within the ensemble are other important aspects distinguishing different orchestral interpretations from one another. While it is hard to model these higher-level aspects as explicit audio features, they can usually be noted visually in spectrogram plots. We therefore propose a method to compare orchestra performances by examining visual spectrogram characteristics. Inspired by eigenfaces in human face recognition, we apply Principal Components Analysis on synchronized performance fragments to localize areas of cross-performance variation in time and frequency. We discuss how this information can be used to examine performer differences, and how beyond pairwise comparison, relative differences can be studied between multiple performances in a corpus at once. © Cynthia C. S. Liem, Alan Hanjalic.","","Face recognition; Information retrieval; Spectrographs; Comparative analysis; Computer assisted; Expressive performance; Human face recognition; Pair-wise comparison; Performance variations; Principal components analysis; Time and frequencies; Principal component analysis","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Gupta S.; Srinivasamurthy A.; Kumar M.; Murthy H.A.; Serra X.","Gupta, Swapnil (57215280029); Srinivasamurthy, Ajay (55583336200); Kumar, Manoj (57209800737); Murthy, Hema A. (57200197348); Serra, Xavier (55892979900)","57215280029; 55583336200; 57209800737; 57200197348; 55892979900","Discovery of syllabic percussion patterns in tabla solo recordings","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022056941&partnerID=40&md5=d5a89ec6a8d3f318b602631dc3a997c0","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; DONlab, Indian Institute of Technology Madras, Chennai, India","Gupta S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kumar M., DONlab, Indian Institute of Technology Madras, Chennai, India; Murthy H.A., DONlab, Indian Institute of Technology Madras, Chennai, India; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We address the unexplored problem of percussion pattern discovery in Indian art music. Percussion in Indian art music uses onomatopoeic oral mnemonic syllables for the transmission of repertoire and technique. This is utilized for the task of percussion pattern discovery from audio recordings. From a parallel corpus of audio and expert curated scores for 38 tabla solo recordings, we use the scores to build a set of most frequent syllabic patterns of different lengths. From this set, we manually select a subset of musically representative query patterns. To discover these query patterns in an audio recording, we use syllable-level hidden Markov models (HMM) to automatically transcribe the recording into a syllable sequence, in which we search for the query pattern instances using a Rough Longest Common Subsequence (RLCS) approach. We show that the use of RLCS makes the approach robust to errors in automatic transcription, significantly improving the pattern recall rate and F-measure. We further propose possible enhancements to improve the results. © Swapnil Gupta, Ajay Srinivasamurthy, Manoj Kumar, Hema A. Murthy, Xavier Serra.","","Hidden Markov models; Information retrieval; Musical instruments; Transcription; Automatic transcription; F measure; Longest common subsequences; Parallel corpora; Pattern discovery; Pattern recall; Query patterns; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Oramas S.; Espinosa-Anke L.; Lawlor A.; Serra X.; Saggion H.","Oramas, Sergio (55582112200); Espinosa-Anke, Luis (56242866900); Lawlor, Aonghus (36878695200); Serra, Xavier (55892979900); Saggion, Horacio (6602505471)","55582112200; 56242866900; 36878695200; 55892979900; 6602505471","Exploring customer reviews for music genre classification and evolutionary studies","2016","Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021923306&partnerID=40&md5=7897216fb862065646c7301ae3c34008","Music Technology Group, Universitat Pompeu Fabra, Spain; TALN Group, Universitat Pompeu Fabra, Spain; Insight Centre for Data Analytics, University College of Dublin, Ireland","Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Espinosa-Anke L., TALN Group, Universitat Pompeu Fabra, Spain; Lawlor A., Insight Centre for Data Analytics, University College of Dublin, Ireland; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain; Saggion H., TALN Group, Universitat Pompeu Fabra, Spain","In this paper, we explore a large multimodal dataset of about 65k albums constructed from a combination of Amazon customer reviews, MusicBrainz metadata and AcousticBrainz audio descriptors. Review texts are further enriched with named entity disambiguation along with polarity information derived from an aspect-based sentiment analysis framework. This dataset constitutes the cornerstone of two main contributions: First, we perform experiments on music genre classification, exploring a variety of feature types, including semantic, sentimental and acoustic features. These experiments show that modeling semantic information contributes to outperforming strong bag-of-words baselines. Second, we provide a diachronic study of the criticism of music genres via a quantitative analysis of the polarity associated to musical aspects over time. Our analysis hints at a potential correlation between key cultural and geopolitical events and the language and evolving sentiments found in music reviews. © Sergio Oramas, Luis Espinosa-Anke, Aonghus Lawlor, Xavier Serra, Horacio Saggion.","","Information retrieval; Large dataset; Semantics; Sentiment analysis; Acoustic features; Customer review; Feature types; Geopolitical events; Model semantics; Multi-modal dataset; Music genre classification; Named entity disambiguations; Classification (of information)","","","17th International Society for Music Information Retrieval Conference, ISMIR 2016","7 August 2016 through 11 August 2016","New York","149380"
"Silva D.F.; Souza V.M.A.; Batista G.E.A.P.A.","Silva, Diego F. (55585876200); Souza, Vinícius M.A. (55990873100); Batista, Gustavo E.A.P.A. (55062789000)","55585876200; 55990873100; 55062789000","Music shapelets for fast cover song recognition","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029375687&partnerID=40&md5=6bb0f7bd4335c050493b3d9f7e99c3a4","Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Souza V.M.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","A cover song is a new performance or recording of a previously recorded music by an artist other than the original one. The automatic identification of cover songs is useful for a wide range of tasks, from fans looking for new versions of their favorite songs to organizations involved in licensing copyrighted songs. This is a difficult task given that a cover may differ from the original song in key, timbre, tempo, structure, arrangement and even language of the vocals. Cover song identification has attracted some attention recently. However, most of the state-of-the-art approaches are based on similarity search, which involves a large number of similarity computations to retrieve potential cover versions for a query recording. In this paper, we adapt the idea of time series shapelets for content-based music retrieval. Our proposal adds a training phase that finds small excerpts of feature vectors that best describe each song. We demonstrate that we can use such small segments to identify cover songs with higher identification rates and more than one order of magnitude faster than methods that use features to describe the whole music. © Diego F. Silva, Vinícius M. A. Souza, Gustavo E. A. P. A. Batista.","","Automation; Information retrieval; Automatic identification; Content-based music retrieval; Cover song identifications; Feature vectors; Identification rates; Similarity computation; Similarity search; State-of-the-art approach; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Repetto R.C.; Gong R.; Kroher N.; Serra X.","Repetto, Rafael Caro (57200495266); Gong, Rong (57200496600); Kroher, Nadine (56407186200); Serra, Xavier (55892979900)","57200495266; 57200496600; 56407186200; 55892979900","Comparison of the singing style of two Jingju schools","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021959833&partnerID=40&md5=b987caa32c80114e47bae860180cd7bd","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gong R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kroher N., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Performing schools (liupai) in jingju (also known as Peking or Beijing opera) are one of the most important elements for the appreciation of this genre among connoisseurs. In the current paper, we study the potential of MIR techniques for supporting and enhancing musicological descriptions of the singing style of two of the most renowned jingju schools for the dan role-type, namely Mei and Cheng schools. To this aim, from the characteristics commonly used for describing singing style in musicological literature, we have selected those that can be studied using standard audio features. We have selected eight recordings from our jingju music research corpus and have applied current algorithms for the measurement of the selected features. Obtained results support the descriptions from musicological sources in all cases but one, and also add precision to them by providing specific measurements. Besides, our methodology suggests some characteristics not accounted for in our musicological sources. Finally, we discuss the need for engaging jingju experts in our future research and applying this approach for musicological and educational purposes as a way of better validating our methodology. © Rafael Caro Repetto, Rong Gong, Nadine Kroher, Xavier Serra.","","Applied current; Audio features; Singing styles; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Pugin L.; Zitellini R.; Roland P.","Pugin, Laurent (23009752900); Zitellini, Rodolfo (55825734700); Roland, Perry (55583223600)","23009752900; 55825734700; 55583223600","Verovio: A library for engraving MEI music notation into SVG","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","44","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034269070&partnerID=40&md5=7c7185988d10ba74f67d722168a0431f","Swiss RISM Office, Switzerland; University of Virginia, United States","Pugin L., Swiss RISM Office, Switzerland; Zitellini R., Swiss RISM Office, Switzerland; Roland P., University of Virginia, United States","Rendering symbolic music notation is a common component of many MIR applications, and many tools are available for this task. There is, however, a need for a tool that can natively render the Music Encoding Initiative (MEI) notation encodings that are increasingly used in music research projects. In this paper, we present Verovio, a library and toolkit for rendering MEI. A significant advantage of Verovio is that it implements MEI’s structure internally, making it the best suited solution for rendering features that make MEI unique. Verovio is designed as a fast, portable, lightweight tool written in pure standard C++ with no dependencies on third-party frameworks or libraries. It can be used as a command-line rendering tool, as a library, or it can be compiled to JavaScript using the Emscripten LLVM-to-JavaScript compiler. This last option is particularly interesting because it provides a complete in-browser music MEI typesetter. The SVG output from Verovio is organized in such a way that the MEI structure is preserved as much as possible. Since every graphic in SVG is an XML element that is easily addressable, Verovio is particularly well-suited for interactive applications, especially in web browsers. Verovio is available under the GPL open-source license. © Laurent Pugin, Rodolfo Zitellini, Perry Roland.","","C++ (programming language); Encoding (symbols); Information retrieval; Web browsers; Command line; Encodings; In browsers; Interactive applications; Javascript; Music notation; Open source license; Third parties; Rendering (computer graphics)","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Saurel P.; Rousseaux F.; Danger M.","Saurel, Pierre (24722046800); Rousseaux, Francis (12345621400); Danger, Marc (57210203700)","24722046800; 12345621400; 57210203700","On the changing regulations of privacy and personal information in MIR","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047245776&partnerID=40&md5=e99bb75f81933638fc72f112a215f6d2","Université Paris-Sorbonne, France; IRCAM, France; ADAMI, France","Saurel P., Université Paris-Sorbonne, France; Rousseaux F., IRCAM, France; Danger M., ADAMI, France","In recent years, MIR research has continued to focus more and more on user feedback, human subjects data, and other forms of personal information. Concurrently, the European Union has adopted new, stringent regulations to take effect in the coming years regarding how such information can be collected, stored and manipulated, with equally strict penalties for being found in violation of the law. Here, we provide a summary of these changes, consider how they relate to our data sources and research practices, and identify promising methodologies that may serve researchers well, both in order to be in compliance with the law and conduct more subject-friendly research. We additionally provide a case study of how such changes might affect a recent human subjects project on the topic of style, and conclude with a few recommendations for the near future. This paper is not intended to be legal advice: our personal legal interpretations are strictly mentioned for illustration purpose, and reader should seek proper legal counsel. © Pierre Saurel, Francis Rousseaux, Marc Danger.","","Data privacy; Information retrieval; Data-sources; European union; Human subjects; Legal advice; Legal counsels; Personal information; Stringent regulations; User feedback; Laws and legislation","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sutcliffe R.; Crawford T.; Fox C.; Root D.L.; Hovy E.; Lewis R.","Sutcliffe, Richard (8878266300); Crawford, Tim (15054056900); Fox, Chris (7402165636); Root, Deane L. (57210355019); Hovy, Eduard (6602910705); Lewis, Richard (55457221800)","8878266300; 15054056900; 7402165636; 57210355019; 6602910705; 55457221800","Relating natural language text to musical passages","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006247222&partnerID=40&md5=0b007103d83e2ae4c3919c3a1b54d36b","School of CSEE, University of Essex, Colchester, United Kingdom; Dept of Computing Goldsmiths, University of London, United Kingdom; Department of Music, University of Pittsburgh, Pittsburgh, PA, United States; Lang Technologies Inst, Carnegie-Mellon Univ, Pittsburgh, PA, United States","Sutcliffe R., School of CSEE, University of Essex, Colchester, United Kingdom; Crawford T., Dept of Computing Goldsmiths, University of London, United Kingdom; Fox C., School of CSEE, University of Essex, Colchester, United Kingdom; Root D.L., Department of Music, University of Pittsburgh, Pittsburgh, PA, United States; Hovy E., Lang Technologies Inst, Carnegie-Mellon Univ, Pittsburgh, PA, United States; Lewis R., Dept of Computing Goldsmiths, University of London, United Kingdom","There is a vast body of musicological literature containing detailed analyses of musical works. These texts make frequent references to musical passages in scores by means of natural language phrases. Our long-term aim is to investigate whether these phrases can be linked automatically to the musical passages to which they refer. As a first step, we have organised for two years running a shared evaluation in which participants must develop software to identify passages in a MusicXML score based on a short noun phrase in English. In this paper, we present the rationale for this work, discuss the kind of references to musical passages which can occur in actual scholarly texts, describe the first two years of the evaluation and finally appraise the results to establish what progress we have made. © R. Sutcliffe, T. Crawford, C. Fox, D.L. Root, E. Hovy and R. Lewis.","","Natural language text; Natural languages; Noun phrase; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Wang C.-I.; Hsu J.; Dubnov S.","Wang, Cheng-i (56668972700); Hsu, Jennifer (57188593547); Dubnov, Shlomo (6602142600)","56668972700; 57188593547; 6602142600","Music pattern discovery with variable Markov oracle: A unified approach to symbolic and audio representations","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005958076&partnerID=40&md5=09039250d364adf18168e8c6091a7356","Music Department, University of California, San Diego, United States","Wang C.-I., Music Department, University of California, San Diego, United States; Hsu J., Music Department, University of California, San Diego, United States; Dubnov S., Music Department, University of California, San Diego, United States","This paper presents a framework for automatically discovering patterns in a polyphonic music piece. The proposed framework is capable of handling both symbolic and audio representations. Chroma features are post-processed with heuristics stemming from musical knowledge and fed into the pattern discovery framework. The pattern-finding algorithm is based on Variable Markov Oracle. The Variable Markov Oracle data structure is capable of locating repeated suffixes within a time series, thus making it an appropriate tool for the pattern discovery task. Evaluation of the proposed framework is performed on the JKU Patterns Development Dataset with state of the art performance. © Cheng-i Wang, Jennifer Hsu and Shlomo Dubnov.","","Information retrieval; Audio representation; Chroma features; Finding algorithm; Pattern discovery; Polyphonic music; State-of-the-art performance; Unified approach; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Barbancho I.; Tzanetakis G.; Tardón L.J.; Driessen P.F.; Barbancho A.M.","Barbancho, Isabel (6602638932); Tzanetakis, George (6602262192); Tardón, Lorenzo J. (6602405058); Driessen, Peter F. (7006761168); Barbancho, Ana M. (6602362530)","6602638932; 6602262192; 6602405058; 7006761168; 6602362530","Estimation of the direction of strokes and arpeggios","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052464452&partnerID=40&md5=b56dbeeda14838cebe19d99e58378cdd","Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain; University of Victoria, Department of Computer Science, Victoria, Canada","Barbancho I., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain; Tzanetakis G., University of Victoria, Department of Computer Science, Victoria, Canada; Tardón L.J., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain; Driessen P.F., University of Victoria, Department of Computer Science, Victoria, Canada; Barbancho A.M., Universidad de Málaga, ATIC Research Group, ETSI Telecomunicación, Dpt. Ingeniería de Comunicaciones, Málaga, 29071, Spain","Whenever a chord is played in a musical instrument, the notes are not commonly played at the same time. Actually, in some instruments, it is impossible to trigger multiple notes simultaneously. In others, the player can consciously select the order of the sequence of notes to play to create a chord. In either case, the notes in the chord can be played very fast, and they can be played from the lowest to the highest pitch note (upstroke) or from the highest to the lowest pitch note (downstroke). In this paper, we describe a system to automatically estimate the direction of strokes and arpeggios from audio recordings. The proposed system is based on the analysis of the spectrogram to identify meaningful changes. In addition to the estimation of the up or down stroke direction, the proposed method provides information about the number of notes that constitute the chord, as well as the chord playing speed. The system has been tested with four different instruments: guitar, piano, autoharp and organ. © Isabel Barbancho, George Tzanetakis, Lorenzo J. Tardón, Peter F. Driessen, Ana M. Barbancho.","","Flight dynamics; Information retrieval; Musical instruments; Downstrokes; Spectrograms; Audio recordings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Chen Y.-P.; Su L.; Yang Y.-H.","Chen, Yuan-Ping (57210233661); Su, Li (55966919100); Yang, Yi-Hsuan (55218558400)","57210233661; 55966919100; 55218558400","Electric guitar playing technique detection in real-world recordings based on F0 sequence pattern recognition","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990981376&partnerID=40&md5=c3f25d281fbd2b06e052aec4d4d4f227","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Chen Y.-P., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Specifically, we treat the task as a time sequence pattern recognition problem, and develop a two-stage framework for detecting five fundamental playing techniques used by the electric guitar. Given an audio track, the first stage identifies prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classifier to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in five studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment. © Yuan-Ping Chen, Li Su, Yi-Hsuan Yang.","","Audio recordings; Information retrieval; Musical instruments; Transcription; Audio track; Electric guitar; F-score; Melody contours; Playing techniques; Real-world; Time sequences; Two-fold-cross-validation; Pattern recognition","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Bazzica A.; Liem C.C.S.; Hanjalic A.","Bazzica, Alessio (35172575500); Liem, Cynthia C.S. (21733247100); Hanjalic, Alan (6701775211)","35172575500; 21733247100; 6701775211","Exploiting instrument-wise playing/non-playing labels for score synchronization of symphonic music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023765922&partnerID=40&md5=d42901b0000e8c5ef4e60739688e0ae4","Delft University of Technology, Netherlands","Bazzica A., Delft University of Technology, Netherlands; Liem C.C.S., Delft University of Technology, Netherlands; Hanjalic A., Delft University of Technology, Netherlands","Synchronization of a score to an audio-visual music performance recording is usually done by solving an audio-to-MIDI alignment problem. In this paper, we focus on the possibility to represent both the score and the performance using information about which instrument is active at a given time stamp. More specifically, we investigate to what extent instrument-wise “playing” (P) and “non-playing” (NP) labels are informative in the synchronization process and what role the visual channel can have for the extraction of P/NP labels. After introducing the P/NP-based representation of the music piece, both at the score and performance level, we define an efficient way of computing the distance between the two representations, which serves as input for the synchronization step based on dynamic time warping. In parallel with assessing the effectiveness of the proposed representation, we also study its robustness when missing and/or erroneous labels occur. Our experimental results show that P/NP-based music piece representation is informative for performance-to-score synchronization and may benefit the existing audio-only approaches. © Alessio Bazzica, Cynthia C. S. Liem, Alan Hanjalic.","","Information retrieval; Synchronization; Alignment Problems; Audio-visual; ON dynamics; Performance level; Symphonic music; Synchronization process; Time stamps; Visual channels; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Grill T.; Schlüter J.","Grill, Thomas (6506275367); Schlüter, Jan (55063593300)","6506275367; 55063593300","Music boundary detection using neural networks on combined features and two-level annotations","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009059267&partnerID=40&md5=1570f1976d1c5536e1bb3fd9862a4fe2","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Grill T., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","The determination of structural boundaries is a key task for understanding the structure of a musical piece, but it is also highly ambiguous. Recently, Convolutional Neural Networks (CNN) trained on spectrogram features and human annotations have been successfully used to tackle the problem, but still fall clearly behind human performance. We expand on the CNN approach by combining spectrograms with self-similarity lag matrices as audio features, thereby capturing more facets of the underlying structural information. Furthermore, in order to consider the hierarchical nature of structural organization, we explore different strategies to learn from the two-level annotations of main and secondary boundaries available in the SALAMI structural annotation dataset. We show that both measures improve boundary recognition performance, resulting in a significant improvement over the previous state of the art. As a side-effect, our algorithm can predict boundaries on two different structural levels, equivalent to the training data. © Thomas Grill and Jan Schlüter.","","Neural networks; Spectrographs; Boundary detection; Boundary recognition; Combined features; Convolutional neural network; Structural boundary; Structural information; Structural levels; Structural organization; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Bittner R.M.; Salamon J.; Essid S.; Bello J.P.","Bittner, Rachel M. (55659619600); Salamon, Justin (55184866100); Essid, Slim (16033218700); Bello, Juan P. (7102889110)","55659619600; 55184866100; 16033218700; 7102889110","Melody extraction by contour classification","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008455727&partnerID=40&md5=2c82d44c632018bced7195a69f6a3df7","Music and Audio Research Lab, New York University, United States; Center for Urban Science and Progress, New York University, United States; Télécom Paris-Tech, France","Bittner R.M., Music and Audio Research Lab, New York University, United States; Salamon J., Music and Audio Research Lab, New York University, United States, Center for Urban Science and Progress, New York University, United States; Essid S., Télécom Paris-Tech, France; Bello J.P., Music and Audio Research Lab, New York University, United States","Due to the scarcity of labeled data, most melody extraction algorithms do not rely on fully data-driven processing blocks but rather on careful engineering. For example, the Melodia melody extraction algorithm employs a pitch contour selection stage that relies on a number of heuristics for selecting the melodic output. In this paper we explore the use of a discriminative model to perform purely data-driven melodic contour selection. Specifically, a discriminative binary classifier is trained to distinguish melodic from non-melodic contours. This classifier is then used to predict likelihoods for a track’s extracted contours, and these scores are decoded to generate a single melody output. The results are compared with the Melodia algorithm and with a generative model used in a previous study. We show that the discriminative model outperforms the generative model in terms of contour classification accuracy, and the melody output from our proposed system performs comparatively to Melodia. The results are complemented with error analysis and avenues for future improvements. © Rachel M. Bittner, Justin Salamon, Slim Essid, Juan P. Bello.","","Extraction; Information retrieval; Binary classifiers; Classification accuracy; Data-driven processing; Discriminative models; Future improvements; Generative model; Melody extractions; Selection stages; Data handling","R.M. Bittner; Music and Audio Research Lab, New York University, United States; email: rachel.bittner@nyu.edu","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Vad B.; Boland D.; Williamson J.; Murray-Smith R.; Steffensen P.B.","Vad, Beatrix (57210232902); Boland, Daniel (39360893500); Williamson, John (8678439600); Murray-Smith, Roderick (6602499837); Steffensen, Peter Berg (57210234792)","57210232902; 39360893500; 8678439600; 6602499837; 57210234792","Design and evaluation of a probabilistic music projection interface","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011647502&partnerID=40&md5=b23d96bddca43a90fea11118afc349b2","School of Computing Science, University of Glasgow, United Kingdom; Syntonetic A/S, Copenhagen, Denmark","Vad B., School of Computing Science, University of Glasgow, United Kingdom; Boland D., School of Computing Science, University of Glasgow, United Kingdom; Williamson J., School of Computing Science, University of Glasgow, United Kingdom; Murray-Smith R., School of Computing Science, University of Glasgow, United Kingdom; Steffensen P.B., Syntonetic A/S, Copenhagen, Denmark","We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist generation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34-dimensional feature space. We use a nonlinear dimensionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilistic mappings of selected features and their uncertainty. We evaluated the system in a longitudinal trial in users’ homes over several weeks. Users said they had fun with the interface and liked the casual nature of the playlist generation. Users preferred to generate playlists from a local neighbourhood of the map, rather than from a trajectory, using neighbourhood selection more than three times more often than path selection. Probabilistic highlighting of subjective features led to more focused exploration in mouse activity logs, and 6 of 8 users said they preferred the probabilistic highlighting mode. © Beatrix Vad, Daniel Boland, John Williamson, Roderick Murray-Smith, Peter Berg Steffensen.","","Information retrieval; Sound recording; Activity logs; Audio features; Design and evaluations; Feature space; Neighbourhood; Nonlinear dimensionality reduction; Path selection; Mammals","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lehner B.; Widmer G.","Lehner, Bernhard (7003282869); Widmer, Gerhard (7004342843)","7003282869; 7004342843","Monaural blind source separation in the context of vocal detection","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986187419&partnerID=40&md5=98dd86cc17a16eb871b41e209be71c2f","Department of Computational Perception, Johannes Kepler University of Linz, Austria","Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","In this paper, we evaluate the usefulness of several monaural blind source separation (BSS) algorithms in the context of vocal detection (VD). BSS is the problem of recovering several sources, given only a mixture. VD is the problem of automatically identifying the parts in a mixed audio signal, where at least one person is singing. We compare the results of three different strategies for utilising the estimated singing voice signals from four state-of-the-art source separation algorithms. In order to assess the performance of those strategies on an internal data set, we use two different feature sets, each fed to two different classifiers. After selecting the most promising approach, the results on two publicly available data sets are presented. In an additional experiment, we use the improved VD for a simple post-processing technique: For the final estimation of the source signals, we decide to use either silence, or the mixed, or the separated signals, according to the VD. The results of traditionally used BSS evaluation methods suggest that this is useful for both the estimated background signals, as well as for the estimated vocals. © Bernhard Lehner, Gerhard Widmer.","","Classification (of information); Information retrieval; Petroleum reservoir evaluation; Additional experiments; Background signals; Evaluation methods; Post-processing techniques; Separation algorithms; Singing voices; State of the art; Vocal detection; Blind source separation","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Schlüter J.; Grill T.","Schlüter, Jan (55063593300); Grill, Thomas (6506275367)","55063593300; 6506275367","Exploring data augmentation for improved singing voice detection with neural networks","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","152","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973376414&partnerID=40&md5=5f6ffe91142a6f9fd5c15caf8de1ff15","Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Grill T., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","In computer vision, state-of-the-art object recognition systems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically explored for music signals. Using the problem of singing voice detection with neural networks as an example, we apply a range of label-preserving audio transformations to assess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shifting to be the most helpful augmentation method. Combined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public datasets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval. © Jan Schlüter and Thomas Grill.","","Classification (of information); Continuous speech recognition; Information retrieval; Object recognition; Augmentation methods; Classification errors; Data augmentation; Image transformations; Music information retrieval; Object recognition systems; Singing voice detection; Training data sets; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Laplante A.","Laplante, Audrey (37110930300)","37110930300","Improving music recommender systems: What can we learn from research on music tastes?","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060993339&partnerID=40&md5=7b6e06ac96b29531fb95a871d99acc6f","École de Bibliothéconomie et des Sciences de l’Information, Université de Montréal, Canada","Laplante A., École de Bibliothéconomie et des Sciences de l’Information, Université de Montréal, Canada","The success of a music recommender system depends on its ability to predict how much a particular user will like or dislike each item in its catalogue. However, such predictions are difficult to make accurately due to the complex nature of music tastes. In this paper, we review the literature on music tastes from social psychology and sociology of music to identify the correlates of music tastes and to understand how music tastes are formed and evolve through time. Research shows associations between music preferences and a wide variety of sociodem-ographic and individual characteristics, including personality traits, values, ethnicity, gender, social class, and political orientation. It also reveals the importance of social influences on music tastes, more specifically from family and peers, as well as the central role of music tastes in the construction of personal and social identities. Suggestions for the design of music recommender systems are made based on this literature review. © Audrey Laplante.","","Individual characteristics; Literature reviews; Music preferences; Music recommender systems; Personality traits; Political orientation; Social influence; Social psychology; Recommender systems","A. Laplante; École de bibliothéconomie et des sciences de l’information, Université de Montréal, Canada; email: audrey.laplante@umontreal.ca","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Panteli M.; Bogaards N.; Honingh A.","Panteli, Maria (55915922100); Bogaards, Niels (55360332100); Honingh, Aline (14032798400)","55915922100; 55360332100; 14032798400","Modeling rhythm similarity for electronic dance music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030170247&partnerID=40&md5=b0ab11ec76fbdd3ef787b8f3fcc55ccf","University of Amsterdam, Amsterdam, Netherlands; Elephantcandy, Amsterdam, Netherlands","Panteli M., University of Amsterdam, Amsterdam, Netherlands; Bogaards N., Elephantcandy, Amsterdam, Netherlands; Honingh A., University of Amsterdam, Amsterdam, Netherlands","A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop’, a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between segments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most notably: attack phase of onsets, periodicity of rhythmic elements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, after which the similarity between segments can be calculated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the overall performance of the model with perceptual ratings of rhythm similarity. © Maria Panteli, Niels Bogaards, Aline Honingh.","","Electronic musical instruments; Information retrieval; Feature vectors; Onset detection; Computer music","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Singhi A.; Brown D.G.","Singhi, Abhishek (57210207767); Brown, Daniel G. (55738804200)","57210207767; 55738804200","On cultural, textual and experiential aspects of music mood","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041407959&partnerID=40&md5=1aaeff66d472aa9f750f71e4e0d0cc5f","University of Waterloo, Cheriton School of Computer Science, Canada","Singhi A., University of Waterloo, Cheriton School of Computer Science, Canada; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada","We study the impact of the presence of lyrics on music mood perception for both Canadian and Chinese listeners by conducting a user study of Canadians not of Chinese origin, Chinese-Canadians, and Chinese people who have lived in Canada for fewer than three years. While our original hypotheses were largely connected to cultural components of mood perception, we also analyzed how stable mood assignments were when listeners could read the lyrics of recent popular English songs they were hearing versus when they only heard the songs. We also showed the lyrics of some songs to participants without playing the recorded music. We conclude that people assign different moods to the same song in these three scenarios. People tend to assign a song to the mood cluster that includes “melancholy” more often when they read the lyrics without listening to it, and having access to the lyrics does not help reduce the difference in music mood perception between Canadian and Chinese listeners significantly. Our results cause us to question the idea that songs have “inherent mood”. Rather, we suggest that the mood depends on both cultural and experiential context. © Abhishek Singhi, Daniel G. Brown.","","Information retrieval; Chinese people; User study; Audition","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Jun S.; Rho S.; Hwang E.","Jun, Sanghoon (26655735600); Rho, Seungmin (10738984000); Hwang, Eenjun (7101826788)","26655735600; 10738984000; 7101826788","Geographical region mapping scheme based on musical preferences","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069933535&partnerID=40&md5=51ffbb27f76c10bd0aada4461d7714f4","Korea University, South Korea; Sungkyul University, South Korea","Jun S., Korea University, South Korea; Rho S., Sungkyul University, South Korea; Hwang E., Korea University, South Korea","Many countries and cities in the world tend to have different types of preferred or popular music, such as pop, K-pop, and reggae. Music-related applications utilize geographical proximity for evaluating the similarity of music preferences between two regions. Sometimes, this can lead to incorrect results due to other factors such as culture and religion. To solve this problem, in this paper, we propose a scheme for constructing a music map in which regions are positioned close to one another depending on the similarity of the musical preferences of their populations. That is, countries or cities in a traditional map are rearranged in the music map such that regions with similar musical preferences are close to one another. To do this, we collect users’ music play history and extract popular artists and tag information from the collected data. Similarities among regions are calculated using the tags and their frequencies. And then, an iterative algorithm for rearranging the regions into a music map is applied. We present a method for constructing the music map along with some experimental results. © Sanghoon Jun, Seungmin Rho, Eenjun Hwang.","","Information retrieval; Iterative methods; Geographical proximity; Iterative algorithm; Mapping scheme; Music preferences; Popular music; Geographical regions","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Wang J.-C.; Yen M.-C.; Yang Y.-H.; Wang H.-M.","Wang, Ju-Chiang (36609288900); Yen, Ming-Chi (57266880100); Yang, Yi-Hsuan (55218558400); Wang, Hsin-Min (8297293300)","36609288900; 57266880100; 55218558400; 8297293300","Automatic set list identification and song segmentation for full-length concert videos","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069917740&partnerID=40&md5=c2e3bdb30d23ec1a44109d096b602c2c","Academia Sinica, Taipei, Taiwan; University of California, San Diego, CA, United States","Wang J.-C., Academia Sinica, Taipei, Taiwan, University of California, San Diego, CA, United States; Yen M.-C., Academia Sinica, Taipei, Taiwan; Yang Y.-H., Academia Sinica, Taipei, Taiwan; Wang H.-M., Academia Sinica, Taipei, Taiwan","Recently, plenty of full-length concert videos have become available on video-sharing websites such as YouTube. As each video generally contains multiple songs, natural questions that arise include “what is the set list?” and “when does each song begin and end?” Indeed, many full concert videos on YouTube contain song lists and timecodes contributed by uploaders and viewers. However, newly uploaded content and videos of lesser-known artists typically lack this metadata. Manually labeling such metadata would be labor-intensive, and thus an automated solution is desirable. In this paper, we define a novel research problem, automatic set list segmentation of full concert videos, which calls for techniques in music information retrieval (MIR) such as audio fingerprinting, cover song identification, musical event detection, music alignment, and structural segmentation. Moreover, we propose a greedy approach that sequentially identifies a song from a database of studio versions and simultaneously estimates its probable boundaries in the concert. We conduct preliminary evaluations on a collection of 20 full concerts and 1,152 studio tracks. Our result demonstrates the effectiveness of the proposed greedy algorithm. © Ju-Chiang Wang, Ming-Chi Yen, Yi-Hsuan Yang, and Hsin-Min Wang.","","Information retrieval; Metadata; Studios; Audio fingerprinting; Automated solutions; Cover song identifications; Greedy approaches; Music information retrieval; Musical event detection; Research problems; Video sharing websites; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"McFee B.; Humphrey E.J.; Bello J.P.","McFee, Brian (34875379700); Humphrey, Eric J. (55060792500); Bello, Juan P. (7102889110)","34875379700; 55060792500; 7102889110","A software framework for musical data augmentation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","91","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996516893&partnerID=40&md5=3dc5ad87eb9e2ff67145aa40049237ac","Center for Data Science, New York University, United States; Music and Audio Research Laboratory, New York University, United States; MuseAmi, Inc., United States","McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Humphrey E.J., Music and Audio Research Laboratory, New York University, United States, MuseAmi, Inc., United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of “data augmentation” — supplementing a training set with carefully perturbed samples — has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals. © Brian McFee, Eric J. Humphrey, Juan P. Bello.","","Audio acoustics; Computer programming; Information retrieval; Annotated training data; Data augmentation; Large-scale machine learning; Predictive models; Proof of concept; Robust systems; Software frameworks; Training sets; Learning systems","B. McFee; Center for Data Science, New York University, United States; email: brian.mcfee@nyu.edu","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Lee C.-L.; Lin Y.-T.; Yao Z.-R.; Lee F.-Y.; Wu J.-L.","Lee, Chuan-Lung (56668957100); Lin, Yin-Tzu (57209831317); Yao, Zun-Ren (57210234761); Lee, Feng-Yi (57210231856); Wu, Ja-Ling (7409250086)","56668957100; 57209831317; 57210234761; 57210231856; 7409250086","Automatic mashup creation by considering both vertical and horizontal mashabilities","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966890182&partnerID=40&md5=e1060a261bb6719d627015b720d738bf","Communications and Multimedia Laboratory, National Taiwan University, Taiwan","Lee C.-L., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Lin Y.-T., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Yao Z.-R., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Lee F.-Y., Communications and Multimedia Laboratory, National Taiwan University, Taiwan; Wu J.-L., Communications and Multimedia Laboratory, National Taiwan University, Taiwan","In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by overlaying music segments on one single base track, the proposed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: “harmonic change balance” and “volume weight” have been considered. On the horizontal side, the methods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encountered and found the proper solution to each of them. Subjective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listening enjoyment. Besides, by taking the newly proposed vertical mashability measurement into account, the improvement in user satisfaction is statistically significant. © Chuan-Lung Lee, Yin-Tzu Lin, Zun-Ren Yao, Feng-Yi Lee and Ja-Ling Wu.","","Mash-up; Mashups; Music segments; Proper solutions; Subjective evaluations; Track segments; User satisfaction; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Frisson C.; Dupont S.; Yvart W.; Riche N.; Siebert X.; Dutoit T.","Frisson, Christian (35104875400); Dupont, Stéphane (7007183914); Yvart, Willy (56025453500); Riche, Nicolas (52264258100); Siebert, Xavier (57200099245); Dutoit, Thierry (36022249200)","35104875400; 7007183914; 56025453500; 52264258100; 57200099245; 36022249200","A proximity grid optimization method to improve audio search for sound design","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069929331&partnerID=40&md5=18a7c3eeca3d6f917ca8b602751e1dd2","Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium","Frisson C., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Dupont S., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Yvart W., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Riche N., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Siebert X., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium; Dutoit T., Numediart Institute, University of Mons, Boulevard Dolez 31, Mons, 7000, Belgium","Sound designers organize their sound libraries either with dedicated applications (often featuring spreadsheet views), or with default file browsers. Content-based research applications have been favoring cloud-like similarity layouts. We propose a solution combining the advantages of these: after feature extraction and dimension reduction (Student-t Stochastic Neighbor Embedding), we apply a proximity grid, optimized to preserve nearest neighborhoods between the adjacent cells. By counting direct vertical / horizontal / diagonal neighbors, we compare this solution over a standard layout: a grid ordered by filename. Our evaluation is performed on subsets of the One Laptop Per Child sound library, either selected by thematic folders, or filtered by tag. We also compare 3 layouts (grid by filename without visual icons, with visual icons, and proximity grid) by a user evaluation through known-item search tasks. This optimization method can serve as a human-readable metric for the comparison of dimension reduction techniques. © Christian Frisson, Stéphane Dupont, Willy Yvart, Nicolas Riche, Xavier Siebert, Thierry Dutoit.","","Information retrieval; Stochastic systems; Dimension reduction; Dimension reduction techniques; Grid optimization methods; Known item searches; Nearest neighborhood; Optimization method; Research applications; Stochastic neighbor embedding; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sturm B.L.; Bardeli R.; Langlois T.; Emiya V.","Sturm, Bob L. (14014190500); Bardeli, Rolf (23097308000); Langlois, Thibault (6602432323); Emiya, Valentin (18041964000)","14014190500; 23097308000; 6602432323; 18041964000","Formalizing the problem of music description","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906272&partnerID=40&md5=acf0feacd6ca214cd10141a2407d73c3","Aalborg University, Denmark; Fraunhofer IAIS, Germany; Lisbon University, Portugal; Aix-Marseille Université, CNRS UMR 7279 LIF, France","Sturm B.L., Aalborg University, Denmark; Bardeli R., Fraunhofer IAIS, Germany; Langlois T., Lisbon University, Portugal; Emiya V., Aix-Marseille Université, CNRS UMR 7279 LIF, France","The lack of a formalism for “the problem of music description” results in, among other things: ambiguity in what problem a music description system must address, how it should be evaluated, what criteria define its success, and the paradox that a music description system can reproduce the “ground truth” of a music dataset without attending to the music it contains. To address these issues, we formalize the problem of music description such that all elements of an instance of it are made explicit. This can thus inform the building of a system, and how it should be evaluated in a meaningful way. We provide illustrations of this formalism applied to three examples drawn from the literature. © Bob L. Sturm, Rolf Bardeli, Thibault Langlois, Valentin Emiya.","","Ground truth; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Hsu L.-C.; Wang Y.-L.; Lin Y.-J.; Su A.W.Y.; Metcalf C.D.","Hsu, Ling-Chi (56939825700); Wang, Yu-Lin (36919548200); Lin, Yi-Ju (56022124700); Su, Alvin W.Y. (8433191700); Metcalf, Cheryl D. (23474785700)","56939825700; 36919548200; 56022124700; 8433191700; 23474785700","Detection of motor changes in violin playing by EMG signals","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069928086&partnerID=40&md5=4b5d878d16fab75a219cda34938809de","Department of CSIE, National Cheng-Kung University, Taiwan; Faculty of Health Sciences, University of Southampton, United Kingdom","Hsu L.-C., Department of CSIE, National Cheng-Kung University, Taiwan; Wang Y.-L., Department of CSIE, National Cheng-Kung University, Taiwan; Lin Y.-J., Department of CSIE, National Cheng-Kung University, Taiwan; Su A.W.Y., Department of CSIE, National Cheng-Kung University, Taiwan; Metcalf C.D., Faculty of Health Sciences, University of Southampton, United Kingdom","Playing a music instrument relies on the harmonious body movements. Motor sequences are trained to achieve the perfect performances in musicians. Thus, the information from audio signal is not enough to understand the sensorimotor programming in players. Recently, the investigation of muscular activities of players during performance has attracted our interests. In this work, we propose a multi-channel system that records the audio sounds and electromyography (EMG) signal simultaneously and also develop algorithms to analyze the music performance and discover its relation to player’s motor sequences. The movement segment was first identified by the information of audio sounds, and the direction of violin bowing was detected by the EMG signal. Six features were introduced to reveal the variations of muscular activities during violin playing. With the additional information of the audio signal, the proposed work could efficiently extract the period and detect the direction of motor changes in violin bowing. Therefore, the proposed work could provide a better understanding of how players activate the muscles to organize the multi-joint movement during violin performance. © L.C. Hsu, Y.J. Lin, Y.L. Wang, A.W.Y. Su, C.D. Metcalf.","","Information retrieval; Musical instruments; Audio signal; Body movements; EMG signal; Multi-joint; Multichannel system; Muscular activities; Music performance; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Humphrey E.J.; Salamon J.; Nieto O.; Forsyth J.; Bittner R.M.; Bello J.P.","Humphrey, Eric J. (55060792500); Salamon, Justin (55184866100); Nieto, Oriol (55583364500); Forsyth, Jon (55566432300); Bittner, Rachel M. (55659619600); Bello, Juan P. (7102889110)","55060792500; 55184866100; 55583364500; 55566432300; 55659619600; 7102889110","JAMS: A JSON annotated music specification for reproducible MIR research","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066080965&partnerID=40&md5=7373d22ea5c7ef9283fdecf00e1b3833","Music and Audio Research Lab, New York University, New York, United States; Center for Urban Science and Progress, New York University, New York, United States","Humphrey E.J., Music and Audio Research Lab, New York University, New York, United States; Salamon J., Music and Audio Research Lab, New York University, New York, United States, Center for Urban Science and Progress, New York University, New York, United States; Nieto O., Music and Audio Research Lab, New York University, New York, United States; Forsyth J., Music and Audio Research Lab, New York University, New York, United States; Bittner R.M., Music and Audio Research Lab, New York University, New York, United States; Bello J.P., Music and Audio Research Lab, New York University, New York, United States","The continued growth of MIR is motivating more complex annotation data, consisting of richer information, multiple annotations for a given task, and multiple tasks for a given music signal. In this work, we propose JAMS, a JSON-based music annotation format capable of addressing the evolving research requirements of the community, based on the three core principles of simplicity, structure and sustainability. It is designed to support existing data while encouraging the transition to more consistent, comprehensive, well-documented annotations that are poised to be at the crux of future MIR research. Finally, we provide a formal schema, software tools, and popular datasets in the proposed format to lower barriers to entry, and discuss how now is a crucial time to make a concerted effort toward sustainable annotation standards. © Eric J. Humphrey, Justin Salamon, Oriol Nieto, Jon Forsyth, Rachel M. Bittner, Juan P. Bello.","","Multiple tasks; Music signals; Information retrieval","E.J. Humphrey; Music and Audio Research Lab, New York University, New York, United States; email: ejhumphrey@nyu.edu","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Hu X.; Lee J.H.; Choi K.; Downie J.S.","Hu, Xiao (55496358400); Lee, Jin Ha (57190797465); Choi, Kahyun (56452038000); Downie, J. Stephen (7102932568)","55496358400; 57190797465; 56452038000; 7102932568","A cross-cultural study of mood in K-pop songs","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041843979&partnerID=40&md5=b7d7ff299a5b8f079ea2f85dcb8a10e9","University of Hong Kong, Hong Kong; University of Washington, United States; University of Illinois, United States","Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Downie J.S., University of Illinois, United States","Prior research suggests that music mood is one of the most important criteria when people look for music—but the perception of mood may be subjective and can be influenced by many factors including the listeners’ cultural background. In recent years, the number of studies of music mood perceptions by various cultural groups and of automated mood classification of music from different cultures has been increasing. However, there has yet to be a well-established testbed for evaluating cross-cultural tasks in Music Information Retrieval (MIR). Moreover, most existing datasets in MIR consist mainly of Western music and the cultural backgrounds of the annotators were mostly not taken into consideration or were limited to one cultural group. In this study, we built a collection of 1,892 K-pop (Korean Pop) songs with mood annotations collected from both Korean and American listeners, based on three different mood models. We analyze the differences and similarities between the mood judgments of the two listener groups, and propose potential MIR tasks that can be evaluated on this dataset. © Xiao Hu, Jin Ha Lee, Kahyun Choi, J. Stephen Downie.","","Cross-cultural study; Cultural backgrounds; Cultural groups; Music information retrieval; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Molina E.; Tardón L.J.; Barbancho I.; Barbancho A.M.","Molina, Emilio (55967369200); Tardón, Lorenzo J. (6602405058); Barbancho, Isabel (6602638932); Barbancho, Ana M. (6602362530)","55967369200; 6602405058; 6602638932; 6602362530","The importance of F0 tracking in query-by-singing-humming","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069901261&partnerID=40&md5=122920d022531fb6a042c2b3acc6ffbd","Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","Molina E., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho I., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","In this paper, we present a comparative study of several state-of-the-art F0 trackers applied to the context of query-by-singing-humming (QBSH). This study has been carried out using the well known, freely available, MIR-QBSH dataset in different conditions of added pub-style noise and smartphone-style distortion. For audio-to-MIDI melodic matching, we have used two state-of-the-art systems and a simple, easily reproducible baseline method. For the evaluation, we measured the QBSH performance for 189 different combinations of F0 tracker, noise/distortion conditions and matcher. Additionally, the overall accuracy of the F0 transcriptions (as defined in MIREX) was also measured. In the results, we found that F0 tracking overall accuracy correlates with QBSH performance, but it does not totally measure the suitability of a pitch vector for QBSH. In addition, we also found clear differences in robustness to F0 transcription errors between different matchers. © Emilio Molina, Lorenzo J. Tardón, Isabel Barbancho, Ana M. Barbancho.","","Audio systems; Transcription; Baseline methods; Comparative studies; Overall accuracies; Query-by-singing; State of the art; Two-state; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Arzt A.; Widmer G.; Sonnleitner R.","Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843); Sonnleitner, Reinhard (55566668400)","36681791200; 7004342843; 55566668400","Tempo- and transposition-invariant identification of piece and score position","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059771094&partnerID=40&md5=3ba1be6fc03f11e43e6b546d3d979650","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present an algorithm that, given a very small snippet of an audio performance and a database of musical scores, quickly identifies the piece and the position in the score. The algorithm is both tempo- and transposition-invariant. We approach the problem by extending an existing tempo-invariant symbolic fingerprinting method, replacing the absolute pitch information in the fingerprints with a relative representation. Not surprisingly, this leads to a big decrease in the discriminative power of the fingerprints. To overcome this problem, we propose an additional verification step to filter out the introduced noise. Finally, we present a simple tracking algorithm that increases the retrieval precision for longer queries. Experiments show that both modifications improve the results, and make the new algorithm usable for a wide range of applications. © Andreas Arzt, Gerhard Widmer, Reinhard Sonnleitner.","","Information retrieval; Absolute pitch; Discriminative power; Fingerprinting methods; Musical score; Tracking algorithm; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"McFee B.; Nieto O.; Bello J.P.","McFee, Brian (34875379700); Nieto, Oriol (55583364500); Bello, Juan P. (7102889110)","34875379700; 55583364500; 7102889110","Hierarchical evaluation of segment boundary detection","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989225106&partnerID=40&md5=3a518ede1edf13c0ae18c25826495879","Center for Data Science, New York University, United States; Music and Audio Research Laboratory, New York University, United States","McFee B., Center for Data Science, New York University, United States, Music and Audio Research Laboratory, New York University, United States; Nieto O., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","Structure in music is traditionally analyzed hierarchically: large-scale sections can be sub-divided and refined down to the short melodic ideas at the motivic level. However, typical algorithmic approaches to structural annotation produce flat temporal partitions of a track, which are commonly evaluated against a similarly flat, human-produced annotation. Evaluating structure analysis as represented by flat annotations effectively discards all notions of structural depth in the evaluation. Although collections of hierarchical structure annotations have been recently published, no techniques yet exist to measure an algorithm’s accuracy against these rich structural annotations. In this work, we propose a method to evaluate structural boundary detection with hierarchical annotations. The proposed method transforms boundary detection into a ranking problem, and facilitates the comparison of both flat and hierarchical annotations. We demonstrate the behavior of the proposed method with various synthetic and real examples drawn from the SALAMI dataset. © Brian McFee, Oriol Nieto, Juan P. Bello.","","Algorithmic approach; Boundary detection; Hierarchical evaluation; Hierarchical structures; Ranking problems; Structural boundary; Structure analysis; Temporal partitions; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Wu C.-W.; Lerch A.","Wu, Chih-Wei (57188865070); Lerch, Alexander (22034963000)","57188865070; 22034963000","Drum transcription using partially fixed non-negative matrix factorization with template adaptation","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973278728&partnerID=40&md5=306c5ea7fbce6b8bcb7bca0d7c65a097","Georgia Institute of Technology, Center for Music Technology, Georgia","Wu C.-W., Georgia Institute of Technology, Center for Music Technology, Georgia; Lerch A., Georgia Institute of Technology, Center for Music Technology, Georgia","In this paper, a template adaptive drum transcription algorithm using partially fixed Non-negative Matrix Factorization (NMF) is presented. The proposed method detects percussive events in complex mixtures of music with a minimal training set. The algorithm decomposes the music signal into two dictionaries: a percussive dictionary initialized with pre-defined drum templates and a harmonic dictionary initialized with undefined entries. The harmonic dictionary is adapted to the non-percussive music content in a standard NMF procedure. The percussive dictionary is adapted to each individual signal in an iterative scheme: it is fixed during the decomposition process, and is updated based on the result of the previous convergence. Two template adaptation methods are proposed to provide more flexibility and robustness in the case of unknown data. The performance of the proposed system has been evaluated and compared to state of the art systems. The results show that template adaptation improves the transcription performance, and the detection accuracy is in the same range as more complex systems. © Chih-Wei Wu, Alexander Lerch.","","Factorization; Information retrieval; Iterative methods; Signal processing; Transcription; Decomposition process; Detection accuracy; Harmonic dictionaries; Iterative schemes; Minimal training; Nonnegative matrix factorization; State-of-the-art system; Template adaptations; Matrix algebra","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Antila C.; Cumming J.","Antila, Christopher (57210206748); Cumming, Julie (53463213000)","57210206748; 53463213000","The VIS framework: Analyzing counterpoint in large datasets","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069906709&partnerID=40&md5=9701a3a33b3db6107328b28a9d676c27","McGill University, Canada","Antila C., McGill University, Canada; Cumming J., McGill University, Canada","The VIS Framework for Music Analysis is a modular Python library designed for “big data” queries in symbolic musical data. Initially created as a tool for studying musical style change in counterpoint, we have built on the music21 and pandas libraries to provide the foundation for much more. We describe the musicological needs that inspired the creation and growth of the VIS Framework, along with a survey of similar previous research. To demonstrate the effectiveness of our analytic approach and software, we present a sample query showing that the most commonly repeated contrapuntal patterns vary between three related style periods. We also emphasize our adaptation of typical n-gram-based research in music, our implementation strategy in VIS, and the flexibility of this approach for future researchers. © Christopher Antila and Julie Cumming.","","Information retrieval; Analytic approach; Implementation strategies; Large datasets; Music analysis; N-grams; Large dataset","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Gómez-Marín D.; Jordà S.; Herrera P.","Gómez-Marín, Daniel (57203584097); Jordà, Sergi (8228030900); Herrera, Perfecto (24824250300)","57203584097; 8228030900; 24824250300","Pad and sad: Two awareness-weighted rhythmic similarity distances","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003552787&partnerID=40&md5=c3d307c78ebb829a05033eb8a12e025f","Universitat Pompeu Fabra, Spain","Gómez-Marín D., Universitat Pompeu Fabra, Spain; Jordà S., Universitat Pompeu Fabra, Spain; Herrera P., Universitat Pompeu Fabra, Spain","Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as being in time without discriminating the importance of some regions over others. In a previously reported experiment we observed that measures of similarity may differ given the presence or absence of a pulse inducing sound and the importance of those measures is not constant along the pattern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the perceptual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the temporal positions of the beat along the bar. We show that with these improvements, the correlation between the previously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops. © Daniel Gómez-Marín, Sergi Jordà, Perfecto Herrera.","","Non-homogeneous; Similarity distance; Similarity metrics; Temporal position; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Arjannikov T.; Zhang J.Z.","Arjannikov, Tom (57189356387); Zhang, John Z. (16551442100)","57189356387; 16551442100","An association-based approach to genre classification in music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061492495&partnerID=40&md5=c1919202ad8134a417906354ffbfaa80","University of Lethbridge, Canada","Arjannikov T., University of Lethbridge, Canada; Zhang J.Z., University of Lethbridge, Canada","Music Information Retrieval (MIR) is a multi-disciplin-ary research area that aims to automate the access to large-volume music data, including browsing, retrieval, storage, etc. The work that we present in this paper tackles a nontrivial problem in the field, namely music genre classification, which is one of the core tasks in MIR. In our proposed approach, we make use of association analysis to study and predict music genres based on the acoustic features extracted directly from music. In essence, we build an associative classifier, which finds inherent associations between content-based features and individual genres and then uses them to predict the genre(s) of a new music piece. We demonstrate the feasibility of our approach through a series of experiments using two publicly available music datasets. One of them is the largest available in MIR and contains real world data, while the other has been widely used and provides a good benchmarking basis. We show the effectiveness of our approach and discuss various related issues. In addition, due to its associative nature, our classifier can assign multiple genres to a single music piece; hopefully this would offer insights into the prevalent multi-label situation in genre classification. © Tom Arjannikov, John Z. Zhang.","","Digital storage; Information retrieval; Acoustic features; Association analysis; Associative classifiers; Content-based features; Genre classification; Large volumes; Music genre classification; Music information retrieval; Classification (of information)","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Nieto O.; Farbood M.M.; Jehan T.; Bello J.P.","Nieto, Oriol (55583364500); Farbood, Morwaread M. (7801537706); Jehan, Tristan (6504476037); Bello, Juan Pablo (7102889110)","55583364500; 7801537706; 6504476037; 7102889110","Perceptual analysis of the f-measure for evaluating section boundaries in music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066072277&partnerID=40&md5=a0c6a17271bb5b376cfbc4249f44664f","Music and Audio Research Lab, New York University, United States; Echo Nest, United States","Nieto O., Music and Audio Research Lab, New York University, United States; Farbood M.M., Music and Audio Research Lab, New York University, United States; Jehan T., Echo Nest, United States; Bello J.P., Music and Audio Research Lab, New York University, United States","In this paper, we aim to raise awareness of the limitations of the F-measure when evaluating the quality of the boundaries found in the automatic segmentation of music. We present and discuss the results of various experiments where subjects listened to different musical excerpts containing boundary indications and had to rate the quality of the boundaries. These boundaries were carefully generated from state-of-the-art segmentation algorithms as well as human-annotated data. The results show that humans tend to give more relevance to the precision component of the F-measure rather than the recall component, therefore making the classical F-measure not as perceptually informative as currently assumed. Based on the results of the experiments, we discuss the potential of an alternative evaluation based on the F-measure that emphasizes precision over recall, making the section boundary evaluation more expressive and reliable. © Oriol Nieto, Morwaread M. Farbood, Tristan Jehan, Juan Pablo Bello.","","Automatic segmentations; Boundary evaluation; F measure; Perceptual analysis; Precision components; Segmentation algorithms; State of the art; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Freedman D.; Kohler E.; Tutschku H.","Freedman, Dylan (57194869433); Kohler, Eddie (9133554700); Tutschku, Hans (36969288000)","57194869433; 9133554700; 36969288000","Correlating extracted and ground-truth harmonic data in music retrieval tasks","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988000027&partnerID=40&md5=1a9d924be03e1c99e85756b929373f97","Harvard University, United States","Freedman D., Harvard University, United States; Kohler E., Harvard University, United States; Tutschku H., Harvard University, United States","We show that traditional music information retrieval tasks with well-chosen parameters perform similarly using computationally extracted chord annotations and ground-truth annotations. Using a collection of Billboard songs with provided ground-truth chord labels, we use established chord identification algorithms to produce a corresponding extracted chord label dataset. We implement methods to compare chord progressions between two songs on the basis of their optimal local alignment scores. We create a set of chord progression comparison parameters defined by chord distance metrics, gap costs, and normalization measures and run a black-box global optimization algorithm to stochastically search for the best parameter set to maximize the rank correlation for two harmonic retrieval tasks across the ground-truth and extracted chord Billboard datasets. The first task evaluates chord progression similarity between all pairwise combinations of songs, separately ranks results for ground-truth and extracted chord labels, and returns a rank correlation coefficient. The second task queries the set of songs with fabricated chord progressions, ranks each query’s results across ground-truth and extracted chord labels, and returns rank correlations. The end results suggest that practical retrieval systems can be constructed to work effectively without the guide of human ground-truthing. © Dylan Freedman, Eddie Kohler, Hans Tutschku.","","Global optimization; Chord identification; Comparison parameters; Global optimization algorithm; Harmonic retrieval; Music information retrieval; Pair-wise combinations; Rank correlation coefficient; Retrieval systems; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Flexer A.","Flexer, Arthur (7004555682)","7004555682","Improving visualization of high-dimensional music similarity spaces","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987969516&partnerID=40&md5=38b6a4fa885f6defc3e948f60fb5cf13","Austrian Research Institute for Artificial Intelligence, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence, Austria","Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that appear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phenomenon impacts three popular approaches to compute two-dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces before low dimensional projection. © Arthur Flexer.","","Information retrieval; Empirical studies; High dimensional spaces; High-dimensional; Intuitive exploration; Low dimensional; Measuring distances; Music similarity; Similarity spaces; Visualization","A. Flexer; Austrian Research Institute for Artificial Intelligence, Austria; email: arthur.flexer@ofai.at","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Urbano J.; Bogdanov D.; Herrera P.; Gómez E.; Serra X.","Urbano, Julián (36118414700); Bogdanov, Dmitry (35748642000); Herrera, Perfecto (24824250300); Gómez, Emilia (14015483200); Serra, Xavier (55892979900)","36118414700; 35748642000; 24824250300; 14015483200; 55892979900","What is the effect of audio quality on the robustness of MFCCs and chroma features?","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028044145&partnerID=40&md5=8b376f6c7390888f4386ce2977cf45b3","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Urbano J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio files encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may affect the computation of descriptors. This raises the question of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classification. © J.Urbano, D.Bogdanov, P.Herrera, E.Gómez and X.Serra.","","Encoding (symbols); Information retrieval; Audio analysis; Audio quality; Audio signal; Chroma features; Genre classification; Music information retrieval; Original signal; Sampling rates; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Benetos E.; Weyde T.","Benetos, Emmanouil (16067946900); Weyde, Tillman (24476899500)","16067946900; 24476899500","An efficient temporally-constrained probabilistic model for multiple-instrument music transcription","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","47","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973291866&partnerID=40&md5=443d2e516e4b0a3526839b91e6a4b473","Centre for Digital Music, Queen Mary University of London, United Kingdom; Department of Computer Science, City University, London, United Kingdom","Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Weyde T., Department of Computer Science, City University, London, United Kingdom","In this paper, an efficient, general-purpose model for multiple instrument polyphonic music transcription is proposed. The model is based on probabilistic latent component analysis and supports the use of sound state spectral templates, which represent the temporal evolution of each note (e.g. attack, sustain, decay). As input, a variable-Q transform (VQT) time-frequency representation is used. Computational efficiency is achieved by supporting the use of pre-extracted and pre-shifted sound state templates. Two variants are presented: without temporal constraints and with hidden Markov model-based constraints controlling the appearance of sound states. Experiments are performed on benchmark transcription datasets: MAPS, TRIOS, MIREX multiF0, and Bach10; results on multi-pitch detection and instrument assignment show that the proposed models outperform the state-of-the-art for multiple-instrument transcription and is more than 20 times faster compared to a previous sound state-based model. We finally show that a VQT representation can lead to improved multi-pitch detection performance compared with constant-Q representations. © Emmanouil Benetos, Tillman Weyde.","","Chemical detection; Computational efficiency; Hidden Markov models; Information retrieval; Multiple instruments; Music transcription; Probabilistic latent component analysis; Probabilistic modeling; State-based models; Temporal constraints; Temporal evolution; Time-frequency representations; Transcription","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Prätzlich T.; Müller M.","Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873)","55582692100; 7404689873","Frame-level audio segmentation for abridged musical works","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017651109&partnerID=40&md5=71b6e676dff79c15ae9dc5c0792e0c64","International Audio Laboratories, Erlangen, Germany","Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Large-scale musical works such as operas may last several hours and typically involve a huge number of musicians. For such compositions, one often finds different arrangements and abridged versions (often lasting less than an hour), which can also be performed by smaller ensembles. Abridged versions still convey the flavor of the musical work containing the most important excerpts and melodies. In this paper, we consider the task of automatically segmenting an audio recording of a given version into semantically meaningful parts. Following previous work, the general strategy is to transfer a reference segmentation of the original complete work to the given version. Our main contribution is to show how this can be accomplished when dealing with strongly abridged versions. To this end, opposed to previously suggested segment-level matching procedures, we adapt a frame-level matching approach for transferring the reference segment information to the unknown version. Considering the opera “Der Freischütz” as an example scenario, we discuss how to balance out flexibility and robustness properties of our proposed frame-level segmentation procedure. © Thomas Prätzlich, Meinard Müller.","","Audio recordings; Information retrieval; Audio segmentation; Level matching; Robustness properties; Segmentation procedure; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Grohganz H.; Clausen M.; Müller M.","Grohganz, Harald (57209036668); Clausen, Michael (56225233200); Müller, Meinard (7404689873)","57209036668; 56225233200; 7404689873","Estimating musical time information from performed MIDI files","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059785340&partnerID=40&md5=3f45fd45cdf7f859dbb3ca23a8a7b5a1","Bonn University, Germany; International Audio Laboratories, Erlangen, Germany","Grohganz H., Bonn University, Germany; Clausen M., Bonn University, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","Even though originally developed for exchanging control commands between electronic instruments, MIDI has been used as quasi standard for encoding and storing score-related parameters. MIDI allows for representing musical time information as specified by sheet music as well as physical time information that reflects performance aspects. However, in many of the available MIDI files the musical beat and tempo information is set to a preset value with no relation to the actual music content. In this paper, we introduce a procedure to determine the musical beat grid from a given performed MIDI file. As one main contribution, we show how the global estimate of the time signature can be used to correct local errors in the pulse grid estimation. Different to MIDI quantization, where one tries to map MIDI note onsets onto a given musical pulse grid, our goal is to actually estimate such a grid. In this sense, our procedure can be used in combination with existing MIDI quantization procedures to convert performed MIDI files into semantically enriched score-like MIDI files. © Harald Grohganz, Michael Clausen, Meinard Müller.","","Electronics industry; Information retrieval; Control command; Electronic instruments; Global estimate; Local error; Music contents; Performance aspects; Physical time; Time information; Electronic musical instruments","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Cancino Chacón C.E.; Lattner S.; Grachten M.","Cancino Chacón, Carlos Eduardo (55968703500); Lattner, Stefan (56989751900); Grachten, Maarten (8974600000)","55968703500; 56989751900; 8974600000","Developing tonal perception through unsupervised learning","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058074677&partnerID=40&md5=bc189da49fa792e2350934ea65b9145c","Austrian Research Institute for Artificial Intelligence, Austria","Cancino Chacón C.E., Austrian Research Institute for Artificial Intelligence, Austria; Lattner S., Austrian Research Institute for Artificial Intelligence, Austria; Grachten M., Austrian Research Institute for Artificial Intelligence, Austria","The perception of tonal structure in music seems to be rooted both in low-level perceptual mechanisms and in enculturation, the latter accounting for cross-cultural differences in perceived tonal structure. Unsupervised machine learning methods are a powerful tool for studying how musical concepts may emerge from exposure to music. In this paper, we investigate to what degree tonal structure can be learned from musical data by unsupervised training of a Restricted Boltzmann Machine, a generative stochastic neural network. We show that even based on a limited set of musical data, the model learns several aspects of tonal structure. Firstly, the model learns an organization of musical material from different keys that conveys the topology of the circle of fifths (CoF). Although such a topology can be learned using principal component analysis (PCA) when using pitch-only representations, we found that using a pitch-duration representation impedes the extraction of the CoF topology much more for PCA than for the RBM. Furthermore, we replicate probe-tone experiments by Krumhansl and Shepard, measuring the organization of tones within a key in human perception. We find that the responses of the RBM share qualitative characteristics with those of both trained and untrained listeners. © Carlos Eduardo Cancino Chacón, Stefan Lattner, Maarten Grachten.","","Information retrieval; Machine learning; Stochastic systems; Topology; Cross-cultural differences; Musical materials; Perceptual mechanism; Qualitative characteristics; Restricted boltzmann machine; Stochastic neural network; Unsupervised machine learning; Unsupervised training; Principal component analysis","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Carabias-Orti J.J.; Rodriguez-Serrano F.J.; Vera-Candeas P.; Ruiz-Reyes N.; Cañadas-Quesada F.J.","Carabias-Orti, J.J. (25926912600); Rodriguez-Serrano, F.J. (36722395200); Vera-Candeas, P. (57216710350); Ruiz-Reyes, N. (8906410300); Cañadas-Quesada, F.J. (16174333600)","25926912600; 36722395200; 57216710350; 8906410300; 16174333600","An audio to score alignment framework using spectral factorization and dynamic time warping","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009484871&partnerID=40&md5=2d03876fd0b2ce167f87dad0a0f8b9df","Music Technology Group (MTG), Universitat Pompeu Fabra, Spain; Polytechnical School of Linares, Universidad de Jaen, Spain","Carabias-Orti J.J., Music Technology Group (MTG), Universitat Pompeu Fabra, Spain; Rodriguez-Serrano F.J., Polytechnical School of Linares, Universidad de Jaen, Spain; Vera-Candeas P., Polytechnical School of Linares, Universidad de Jaen, Spain; Ruiz-Reyes N., Polytechnical School of Linares, Universidad de Jaen, Spain; Cañadas-Quesada F.J., Polytechnical School of Linares, Universidad de Jaen, Spain","In this paper, we present an audio to score alignment framework based on spectral factorization and online Dynamic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factorization (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposition method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be interpreted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance. © J.J. Carabias-Orti, F.J. Rodriguez-Serrano, P. Vera-Candeas, N. Ruiz-Reyes, F.J. Cañadas-Quesada.","","Alignment; Factorization; Information retrieval; Signal processing; Basis functions; Dynamic time warping; Musical score; Nonnegative matrix factorization; Signal decomposition; Spectral factorizations; Spectral patterns; Spectrograms; Matrix algebra","J.J. Carabias-Orti; Music Technology Group (MTG), Universitat Pompeu Fabra, Spain; email: julio.carabias@upf.edu","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Srinivasamurthy A.; Holzapfel A.; Cemgil A.T.; Serra X.","Srinivasamurthy, Ajay (55583336200); Holzapfel, Andre (18041818000); Cemgil, Ali Taylan (15130945100); Serra, Xavier (55892979900)","55583336200; 18041818000; 15130945100; 55892979900","Particle filters for efficient meter tracking with dynamic Bayesian networks","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973317973&partnerID=40&md5=b2bbb4319247f3dcf342730d36b375bb","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey","Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey; Cemgil A.T., Dept. of Computer Engineering, Boğaziçi University, Istanbul, Turkey; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Recent approaches in meter tracking have successfully applied Bayesian models. While the proposed models can be adapted to different musical styles, the applicability of these flexible methods so far is limited because the application of exact inference is computationally demanding. More efficient approximate inference algorithms using particle filters (PF) can be developed to overcome this limitation. In this paper, we assume that the type of meter of a piece is known, and use this knowledge to simplify an existing Bayesian model with the goal of incorporating a more diverse observation model. We then propose Particle Filter based inference schemes for both the original model and the simplification. We compare the results obtained from exact and approximate inference in terms of meter tracking accuracy as well as in terms of computational demands. Evaluations are performed using corpora of Carnatic music from India and a collection of Ballroom dances. We document that the approximate methods perform similar to exact inference, at a lower computational cost. Furthermore, we show that the inference schemes remain accurate for long and full length recordings in Carnatic music. © Ajay Srinivasamurthy, Andre Holzapfel, Ali Taylan Cemgil, Xavier Serra.","","Distributed computer systems; Inference engines; Information retrieval; Monte Carlo methods; Approximate inference; Approximate methods; Computational costs; Computational demands; Dynamic Bayesian networks; Exact and approximate inferences; Observation model; Tracking accuracy; Bayesian networks","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Sigtia S.; Boulanger-Lewandowski N.; Dixon S.","Sigtia, Siddharth (56304321500); Boulanger-Lewandowski, Nicolas (45560980800); Dixon, Simon (7201479437)","56304321500; 45560980800; 7201479437","Audio chord recognition with a hybrid recurrent neural network","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","48","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971282541&partnerID=40&md5=3c29eee9fdbe01d45989f759384cd7dc","Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada","Sigtia S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Dixon S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom","In this paper, we present a novel architecture for audio chord estimation using a hybrid recurrent neural network. The architecture replaces hidden Markov models (HMMs) with recurrent neural network (RNN) based language models for modelling temporal dependencies between chords. We demonstrate the ability of feed forward deep neural networks (DNNs) to learn discriminative features directly from a time-frequency representation of the acoustic signal, eliminating the need for a complex feature extraction stage. For the hybrid RNN architecture, inference over the output variables of interest is performed using beam search. In addition to the hybrid model, we propose a modification to beam search using a hash table which yields improved results while reducing memory requirements by an order of magnitude, thus making the proposed model suitable for real-time applications. We evaluate our model's performance on a dataset with publicly available annotations and demonstrate that the performance is comparable to existing state of the art approaches for chord recognition. © Siddharth Sigtia, Nicolas Boulanger-Lewandowski, Simon Dixon.","","Deep neural networks; Feedforward neural networks; Hidden Markov models; Information retrieval; Modeling languages; Network architecture; Audio chord estimations; Discriminative features; Hidden markov models (HMMs); Memory requirements; Real-time application; Recurrent neural network (RNN); State-of-the-art approach; Time-frequency representations; Recurrent neural networks","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Giraud M.; Levé F.; Mercier F.; Rigaudière M.; Thorez D.","Giraud, Mathieu (8700367400); Levé, Florence (55893852300); Mercier, Florent (57210202635); Rigaudière, Marc (57210204716); Thorez, Donatien (57210212284)","8700367400; 55893852300; 57210202635; 57210204716; 57210212284","Towards modeling texture in symbolic data","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069909689&partnerID=40&md5=fe7ebf547c43889de89f5f47a8230b73","LIFL, CNRS, Univ. Lille 1, Lille 3, France; MIS, UPJV, Amiens LIFL, Univ. Lille 1, France; Univ. Lille 1, France; Univ. Lorraine, France","Giraud M., LIFL, CNRS, Univ. Lille 1, Lille 3, France; Levé F., MIS, UPJV, Amiens LIFL, Univ. Lille 1, France; Mercier F., Univ. Lille 1, France; Rigaudière M., Univ. Lorraine, France; Thorez D., Univ. Lille 1, France","Studying texture is a part of many musicological analy-ses. The change of texture plays an important role in the cognition of musical structures. Texture is a feature commonly used to analyze musical audio data, but it is rarely taken into account in symbolic studies. We propose to formalize the texture in classical Western instrumental music as melody and accompaniment layers, and provide an algorithm able to detect homorhythmic layers in polyphonic data where voices are not separated. We present an evaluation of these methods for parallel motions against a ground truth analysis of ten instrumental pieces, including the first movements of the six quatuors op. 33 by Haydn. © Mathieu Giraud, Florence Levé, Florent Mercier, Marc Rigaudière, Donatien Thorez.","","Audio acoustics; Information retrieval; Ground truth; Musical audio data; Musical structures; Parallel motion; Symbolic data; Textures","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Li P.-C.; Su L.; Yang Y.-H.; Su A.W.Y.","Li, Pei-Ching (57195930038); Su, Li (55966919100); Yang, Yi-Hsuan (55218558400); Su, Alvin W.Y. (8433191700)","57195930038; 55966919100; 55218558400; 8433191700","Analysis of expressive musical terms in violin using score-informed and expression-based audio features","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989244870&partnerID=40&md5=0fb9ce09a01d2453e9e9aa90343afc70","SCREAM Lab, Department of CSIE, National Cheng-Kung University, Taiwan; MAC Lab., CITI, Academia Sinica, Taiwan","Li P.-C., SCREAM Lab, Department of CSIE, National Cheng-Kung University, Taiwan; Su L., MAC Lab., CITI, Academia Sinica, Taiwan; Yang Y.-H., MAC Lab., CITI, Academia Sinica, Taiwan; Su A.W.Y., SCREAM Lab, Department of CSIE, National Cheng-Kung University, Taiwan","The manipulation of different interpretational factors, including dynamics, duration, and vibrato, constitutes the realization of different expressions in music. Therefore, a deeper understanding of the workings of these factors is critical for advanced expressive synthesis and computer-aided music education. In this paper, we propose the novel task of automatic expressive musical term classification as a direct means to study the interpretational factors. Specifically, we consider up to 10 expressive musical terms, such as Scherzando and Tranquillo, and compile a new dataset of solo violin excerpts featuring the realization of different expressive terms by different musicians for the same set of classical music pieces. Under a score-informed scheme, we design and evaluate a number of note-level features characterizing the interpretational aspects of music for the classification task. Our evaluation shows that the proposed features lead to significantly higher classification accuracy than a baseline feature set commonly used in music information retrieval tasks. Moreover, taking the contrast of feature values between an expressive and its corresponding non-expressive version (if given) of a music piece greatly improves the accuracy in classifying the presented expressive one. We also draw insights from analyzing the feature relevance and the class-wise accuracy of the prediction. © Pei-Ching Li, Li Su, Yi-Hsuan Yang, Alvin W. Y. Su. Li-.","","Audio acoustics; Computer aided instruction; Information retrieval; Classical musics; Classification accuracy; Classification tasks; Expressive synthesis; Feature relevance; Music education; Music information retrieval; Term classification; Classification (of information)","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kruspe A.M.","Kruspe, Anna M. (54971261400)","54971261400","Keyword spotting in a-capella singing","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048382424&partnerID=40&md5=8057ef3027929d15c1ed2febdbbfd961","Fraunhofer IDMT, Ilmenau, Germany; Johns Hopkins University, Baltimore, MD, United States","Kruspe A.M., Fraunhofer IDMT, Ilmenau, Germany, Johns Hopkins University, Baltimore, MD, United States","Keyword spotting (or spoken term detection) is an interesting task in Music Information Retrieval that can be applied to a number of problems. Its purposes include topical search and improvements for genre classification. Keyword spotting is a well-researched task on pure speech, but state-of-the-art approaches cannot be easily transferred to singing because phoneme durations have much higher variations in singing. To our knowledge, no keyword spotting system for singing has been presented yet. We present a keyword spotting approach based on keyword-filler Hidden Markov Models (HMMs) and test it on a-capella singing and spoken lyrics. We test Mel-Frequency Cepstral Coefficents (MFCCs), Perceptual Linear Predictive Features (PLPs), and Temporal Patterns (TRAPs) as front ends. These features are then used to generate phoneme posteriors using Multilayer Perceptrons (MLPs) trained on speech data. The phoneme posteriors are then used as the system input. Our approach produces useful results on a-capella singing, but depend heavily on the chosen keyword. We show that results can be further improved by training the MLP on a-capella data. We also test two post-processing methods on our phoneme posteriors before the keyword spotting step. First, we average the posteriors of all three feature sets. Second, we run the three concatenated posteriors through a fusion classifier. © Anna M. Kruspe.","","Information retrieval; Genre classification; Hidden markov models (HMMs); Keyword spotting systems; Multi-layer perceptrons (MLPs); Music information retrieval; Postprocessing methods; Spoken term detections; State-of-the-art approach; Hidden Markov models","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Chen C.-T.; Jang J.-S.R.; Lu C.-H.","Chen, Chun-Ta (54784056600); Jang, Jyh-Shing Roger (7402965041); Lu, Chun-Hung (56174622600)","54784056600; 7402965041; 56174622600","Improved query-by-tapping via tempo alignment","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069933648&partnerID=40&md5=e20460c5d29abdb76982f481b98791a7","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Taiwan University, Taipei, Taiwan; Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taiwan","Chen C.-T., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Department of Computer Science, National Taiwan University, Taipei, Taiwan; Lu C.-H., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taiwan","Query by tapping (QBT) is a content-based music retrieval method that can retrieve a song by taking the user’s tapping or clapping at the note onsets of the intended song in the database for comparison. This paper proposes a new query-by-tapping algorithm that aligns the IOI (inter-onset interval) vector of the query sequence with songs in the dataset by building an IOI ratio matrix, and then applies a dynamic programming (DP) method to compute the optimum path with minimum cost. Experiments on different datasets indicate that our algorithm outperforms other previous approaches in accuracy (top-10 and MRR), with a speedup factor of 3 in computation. With the advent of personal handheld devices, QBT provides an interesting and innovative way for music retrieval by shaking or tapping the devices, which is also discussed in the paper. © Chun-Ta Chen, Jyh-Shing Roger Jang, Chun-Hung Lu.","","Dynamic programming; Information retrieval; Content-based music retrieval; Hand held device; Inter onset intervals; Minimum cost; Music retrieval; Optimum paths; Query sequence; Speed-up factors; Query processing","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Cunningham S.J.; Nichols D.M.; Bainbridge D.; Ali H.","Cunningham, Sally Jo (7201937110); Nichols, David M. (10044366000); Bainbridge, David (8756864800); Ali, Hasan (57210205350)","7201937110; 10044366000; 8756864800; 57210205350","Social music in cars","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012024481&partnerID=40&md5=aabd53af66c77327c57bbef84bd4ccdc","Department of Computer Science, University of Waikato, New Zealand","Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand; Nichols D.M., Department of Computer Science, University of Waikato, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, New Zealand; Ali H., Department of Computer Science, University of Waikato, New Zealand","This paper builds an understanding of how music is currently experienced by a social group travelling together in a car—how songs are chosen for playing, how music both reflects and influences the group’s mood and social interaction, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of ethnographic data (participant observations and interviews) focusing primarily on the experience of in-car music on moderate length and long trips. We suggest features and functionality for music software to enhance the social experience when travelling in cars, and prototype and test a user interface based on design suggestions drawn from the data. © S.J. Cunningham, D.M. Nichols, D. Bainbridge, H. Ali.","","Information retrieval; Software testing; Design suggestions; Fine grained; Hardware/software; Participant observations; Qualitative analysis; Social groups; Social interactions; User interfaces","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Melenhorst M.S.; Liem C.C.S.","Melenhorst, Mark S. (55971366400); Liem, Cynthia C.S. (21733247100)","55971366400; 21733247100","Put the concert attendee in the spotlight. A user-centered design and development approach for classical concert applications","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978652176&partnerID=40&md5=6d7b6e97c2acf02270b5cae67e576e0e","Delft University of Technology, Multimedia Computing Group, Netherlands","Melenhorst M.S., Delft University of Technology, Multimedia Computing Group, Netherlands; Liem C.C.S., Delft University of Technology, Multimedia Computing Group, Netherlands","As the importance of real-life use cases in the music information retrieval (MIR) field is increasing, so does the importance of understanding user needs. The development of innovative real-life applications that draw on MIR technology requires a user-centered design and development approach that assesses user needs and aligns them with technological and academic ambitions in the MIR domain. In this paper we present such an approach, and apply it to the development of technological applications to enrich classical symphonic concerts. A user-driven approach is particularly important in this area, as orchestras need to innovate the concert experience to meet the needs and expectations of younger generations without alienating the current audience. We illustrate this approach with the results of five focus groups for three audience segments, which allow us to formulate informed user requirements for classical concert applications. © Mark S. Melenhorst, Cynthia C. S. Liem.","","Information retrieval; Focus groups; Music information retrieval; Real-life applications; Technological applications; User need; User requirements; User-driven approach; Younger generations; User centered design","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Caetano M.; Wiering F.","Caetano, Marcelo (8899555700); Wiering, Frans (8976178100)","8899555700; 8976178100","Theoretical framework of a computational model of auditory memory for music emotion recognition","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058018796&partnerID=40&md5=1af66fea94dcdc0bc1fc331af205bdf9","Sound and Music Computing Group INESC TEC, Porto, Portugal; Dep. Information and Computing Sciences, Utrecht University, Netherlands","Caetano M., Sound and Music Computing Group INESC TEC, Porto, Portugal; Wiering F., Dep. Information and Computing Sciences, Utrecht University, Netherlands","The bag of frames (BOF) approach commonly used in music emotion recognition (MER) has several limitations. The semantic gap is believed to be responsible for the glass ceiling on the performance of BOF MER systems. However, there are hardly any alternative proposals to address it. In this article, we introduce the theoretical framework of a computational model of auditory memory that incorporates temporal information into MER systems. We advocate that the organization of auditory memory places time at the core of the link between musical meaning and musical emotions. The main goal is to motivate MER researchers to develop an improved class of systems capable of overcoming the limitations of the BOF approach and coping with the inherent complexity of musical emotions. © Marcelo Caetano, Frans Wiering.","","Computation theory; Computational methods; Information retrieval; Semantics; Speech recognition; Auditory memory; Computational model; Glass ceiling; Inherent complexity; Music emotions; Musical emotion; Temporal information; Theoretical framework; Sensory perception","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Raffel C.; McFee B.; Humphrey E.J.; Salamon J.; Nieto O.; Liang D.; Ellis D.P.W.","Raffel, Colin (55354986300); McFee, Brian (34875379700); Humphrey, Eric J. (55060792500); Salamon, Justin (55184866100); Nieto, Oriol (55583364500); Liang, Dawen (55586031200); Ellis, Daniel P.W. (13609089200)","55354986300; 34875379700; 55060792500; 55184866100; 55583364500; 55586031200; 13609089200","mir_eval: A transparent implementation of common MIR metrics","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","313","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066108850&partnerID=40&md5=b973b7b12c48b2e2c72a5a1c10737e8b","LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Center for Jazz Studies, Columbia University, New York, United States; Music and Audio Research Lab, New York University, New York, United States; Center for Urban Science and Progress, New York University, New York, United States","Raffel C., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; McFee B., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States, Center for Jazz Studies, Columbia University, New York, United States; Humphrey E.J., Music and Audio Research Lab, New York University, New York, United States; Salamon J., Music and Audio Research Lab, New York University, New York, United States, Center for Urban Science and Progress, New York University, New York, United States; Nieto O., Music and Audio Research Lab, New York University, New York, United States; Liang D., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States","Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir_eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir_eval and quantitatively compare each to existing implementations. When the scores reported by mir_eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir_eval’s architecture, design, and intended use. © Colin Raffel, Brian McFee, Eric J. Humphrey, Justin Salamon, Oriol Nieto, Dawen Liang, Daniel P. W. Ellis.","","Information retrieval; Open systems; Extract informations; Music data; Open source software","C. Raffel; LabROSA, Dept. of Electrical Engineering, Columbia University, New York, United States; email: craffel@gmail.com","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Abeßer J.; Cano E.; Frieler K.; Pfleiderer M.; Zaddach W.-G.","Abeßer, Jakob (36607532300); Cano, Estefanía (51161075800); Frieler, Klaus (32667638400); Pfleiderer, Martin (57094855700); Zaddach, Wolf-Georg (57189304661)","36607532300; 51161075800; 32667638400; 57094855700; 57189304661","Score-informed analysis of intonation and pitch modulation in jazz solos","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007458423&partnerID=40&md5=7ca526a1da69865f17d31fbf91ff8e8f","Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany","Abeßer J., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany, Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Cano E., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Frieler K., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; Pfleiderer M., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; Zaddach W.-G., Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany","The paper presents new approaches for analyzing the characteristics of intonation and pitch modulation of woodwind and brass solos in jazz recordings. To this end, we use score-informed analysis techniques for source separation and fundamental frequency tracking. After splitting the audio into a solo and a backing track, a reference tuning frequency is estimated from the backing track. Next, we compute the fundamental frequency contour for each tone in the solo and a set of features describing its temporal shape. Based on this data, we first investigate, whether the tuning frequencies of jazz recordings changed over the decades of the last century. Second, we analyze whether the intonation is artist-specific. Finally, we examine how the modulation frequency of vibrato tones depends on contextual parameters such as pitch, duration, and tempo as well as the performing artist. © Jakob Abeßer, Estefanía Cano, Klaus Frieler, Martin Pfleiderer, Wolf-Georg Zaddach.","","Information retrieval; Modulation; Natural frequencies; Source separation; Analysis techniques; Fundamental frequencies; Fundamental frequency contour; Modulation frequencies; New approaches; Temporal shape; Tuning frequency; Audio recordings","J. Abeßer; Jazzomat Research Project, University of Music Franz Liszt, Weimar, Germany; email: jakob.abesser@idmt.fraunhofer.de","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Bittner R.; Salamon J.; Tierney M.; Mauch M.; Cannam C.; Bello J.","Bittner, Rachel (55659619600); Salamon, Justin (55184866100); Tierney, Mike (56769171100); Mauch, Matthias (36461512900); Cannam, Chris (36730835200); Bello, Juan (7102889110)","55659619600; 55184866100; 56769171100; 36461512900; 36730835200; 7102889110","MedleyDB: A multitrack dataset for annotation-intensive MIR research","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","233","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057325970&partnerID=40&md5=ff8c9aa9802f15136c013065e48c5c5f","Music and Audio Research Lab, New York University, United States; Center for Urban Science and Progress, New York University, United States; Centre for Digital Music, Queen Mary University of London, United Kingdom","Bittner R., Music and Audio Research Lab, New York University, United States; Salamon J., Music and Audio Research Lab, New York University, United States, Center for Urban Science and Progress, New York University, United States; Tierney M., Music and Audio Research Lab, New York University, United States; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Cannam C., Centre for Digital Music, Queen Mary University of London, United Kingdom; Bello J., Music and Audio Research Lab, New York University, United States","We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research. © Rachel Bittner, Justin Salamon, Mike Tierney, Matthias Mauch, Chris Cannam, Juan Bello.","","Extraction; Information retrieval; Source separation; Curation; Current test; Gain insight; Instrument recognition; Melody extractions; Multi-track recording; State of the art; Statistical tests","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Moore J.L.; Joachims T.; Turnbull D.","Moore, Joshua L. (54995503400); Joachims, Thorsten (6602804136); Turnbull, Douglas (8380095700)","54995503400; 6602804136; 8380095700","Taste space versus the world: An embedding analysis of listening habits and geography","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016290977&partnerID=40&md5=199525de708141f58f17ab9f5e2ee4c8","Cornell University, Dept. of Computer Science, United States; Ithaca College, Dept. of Computer Science, United States","Moore J.L., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","Probabilistic embedding methods provide a principled way of deriving new spatial representations of discrete objects from human interaction data. The resulting assignment of objects to positions in a continuous, low-dimensional space not only provides a compact and accurate predictive model, but also a compact and flexible representation for understanding the data. In this paper, we demonstrate how probabilistic embedding methods reveal the “taste space” in the recently released Million Musical Tweets Dataset (MMTD), and how it transcends geographic space. In particular, by embedding cities around the world along with preferred artists, we are able to distill information about cultural and geographical differences in listening patterns into spatial representations. These representations yield a similarity metric among city pairs, artist pairs, and city-artist pairs, which can then be used to draw conclusions about the similarities and contrasts between taste space and geographic location. © Joshua L. Moore, Thorsten Joachims, Douglas Turnbull.","","Information retrieval; Discrete objects; Embedding method; Geographic location; Human interactions; Low-dimensional spaces; Predictive modeling; Similarity metrics; Spatial representations; Embeddings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Oramas S.; Gómez F.; Gómez E.; Mora J.","Oramas, Sergio (55582112200); Gómez, Francisco (57196622778); Gómez, Emilia (14015483200); Mora, Joaquín (55582857600)","55582112200; 57196622778; 14015483200; 55582857600","FlaBase: Towards the creation of a flamenco music knowledge base","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995603955&partnerID=40&md5=4c2924a968d3a2ae0d826f4c2314d22f","Music Technology Group, Universitat Pompeu Fabra, Spain; Technical University of Madrid, Spain; Faculty of Psychology, University of Sevilla, Spain","Oramas S., Music Technology Group, Universitat Pompeu Fabra, Spain; Gómez F., Technical University of Madrid, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Mora J., Faculty of Psychology, University of Sevilla, Spain","Online information about flamenco music is scattered over different sites and knowledge bases. Unfortunately, there is no common repository that indexes all these data. In this work, information related to flamenco music is gathered from general knowledge bases (e.g., Wikipedia, DBpedia), music encyclopedias (e.g., MusicBrainz), and specialized flamenco websites, and is then integrated into a new knowledge base called FlaBase. As resources from different data sources do not share common identifiers, a process of pair-wise entity resolution has been performed. FlaBase contains information about 1,174 artists, 76 palos (flamenco genres), 2,913 albums, 14,078 tracks, and 771 Andalusian locations. It is freely available in RDF and JSON formats. In addition, a method for entity recognition and disambiguation for FlaBase has been created. The system can recognize and disambiguate FlaBase entity references in Spanish texts with an f-measure value of 0.77. We applied it to biographical texts present in Flabase. By using the extracted information, the knowledge base is populated with relevant information and a semantic graph is created connecting the entities of FlaBase. Artists relevance is then computed over the graph and evaluated according to a flamenco expert criteria. Accuracy of results shows a high degree of quality and completeness of the knowledge base. © Sergio Oramas1, Francisco Gómez2, Emilia Gómez1, Joaquín Mora3.","","Information retrieval; Semantic Web; Semantics; Entity recognition; Entity resolutions; General knowledge; Knowledge base; Knowledge basis; Music knowledge; On-line information; Semantic graphs; Knowledge based systems","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Porter A.; Bogdanov D.; Kaye R.; Tsukanov R.; Serra X.","Porter, Alastair (55582507300); Bogdanov, Dmitry (35748642000); Kaye, Robert (57210235695); Tsukanov, Roman (57210232437); Serra, Xavier (55892979900)","55582507300; 35748642000; 57210235695; 57210232437; 55892979900","AcousticBrainz: A community platform for gathering music information obtained from audio","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994009403&partnerID=40&md5=0f61acabc67e81bf4551601e5312e0f2","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; MetaBrainz Foundation, United States","Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain, MetaBrainz Foundation, United States; Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Kaye R., MetaBrainz Foundation, United States; Tsukanov R., MetaBrainz Foundation, United States; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We introduce the AcousticBrainz project, an open platform for gathering music information. At its core, AcousticBrainz is a database of music descriptors computed from audio recordings using a number of state-of-the-art Music Information Retrieval algorithms. Users run a supplied feature extractor on audio files and upload the analysis results to the AcousticBrainz server. All submissions include a MusicBrainz identifier allowing them to be linked to various sources of editorial information. The feature extractor is based on the open source Essentia audio analysis library. From the data submitted by the community, we run classifiers aimed at adding musically relevant semantic information. These classifiers can be developed by the community using tools available on the AcousticBrainz website. All data in AcousticBrainz is freely available and can be accessed through the website or API. For AcousticBrainz to be successful we need to have an active community that contributes to and uses this platform, and it is this community that will define the actual uses and applications of its data. © Alastair Porter, Dmitry Bogdanov, Robert Kaye, Roman Tsukanov, Xavier Serra.","","Audio recordings; Classification (of information); Information retrieval; Semantics; Websites; Audio analysis; Feature extractor; Music information; Music information retrieval; Number of state; Open platforms; Open sources; Semantic information; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Prockup M.; Ehmann A.F.; Gouyon F.; Schmidt E.M.; Celma O.; Kim Y.E.","Prockup, Matthew (37089471200); Ehmann, Andreas F. (8988651500); Gouyon, Fabien (8373002800); Schmidt, Erik M. (36053813000); Celma, Oscar (12804596800); Kim, Youngmoo E. (24724623000)","37089471200; 8988651500; 8373002800; 36053813000; 12804596800; 24724623000","Modeling genre with the music genome project: Comparing human-labeled attributes and audio features","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977927583&partnerID=40&md5=ce355609fcb2a44d34f48f37c8595115","Drexel University, United States; Pandora Media Inc., United States","Prockup M., Drexel University, United States, Pandora Media Inc., United States; Ehmann A.F., Pandora Media Inc., United States; Gouyon F., Pandora Media Inc., United States; Schmidt E.M., Pandora Media Inc., United States; Celma O., Pandora Media Inc., United States; Kim Y.E., Drexel University, United States","Genre provides one of the most convenient categorizations of music, but it is often regarded as a poorly defined or largely subjective musical construct. In this work, we provide evidence that musical genres can to a large extent be objectively modeled via a combination of musical attributes. We employ a data-driven approach utilizing a subset of 48 hand-labeled musical attributes comprising instrumentation, timbre, and rhythm across more than one million examples from Pandora® Internet Radio’s Music Genome Project®. A set of audio features motivated by timbre and rhythm are then implemented to model genre both directly and through audio-driven models derived from the hand-labeled musical attributes. In most cases, machine learning models built directly from hand-labeled attributes outperform models based on audio features. Among the audio-based models, those that combine audio features and learned musical attributes perform better than those derived from audio features alone. © Matthew Prockup, Andreas F. Ehmann, Fabien Gouyon Erik M. Schmidt, Oscar Celma, and Youngmoo E. Kim.","","Computer music; Genes; Information retrieval; Audio features; Audio-based; Data-driven approach; Genome projects; Internet radio; Machine learning models; Musical genre; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"van den Oord A.; Dieleman S.; Schrauwen B.","van den Oord, Aäron (55329476300); Dieleman, Sander (55418865400); Schrauwen, Benjamin (22941905500)","55329476300; 55418865400; 22941905500","Transfer learning by supervised pre-training for audio-based music classification","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","51","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055025883&partnerID=40&md5=1cddf939824e402ab2094ea05ed49b5c","Electronics and Information Systems department, Ghent University, Belgium","van den Oord A., Electronics and Information Systems department, Ghent University, Belgium; Dieleman S., Electronics and Information Systems department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems department, Ghent University, Belgium","Very few large-scale music research datasets are publicly available. There is an increasing need for such datasets, because the shift from physical to digital distribution in the music industry has given the listener access to a large body of music, which needs to be cataloged efficiently and be easily browsable. Additionally, deep learning and feature learning techniques are becoming increasingly popular for music information retrieval applications, and they typically require large amounts of training data to work well. In this paper, we propose to exploit an available large-scale music dataset, the Million Song Dataset (MSD), for classification tasks on other datasets, by reusing models trained on the MSD for feature extraction. This transfer learning approach, which we refer to as supervised pre-training, was previously shown to be very effective for computer vision problems. We show that features learned from MSD audio fragments in a supervised manner, using tag labels and user listening data, consistently outperform features learned in an unsupervised manner in this setting, provided that the learned feature extractor is of limited complexity. We evaluate our approach on the GTZAN, 1517-Artists, Unique and Magnatagatune datasets. © Aäron van den Oord, Sander Dieleman, Benjamin Schrauwen.","","Audio acoustics; Deep learning; Information retrieval; Large dataset; Classification tasks; Computer vision problems; Digital distribution; Feature extractor; Feature learning; Music classification; Music information retrieval; Transfer learning; Classification (of information)","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Repetto R.C.; Serra X.","Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57200495266; 55892979900","Creating a corpus of Jingju (Beijing opera) music and possibilities for melodic analysis","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054970902&partnerID=40&md5=8dc605e6fcd9f6fc3a299e530aecea02","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Jingju (Beijing opera) is a Chinese traditional performing art form in which theatrical and musical elements are intimately combined. As an oral tradition, its musical dimension is the result of the application of a series of predefined conventions and it offers unique concepts for musicological research. Computational analyses of jingju music are still scarce, and only a few studies have dealt with it from an MIR perspective. In this paper we present the creation of a corpus of jingju music in the framework of the CompMusic project that is formed by audio, editorial metadata, lyrics and scores. We discuss the criteria followed for the acquisition of the data, describe the content of the corpus, and evaluate its suitability for computational and musicological research. We also identify several research problems that can take advantage of this corpus in the context of computational musicology, especially for melodic analysis, and suggest approaches for future work. © Rafael Caro Repetto, Xavier Serra.","","Information retrieval; Computational analysis; Oral tradition; Performing arts; Research problems; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Molina E.; Barbancho A.M.; Tardón L.J.; Barbancho I.","Molina, Emilio (55967369200); Barbancho, Ana M. (6602362530); Tardón, Lorenzo J. (6602405058); Barbancho, Isabel (6602638932)","55967369200; 6602362530; 6602405058; 6602638932","Evaluation framework for automatic singing transcription","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038600955&partnerID=40&md5=1f50c7f63d1f5383d86e2565349328a9","Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","Molina E., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho A.M., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Tardón L.J., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain; Barbancho I., Universidad de Málaga, ATIC Research Group, Andalucía Tech, ETSI Telecomunicación, Campus de Teatinos s/n, Málaga, 29071, Spain","In this paper, we analyse the evaluation strategies used in previous works on automatic singing transcription, and we present a novel, comprehensive and freely available evaluation framework for automatic singing transcription. This framework consists of a cross-annotated dataset and a set of extended evaluation measures, which are integrated in a Matlab toolbox. The presented evaluation measures are based on standard MIREX note-tracking measures, but they provide extra information about the type of errors made by the singing transcriber. Finally, a practical case of use is presented, in which the evaluation framework has been used to perform a comparison in detail of several state-of-the-art singing transcribers. © Emilio Molina, Ana M. Barbancho, Lorenzo J. Tardón, Isabel Barbancho.","","Information retrieval; Evaluation framework; Evaluation measures; Evaluation strategies; Matlab toolboxes; State of the art; Transcription","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Silva D.F.; Rossi R.G.; Rezende S.O.; Batista G.E.A.P.A.","Silva, Diego F. (55585876200); Rossi, Rafael G. (37013907300); Rezende, Solange O. (7005154055); Batista, Gustavo E.A.P.A. (55062789000)","55585876200; 37013907300; 7005154055; 55062789000","Music classification by transductive learning using bipartite heterogeneous networks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055483672&partnerID=40&md5=f58b5becf1836e07d357c9d6d2fb9dd5","Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Rossi R.G., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Rezende S.O., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil","The popularization of music distribution in electronic format has increased the amount of music with incomplete metadata. The incompleteness of data can hamper some important tasks, such as music and artist recommendation. In this scenario, transductive classification can be used to classify the whole dataset considering just few labeled instances. Usually transductive classification is performed through label propagation, in which data are represented as networks and the examples propagate their labels through their connections. Similarity-based networks are usually applied to model data as network. However, this kind of representation requires the definition of parameters, which significantly affect the classification accuracy, and presents a high cost due to the computation of similarities among all dataset instances. In contrast, bipartite heterogeneous networks have appeared as an alternative to similarity-based networks in text mining applications. In these networks, the words are connected to the documents which they occur. Thus, there is no parameter or additional costs to generate such networks. In this paper, we propose the use of the bipartite network representation to perform transductive classification of music, using a bag-of-frames approach to describe music signals. We demonstrate that the proposed approach outperforms other music classification approaches when few labeled instances are available. © Diego F. Silva, Rafael G. Rossi, Solange O. Rezende, Gustavo E. A. P. A. Batista.","","Electronic musical instruments; Heterogeneous networks; Information retrieval; Additional costs; Bipartite network; Classification accuracy; Electronic formats; Label propagation; Music classification; Music distribution; Transductive learning; Classification (of information)","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Khlif A.; Sethu V.","Khlif, Anis (57210232867); Sethu, Vidhyasaharan (16239802100)","57210232867; 16239802100","An iterative multi range non-negative matrix factorization algorithm for polyphonic music transcription","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010872946&partnerID=40&md5=479f3f1c46eb392b6f1ebbcfd25decb8","École des Mines ParisTech, France; University of New South Wales, Australia","Khlif A., École des Mines ParisTech, France; Sethu V., University of New South Wales, Australia","This article presents a novel iterative algorithm based on Non-negative Matrix Factorisation (NMF) that is particularly well suited to the task of automatic music transcription (AMT). Compared with previous NMF based techniques, this one does not aim at factorizing the time-frequency representation of the entire musical signal into a combination of the possible set of notes. Instead, the proposed algorithm proceeds iteratively by initially decomposing a part of the time-frequency representation into a combination of a small subset of all possible notes then reinvesting this information in the following step involving a large subset of notes. Specifically, starting with the lowest octave of notes that is of interest, each iteration increases the set of notes under consideration by an octave. The resolution of a lower dimensionality problem used to properly initialize matrices for a more complex problem, results in a gain of some percent in the transcription accuracy. © Anis Khlif, Vidhyasaharan Sethu.","","Factorization; Information retrieval; Iterative methods; Matrix algebra; Transcription; Automatic music transcription; Complex problems; Iterative algorithm; Musical signals; Non-negative matrix factorisation; Non-negative matrix factorization algorithms; Polyphonic music; Time-frequency representations; Computer music","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Laaksonen A.","Laaksonen, Antti (36696605500)","36696605500","Automatic melody transcription based on chord transcription","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057932221&partnerID=40&md5=6a3c2fa9a77dc6bb8c040a52d5207b0e","Department of Computer Science, University of Helsinki, Finland","Laaksonen A., Department of Computer Science, University of Helsinki, Finland","This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord transcription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To evaluate the algorithm, we present a new ground truth dataset that consists of 1,5 hours of audio excerpts together with hand-made melody and chord transcriptions. © Antti Laaksonen.","","Audio acoustics; Information retrieval; Ground-truth dataset; Hand made; Transcription","A. Laaksonen; Department of Computer Science, University of Helsinki, Finland; email: ahslaaks@cs.helsinki.fi","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Gulati S.; Serrà J.; Serra X.","Gulati, Sankalp (37087243200); Serrà, Joan (35749172500); Serra, Xavier (55892979900)","37087243200; 35749172500; 55892979900","Improving melodic similarity in Indian art music using culture-specific melodic characteristics","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973306872&partnerID=40&md5=4ba8c9daad88a1f314ee406b2b2401d1","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Telefonica Research, Barcelona, Spain","Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Telefonica Research, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Detecting the occurrences of rāgs’ characteristic melodic phrases from polyphonic audio recordings is a fundamental task for the analysis and retrieval of Indian art music. We propose an abstraction process and a complexity weighting scheme which improve melodic similarity by exploiting specific melodic characteristics in this music. In addition, we propose a tetrachord normalization to handle transposed phrase occurrences. The melodic abstraction is based on the partial transcription of the steady regions in the melody, followed by a duration truncation step. The proposed complexity weighting accounts for the differences in the melodic complexities of the phrases, a crucial aspect known to distinguish phrases in Carnatic music. For evaluation we use over 5 hours of audio data comprising 625 annotated melodic phrases belonging to 10 different phrase categories. Results show that the proposed melodic abstraction and complexity weighting schemes significantly improve the phrase detection accuracy, and that tetrachord normalization is a successful strategy for dealing with transposed phrase occurrences in Carnatic music. In the future, it would be worthwhile to explore the applicability of the proposed approach to other melody dominant music traditions such as Flamenco, Beijing opera and Turkish Makam music. © Sankalp Gulati, Joan Serrà and Xavier Serra.","","Abstracting; Audio acoustics; Information retrieval; Abstraction process; Audio data; Detection accuracy; Melodic similarity; Steady region; Turkishs; Weighting scheme; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Huang P.-S.; Kim M.; Hasegawa-Johnson M.; Smaragdis P.","Huang, Po-Sen (47061231500); Kim, Minje (13204925700); Hasegawa-Johnson, Mark (6602106855); Smaragdis, Paris (6507529311)","47061231500; 13204925700; 6602106855; 6507529311","Singing-voice separation from monaural recordings using deep recurrent neural networks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","99","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046988721&partnerID=40&md5=3286c4aa59dbb85dc35f4de680ef36d8","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States; Department of Computer Science, University of Illinois at Urbana-Champaign, United States; Adobe Research, United States","Huang P.-S., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States; Kim M., Department of Computer Science, University of Illinois at Urbana-Champaign, United States; Hasegawa-Johnson M., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States; Smaragdis P., Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, United States, Department of Computer Science, University of Illinois at Urbana-Champaign, United States, Adobe Research, United States","Monaural source separation is important for many real world applications. It is challenging since only single channel information is available. In this paper, we explore using deep recurrent neural networks for singing voice separation from monaural recordings in a supervised setting. Deep recurrent neural networks with different temporal connections are explored. We propose jointly optimizing the networks for multiple source signals by including the separation step as a nonlinear operation in the last layer. Different discriminative training objectives are further explored to enhance the source to interference ratio. Our proposed system achieves the state-of-the-art performance, 2.30~2.48 dB GNSDR gain and 4.32~5.42 dB GSIR gain compared to previous models, on the MIR-1K dataset. © Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis.","","Audio recordings; Deep neural networks; Information retrieval; Separation; Source separation; Discriminative training; Interference ratio; Multiple source; Nonlinear operation; Real-world; Singing voice separations; Single channels; State-of-the-art performance; Recurrent neural networks","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Li B.; Duan Z.","Li, Bochen (57191924185); Duan, Zhiyao (24450312900)","57191924185; 24450312900","Score following for piano performances with sustain-pedal effects","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994840537&partnerID=40&md5=e5f093b7fab1c17cb6857d149512a4a5","Audio Information Research (AIR) Lab, University of Rochester, Department of Electrical and Computer Engineering, United States","Li B., Audio Information Research (AIR) Lab, University of Rochester, Department of Electrical and Computer Engineering, United States; Duan Z., Audio Information Research (AIR) Lab, University of Rochester, Department of Electrical and Computer Engineering, United States","One challenge in score following (i.e., mapping audio frames to score positions in real time) for piano performances is the mismatch between audio and score caused by the usage of the sustain pedal. When the pedal is pressed, notes played will continue to sound until the string vibration naturally ceases. This makes the notes longer than their notated lengths and overlap with later notes. In this paper, we propose an approach to address this problem. Given that the most competitive wrong score positions for each audio frame are the ones before the correct position due to the sustained sounds, we remove partials of sustained notes and only retain partials of “new notes” in the audio representation. This operation reduces sustain-pedal effects by weakening the match between the audio frame and previous wrong score positions, hence encourages the system to align to the correct score position. We implement this idea based on a state-of-the-art score following framework. Experiments on synthetic and real piano performances from the MAPS dataset show significant improvements on both alignment accuracy and robustness. © Bochen Li, Zhiyao Duan.","","Information retrieval; Musical instruments; Alignment accuracy; Audio frames; Audio representation; Real time; Score-following; State of the art; String vibrations; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Zhang S.; Repetto R.C.; Serra X.","Zhang, Shuo (57195678445); Repetto, Rafael Caro (57200495266); Serra, Xavier (55892979900)","57195678445; 57200495266; 55892979900","Study of the similarity between linguistic tones and melodic pitch contours in Beijing opera singing","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054973049&partnerID=40&md5=228c40edda7e083d8ffaa0ccdeb1f1a0","Music Technology Group, Universitat Pompeu Fabra, Spain","Zhang S., Music Technology Group, Universitat Pompeu Fabra, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Spain","Features of linguistic tone contours are important factors that shape the distinct melodic characteristics of different genres of Chinese opera. In Beijing opera, the presence of a two-dialectal tone system makes the tone-melody relationship more complex. In this paper, we propose a novel data-driven approach to analyze syllable-sized tone-pitch contour similarity in a corpus of Beijing Opera (381 arias) with statistical modeling and machine learning methods. A total number of 1,993 pitch contour units and attributes were extracted from a selection of 20 arias. We then build Smoothing Spline ANOVA models to compute matrixes of average melodic contour curves by tone category and other attributes. A set of machine learning and statistical analysis methods are applied to 30-point pitch contour vectors as well as dimensionality-reduced representations using Symbolic Aggregate approXimation(SAX). The results indicate an even mixture of shapes within all tone categories, with the absence of evidence for a predominant dialectal tone system in Beijing opera. We discuss the key methodological issues in melody-tone analysis and future work on pair-wise contour unit analysis. © Shuo Zhang, Rafael Caro Repetto, Xavier Serra.","","Curve fitting; Information retrieval; Machine learning; Statistical methods; Contour curves; Data-driven approach; Machine learning methods; Reduced representation; Smoothing spline; Statistical analysis methods; Statistical modeling; Symbolic aggregate approximation (SAX); Linguistics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Duan Z.; Temperley D.","Duan, Zhiyao (24450312900); Temperley, David (6602761758)","24450312900; 6602761758","Note-level music transcription by maximum likelihood sampling","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028047705&partnerID=40&md5=840f7bf0ae4a4813c4ca9b68e9397e69","University of Rochester, Dept. Electrical and Computer Engineering, United States; University of Rochester, Eastman School of Music, United States","Duan Z., University of Rochester, Dept. Electrical and Computer Engineering, United States; Temperley D., University of Rochester, Eastman School of Music, United States","Note-level music transcription, which aims to transcribe note events (often represented by pitch, onset and offset times) from music audio, is an important intermediate step towards complete music transcription. In this paper, we present a note-level music transcription system, which is built on a state-of-the-art frame-level multi-pitch estimation (MPE) system. Preliminary note-level transcription achieved by connecting pitch estimates into notes often lead to many spurious notes due to MPE errors. In this paper, we propose to address this problem by randomly sampling notes in the preliminary note-level transcription. Each sample is a subset of all notes and is viewed as a note-level transcription candidate. We evaluate the likelihood of each candidate using the MPE model, and select the one with the highest likelihood as the final transcription. The likelihood treats notes in a transcription as a whole and favors transcriptions with less spurious notes. Experiments conducted on 110 pieces of J.S. Bach chorales with polyphony from 2 to 4 show that the proposed sampling scheme significantly improves the transcription performance from the preliminary approach. The proposed system also significantly outperforms two other state-of-the-art systems in both frame-level and note-level transcriptions. © Zhiyao Duan, David Temperley.","","Audio acoustics; Information retrieval; Maximum likelihood estimation; Multi-pitch estimations; Music transcription; Preliminary approach; Sampling schemes; State of the art; State-of-the-art system; Transcription","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Yadati K.; Larson M.; Liem C.C.S.; Hanjalic A.","Yadati, Karthik (54929907400); Larson, Martha (56574709600); Liem, Cynthia C.S. (21733247100); Hanjalic, Alan (6701775211)","54929907400; 56574709600; 21733247100; 6701775211","Detecting drops in electronic dance music: Content based approaches to a socially significant music event","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040762482&partnerID=40&md5=e3cb4bc799c93725825a40cea266d950","Delft University of Technology, Netherlands","Yadati K., Delft University of Technology, Netherlands; Larson M., Delft University of Technology, Netherlands; Liem C.C.S., Delft University of Technology, Netherlands; Hanjalic A., Delft University of Technology, Netherlands","Electronic dance music (EDM) is a popular genre of music. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the number of users who leave comments in the general neighborhood of drop events in music on online audio distribution platforms like SoundCloud. The variability that characterizes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage approach to drop detection that first models the sound characteristics during drop events and then incorporates temporal structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the proposed method for combining content analysis and social activity to detect events in music recordings. © Karthik Yadati, Martha Larson, Cynthia C. S. Liem, Alan Hanjalic.","","Audio acoustics; Drops; Electronic musical instruments; Information retrieval; Content analysis; Content-based approach; Drop detection; Music recording; Social activities; Temporal structures; Two stage approach; Watershed moment; Audio recordings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Flexer A.","Flexer, Arthur (7004555682)","7004555682","On inter-rater agreement in audio music similarity","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066081175&partnerID=40&md5=2a280609331eb39d08c77e862bae746d","Austrian Research Institute for Artificial Intelligence (OFAI), Freyung 6/6, Vienna, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Freyung 6/6, Vienna, Austria","One of the central tasks in the annual MIREX evaluation campaign is the”Audio Music Similarity and Retrieval (AMS)” task. Songs which are ranked as being highly similar by algorithms are evaluated by human graders as to how similar they are according to their subjective judgment. By analyzing results from the AMS tasks of the years 2006 to 2013 we demonstrate that: (i) due to low inter-rater agreement there exists an upper bound of performance in terms of subjective gradings; (ii) this upper bound has already been achieved by participating algorithms in 2009 and not been surpassed since then. Based on this sobering result we discuss ways to improve future evaluations of audio music similarity. © Arthur Flexer.","","Information retrieval; Audio music; Inter-rater agreements; Upper Bound; Audio acoustics","A. Flexer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Freyung 6/6, Austria; email: arthur.flexer@ofai.at","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Aljanaki A.; Wiering F.; Veltkamp R.C.","Aljanaki, Anna (56410808700); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","56410808700; 8976178100; 7003421646","Computational modeling of induced emotion using GEMs","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061439478&partnerID=40&md5=42961bd554eb36c17bb947e82c45695d","Utrecht University, Netherlands","Aljanaki A., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","Most researchers in the automatic music emotion recognition field focus on the two-dimensional valence and arousal model. This model though does not account for the whole diversity of emotions expressible through music. Moreover, in many cases it might be important to model induced (felt) emotion, rather than perceived emotion. In this paper we explore a multidimensional emotional space, the Geneva Emotional Music Scales (GEMS), which addresses these two issues. We collected the data for our study using a game with a purpose. We exploit a comprehensive set of features from several state-of-the-art toolboxes and propose a new set of harmonically motivated features. The performance of these feature sets is compared. Additionally, we use expert human annotations to explore the dependency between musicologically meaningful characteristics of music and emotional categories of GEMS, demonstrating the need for algorithms that can better approximate human perception. © Anna Aljanaki, Frans Wiering, Remco C. Veltkamp.","","Gems; Information retrieval; Computational model; Feature sets; Game with a purpose; Human annotations; Human perception; Music emotions; State of the art; Behavioral research","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Van Balen J.; Bountouridis D.; Wiering F.; Veltkamp R.","Van Balen, Jan (55874307000); Bountouridis, Dimitrios (36607463900); Wiering, Frans (8976178100); Veltkamp, Remco (7003421646)","55874307000; 36607463900; 8976178100; 7003421646","Cognition-inspired descriptors for scalable cover song retrieval","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042128674&partnerID=40&md5=617dd7fe7bfd2979d24ba7388351d043","Utrecht University, Department of Information and Computing Sciences, Netherlands","Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Bountouridis D., Utrecht University, Department of Information and Computing Sciences, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R., Utrecht University, Department of Information and Computing Sciences, Netherlands","Inspired by representations used in music cognition studies and computational musicology, we propose three simple and interpretable descriptors for use in mid- to high-level computational analysis of musical audio and applications in content-based retrieval. We also argue that the task of scalable cover song retrieval is very suitable for the development of descriptors that effectively capture musical structures at the song level. The performance of the proposed descriptions in a cover song problem is presented. We further demonstrate that, due to the musically-informed nature of the descriptors, an independently established model of stability and variation in covers songs can be integrated to improve performance. © Jan Van Balen, Dimitrios Bountouridis, Frans Wiering, Remco Veltkamp.","","Content based retrieval; Computational analysis; Cover songs; Descriptors; Improve performance; Musical audio; Musical structures; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Wu B.; Horner A.; Lee C.","Wu, Bin (55943143800); Horner, Andrew (56244632900); Lee, Chung (55600992100)","55943143800; 56244632900; 55600992100","Emotional predisposition of musical instrument timbres with static spectra","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059071611&partnerID=40&md5=fe360ea5d2d9349d0ed929defd8e400f","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Information Systems Technology and Design Pillar Singapore University of Technology and Design, 20 Dover Drive, Singapore, 138682, Singapore","Wu B., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Horner A., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Lee C., Information Systems Technology and Design Pillar Singapore University of Technology and Design, 20 Dover Drive, Singapore, 138682, Singapore","Music is one of the strongest triggers of emotions. Recent studies have shown strong emotional predispositions for musical instrument timbres. They have also shown significant correlations between spectral centroid and many emotions. Our recent study on spectral centroid-equalized tones further suggested that the even/odd harmonic ratio is a salient timbral feature after attack time and brightness. The emergence of the even/odd harmonic ratio motivated us to go a step further: to see whether the spectral shape of musical instruments alone can have a strong emotional predisposition. To address this issue, we conducted followup listening tests of static tones. The results showed that the even/odd harmonic ratio again significantly correlated with most emotions, consistent with the theory that static spectral shapes have a strong emotional predisposition. © Bin Wu, Andrew Horner, Chung Lee.","","Harmonic analysis; Information retrieval; Harmonic ratios; Listening tests; Spectral shapes; Musical instruments","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Raffel C.; Ellis D.P.W.","Raffel, Colin (55354986300); Ellis, Daniel P.W. (13609089200)","55354986300; 13609089200","Large-scale content-based matching of MIDI and audio files","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973305560&partnerID=40&md5=47ef0b0e05129933690fac22a297835b","LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States","Raffel C., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Department of Electrical Engineering, Columbia University, New York, NY, United States","MIDI files, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks. We present a system which can efficiently match and align MIDI files to entries in a large corpus of audio content based solely on content, i.e., without using any metadata. The core of our approach is a convolutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efficiently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI files to the Million Song Dataset. © Colin Raffel, Daniel P. W. Ellis.","","Audio recordings; Information retrieval; Vector spaces; Audio content; Content-based matching; Convolutional networks; Cross modality; Feature matrices; Hamming space; Large-scale dynamics; Music information retrieval; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Deruty E.; Pachet F.","Deruty, Emmanuel (35503270700); Pachet, François (6701441655)","35503270700; 6701441655","The MIR perspective on the evolution of dynamics in mainstream music","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958070138&partnerID=40&md5=f9612257f816fc4e47aa305d771be059","Sony Computer Science Laboratory, Akoustic Arts, Paris, France; Sony Computer Science Laboratory, Paris, France","Deruty E., Sony Computer Science Laboratory, Akoustic Arts, Paris, France; Pachet F., Sony Computer Science Laboratory, Paris, France","Understanding the evolution of mainstream music is of high interest for the music production industry. In this context, we argue that a MIR perspective may be used to highlight, in particular, relations between dynamics and various properties of mainstream music. We illustrate this claim with two results obtained from a diachronic analysis performed on 7200 tracks released between 1967 and 2014. This analysis suggests that 1) the so-called “loudness war” has peaked in 2007, and 2) its influence has been important enough to override the impact of genre on dynamics. In other words, dynamics in mainstream music are primarily related to a track’s year of release, rather than to its genre. © Emmanuel Deruty François Pachet.","","Information retrieval; Music production; ON dynamics; Dynamics","E. Deruty; Sony Computer Science Laboratory, Akoustic Arts, Paris, France; email: emmanuel.deruty@gmail.com","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Böck S.; Krebs F.; Widmer G.","Böck, Sebastian (55413719000); Krebs, Florian (7006192702); Widmer, Gerhard (7004342843)","55413719000; 7006192702; 7004342843","Accurate tempo estimation based on recurrent neural networks and resonating comb filters","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994647104&partnerID=40&md5=e8f4b49f80e46cc2d230aecd9eb2d267","Department of Computational Perception Johannes Kepler University, Linz, Austria","Böck S., Department of Computational Perception Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception Johannes Kepler University, Linz, Austria","In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike existing (comb filter based) approaches, we do not use handcrafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as input to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing different accent bands, modelling metrical relations, categoris-ing the excerpts into slow / fast or any other advanced processing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator’s histogram peak. © Sebastian Böck, Florian Krebs and Gerhard Widmer.","","Filter banks; Information retrieval; Recurrent neural networks; Audio signal; Multiple time scale; Post processing; State-of-the-art performance; Tempo estimations; Comb filters","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Singhi A.; Brown D.G.","Singhi, Abhishek (57210207767); Brown, Daniel G. (55738804200)","57210207767; 55738804200","Are poetry and lyrics all that different?","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015318084&partnerID=40&md5=1a08f3f257826cea72ba7ab74f102866","University of Waterloo, Cheriton School of Computer Science, Canada","Singhi A., University of Waterloo, Cheriton School of Computer Science, Canada; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada","We hypothesize that different genres of writing use different adjectives for the same concept. We test our hypothesis on lyrics, articles and poetry. We use the English Wikipedia and over 13,000 news articles from four leading newspapers for the article data set. Our lyrics data set consists of lyrics of more than 10,000 songs by 56 popular English singers, and our poetry dataset is made up of more than 20,000 poems from 60 famous poets. We find the probability distribution of synonymous adjectives in all the three different categories and use it to predict if a document is an article, lyrics or poetry given its set of adjectives. We achieve an accuracy level of 67% for lyrics, 80% for articles and 57% for poetry. Using these probability distribution we show that adjectives more likely to be used in lyrics are more rhymable than those more likely to be used in poetry, but they do not differ significantly in their semantic orientations. Furthermore we show that our algorithm is successfully able to detect poetic lyricists like Bob Dylan from non-poetic ones like Bryan Adams, as their lyrics are more often misclassified as poetry. © Abhishek Singhi, Daniel G. Brown.","","Information retrieval; Semantics; Accuracy level; Data set; News articles; Semantic orientation; Wikipedia; Probability distributions","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Krebs F.; Böck S.; Widmer G.","Krebs, Florian (7006192702); Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","7006192702; 55413719000; 7004342843","An efficient state-space model for joint tempo and meter tracking","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993966282&partnerID=40&md5=7325f53ec11ca914dd4090ea59834890","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Dynamic Bayesian networks (e.g., Hidden Markov Models) are popular frameworks for meter tracking in music because they are able to incorporate prior knowledge about the dynamics of rhythmic parameters (tempo, meter, rhythmic patterns, etc.). One popular example is the bar pointer model, which enables joint inference of these rhythmic parameters from a piece of music. While this allows the mutual dependencies between these parameters to be exploited, it also increases the computational complexity of the models. In this paper, we propose a new state-space discretisation and tempo transition model for this class of models that can act as a drop-in replacement and not only increases the beat and downbeat tracking accuracy, but also reduces time and memory complexity drastically. We incorporate the new model into two state-of-the-art beat and meter tracking systems, and demonstrate its superiority to the original models on six datasets. © Florian Krebs, Sebastian Böck, and Gerhard Widmer.","","Bayesian networks; Complex networks; Information retrieval; State space methods; Dynamic Bayesian networks; Memory complexity; Mutual dependencies; Rhythmic patterns; State - space models; Tracking accuracy; Tracking system; Transition model; Hidden Markov models","F. Krebs; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: florian.krebs@jku.at","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Martorell A.; Gómez E.","Martorell, Agustín (41661868300); Gómez, Emilia (14015483200)","41661868300; 14015483200","Systematic multi-scale set-class analysis","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069932760&partnerID=40&md5=14a5ec53f7660749ccc3cb887fe3763b","Universitat Pompeu Fabra, Spain","Martorell A., Universitat Pompeu Fabra, Spain; Gómez E., Universitat Pompeu Fabra, Spain","This work reviews and elaborates a methodology for hierarchical multi-scale set-class analysis of music pieces. The method extends the systematic segmentation and representation of Sapp’s ‘keyscapes’ to the description stage, by introducing a set-class level of description. This provides a systematic, mid-level, and standard analytical lexicon, which allows the description of any notated music based on fixed temperaments. The method benefits from the representation completeness, the compromise between generalisation and discrimination of the set-class spaces, and the access to hierarchical inclusion relations over time. The proposed class-matrices are multidimensional time series encoding the pitch content of every possible music segment over time, regardless the involved time-scales, in terms of a given set-class space. They provide the simplest information mining methods with the ability of capturing sophisticated tonal relations. The proposed class-vectors, quantifying the presence of every possible set-class in a piece, are discussed for advanced explorations of corpora. The compromise between dimensionality and informativeness provided by the class-matrices and class-vectors, is discussed in relation with standard content-based tonal descriptors, and music information retrieval applications. © Agustín Martorell, Emilia Gómez.","","Mining; Class analysis; Generalisation; Inclusion relation; Information mining; Informative ness; Multidimensional time series; Music information retrieval; Music segments; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Su L.; Yu L.-F.; Yang Y.-H.","Su, Li (55966919100); Yu, Li-Fan (56304353100); Yang, Yi-Hsuan (55218558400)","55966919100; 56304353100; 55218558400","Sparse cepstral and phase codes for guitar playing technique classification","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034833748&partnerID=40&md5=4f96ecf3c4fee9765023a51cc375f1b5","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yu L.-F., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Automatic recognition of guitar playing techniques is challenging as it is concerned with subtle nuances of guitar timbres. In this paper, we investigate this research problem by a comparative study on the performance of features extracted from the magnitude spectrum, cepstrum and phase derivatives such as group-delay function (GDF) and instantaneous frequency deviation (IFD) for classifying the playing techniques of electric guitar recordings. We consider up to 7 distinct playing techniques of electric guitar and create a new individual-note dataset comprising of 7 types of guitar tones for each playing technique. The dataset contains 6,580 clips and 11,928 notes. Our evaluation shows that sparse coding is an effective means of mining useful patterns from the primitive time-frequency representations and that combining the sparse representations of logarithm cepstrum, GDF and IFD leads to the highest average F-score of 71.7%. Moreover, from analyzing the confusion matrices we find that cepstral and phase features are particularly important in discriminating highly similar techniques such as pull-off, hammer-on and bending. We also report a preliminary study that demonstrates the potential of the proposed methods in automatic transcription of real-world electric guitar solos. © Li Su, Li-Fan Yu and Yi-Hsuan Yang.","","Group delay; Information retrieval; Automatic recognition; Automatic transcription; Comparative studies; Group delay functions; Instantaneous frequency; Magnitude spectrum; Sparse representation; Time-frequency representations; Musical instruments","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Venkataramani S.; Nayak N.; Rao P.; Velmurugan R.","Venkataramani, Shrikant (57194871179); Nayak, Nagesh (55213440900); Rao, Preeti (35180193500); Velmurugan, Rajbabu (55793189564)","57194871179; 55213440900; 35180193500; 55793189564","Vocal separation using singer-vowel priors obtained from polyphonic audio","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028075336&partnerID=40&md5=cda8a5804e210621a9c075796e040d0f","Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India; Sensibol Audio Technologies Pvt. Ltd, India","Venkataramani S., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India; Nayak N., Sensibol Audio Technologies Pvt. Ltd, India; Rao P., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India; Velmurugan R., Department of Electrical Engineering, IIT Bombay, Mumbai, 400076, India","Single-channel methods for the separation of the lead vocal from mixed audio have traditionally included harmonic-sinusoidal modeling and matrix decomposition methods, each with its own strengths and shortcomings. In this work we use a hybrid framework to incorporate prior knowledge about singer and phone identity to achieve the superior separation of the lead vocal from the instrumental background. Singer specific dictionaries learned from available polyphonic recordings provide the soft mask that effectively attenuates the bleeding-through of accompanying melodic instruments typical of purely harmonic-sinusoidal model based separation. The dictionary learning uses NMF optimization across a training set of mixed signal utterances while keeping the vocal signal bases constant across the utterances. A soft mask is determined for each test mixed utterance frame by imposing sparseness constraints in the NMF partial co-factorization. We demonstrate significant improvements in reconstructed signal quality arising from the more accurate estimation of singer-vowel spectral envelope. © Shrikant Venkataramani, Nagesh Nayak, Preeti Rao, Rajbabu Velmurugan.","","Information retrieval; Linguistics; Accurate estimation; Dictionary learning; Hybrid framework; Matrix decomposition; Single channel methods; Sinusoidal model; Sparseness constraints; Spectral envelopes; Separation","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Kong L.W.; Lee T.","Kong, Lam Wang (55954212400); Lee, Tan (7501439194)","55954212400; 7501439194","Automatic key partition based on tonal organization information of classical music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069902172&partnerID=40&md5=2fbe87c813b48244b8ee7772acbbb010","Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong SAR, China","Kong L.W., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong SAR, China; Lee T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong SAR, China","Key information is a useful information for tonal music analysis. It is related to chord progressions, which follows some specific structures and rules. In this paper, we describe a generative account of chord progression consisting of phrase-structure grammar rules proposed by Martin Rohrmeier. With some modifications, these rules can be used to partition a chord symbol sequence into different key areas, if modulation occurs. Exploiting tonal grammar rules, the most musically sensible key partition of chord sequence is derived. Some examples of classical music excerpts are evaluated. This rule-based system is compared against another system which is based on dynamic programming of harmonic-hierarchy information. Using Kostka-Payne corpus as testing data, the experimental result shows that our system is better in terms of key detection accuracy. © Lam Wang Kong, Tan Lee.","","Dynamic programming; Information retrieval; Chord sequence; Classical musics; Detection accuracy; Grammar rules; IF modulation; Phrase structure grammars; Symbol sequences; Testing data; Information use","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Nunes L.; Rocamora M.; Jure L.; Biscainho L.W.P.","Nunes, Leonardo (25641670100); Rocamora, Martín (55347707700); Jure, Luis (55583332100); Biscainho, Luiz W.P. (6603071473)","25641670100; 55347707700; 55583332100; 6603071473","Beat and downbeat tracking based on rhythmic patterns applied to the Uruguayan candombe drumming","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002676697&partnerID=40&md5=e56ce4af9cd9ce309df113a4ab2f97fb","ATL-Brazil, Microsoft, United States; Universidad de la República, Uruguay; Federal Univ. of Rio de Janeiro, Brazil","Nunes L., ATL-Brazil, Microsoft, United States; Rocamora M., Universidad de la República, Uruguay; Jure L., Universidad de la República, Uruguay; Biscainho L.W.P., Federal Univ. of Rio de Janeiro, Brazil","Computational analysis of the rhythmic/metrical structure of music from recorded audio is a hot research topic in music information retrieval. Recent research has explored the explicit modeling of characteristic rhythmic patterns as a way to improve upon existing beat-tracking algorithms, which typically fail on dealing with syncopated or polyrhythmic music. This work takes the Uruguayan Candombe drumming (an afro-rooted rhythm from Latin America) as a case study. After analyzing the aspects that make this music genre troublesome for usual algorithmic approaches and describing its basic rhythmic patterns, the paper proposes a supervised scheme for rhythmic pattern tracking that aims at finding the metric structure from a Candombe recording, including beat and downbeat phases. Then it evaluates and compares the performance of the method with those of general-purpose beat-tracking algorithms through a set of experiments involving a database of annotated recordings totaling over two hours of audio. The results of this work reinforce the advantages of tracking rhythmic patterns (possibly learned from annotated music) when it comes to automatically following complex rhythms. A software implementation of the proposal as well as the annotated database utilized are available to the research community with the publication of this paper. © Leonardo Nunes, Martín Rocamora, Luis Jure, Luiz W. P. Biscainho.","","Audio acoustics; Computer music; Information retrieval; Tracking (position); Algorithmic approach; Annotated database; Computational analysis; Hot research topics; Music information retrieval; Recent researches; Research communities; Software implementation; Audio recordings","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Hu X.; Lee J.H.; Wong L.K.Y.","Hu, Xiao (55496358400); Lee, Jin Ha (57190797465); Wong, Leanne Ka Yan (57210214529)","55496358400; 57190797465; 57210214529","Music information behaviors and system preferences of university students in Hong kong","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064493843&partnerID=40&md5=b260bb6975a173ac63d67be73d48687b","University of Hong Kong, Hong Kong; University of Washington, United States","Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Wong L.K.Y., University of Hong Kong, Hong Kong","This paper presents a user study on music information needs and behaviors of university students in Hong Kong. A mix of quantitative and qualitative methods was used. A survey was completed by 101 participants and supplemental interviews were conducted in order to investigate users’ music information related activities. We found that university students in Hong Kong listened to music frequently and mainly for the purposes of entertainment, singing and playing instruments, and stress reduction. This user group often searches for music with multiple methods, but common access points like genre and time period were rarely used. Sharing music with people in their online social networks such as Facebook and Weibo was a common activity. Furthermore, the popularity of smartphones prompted the need for streaming music and mobile music applications. We also examined users’ preferences on music services available in Hong Kong such as YouTube and KKBox, as well as the characteristics liked and disliked by the users. The results not only offer insights into non-Western users’ music behaviors but also for designing online music services for young music listeners in Hong Kong. © Xiao Hu, Jin Ha Lee, Leanne Ka Yan Wong.","","Information retrieval; Social networking (online); Students; Access points; Multiple methods; Music information; On-line social networks; Online music services; Quantitative and qualitative methods; Stress reduction; University students; Surveys","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Glazyrin N.","Glazyrin, Nikolay (56300572100)","56300572100","Towards automatic content-based separation of DJ mixes into single tracks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028080646&partnerID=40&md5=280ef71c6d671032fc33da09c7c2d458","Ural Federal University, Russian Federation","Glazyrin N., Ural Federal University, Russian Federation","DJ mixes and radio show recordings constitute an important and underexploited music and data source. In this paper we try to approach the problem of separation of a continuous DJ mix into single tracks or timestamping a mix. Sharing some aspects with the task of structural segmentation, this problem has a number of distinctive features that make difficulties for structural segmentation algorithms designed to work with a single track. We use the information derived from spectrum data to separate tracks from each other. We show that the metadata that usually comes with DJ mixes can be exploited to improve the separation. An iterative algorithm that can consider both content-based data and user provided metadata is proposed and evaluated on a collection of freely available timestamped DJ mix recordings of various styles. © Nikolay Glazyrin.","","Audio recordings; Information retrieval; Iterative methods; Metadata; Automatic content; Content-based; Data-source; Iterative algorithm; Segmentation algorithms; Single-tracks; Timestamping; Separation","N. Glazyrin; Ural Federal University, Russian Federation; email: nglazyrin@gmail.com","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Morchid M.; Dufour R.; Linarès G.","Morchid, Mohamed (55697911300); Dufour, Richard (26664481000); Linarès, Georges (6603063660)","55697911300; 26664481000; 6603063660","A combined thematic and acoustic approach for a music recommendation service in TV commercials","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069922280&partnerID=40&md5=65c828cea61032ac69b1efcf372ea329","LIA - University of Avignon, France","Morchid M., LIA - University of Avignon, France; Dufour R., LIA - University of Avignon, France; Linarès G., LIA - University of Avignon, France","Most of modern advertisements contain a song to illustrate the commercial message. The success of a product, and its economic impact, can be directly linked to this choice. Finding the most appropriate song is usually made manually. Nonetheless, a single person is not able to listen and choose the best music among millions. The need for an automatic system for this particular task becomes increasingly critical. This paper describes the LIA music recommendation system for advertisements using both textual and acoustic features. This system aims at providing a song to a given commercial video and was evaluated in the context of the MediaEval 2013 Soundtrack task [14]. The goal of this task is to predict the most suitable soundtrack from a list of candidate songs, given a TV commercial. The organizers provide a development dataset including multimedia features. The initial assumption of the proposed system is that commercials which sell the same type of product, should also share the same music rhythm. A two-fold system is proposed: find commercials with close subjects in order to determine the mean rhythm of this subset, and then extract, from the candidate songs, the music which better corresponds to this mean rhythm. © Mohamed Morchid, Richard Dufour, Georges Linarès.","","Sound recording; Acoustic approaches; Acoustic features; Automatic systems; Commercial messages; Commercial video; Multimedia features; Music recommendation; Music Recommendation System; Recommender systems","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Thompson L.; Mauch M.; Dixon S.","Thompson, Lucas (57210212432); Mauch, Matthias (36461512900); Dixon, Simon (7201479437)","57210212432; 36461512900; 7201479437","Drum transcription via classification of bar-level rhythmic patterns","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019997798&partnerID=40&md5=1b07bda9bdf7489f1d4312e33f0a5f1f","Centre for Digital Music, Queen Mary University of London, United Kingdom","Thompson L., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We propose a novel method for automatic drum transcription from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognising individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhythmic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six different drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly below that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment. © Lucas Thompson, Matthias Mauch and Simon Dixon.","","Information retrieval; Statistical tests; Support vector machines; Transcription; Acoustic performance; Audio similarities; Classified pattern; Learned patterns; Musical rhythm; Precision and recall; Rhythmic patterns; Synthesised; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Stephen Downie J.; Hu X.; Lee J.H.; Choi K.; Cunningham S.J.; Hao Y.","Stephen Downie, J. (7102932568); Hu, Xiao (55496358400); Lee, Jin Ha (57190797465); Choi, Kahyun (56452038000); Cunningham, Sally Jo (7201937110); Hao, Yun (56147494900)","7102932568; 55496358400; 57190797465; 56452038000; 7201937110; 56147494900","Ten years of MIREX: Reflections, challenges and opportunities","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047950429&partnerID=40&md5=7b78b146f0a38913fc16ed09ed07bd77","University of Illinois, United States; University of Hong Kong, Hong Kong; University of Washington, United States; University of Waikato, New Zealand","Stephen Downie J., University of Illinois, United States; Hu X., University of Hong Kong, Hong Kong; Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Cunningham S.J., University of Waikato, New Zealand; Hao Y., University of Illinois, United States","The Music Information Retrieval Evaluation eXchange (MIREX) has been run annually since 2005, with the October 2014 plenary marking its tenth iteration. By 2013, MIREX has evaluated approximately 2000 individual music information retrieval (MIR) algorithms for a wide range of tasks over 37 different test collections. MIREX has involved researchers from over 29 different countries with a median of 109 individual participants per year. This paper summarizes the history of MIREX from its earliest planning meeting in 2001 to the present. It reflects upon the administrative, financial, and technological challenges MIREX has faced and describes how those challenges have been surmounted. We propose new funding models, a distributed evaluation framework, and more holistic user experience evaluation tasks—some evolutionary, some revolutionary—for the continued success of MIREX. We hope that this paper will inspire MIR community members to contribute their ideas so MIREX can have many more successful years to come. © J. Stephen Downie, Xiao Hu, Jin Ha Lee, Kahyun Choi, Sally Jo Cunningham, Yun Hao.","","Information retrieval; Iterative methods; Distributed evaluation; Music information retrieval; Planning meetings; Technological challenges; Test Collection; User experience evaluations; Insecticides","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Driedger J.; Prätzlich T.; Müller M.","Driedger, Jonathan (55582371700); Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873)","55582371700; 55582692100; 7404689873","Let it bee – Towards NMF-inspired audio mosaicing","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984816257&partnerID=40&md5=d963ac73d91ff56dc9b27709bf2fafd6","International Audio Laboratories, Erlangen, Germany","Driedger J., International Audio Laboratories, Erlangen, Germany; Prätzlich T., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","A swarm of bees buzzing “Let it be” by the Beatles or the wind gently howling the romantic “Gute Nacht” by Schubert – these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automatically generating audio mosaics with the objective to preserve the source’s timbre in the mosaic. Inspired by algorithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target recording. However, when applying the original NMF procedure, the resulting mosaic does not adequately reflect the source’s timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source’s timbral characteristics in the resulting mosaic. © Jonathan Driedger, Thomas Prätzlich, Meinard Müller.","","Audio recordings; Chemical activation; Factorization; Information retrieval; Iterative methods; Matrix algebra; Spectrographs; Activation matrices; Diagonal structure; Iterative learning; Mosaicing; Nonnegative matrix factorization; Spectrograms; Technical contribution; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Miron M.; Carabias-Orti J.J.; Janer J.","Miron, Marius (55585881400); Carabias-Orti, Julio José (25926912600); Janer, Jordi (35068089300)","55585881400; 25926912600; 35068089300","Audio-to-score alignment at note level for orchestral recordings","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013215748&partnerID=40&md5=839ebd2df3e20ed8f17ca8168962c425","Music Technology Group, Universitat Pompeu Fabra, Spain","Miron M., Music Technology Group, Universitat Pompeu Fabra, Spain; Carabias-Orti J.J., Music Technology Group, Universitat Pompeu Fabra, Spain; Janer J., Music Technology Group, Universitat Pompeu Fabra, Spain","In this paper we propose an offline method for refining audio-to-score alignment at the note level in the context of orchestral recordings. State-of-the-art score alignment systems estimate note onsets with a low time resolution, and without detecting note offsets. For applications such as score-informed source separation we need a precise alignment at note level. Thus, we propose a novel method that refines alignment by determining the note onsets and offsets in complex orchestral mixtures by combining audio and image processing techniques. First, we introduce a note-wise pitch salience function that weighs the harmonic contribution according to the notes present in the score. Second, we perform image binarization and blob detection based on connectivity rules. Then, we pick the best combination of blobs, using dynamic programming. We finally obtain onset and offset times from the boundaries of the most salient blob. We evaluate our method on a dataset of Bach chorales, showing that the proposed approach can accurately estimate note onsets and offsets. © Marius Miron, Julio José Carabias-Orti, Jordi Janer.","","Audio recordings; Dynamic programming; Image processing; Information retrieval; Source separation; Alignment system; Image binarization; Image processing technique; Off-line methods; Precise alignments; Score-informed source separations; State of the art; Time resolution; Alignment","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Zhou X.; Lerch A.","Zhou, Xinquan (57210231875); Lerch, Alexander (22034963000)","57210231875; 22034963000","Chord detection using deep learning","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","47","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978854736&partnerID=40&md5=9dc00781fcbe961953f8c3677d3d5eb0","Center for Music Technology, Georgia Institute of Technology, Georgia","Zhou X., Center for Music Technology, Georgia Institute of Technology, Georgia; Lerch A., Center for Music Technology, Georgia Institute of Technology, Georgia","In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and configurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classification. © Xinquan Zhou, Alexander Lerch.","","Information retrieval; Network architecture; High-level features; Learning to learn; Pre-processing; State-of-the-art system; Deep learning","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Arzt A.; Widmer G.","Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","36681791200; 7004342843","Real-time music tracking using multiple performances as a reference","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006887569&partnerID=40&md5=fe87ae4ac4b7adacb32503edb7ceacd8","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In general, algorithms for real-time music tracking directly use a symbolic representation of the score, or a synthesised version thereof, as a reference for the on-line alignment process. In this paper we present an alternative approach. First, different performances of the piece in question are collected and aligned (off-line) to the symbolic score. Then, multiple instances of the on-line tracking algorithm (each using a different performance as a reference) are used to follow the live performance, and their output is combined to come up with the current position in the score. As the evaluation shows, this strategy improves both the robustness and the precision, especially on pieces that are generally hard to track (e.g. pieces with extreme, abrupt tempo changes, or orchestral pieces with a high degree of polyphony). Finally, we describe a real-world application, where this music tracking algorithm was used to follow a world-famous orchestra in a concert hall in order to show synchronised visual content (the sheet music, explanatory text and videos) to members of the audience. © Andreas Arzt, Gerhard Widmer.","","Tracking (position); Concert hall; Multiple instances; On-line tracking; Real-world; Symbolic representation; Synthesised; Tracking algorithm; Visual content; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Duval E.; van Berchum M.; Jentzsch A.; Parra Chico G.A.; Drakos A.","Duval, Erik (7006487422); van Berchum, Marnix (56175640800); Jentzsch, Anja (57217945793); Parra Chico, Gonzalo Alberto (57210235904); Drakos, Andreas (24398372400)","7006487422; 56175640800; 57217945793; 57210235904; 24398372400","Musicology of early music with europeana tools and services","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005994076&partnerID=40&md5=2083b0f5d422abc02e1062aa1dfe5f32","Dept. of Computer Science, KU Leuven, B, Belgium; KNAW-DANS, Utrecht University, Netherlands; Open Knowledge Foundation, D, United Kingdom; AgroKnow, Greece","Duval E., Dept. of Computer Science, KU Leuven, B, Belgium; van Berchum M., KNAW-DANS, Utrecht University, Netherlands; Jentzsch A., Open Knowledge Foundation, D, United Kingdom; Parra Chico G.A., Dept. of Computer Science, KU Leuven, B, Belgium; Drakos A., AgroKnow, Greece","The Europeana repository hosts large collections of digitized music manuscripts and prints. This paper investigates how tools and services for this repository can enable Early Music musicologists to carry out their research in a more effective or efficient way, or to carry out research that is impossible to do without such tools or services. We report on the methodology, user-centered development of a suite of tools that we have integrated loosely, in order to experiment with this specific target audience and an evaluation of the impact that such tools may have on how these musicologists carry out their research. Positive feedback relates to the automation of data sharing between the loosely coupled tools and support for an integrated workflow. Participants in this study wanted to have the ability to work not only with individual items, but also with collections of such items. The use of search facets to filter, and visualization around time and place were positively evaluated, as was the use of Optical Music Recognition and computer-supported analysis of music scores. The musicologists were not convinced of the value of activity streams. They also wanted a less strictly linear organization of their workflow and the ability to not only consume items from the repository, but to also push their research results back into the Europeana repository. © Erik Duval, Marnix van Berchum, Anja Jentzsch,.","","Activity streams; Integrated workflow; Linear organization; Loosely coupled; Optical music recognition; Research results; Target audience; User-centered development; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Kaneshiro B.; Dmochowski J.P.","Kaneshiro, Blair (56884405500); Dmochowski, Jacek P. (35742934900)","56884405500; 35742934900","Neuroimaging methods for music information retrieval: Current findings and future prospects","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988033894&partnerID=40&md5=cbb74b49b792297b64ef608db72b973d","Center for Computer Research in Music and Acoustics, Stanford University, Stanford, CA, United States; Department of Psychology, Stanford University, Stanford, CA, United States","Kaneshiro B., Center for Computer Research in Music and Acoustics, Stanford University, Stanford, CA, United States; Dmochowski J.P., Department of Psychology, Stanford University, Stanford, CA, United States","Over the past decade and a half, music information retrieval (MIR) has grown into a robust, cross-disciplinary field spanning a variety of research domains. Collaborations between MIR and neuroscience researchers, however, are still rare, and to date only a few studies using approaches from one domain have successfully reached an audience in the other. In this paper, we take an initial step toward bridging these two fields by reviewing studies from the music neuroscience literature, with an emphasis on imaging modalities and analysis techniques that might be of practical interest to the MIR community. We show that certain approaches currently used in a neuroscientific setting align with those used in MIR research, and discuss implications for potential areas of future research. We additionally consider the impact of disparate research objectives between the two fields, and how such a discrepancy may have hindered cross-discipline output thus far. It is hoped that a heightened awareness of this literature will foster interaction and collaboration between MIR and neuroscience researchers, leading to advances in both fields that would not have been achieved independently. © Blair Kaneshiro, Jacek P. Dmochowski.","","Neuroimaging; Neurophysiology; Analysis techniques; Cross-disciplinary; Cross-disciplines; Future prospects; Imaging modality; Music information retrieval; Neuroscience literature; Research objectives; Information retrieval","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Church M.; Cuthbert M.S.","Church, Maura (57210204014); Cuthbert, Michael Scott (15724599200)","57210204014; 15724599200","Improving rhythmic transcriptions via probability models applied post-OMR","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056669908&partnerID=40&md5=c82ec08e4cfcfba1887746c10ff635b8","Applied Math, Harvard University, Google Inc., United States; Music and Theater Arts, M.I.T, United States","Church M., Applied Math, Harvard University, Google Inc., United States; Cuthbert M.S., Music and Theater Arts, M.I.T, United States","Despite many improvements in the recognition of graphical elements, even the best implementations of Optical Music Recognition (OMR) introduce inaccuracies in the resultant score. These errors, particularly rhythmic errors, are time consuming to fix. Most musical compositions repeat rhythms between parts and at various places throughout the score. Information about rhythmic self-similarity, however, has not previously been used in OMR systems. This paper describes and implements methods for using the prior probabilities for rhythmic similarities in scores produced by a commercial OMR system to correct rhythmic errors which cause a contradiction between the notes of a measure and the underlying time signature. Comparing the OMR output and post-correction results to hand-encoded scores of 37 polyphonic pieces and movements (mostly drawn from the classical repertory), the system reduces incorrect rhythms by an average of 19% (min: 2%, max: 36%). The paper includes a public release of an implementation of the model in music21 and also suggests future refinements and applications to pitch correction that could further improve the accuracy of OMR systems. © Maura Church, Michael Scott Cuthbert.","","Information retrieval; Graphical elements; Musical composition; Optical music recognition; Prior probability; Probability models; Self-similarities; Errors","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Tang Z.; Black D.A.A.","Tang, Zheng (57189590145); Black, Dawn A.A. (55257515800)","57189590145; 55257515800","Melody extraction from polyphonic audio of Western opera: A method based on detection of the singer’s formant","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020986484&partnerID=40&md5=80f85bb14fad117b82da6f96357f21b2","University of Washington, Department of Electrical Engineering, United States; Queen Mary University of London, Electronic Engineering and Computer Science, United Kingdom","Tang Z., University of Washington, Department of Electrical Engineering, United States; Black D.A.A., Queen Mary University of London, Electronic Engineering and Computer Science, United Kingdom","Current melody extraction approaches perform poorly on the genre of opera [1, 2]. The singer’s formant is defined as a prominent spectral-envelope peak around 3 kHz found in the singing of professional Western opera singers [3]. In this paper we introduce a novel melody extraction algorithm based on this feature for opera signals. At the front end, it automatically detects the singer’s formant according to the Long-Term Average Spectrum (LTAS). This detection function is also applied to the short-term spectrum in each frame to determine the melody. The Fan Chirp Transform (FChT) [4] is used to compute pitch salience as its high time-frequency resolution overcomes the difficulties introduced by vibrato. Subharmonic attenuation is adopted to handle octave errors which are common in opera vocals. We improve the FChT algorithm so that it is capable of correcting outliers in pitch detection. The performance of our method is compared to 5 state-of-the-art melody extraction algorithms on a newly created dataset and parts of the ADC2004 dataset. Our algorithm achieves an accuracy of 87.5% in singer’s formant detection. In the evaluation of melody extraction, it has the best performance in voicing detection (91.6%), voicing false alarm (5.3%) and overall accuracy (82.3%). © Zheng Tang, Dawn A. A. Black.","","Information retrieval; Based on detections; Detection functions; Evaluation of Melodies; Melody extractions; Overall accuracies; Short-term spectrum; Spectral envelopes; Time-frequency resolution; Extraction","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Schramm R.; de Souza Nunes H.; Jung C.R.","Schramm, Rodrigo (56259281600); de Souza Nunes, Helena (57192811941); Jung, Cláudio Rosito (7402016327)","56259281600; 57192811941; 7402016327","Automatic solfège assessment","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008164342&partnerID=40&md5=da36930e755400f57b9be45ab213c3fe","Institute of Informatics, Federal University of Rio Grande do Sul, Brazil; Department of Music, Federal University of Rio Grande do Sul of Music, Brazil","Schramm R., Institute of Informatics, Federal University of Rio Grande do Sul, Brazil; de Souza Nunes H., Department of Music, Federal University of Rio Grande do Sul of Music, Brazil; Jung C.R., Institute of Informatics, Federal University of Rio Grande do Sul, Brazil","This paper presents a note-by-note approach for automatic solfège assessment. The proposed system uses melodic transcription techniques to extract the sung notes from the audio signal, and the sequence of melodic segments is subsequently processed by a two stage algorithm. On the first stage, an aggregation process is introduced to perform the temporal alignment between the transcribed melody and the music score (ground truth). This stage implicitly aggregates and links the best combination of the extracted melodic segments with the expected note in the ground truth. On the second stage, a statistical method is used to evaluate the accuracy of each detected sung note. The technique is implemented using a Bayesian classifier, which is trained using an audio dataset containing individual scores provided by a committee of expert listeners. These individual scores were measured at each musical note, regarding the pitch, onset, and offset accuracy. Experimental results indicate that the classification scheme is suitable to be used as an assessment tool, providing useful feedback to the student. © Rodrigo Schramm, Helena de Souza Nunes, Cláudio Rosito Jung.","","Information retrieval; Transcription; Aggregation process; Assessment tool; Bayesian classifier; Classification scheme; Committee of experts; Individual scores; Temporal alignment; Two-stage algorithm; Classification (of information)","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Aljanaki A.; Wiering F.; Veltkamp R.C.","Aljanaki, Anna (56410808700); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","56410808700; 8976178100; 7003421646","Emotion based segmentation of musical audio","2015","Proceedings of the 16th International Society for Music Information Retrieval Conference, ISMIR 2015","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988327778&partnerID=40&md5=a46205f4017c59f08147f605a2a9aae7","Utrecht University, Netherlands","Aljanaki A., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands","The dominant approach to musical emotion variation detection tracks emotion over time continuously and usually deals with time resolutions of one second. In this paper we discuss the problems associated with this approach and propose to move to bigger time resolutions when tracking emotion over time. We argue that it is more natural from the listener’s point of view to regard emotional variation in music as a progression of emotionally stable segments. In order to enable such tracking of emotion over time it is necessary to segment music at the emotional boundaries. To address this problem we conduct a formal evaluation of different segmentation methods as applied to a task of emotional boundary detection. We collect emotional boundary annotations from three annotators for 52 musical pieces from the RWC music collection that already have structural annotations from the SALAMI dataset. We investigate how well structural segmentation explains emotional segmentation and find that there is a large overlap, though about a quarter of emotional boundaries do not coincide with structural ones. We also study inter-annotator agreement on emotional segmentation. Lastly, we evaluate different unsupervised segmentation methods when applied to emotional boundary detection and find that, in terms of F-measure, the Structural Features method performs best. © Anna Aljanaki, Frans Wiering, Remco C. Veltkamp.","","Image segmentation; Information retrieval; Boundary detection; Music collection; Musical emotion; Musical pieces; Segmentation methods; Structural feature; Time resolution; Unsupervised segmentation method; Audio acoustics","","","16th International Society for Music Information Retrieval Conference, ISMIR 2015","26 October 2015 through 30 October 2015","Malaga","149377"
"Dutta S.; Murthy H.A.","Dutta, Shrey (55002622000); Murthy, Hema A. (57200197348)","55002622000; 57200197348","Discovering typical motifs of a rāga from one-liners of songs in carnatic music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039957811&partnerID=40&md5=f181a03d444ef227585665d2da7e97d4","Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","Dutta S., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, Indian Institute of Technology Madras, India","Typical motifs of a rāga can be found in the various songs that are composed in the same rāga by different composers. The compositions in Carnatic music have a definite structure, the one commonly seen being pallavi, anupallavi and charanam. The tala is also fixed for every song. Taking lines corresponding to one or more cycles of the pallavi, anupallavi and charanam as one-liners, one-liners across different songs are compared using a dynamic programming based algorithm. The density of match between the one-liners and normalized cost along-with a new measure, which uses the stationary points in the pitch contour to reduce the false alarms, are used to determine and locate the matched pattern. The typical motifs of a rāga are then filtered using compositions of various rāgas. Motifs are considered typical if they are present in the compositions of the given rāga and are not found in compositions of other rāgas. © Shrey Dutta, Hema A. Murthy.","","Information retrieval; False alarms; In compositions; Pitch contours; Stationary points; Dynamic programming","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Przyjaciel-Zablocki M.; Hornung T.; Schätzle A.; Gauß S.; Taxidou I.; Lausen G.","Przyjaciel-Zablocki, Martin (44061942000); Hornung, Thomas (35729958800); Schätzle, Alexander (44061881800); Gauß, Sven (57208111158); Taxidou, Io (55976190800); Lausen, Georg (7003279095)","44061942000; 35729958800; 44061881800; 57208111158; 55976190800; 7003279095","MUSE: A music recommendation management system","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069902609&partnerID=40&md5=8dd189f177c9b2cb3923ee6c99618599","Department of Computer Science, University of Freiburg, Germany","Przyjaciel-Zablocki M., Department of Computer Science, University of Freiburg, Germany; Hornung T., Department of Computer Science, University of Freiburg, Germany; Schätzle A., Department of Computer Science, University of Freiburg, Germany; Gauß S., Department of Computer Science, University of Freiburg, Germany; Taxidou I., Department of Computer Science, University of Freiburg, Germany; Lausen G., Department of Computer Science, University of Freiburg, Germany","Evaluating music recommender systems is a highly repetitive, yet non-trivial, task. But it has the advantage over other domains that recommended songs can be evaluated immediately by just listening to them. In this paper, we present MUSE – a music recommendation management system – for solving the typical tasks of an in vivo evaluation. MUSE provides the typical off-the-shelf evaluation algorithms, offers an online evaluation system with automatic reporting, and by integrating online streaming services also a legal possibility to evaluate the quality of recommended songs in real time. Finally, it has a built-in user management system that conforms with state-of-the-art privacy standards. New recommender algorithms can be plugged in comfortably and evaluations can be configured and managed online. © Martin Przyjaciel-Zablocki, Thomas Hornung, Alexander Schätzle, Sven Gauß, Io Taxidou, Georg Lausen.","","Quality control; Evaluation algorithm; Management systems; Music recommendation; Music recommender systems; On-line evaluation; Recommender algorithms; State of the art; Streaming service; Recommender systems","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Wu M.-J.; Jang J.-S.R.; Lu C.-H.","Wu, Ming-Ju (55729401300); Jang, Jyh-Shing Roger (7402965041); Lu, Chun-Hung (56174622600)","55729401300; 7402965041; 56174622600","Gender identification and age estimation of users based on music metadata","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057947447&partnerID=40&md5=bdd1f8f1735ad6095a347687d6a433b2","Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Computer Science Department, National Taiwan University, Taipei, Taiwan; Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taipei, Taiwan","Wu M.-J., Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Computer Science Department, National Taiwan University, Taipei, Taiwan; Lu C.-H., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Institute for Information Industry, Taipei, Taiwan","Music recommendation is a crucial task in the field of music information retrieval. However, users frequently withhold their real-world identity, which creates a negative impact on music recommendation. Thus, the proposed method recognizes users’ real-world identities based on music metadata. The approach is based on using the tracks most frequently listened to by a user to predict their gender and age. Experimental results showed that the approach achieved an accuracy of 78.87% for gender identification and a mean absolute error of 3.69 years for the age estimation of 48403 users, demonstrating its effectiveness and feasibility, and paving the way for improving music recommendation based on such personal information. © Ming-Ju Wu, Jyh-Shing Roger Jang, Chun-Hung Lu.","","Information retrieval; Age estimation; Gender identification; Mean absolute error; Music information retrieval; Music recommendation; Personal information; Real-world; Metadata","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Jiang N.; Müller M.","Jiang, Nanzhu (55552898300); Müller, Meinard (7404689873)","55552898300; 7404689873","Automated methods for analyzing music recordings in sonata form","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063429818&partnerID=40&md5=7b46b9e6e5240e6d752a6b2cd9cc43a8","International Audio Laboratories, Erlangen, Germany","Jiang N., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","The sonata form has been one of the most important large-scale musical structures used since the early Classical period. Typically, the first movements of symphonies and sonatas follow the sonata form, which (in its most basic form) starts with an exposition and a repetition thereof, continues with a development, and closes with a recapitulation. The recapitulation can be regarded as an altered repeat of the exposition, where certain substructures (first and second subject groups) appear in musically modified forms. In this paper, we introduce automated methods for analyzing music recordings in sonata form, where we proceed in two steps. In the first step, we derive the coarse structure by exploiting that the recapitulation is a kind of repetition of the exposition. This requires audio structure analysis tools that are invariant under local modulations. In the second step, we identify finer substructures by capturing relative modulations between the subject groups in exposition and recapitulation. We evaluate and discuss our results by means of the Beethoven piano sonatas. In particular, we introduce a novel visualization that not only indicates the benefits and limitations of our methods, but also yields some interesting musical insights into the data. © 2013 International Society for Music Information Retrieval.","","Data visualization; Information retrieval; Modulation; Automated methods; Coarse structure; Music recording; Musical structures; Novel visualizations; Structure analysis; Audio recordings","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Kirlin P.B.","Kirlin, Phillip B. (55582044600)","55582044600","A data set for computational studies of schenkerian analysis","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977905068&partnerID=40&md5=f8140277461862dd86b23740b9a19ba2","Department of Mathematics and Computer Science, Rhodes College, United States","Kirlin P.B., Department of Mathematics and Computer Science, Rhodes College, United States","Schenkerian analysis, a kind of hierarchical music analysis, is widely used by music theorists. Though it is part of the standard repertoire of analytical techniques, computational studies of Schenkerian analysis have been hindered by the lack of available data sets containing both musical compositions and ground-truth analyses of those compositions. Without such data sets, it is difficult to empirically study the patterns that arise in analyses or rigorously evaluate the performance of intelligent systems for this kind of analysis. To combat this, we introduce the first publicly available large-scale data set of computer-processable Schenkerian analyses. We discuss the choice of musical selections in the data set, the encoding of the music and the corresponding ground-truth analyses, and the possible uses of these data. As an example of the utility of the data set, we present an algorithm that transforms the Schenkerian analyses into hierarchically-organized data structures that are easily manipulated in software. © Phillip B. Kirlin.","","Intelligent systems; Computational studies; Data set; Ground truth; Large scale data sets; Music analysis; Musical composition; Processable; Information retrieval","P.B. Kirlin; Department of Mathematics and Computer Science, Rhodes College, United States; email: kirlinp@rhodes.edu","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Van Balen J.; Burgoyne J.A.; Wiering F.; Veltkamp R.C.","Van Balen, Jan (55874307000); Burgoyne, John Ashley (23007865600); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","55874307000; 23007865600; 8976178100; 7003421646","An analysis of chorus features in popular song","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066062329&partnerID=40&md5=edaa248f95c4d687cf5b19a09eebdf9a","Utrecht University, Department of Information and Computing Sciences, Netherlands; Universiteit van Amsterdam, Institute for Logic, Language and Computation, Netherlands","Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Burgoyne J.A., Universiteit van Amsterdam, Institute for Logic, Language and Computation, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R.C., Utrecht University, Department of Information and Computing Sciences, Netherlands","This paper presents a computational study of the perceptual and musicological audio features that correlate with the structural function of sections in pop songs, specifically the chorus. Choruses have been described as more prominent, more catchy and more memorable than other sections in a song, yet chorus detection applications have always been primarily based on identifying the most-repeated section in a song. Inspired by cognitive research rather than applied signal processing, this computational analysis compiles a list of robust and interpretable features and models their influence on the ‘chorusness’ of a collection of song sections from the Billboard dataset. This is done through the unsupervised learning of a probabilistic graphical model. We show that timbre and timbre variety are more strongly related to chorus qualities than harmony and absolute pitch height. A regression and a classification experiment are performed to quantify these relations. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Absolute pitch; Audio features; Chorus detection; Computational analysis; Computational studies; Popular song; Probabilistic graphical models; Structural function; Signal processing","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Laaksonen A.; Lemström K.","Laaksonen, Antti (36696605500); Lemström, Kjell (7006564183)","36696605500; 7006564183","On finding symbolic themes directly from audio files using dynamic programming","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069838428&partnerID=40&md5=8e017af5f043c1b3c2030efb2e1133ae","Department of Computer Science, University of Helsinki, Finland; Laurea University of Applied Sciences, Finland","Laaksonen A., Department of Computer Science, University of Helsinki, Finland; Lemström K., Department of Computer Science, University of Helsinki, Finland, Laurea University of Applied Sciences, Finland","In this paper our goal is to find occurrences of a theme within a musical work. The theme is given in a symbolic form that is searched for directly in an audio file. We present a dynamic programming algorithm that is related to an existing time-warp invariant algorithm. However, the new algorithm is computationally more efficient than its predecessor, and it can also be used for approximate timescale invariant search. In the latter case the note durations in the query are taken into account, but some time jittering is allowed for. When dealing with audio, these are important properties because the number of possible note events is large and the note positions are not exact. We evaluate the algorithm using a collection of themes from Tchaikovsky’s symphonies. The new approximate time-scaled algorithm seems to be a good choice for this setting. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Audio files; Dynamic programming algorithm; Invariant algorithms; Time Warp; Time-scales; Dynamic programming","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Humphrey E.J.; Nieto O.; Bello J.P.","Humphrey, Eric J. (55060792500); Nieto, Oriol (55583364500); Bello, Juan P. (7102889110)","55060792500; 55583364500; 7102889110","Data driven and discriminative projections for large-scale cover song identification","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006053298&partnerID=40&md5=34e299aed256d1866766aa6a34bec9b6","Music and Audio Research Laboratory, New York University, United States","Humphrey E.J., Music and Audio Research Laboratory, New York University, United States; Nieto O., Music and Audio Research Laboratory, New York University, United States; Bello J.P., Music and Audio Research Laboratory, New York University, United States","The predominant approach to computing document similarity in web scale applications proceeds by encoding task-specific invariance in a vectorized representation, such that the relationship between items can be computed efficiently by a simple scoring function, e.g. Euclidean distance. Here, we improve upon previous work in large-scale cover song identification by using data-driven projections at different time-scales to capture local features and embed summary vectors into a semantically organized space. We achieve this by projecting 2D-Fourier Magnitude Coefficients (2D-FMCs) of beat-chroma patches into a sparse, high dimensional representation which, due to the shift invariance properties of the Fourier Transform, is similar in principle to convolutional sparse coding. After aggregating these local beat-chroma projections, we apply supervised dimensionality reduction to recover an embedding where distance is useful for cover song retrieval. Evaluating on the Million Song Dataset, we find our method outperforms the current state of the art overall, but significantly so for top-k metrics, which indicate improved usability. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Cover song identifications; Different time scale; Dimensionality reduction; Document similarity; Euclidean distance; High-dimensional; Scoring functions; Shift-invariance properties; Vector spaces","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Böck S.; Krebs F.; Widmer G.","Böck, Sebastian (55413719000); Krebs, Florian (7006192702); Widmer, Gerhard (7004342843)","55413719000; 7006192702; 7004342843","A multi-model approach to beat tracking considering heterogeneous music styles","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973298434&partnerID=40&md5=3e1e03cdb993004b6d581d8306b9813c","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new beat tracking algorithm which extends an existing state-of-the-art system with a multi-model approach to represent different music styles. The system uses multiple recurrent neural networks, which are specialised on certain musical styles, to estimate possible beat positions. It chooses the model with the most appropriate beat activation function for the input signal and jointly models the tempo and phase of the beats from this activation function with a dynamic Bayesian network. We test our system on three big datasets of various styles and report performance gains of up to 27% over existing state-of-the-art methods. Under certain conditions the system is able to match even human tapping performance. © Sebastian Böck, Florian Krebs and Gerhard Widmer.","","Bayesian networks; Chemical activation; Information retrieval; Activation functions; Beat tracking; Dynamic Bayesian networks; Multi model; Multiple recurrent neural networks; Performance Gain; State-of-the-art methods; State-of-the-art system; Recurrent neural networks","S. Böck; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: sebastian.boeck@jku.at","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Kosta K.; Song Y.; Fazekas G.; Sandler M.B.","Kosta, Katerina (55582193200); Song, Yading (55582606800); Fazekas, György (37107520200); Sandler, Mark B. (7202740804)","55582193200; 55582606800; 37107520200; 7202740804","A study of cultural dependence of perceived mood in greek music","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028332273&partnerID=40&md5=4b8d559d11cbb6d09096928df2645ffb","Centre for Digital Music, Queen Mary University of London, United Kingdom","Kosta K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University of London, United Kingdom","Several algorithms have been developed in the music information retrieval community for predicting mood in music in order to facilitate organising and accessing large audio collections. Little attention has been paid however to how perceived emotion depends on cultural factors, such as listeners’ acculturation or familiarity with musical background or language. In this study, we examine this dependence in the context of Greek music. A large representative database of Greek songs has been created and sampled observing predefined criteria such as the balance between Eastern and Western influenced musical genres. Listeners were then asked to rate songs according to their perceived mood. We collected continuous ratings of arousal and valence for short song excerpts and also asked participants to select a mood tag from a controlled mood vocabulary that best described the music. We analysed the consistency of ratings between Greek and non-Greek listeners and the relationships between the categorical and dimensional representations of emotions. Our results show that there is a greater agreement in listener’s judgements with Greek background compared to the group with varying background. These findings suggest valuable implications on the future development of mood prediction systems. © 2013 International Society for Music Information Retrieval.","","Behavioral research; Information retrieval; Cultural factors; Dimensional representation; Music information retrieval; Musical genre; Prediction systems; Varying background; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Burgoyne J.A.; de Haas W.B.; Pauwels J.","Burgoyne, John Ashley (23007865600); de Haas, W. Bas (51160955300); Pauwels, Johan (35113648700)","23007865600; 51160955300; 35113648700","On comparative statistics for labelling tasks: What can we learn from MIREX ACE 2013?","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973329302&partnerID=40&md5=4da69c833a8138dad3d747818d11a112","Universiteit van Amsterdam, Netherlands; Universiteit Utrecht, Netherlands; STMS IRCAM, CNRS, UPMC, France","Burgoyne J.A., Universiteit van Amsterdam, Netherlands; de Haas W.B., Universiteit Utrecht, Netherlands; Pauwels J., STMS IRCAM, CNRS, UPMC, France","For MIREX 2013, the evaluation of audio chord estimation (ACE) followed a new scheme. Using chord vocabularies of differing complexity as well as segmentation measures, the new scheme provides more information than the ACE evaluations from previous years. With this new information, however, comes new interpretive challenges. What are the correlations among different songs and, more importantly, different submissions across the new measures? Performance falls off for all submissions as the vocabularies increase in complexity, but does it do so directly in proportion to the number of more complex chords, or are certain algorithms indeed more robust? What are the outliers, song-algorithm pairs where the performance was substantially higher or lower than would be predicted, and how can they be explained? Answering these questions requires moving beyond the Friedman tests that have most often been used to compare algorithms to a richer underlying model. We propose a logistic-regression approach for generating comparative statistics for MIREX ACE, supported with generalised estimating equations (GEES) to correct for repeated measures. We use the MIREX 2013 ACE results as a case study to illustrate our proposed method, including some of interesting aspects of the evaluation that might not apparent from the headline results alone. © John Ashley Burgoyne, W. Bas de Haas, Johan Pauwels.","","Information retrieval; Audio chord estimations; Estimating equations; Friedman test; Logistic regressions; Previous year; Repeated measures; Insecticides","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Rodríguez-López M.; Volk A.; Bountouridis D.","Rodríguez-López, Marcelo (56448500300); Volk, Anja (30567849900); Bountouridis, Dimitrios (36607463900)","56448500300; 30567849900; 36607463900","Multi-strategy segmentation of melodies","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948953110&partnerID=40&md5=5ba8893218e3a712b2d6128a105f5861","Utrecht University, Netherlands","Rodríguez-López M., Utrecht University, Netherlands; Volk A., Utrecht University, Netherlands; Bountouridis D., Utrecht University, Netherlands","Melodic segmentation is a fundamental yet unsolved problem in automatic music processing. At present most melody segmentation models rely on a ‘single strategy’ (i.e. they model a single perceptual segmentation cue). However, cognitive studies suggest that multiple cues need to be considered. In this paper we thus propose and evaluate a ‘multi-strategy’ system to automatically segment symbolically encoded melodies. Our system combines the contribution of different single strategy boundary detection models. First, it assesses the perceptual relevance of a given boundary detection model for a given input melody; then it uses the boundaries predicted by relevant detection models to search for the most plausible segmentation of the melody. We use our system to automatically segment a corpus of instrumental and vocal folk melodies. We compare the predictions to human annotated segments, and to state of the art segmentation methods. Our results show that our system outperforms the state-of-the-art in the instrumental set. © Marcelo Rodríguez-López, Anja Volk, Dimitrios Bountouridis.","","Boundary detection; Detection models; Input melodies; Perceptual segmentations; Segmentation methods; Segmentation models; State of the art; Unsolved problems; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Wu B.; Wun S.; Lee C.; Horner A.","Wu, Bin (55943143800); Wun, Simon (15072214200); Lee, Chung (55600992100); Horner, Andrew (56244632900)","55943143800; 15072214200; 55600992100; 56244632900","Spectral correlates in emotion labeling of sustained musical instrument tones","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055724292&partnerID=40&md5=8200fd8035c43599fe584a6f23442559","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, 20 Dover Drive, 138682, Singapore","Wu B., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Wun S., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Lee C., Information Systems Technology and Design Pillar, Singapore University of Technology and Design, 20 Dover Drive, 138682, Singapore; Horner A., Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong","Music is one of the strongest inducers of emotion in humans. Melody, rhythm, and harmony provide the primary triggers, but what about timbre? Do the musical instruments have underlying emotional characters? For example, is the well-known melancholy sound of the English horn due to its timbre or to how composers use it? Though music emotion recognition has received a lot of attention, researchers have only recently begun considering the relationship between emotion and timbre. To this end, we devised a listening test to compare representative tones from eight different wind and string instruments. The goal was to determine if some tones were consistently perceived as being happier or sadder in pairwise comparisons. A total of eight emotions were tested in the study. The results showed strong underlying emotional characters for each instrument. The emotions Happy, Joyful, Heroic, and Comic were strongly correlated with one another. The violin, trumpet, and clarinet best represented these emotions. Sad and Depressed were also strongly correlated. These two emotions were best represented by the horn and flute. Scary was the emotional outlier of the group, while the oboe had the most emotionally neutral timbre. Also, we found that emotional judgment correlates significantly with average spectral centroid for the more distinctive emotions, including Happy, Joyful, Sad, Depressed, and Shy. These results can provide insights in orchestration, and lay the groundwork for future studies on emotion and timbre. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Listening tests; Music emotions; Pair-wise comparison; String instruments; Musical instruments","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Maezawa A.; Itoyama K.; Yoshii K.; Okuno H.G.","Maezawa, Akira (35753591100); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120); Okuno, Hiroshi G. (7102397930)","35753591100; 18042499100; 7103400120; 7102397930","Bayesian audio alignment based on a unified generative model of music composition and performance","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977918138&partnerID=40&md5=de55ab115ee0fc785f8b50c9dc575233","Yamaha Corporation, Japan; Kyoto University, Japan; Waseda University, Japan","Maezawa A., Yamaha Corporation, Japan, Kyoto University, Japan; Itoyama K., Kyoto University, Japan; Yoshii K., Kyoto University, Japan; Okuno H.G., Waseda University, Japan","This paper presents a new probabilistic model that can align multiple performances of a particular piece of music. Conventionally, dynamic time warping (DTW) and left-to-right hidden Markov models (HMMs) have often been used for audio-to-audio alignment based on a shallow acoustic similarity between performances. Those methods, however, cannot distinguish latent musical structures common to all performances and temporal dynamics unique to each performance. To solve this problem, our model explicitly represents two state sequences: a top-level sequence that determines the common structure inherent in the music itself and a bottom-level sequence that determines the actual temporal fluctuation of each performance. These two sequences are fused into a hierarchical Bayesian HMM and can be learned at the same time from the given performances. Since the top-level sequence assigns the same state for note combinations that repeatedly appear within a piece of music, we can unveil the latent structure of the piece. Moreover, we can easily compare different performances of the same piece by analyzing the bottom-level sequences. Experimental evaluation showed that our method outperformed the conventional methods. © Akira Maezawa, Katsutoshi Itoyama, Kazuyoshi Yoshii, Hiroshi G. Okuno.","","Hidden Markov models; Information retrieval; Acoustic similarities; Conventional methods; Dynamic time warping; Experimental evaluation; Hidden markov models (HMMs); Hierarchical bayesian; Probabilistic modeling; Temporal fluctuation; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Bountouridis D.; Veltkamp R.C.; Van Balen J.","Bountouridis, Dimitrios (36607463900); Veltkamp, Remco C. (7003421646); Van Balen, Jan (55874307000)","36607463900; 7003421646; 55874307000","Placing music artists and songs in time using editorial metadata and web mining techniques","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069838031&partnerID=40&md5=3202a0fe39817ecfbc59e33da010f5ff","Utrecht University, Department of Information and Computing Sciences, Netherlands","Bountouridis D., Utrecht University, Department of Information and Computing Sciences, Netherlands; Veltkamp R.C., Utrecht University, Department of Information and Computing Sciences, Netherlands; Van Balen J., Utrecht University, Department of Information and Computing Sciences, Netherlands","This paper investigates the novel task of situating music artists and songs in time, thereby adding contextual information that typically correlates with an artist’s similarities, collaborations and influences. The proposed method makes use of editorial metadata in conjunction with web mining techniques, aiming to infer an artist’s productivity over time and estimate the original year of release of a song. Experimental evaluation over a set of Dutch and American music confirms the practicality and reliability of the proposed methods. As a consequence, large-scale correlational analyses between artist productivity and other musical characteristics (e.g. versatility, eminence) become possible. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Productivity; Contextual information; Correlational analysis; Experimental evaluation; Novel task; Web Mining; Data mining","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Sasaki S.; Yoshii K.; Nakano T.; Goto M.; Morishima S.","Sasaki, Shoto (55822251400); Yoshii, Kazuyoshi (7103400120); Nakano, Tomoyasu (24344775400); Goto, Masataka (7403505330); Morishima, Shigeo (7005317462)","55822251400; 7103400120; 24344775400; 7403505330; 7005317462","LyricSRadar: A lyrics retrieval system based on latent topics of lyrics","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951980403&partnerID=40&md5=3bf7ee27a185a20e0f5e6346599149e5","Waseda University, Japan; Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Sasaki S., Waseda University, Japan; Yoshii K., Kyoto University, Japan; Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Morishima S., Waseda University, Japan","This paper presents a lyrics retrieval system called LyricsRadar that enables users to interactively browse song lyrics by visualizing their topics. Since conventional lyrics retrieval systems are based on simple word search, those systems often fail to reflect user’s intention behind a query when a word given as a query can be used in different contexts. For example, the wordtearscan appear not only in sad songs (e.g., feel heartrending), but also in happy songs (e.g., weep for joy). To overcome this limitation, we propose to automatically analyze and visualize topics of lyrics by using a well-known text analysis method called latent Dirichlet allocation (LDA). This enables LyricsRadar to offer two types of topic visualization. One is the topic radar chart that visualizes the relative weights of five latent topics of each song on a pentagon-shaped chart. The other is radar-like arrangement of all songs in a two-dimensional space in which song lyrics having similar topics are arranged close to each other. The subjective experiments using 6,902 Japanese popular songs showed that our system can appropriately navigate users to lyrics of interests. © Shoto Sasaki, Kazuyoshi Yoshii, Tomoyasu Nakano, Masataka Goto, Shigeo Morishima.","","Radar; Statistics; Latent dirichlet allocations; Popular song; Relative weights; Retrieval systems; Subjective experiments; Text-analysis methods; Topic visualizations; Two dimensional spaces; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Kell T.; Tzanetakis G.","Kell, Thor (56407130600); Tzanetakis, George (6602262192)","56407130600; 6602262192","Empirical analysis of track selection and ordering in electronic dance music using audio feature extraction","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003731489&partnerID=40&md5=acb565ec694e4f9e114374ecce5b0690","IDMIL, CIRMMT, McGill University, Canada; University of Victoria, Canada","Kell T., IDMIL, CIRMMT, McGill University, Canada; Tzanetakis G., University of Victoria, Canada","Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music. © 2013 International Society for Music Information Retrieval.","","Electronic musical instruments; Feature extraction; Information retrieval; Sound recording; Audio feature extraction; Automatic content; Empirical analysis; Statistically significant difference; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Fourer D.; Rouas J.-L.; Hanna P.; Robine M.","Fourer, Dominique (54683917700); Rouas, Jean-Luc (6506930347); Hanna, Pierre (23134483000); Robine, Matthias (24559139300)","54683917700; 6506930347; 23134483000; 24559139300","Automatic timbre classification of ethnomusicological audio recordings","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985039648&partnerID=40&md5=2a0af71e1619908927ca1d619badab1d","LaBRI, CNRS UMR 5800, University of Bordeaux, France","Fourer D., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Rouas J.-L., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Hanna P., LaBRI, CNRS UMR 5800, University of Bordeaux, France; Robine M., LaBRI, CNRS UMR 5800, University of Bordeaux, France","Automatic timbre characterization of audio signals can help to measure similarities between sounds and is of interest for automatic or semi-automatic databases indexing. The most effective methods use machine learning approaches which require qualitative and diversified training databases to obtain accurate results. In this paper, we introduce a diversified database composed of worldwide non-western instruments audio recordings on which is evaluated an effective timbre classification method. A comparative evaluation based on the well studied Iowa musical instruments database shows results comparable with those of state-of-the-art methods. Thus, the proposed method offers a practical solution for automatic ethnomusicological indexing of a database composed of diversified sounds with various quality. The relevance of audio features for the timbre characterization is also discussed in the context of non-western instruments analysis. © Dominique Fourer, Jean-Luc Rouas, Pierre Hanna, Matthias Robine.","","Audio recordings; Classification (of information); Database systems; Indexing (of information); Information retrieval; Petroleum reservoir evaluation; Audio features; Comparative evaluations; Machine learning approaches; Practical solutions; Semi-automatics; State-of-the-art methods; Timbre classification; Training database; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"McKay C.","McKay, Cory (14033215600)","14033215600","JProductionCritic: An educational tool for detecting technical errors in audio mixes","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069875722&partnerID=40&md5=84655fe6122d871e57eda610575c14c4","Marianopolis College, CIRMMT, Canada","McKay C., Marianopolis College, CIRMMT, Canada","jProductionCritic is an open-source educational framework for automatically detecting technical recording, editing and mixing problems in audio files. It is intended to be used as a learning and proofreading tool by students and amateur producers, and can also assist teachers as a timesaving tool when grading recordings. A number of novel error detection algorithms are implemented by jProductionCritic. Problems detected include edit errors, clipping, noise infiltration, poor use of dynamics, poor track balancing, and many others. The error detection algorithms are highly configurable, in order to meet the varying aesthetics of different musical genres (e.g. Baroque vs. noise music). Effective general-purpose default settings were developed based on experiments with a variety of student pieces, and these settings were then validated using a reserved set of student pieces. jProductionCritic is also designed to serve as an extensible framework to which new detection modules can be easily plugged in. It is hoped that this will help to galvanize MIR research relating to audio production, an area that is currently underrepresented in the MIR literature, and that this work will also help to address the current general lack of educational production software. © 2013 International Society for Music Information Retrieval.","","Acoustic noise; Audio recordings; Error detection; Grading; Information retrieval; Open source software; Signal detection; Default setting; Detection modules; Educational tools; Error detection algorithms; Extensible framework; Musical genre; Production software; Technical errors; Students","C. McKay; Marianopolis College, CIRMMT, Canada; email: cory.mckay@mail.mcgill.ca","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Farrahi K.; Schedl M.; Vall A.; Hauger D.; Tkalčič M.","Farrahi, Katayoun (25926993400); Schedl, Markus (8684865900); Vall, Andreu (56286378600); Hauger, David (42961319900); Tkalčič, Marko (24438438300)","25926993400; 8684865900; 56286378600; 42961319900; 24438438300","Impact of listening behavior on music recommendation","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964729952&partnerID=40&md5=f7eb0cb5d24d2086f36dd36d536df8a4","Goldsmiths, University of London London, United Kingdom; Johannes Kepler University Linz, Austria","Farrahi K., Goldsmiths, University of London London, United Kingdom; Schedl M., Johannes Kepler University Linz, Austria; Vall A., Johannes Kepler University Linz, Austria; Hauger D., Johannes Kepler University Linz, Austria; Tkalčič M., Johannes Kepler University Linz, Austria","The next generation of music recommendation systems will be increasingly intelligent and likely take into account user behavior for more personalized recommendations. In this work we consider user behavior when making recommendations with features extracted from a user’s history of listening events. We investigate the impact of listener’s behavior by considering features such as play counts, “mainstreaminess”, and diversity in music taste on the performance of various music recommendation approaches. The underlying dataset has been collected by crawling social media (specifically Twitter) for listening events. Each user’s listening behavior is characterized into a three dimensional feature space consisting of play count, “mainstreaminess” (i.e. the degree to which the observed user listens to currently popular artists), and diversity (i.e. the diversity of genres the observed user listens to). Drawing subsets of the 28,000 users in our dataset, according to these three dimensions, we evaluate whether these dimensions influence figures of merit of various music recommendation approaches, in particular, collaborative filtering (CF) and CF enhanced by cultural information such as users located in the same city or country. © 2014 International Society for Music Information Retrieval.","","Collaborative filtering; Recommender systems; Social networking (online); Cultural informations; Feature space; Figures of merits; Music recommendation; Music Recommendation System; Personalized recommendation; Three dimensions; User behaviors; Behavioral research","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Choudhury M.; Bhagwan R.; Bali K.","Choudhury, Monojit (55841605200); Bhagwan, Ranjita (6506809871); Bali, Kalika (16052071200)","55841605200; 6506809871; 16052071200","The use of melodic scales in bollywood music: An empirical study","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069854807&partnerID=40&md5=257cb203d1ca6e8943c0578dad9b8890","Microsoft Research Lab, India","Choudhury M., Microsoft Research Lab, India; Bhagwan R., Microsoft Research Lab, India; Bali K., Microsoft Research Lab, India","Hindi film music, which is commonly referred to as “Bollywood” music, is one of the most popular forms of music in the world today. One of the reasons for its popularity has been the willingness of Bollywood composers to adopt and be influenced by various musical forms including Western pop, jazz, rock, and classical music. However, till date, we are unaware of any systematic quantitative analysis of how this genre has changed and evolved over the years since its inception in the early 20th century. In this paper, we study the evolution of Bollywood music with respect to the use of melodic scales. We analyse songs composed over seven decades using a database of top-lists, which reveals many interesting patterns. We also analyze the scale usage patterns in the music of some of the most popular composers, which clearly brings out certain idiosyncrasies and preferences of each of them. © 2013 International Society for Music Information Retrieval.","","Classical musics; Early 20; Empirical studies; Usage patterns; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Nakamura E.; Ono N.; Sagayama S.","Nakamura, Eita (24587601200); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","24587601200; 7202472899; 7004859104","Merged-output HMM for piano fingering of both hands","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961808605&partnerID=40&md5=94c7142c365c38438b71ab2a08388a0f","National Institute of Informatics, Tokyo, 101-8430, Japan; Meiji University, Tokyo, 164-8525, Japan","Nakamura E., National Institute of Informatics, Tokyo, 101-8430, Japan; Ono N., National Institute of Informatics, Tokyo, 101-8430, Japan; Sagayama S., Meiji University, Tokyo, 164-8525, Japan","This paper discusses a piano fingering model for both hands and its applications. One of our motivations behind the study is automating piano reduction from ensemble scores. For this, quantifying the difficulty of piano performance is important where a fingering model of both hands should be relevant. Such a fingering model is proposed that is based on merged-output hidden Markov model and can be applied to scores in which the voice part for each hand is not indicated. The model is applied for decision of fingering for both hands and voice-part separation, automation of which is itself of great use and were previously difficult. A measure of difficulty of performance based on the fingering model is also proposed and yields reasonable results. © Eita Nakamura, Nobutaka Ono, Shigeki Sagayama.","","Hidden Markov models; Information retrieval; ITS applications; Performance based; Musical instruments","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Nakamura T.; Shikata K.; Takamune N.; Kameoka H.","Nakamura, Tomohiko (56303559000); Shikata, Kotaro (57210207709); Takamune, Norihiro (56304348800); Kameoka, Hirokazu (7006771405)","56303559000; 57210207709; 56304348800; 7006771405","Harmonic-temporal factor decomposition incorporating music prior information for informed monaural source separation","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973404822&partnerID=40&md5=e591d5269efe9cc90375685d9efec5c5","Graduate School of Information Science and Technology, University of Tokyo, Japan; NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Japan","Nakamura T., Graduate School of Information Science and Technology, University of Tokyo, Japan; Shikata K., Graduate School of Information Science and Technology, University of Tokyo, Japan; Takamune N., Graduate School of Information Science and Technology, University of Tokyo, Japan; Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Japan, NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Japan","For monaural source separation two main approaches have thus far been adopted. One approach involves applying non-negative matrix factorization (NMF) to an observed magnitude spectrogram, interpreted as a non-negative matrix. The other approach is based on the concept of computational auditory scene analysis (CASA). A CASA-based approach called the “harmonic-temporal clustering (HTC)” aims to cluster the time-frequency components of an observed signal based on a constraint designed according to the local time-frequency structure common in many sound sources (such as harmonicity and the continuity of frequency and amplitude modulations). This paper proposes a new approach for monaural source separation called the “Harmonic-Temporal Factor Decomposition (HTFD)” by introducing a spectrogram model that combines the features of the models employed in the NMF and HTC approaches. We further describe some ideas how to design the prior distributions for the present model to incorporate musically relevant information into the separation scheme. © Tomohiko Nakamura, Kotaro Shikata, Norihiro Takamune, Hirokazu Kameoka.","","Factorization; Harmonic analysis; Information retrieval; Matrix algebra; Separation; Spectrographs; Computational auditory scene analysis; Factor decompositions; Non-negative matrix; Nonnegative matrix factorization; Prior distribution; Prior information; Separation schemes; Temporal clustering; Source separation","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Schoeffler M.; Stöter F.-R.; Bayerlein H.; Edler B.; Herre J.","Schoeffler, Michael (55764668100); Stöter, Fabian-Robert (55764648500); Bayerlein, Harald (57203902641); Edler, Bernd (6603077807); Herre, Jürgen (7006226766)","55764668100; 55764648500; 57203902641; 6603077807; 7006226766","An experiment about estimating the number of instruments in polyphonic music: A comparison between internet and laboratory results","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041551313&partnerID=40&md5=639bdd151f908f99b4643220086515f0","International Audio Laboratories, Erlangen, Germany","Schoeffler M., International Audio Laboratories, Erlangen, Germany; Stöter F.-R., International Audio Laboratories, Erlangen, Germany; Bayerlein H., International Audio Laboratories, Erlangen, Germany; Edler B., International Audio Laboratories, Erlangen, Germany; Herre J., International Audio Laboratories, Erlangen, Germany","Internet experiments in the fields of music perception and music information retrieval are becoming more and more popular. However, not many Internet experiments are compared to laboratory experiments, the consequence being that the effect of the uncontrolled Internet environment on the results is unknown. In this paper the results of an Internet experiment with 1168 participants are compared to those of the same experiment with 62 participants but previously conducted in a controlled environment. The comparison of the Internet and laboratory results enabled us to make a point on whether the Internet can be used for our experiment procedure. The experiment aimed to investigate the listeners ability to correctly estimate the number of instruments being played back in a given excerpt of music. The participants listened to twelve short classical and pop music excerpts each composed using one to six instruments. For each music excerpt the participants were asked how many instruments they could hear and how certain they were about their estimation. © 2013 International Society for Music Information Retrieval.","","Controlled environment; Internet environment; Internet experiments; Laboratory experiments; Many instruments; Music information retrieval; Music perception; Polyphonic music; Information retrieval","M. Schoeffler; International Audio Laboratories, Erlangen, Germany; email: michael.schoeffler@audiolabs-erlangen.com","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Liang D.; Paisley J.; Ellis D.P.W.","Liang, Dawen (55586031200); Paisley, John (35810949000); Ellis, Daniel P.W. (13609089200)","55586031200; 35810949000; 13609089200","Codebook-based scalable music tagging with poisson matrix factorization","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991009696&partnerID=40&md5=b883678d19619145fd29dbe1dee10dea","Department of Electrical Engineering, Columbia University, United States","Liang D., Department of Electrical Engineering, Columbia University, United States; Paisley J., Department of Electrical Engineering, Columbia University, United States; Ellis D.P.W., Department of Electrical Engineering, Columbia University, United States","Automatic music tagging is an important but challenging problem within MIR. In this paper, we treat music tagging as a matrix completion problem. We apply the Poisson matrix factorization model jointly on the vector-quantized audio features and a “bag-of-tags” representation. This approach exploits the shared latent structure between semantic tags and acoustic codewords. Leveraging the recently-developed technique of stochastic variational inference, the model can tractably analyze massive music collections. We present experimental results on the CAL500 dataset and the Million Song Dataset for both annotation and retrieval tasks, illustrating the steady improvement in performance as more data is used. © Dawen Liang, John Paisley, Daniel P. W. Ellis.","","Factorization; Information retrieval; Semantics; Stochastic models; Stochastic systems; Audio features; Automatic Music Tagging; Latent structures; Matrix completion problems; Matrix factorizations; Music collection; Semantic tags; Variational inference; Matrix algebra","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Srinivasamurthy A.; Repetto R.C.; Sundar H.; Serra X.","Srinivasamurthy, Ajay (55583336200); Repetto, Rafael Caro (57200495266); Sundar, Harshavardhan (55378040700); Serra, Xavier (55892979900)","55583336200; 57200495266; 55378040700; 55892979900","Transcription and recognition of syllable based percussion patterns: The case of beijing opera","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994535270&partnerID=40&md5=15630e0e139be078a59fc0aa0276354e","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Speech and Audio Group, Indian Institute of Science, Bangalore, India","Srinivasamurthy A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Repetto R.C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sundar H., Speech and Audio Group, Indian Institute of Science, Bangalore, India; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In many cultures of the world, traditional percussion music uses mnemonic syllables that are representative of the timbres of instruments. These syllables are orally transmitted and often provide a language for percussion in those music cultures. Percussion patterns in these cultures thus have a well defined representation in the form of these syllables, which can be utilized in several computational percussion pattern analysis tasks. We explore a connected word speech recognition based framework that can effectively utilize the syllabic representation for automatic transcription and recognition of audio percussion patterns. In particular, we consider the case of Beijing opera and present a syllable level hidden markov model (HMM) based system for transcription and classification of percussion patterns. The encouraging classification results on a representative dataset of Beijing opera percussion patterns supports our approach and provides further insights on the utility of these syllables for computational description of percussion patterns. © Ajay Srinivasamurthy, Rafael Caro Repetto, Harshavardhan Sundar, Xavier Serra.","","Classification (of information); Hidden Markov models; Information retrieval; Speech recognition; Transcription; Automatic transcription; Classification results; Pattern analysis; Musical instruments","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Schlüter J.","Schlüter, Jan (55063593300)","55063593300","Learning binary codes for efficient large-scale music similarity search","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026602507&partnerID=40&md5=d2a37fed701a6f97f4603fc361524e0e","Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Content-based music similarity estimation provides a way to find songs in the unpopular “long tail” of commercial catalogs. However, state-of-the-art music similarity measures are too slow to apply to large databases, as they are based on finding nearest neighbors among very high-dimensional or non-vector song representations that are difficult to index. In this work, we adopt recent machine learning methods to map such song representations to binary codes. A linear scan over the codes quickly finds a small set of likely neighbors for a query to be refined with the original expensive similarity measure. Although search costs grow linearly with the collection size, we show that for commercial-scale databases and two state-of-the-art similarity measures, this outperforms five previous attempts at approximate nearest neighbor search. When required to return 90% of true nearest neighbors, our method is expected to answer 4.2 1-NN queries or 1.3 50-NN queries per second on a collection of 30 million songs using a single CPU core; an up to 260 fold speedup over a full scan of 90% of the database. © 2013 International Society for Music Information Retrieval.","","Binary codes; Information retrieval; Nearest neighbor search; Query languages; Query processing; Content-based; High-dimensional; Large database; Machine learning methods; Music similarity; Nearest neighbors; Similarity measure; State of the art; Learning systems","J. Schlüter; Austrian Research Institute for Artificial Intelligence, Vienna, Austria; email: jan.schlueter@ofai.at","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Wang S.; Ewert S.; Dixon S.","Wang, Siying (56939449600); Ewert, Sebastian (32667575400); Dixon, Simon (7201479437)","56939449600; 32667575400; 7201479437","Robust joint alignment of multiple versions of a piece of music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973284602&partnerID=40&md5=9bc5a54b36ef1b6aec9e43d8eae5df6c","Queen Mary University of London, United Kingdom","Wang S., Queen Mary University of London, United Kingdom; Ewert S., Queen Mary University of London, United Kingdom; Dixon S., Queen Mary University of London, United Kingdom","Large music content libraries often comprise multiple versions of a piece of music. To establish a link between different versions, automatic music alignment methods map each position in one version to a corresponding position in another version. Due to the leeway in interpreting a piece, any two versions can differ significantly, for example, in terms of local tempo, articulation, or playing style. For a given pair of versions, these differences can be significant such that even state-of-the-art methods fail to identify a correct alignment. In this paper, we present a novel method that increases the robustness for difficult to align cases. Instead of aligning only pairs of versions as done in previous methods, our method aligns multiple versions in a joint manner. This way, the alignment can be computed by comparing each version not only with one but with several versions, which stabilizes the comparison and leads to an increase in alignment robustness. Using recordings from the Mazurka Project, the alignment error for our proposed method was 14% lower on average compared to a state-of-the-art method, with significantly less outliers (standard deviation 53% lower). © Siying Wang, Sebastian Ewert, Simon Dixon.","","Information retrieval; Alignment error; Alignment methods; Music contents; Playing style; Standard deviation; State-of-the-art methods; Alignment","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Liu I.-T.; Lin Y.-T.; Wu J.-L.","Liu, I-Ting (56678338600); Lin, Yin-Tzu (57209831317); Wu, Ja-Ling (7409250086)","56678338600; 57209831317; 7409250086","Music cut and paste: A personalized musical medley generating system","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044504696&partnerID=40&md5=43cbc8b8ca6e177d9fe370e11438be47","Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","Liu I.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Lin Y.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Wu J.-L., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","A musical medley is a piece of music that is composed of parts of existing pieces. Manually creating medley is time consuming because it is not easy to find out proper clips to put in succession and seamlessly connect them. In this work, we propose a framework for creating personalized music medleys from users’ music collection. Unlike existing similar works in which only low-level features are used to select candidate clips and locate possible transition points among clips, we take song structures and song phrasing into account during medley creation. Inspired by the musical dice game, we treat the medley generation process as an audio version of musical dice game. That is, once the analysis on the songs of user collection has been done, the system is able to generate various medleys with different probabilities. This flexibility brings us the ability to create medleys according to the user-specified conditions, such as the medley structure or some must-use clips. The preliminary subjective evaluations showed that the proposed system is effective in selecting connectable clips that preserved chord progression structure. Besides, connecting the clips at phrase boundaries acquired more user preference than previous works did. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Generating system; Generation process; Low-level features; Music collection; Phrase boundary; Song structure; Subjective evaluations; Transition point; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Pauwels J.; Kaiser F.; Peeters G.","Pauwels, Johan (35113648700); Kaiser, Florian (57201809842); Peeters, Geoffroy (22433836000)","35113648700; 57201809842; 22433836000","Combining harmony-based and novelty-based approaches for structural segmentation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052779227&partnerID=40&md5=bcccdb3e45849af7226e6b5b306e58e9","STMS IRCAM-CNRS-UPMC, France","Pauwels J., STMS IRCAM-CNRS-UPMC, France; Kaiser F., STMS IRCAM-CNRS-UPMC, France; Peeters G., STMS IRCAM-CNRS-UPMC, France","This paper describes a novel way to combine a well-proven method of structural segmentation through novelty detection with a recently introduced method based on harmonic analysis. The former system works by looking for peaks in novelty curves derived from self-similarity matrices. The latter relies on the detection of key changes and on the differences in prior probability of chord transitions according to their position in a structural segment. Both approaches are integrated into a probabilistic system that jointly estimates keys, chords and structural boundaries. The novelty curves are herein used as observations. In addition, chroma profiles are used as features for the harmony analysis. These observations are then subjected to a constrained transition model that is musically motivated. An information theoretic justification of this model is also given. Finally, an evaluation of the resulting system is performed. It is shown that the combined system improves the results of both constituting components in isolation. © 2013 International Society for Music Information Retrieval.","","Information theory; Combined system; Constrained transition; Harmony analysis; Novelty detection; Prior probability; Probabilistic systems; Self-similarities; Structural boundary; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cherla S.; Weyde T.; d’Avila Garcez A.","Cherla, Srikanth (24824198800); Weyde, Tillman (24476899500); d’Avila Garcez, Artur (57188767693)","24824198800; 24476899500; 57188767693","Multiple viewpoint melodic prediction with fixed-context neural networks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975204842&partnerID=40&md5=f80e30ed4e93020115cc5c22bf4d6600","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Machine Learning Group, Department of Computer Science, City University London, United Kingdom","Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom","The multiple viewpoints representation is an event-based representation of symbolic music data which offers a means for the analysis and generation of notated music. Previous work using this representation has predominantly relied on n-gram and variable order Markov models for music sequence modelling. Recently the efficacy of a class of distributed models, namely restricted Boltzmann machines, was demonstrated for this purpose. In this paper, we demonstrate the use of two neural network models which use fixed-length sequences of various viewpoint types as input to predict the pitch of the next note in the sequence. The predictive performance of each of these models is comparable to that of models previously evaluated on the same task. We then combine the predictions of individual models using an entropy-weighted combination scheme to improve the overall prediction performance, and compare this with the predictions of a single equivalent model which takes as input all the viewpoint types of each of the individual models in the combination. © Srikanth Cherla, Tillman Weyde and Artur d’Avila Garcez.","","Forecasting; Information retrieval; Markov processes; Event-based representations; Multiple viewpoints; Neural network model; Prediction performance; Predictive performance; Restricted boltzmann machine; Variable Order Markov Models; Weighted combination scheme; Computer music","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Lartillot O.","Lartillot, Olivier (6507137446)","6507137446","In-depth motivic analysis based on multiparametric closed pattern and cyclic sequence mining","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956821076&partnerID=40&md5=c1c46fe66f2eec73ed779f5173d97898","Aalborg University, Department of Architecture, Design and Media Technology, Denmark","Lartillot O., Aalborg University, Department of Architecture, Design and Media Technology, Denmark","The paper describes a computational system for exhaustive but compact description of repeated motivic patterns in symbolic representations of music. The approach follows a method based on closed heterogeneous pattern mining in multiparametrical space with control of pattern cyclicity. This paper presents a much simpler description and justification of this general strategy, as well as significant simplifications of the model, in particular concerning the management of pattern cyclicity. A new method for automated bundling of patterns belonging to same motivic or thematic classes is also presented. The good performance of the method is shown through the analysis of a piece from the JKUPDD database. Ground-truth motives are detected, while additional relevant information completes the ground-truth musicological analysis. The system, implemented in Matlab, is made publicly available as part of MiningSuite, a new open-source framework for audio and music analysis. © Olivier Lartillot.","","Data mining; Information retrieval; Open systems; Closed pattern; Computational system; Cyclic sequences; Ground truth; Music analysis; Open source frameworks; Pattern mining; Symbolic representation; Audio acoustics","O. Lartillot; Aalborg University, Department of Architecture, Design and Media Technology, Denmark; email: olartillot@gmail.com","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Boulanger-Lewandowski N.; Bengio Y.; Vincent P.","Boulanger-Lewandowski, Nicolas (45560980800); Bengio, Yoshua (7003958245); Vincent, Pascal (57203214842)","45560980800; 7003958245; 57203214842","Audio chord recognition with recurrent neural networks","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","129","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054275611&partnerID=40&md5=34ecf2cb2a67b5c045e4a0b8cdbd1857","Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada","Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Bengio Y., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada; Vincent P., Dept. IRO, Université de Montréal, Montréal, H3C 3J7, QC, Canada","In this paper, we present an audio chord recognition system based on a recurrent neural network. The audio features are obtained from a deep neural network optimized with a combination of chromagram targets and chord information, and aggregated over different time scales. Contrarily to other existing approaches, our system incorporates acoustic and musicological models under a single training objective. We devise an efficient algorithm to search for the global mode of the output distribution while taking long-term dependencies into account. The resulting method is competitive with state-of-the-art approaches on the MIREX dataset in the major/minor prediction task. © 2013 International Society for Music Information Retrieval.","","Deep neural networks; Information retrieval; Audio features; Chord recognition; Different time scale; Long-term dependencies; Output distribution; Prediction tasks; State-of-the-art approach; Recurrent neural networks","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Yakar T.B.; Litman R.; Sprechmann P.; Bronstein A.; Sapiro G.","Yakar, Tal Ben (56123302700); Litman, Roee (37077586000); Sprechmann, Pablo (23010370200); Bronstein, Alex (7103163126); Sapiro, Guillermo (7005450011)","56123302700; 37077586000; 23010370200; 7103163126; 7005450011","Bilevel sparse models for polyphonic music transcription","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017397082&partnerID=40&md5=98ca79f75c5ae249532b96e594fbb19d","Tel Aviv University, Israel; Duke University, United States","Yakar T.B., Tel Aviv University, Israel; Litman R., Tel Aviv University, Israel; Sprechmann P., Duke University, United States; Bronstein A., Tel Aviv University, Israel; Sapiro G., Duke University, United States","In this work, we propose a trainable sparse model for automatic polyphonic music transcription, which incorporates several successful approaches into a unified optimization framework. Our model combines unsupervised synthesis models similar to latent component analysis and nonnegative factorization with metric learning techniques that allow supervised discriminative learning. We develop efficient stochastic gradient training schemes allowing unsupervised, semi-, and fully supervised training of the model as well its adaptation to test data. We show efficient fixed complexity and latency approximation that can replace iterative minimization algorithms in time-critical applications. Experimental evaluation on synthetic and real data shows promising initial results. © 2013 International Society for Music Information Retrieval.","","Approximation algorithms; Information retrieval; Iterative methods; Stochastic models; Stochastic systems; Transcription; Discriminative learning; Experimental evaluation; Latent component analysis; Minimization algorithms; Non-negative factorization; Synthetic and real data; Time-critical applications; Unified optimization framework; Learning systems","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Ullrich K.; Schlüter J.; Grill T.","Ullrich, Karen (57202060292); Schlüter, Jan (55063593300); Grill, Thomas (6506275367)","57202060292; 55063593300; 6506275367","Boundary detection in music structure analysis using convolutional neural networks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","60","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007420949&partnerID=40&md5=7dea69f019e885b872f394ff8b933dba","Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Ullrich K., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Grill T., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","The recognition of boundaries, e.g., between chorus and verse, is an important task in music structure analysis. The goal is to automatically detect such boundaries in audio signals so that the results are close to human annotation. In this work, we apply Convolutional Neural Networks to the task, trained directly on mel-scaled magnitude spectrograms. On a representative subset of the SALAMI structural annotation dataset, our method outperforms current techniques in terms of boundary retrieval F-measure at different temporal tolerances: We advance the state-of-the-art from 0.33 to 0.46 for tolerances of ±0.5 seconds, and from 0.52 to 0.62 for tolerances of ±3 seconds. As the algorithm is trained on annotated audio data without the need of expert knowledge, we expect it to be easily adaptable to changed annotation guidelines and also to related tasks such as the detection of song transitions. © Karen Ullrich, Jan Schlüter, and Thomas Grill.","","Fits and tolerances; Information retrieval; Neural networks; Audio signal; Boundary detection; Convolutional neural network; Expert knowledge; Human annotations; Music structure analysis; Spectrograms; State of the art; Convolution","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sébastien V.; Sébastien D.; Conruyt N.","Sébastien, Véronique (36167211300); Sébastien, Didier (24766624700); Conruyt, Noël (24765910100)","36167211300; 24766624700; 24765910100","Annotating works for music education: Propositions for a musical forms and structures ontology and a musical performance ontology","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069854332&partnerID=40&md5=d24ae64f791e1e2da69d598a962e501f","IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France","Sébastien V., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France; Sébastien D., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France; Conruyt N., IREMIA, Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Réunion, France","Web applications and mobile tablets are changing the way musicians practice their instrument. Now, they can access instantaneously thousands of musical scores online and play them while watching their tablet, put on their music stand. However musicians may have difficulties in getting appropriate tips and advice to play the chosen piece correctly. This is why we conceived a collaborative platform to annotate digital scores on tablets in previous work. However, we noticed that the current Music Ontology (MO) do not allow to tag these annotations appropriately. Thus, we present in this paper a proposition for a Musical Forms and Structures Ontology (MFSO) and a Musical Performance Ontology (MPO) based on music practice. A construction methodology and a model are first detailed. Then, a practical use case is presented. Lastly, inherent theoretical and practical difficulties encountered during the ontology framework’s conception are discussed. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Collaborative platform; Construction methodology; Digital score; Music education; Music ontology; Musical performance; Musical score; WEB application; Ontology","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Kaiser F.; Peeters G.","Kaiser, Florian (57201809842); Peeters, Geoffroy (22433836000)","57201809842; 22433836000","A simple fusion method of state and sequence segmentation for music structure discovery","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009854728&partnerID=40&md5=15459ec8fb75cc5026a54891db9b875d","STMS IRCAM-CNRS-UPMC, 1 Place Igor Stravinsky, Paris, 75004, France","Kaiser F., STMS IRCAM-CNRS-UPMC, 1 Place Igor Stravinsky, Paris, 75004, France; Peeters G., STMS IRCAM-CNRS-UPMC, 1 Place Igor Stravinsky, Paris, 75004, France","Methods for music structure segmentation are based on strong assumptions on the acoustical properties of structural segments. These assumptions relate to the novelty, homogeneity, repetition and/or regularity of the content. Each of these assumptions provide a different perspective on the music piece. These assumptions are however often considered separately in the methods. In this paper we propose a method for estimating the music structure segmentation based on the fusion of the novelty and repetition assumptions. This combination of different perspectives on the music pieces allows to generate more coherent acoustic segments and strongly improves the final music structure segmentation’s performance. © 2013 International Society for Music Information Retrieval.","","Acoustical properties; Fusion methods; Music structures; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Vigliensoni G.; Burlet G.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); Burlet, Gregory (55582430200); Fujinaga, Ichiro (9038140900)","55217696900; 55582430200; 9038140900","Optical measure recognition in common music notation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985933779&partnerID=40&md5=22e9d0513a85d05084d03b647689652d","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burlet G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper presents work on the automatic recognition of measures in common Western music notation scores using optical music recognition techniques. It is important to extract the bounding boxes of measures within a music score to facilitate some methods of multimodal navigation of music catalogues. We present an image processing algorithm that extracts the position of barlines on an input music score in order to deduce the number and position of measures on the page. An open-source implementation of this algorithm is made publicly available. In addition, we have created a ground-truth dataset of 100 images of music scores with manually annotated measures. We conducted several experiments using different combinations of values for two critical parameters to evaluate our measure recognition algorithm. Our algorithm obtained an f-score of 91 percent with the optimal set of parameters. Although our implementation obtained results similar to previous approaches, the scope and size of the evaluation dataset is significantly larger. © 2013 International Society for Music Information Retrieval.","","Image processing; Information retrieval; Optical data processing; Automatic recognition; Bounding box; Ground-truth dataset; Image processing algorithm; Music notation; Open source implementation; Optical music recognition; Recognition algorithm; Parameter estimation","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Holzapfel A.; Krebs F.; Srinivasamurthy A.","Holzapfel, Andre (18041818000); Krebs, Florian (7006192702); Srinivasamurthy, Ajay (55583336200)","18041818000; 7006192702; 55583336200","Tracking the “odd”: Meter inference in a culturally diverse music corpus","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973295148&partnerID=40&md5=3db702b9eb0cfb87815917860577d175","New York University Abu Dhabi, United Arab Emirates; Johannes Kepler University, Austria; Universitat Pompeu Fabra, Spain","Holzapfel A., New York University Abu Dhabi, United Arab Emirates; Krebs F., Johannes Kepler University, Austria; Srinivasamurthy A., Universitat Pompeu Fabra, Spain","In this paper, we approach the tasks of beat tracking, downbeat recognition and rhythmic style classification in non-Western music. Our approach is based on a Bayesian model, which infers tempo, downbeats and rhythmic style, from an audio signal. The model can be automatically adapted to rhythmic styles and time signatures. For evaluation, we compiled and annotated a music corpus consisting of eight rhythmic styles from three cultures, containing a variety of meter types. We demonstrate that by adapting the model to specific styles, we can track beats and downbeats in odd meter types like 9/8 or 7/8 with an accuracy significantly improved over the state of the art. Even if the rhythmic style is not known in advance, a unified model is able to recognize the meter and track the beat with comparable results, providing a novel method for inferring the metrical structure in culturally diverse datasets. © Andre Holzapfel, Florian Krebs, Ajay Srinivasamurthy.","","Bayesian networks; Audio signal; Bayesian model; Beat tracking; State of the art; Unified Modeling; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Saari P.; Eerola T.; Fazekas G.; Barthet M.; Lartillot O.; Sandler M.","Saari, Pasi (55937913000); Eerola, Tuomas (6602209042); Fazekas, György (37107520200); Barthet, Mathieu (24723525000); Lartillot, Olivier (6507137446); Sandler, Mark (7202740804)","55937913000; 6602209042; 37107520200; 24723525000; 6507137446; 7202740804","The role of audio and tags in music mood prediction: A study using semantic layer projection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044951126&partnerID=40&md5=458c170c3fdd0904534a8d8ce20942de","Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Centre for Digital Music, Queen Mary University of London, United Kingdom","Saari P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Semantic Layer Projection (SLP) is a method for automatically annotating music tracks according to expressed mood based on audio. We evaluate this method by comparing it to a system that infers the mood of a given track using associated tags only. SLP differs from conventional auto-tagging algorithms in that it maps audio features to a low-dimensional semantic layer congruent with the circumplex model of emotion, rather than training a model for each tag separately. We build the semantic layer using two large-scale data sets – crowd-sourced tags from Last.fm, and editorial annotations from the I Like Music (ILM) production music corpus – and use subsets of these corpora to train SLP for mapping audio features to the semantic layer. The performance of the system is assessed in predicting mood ratings on continuous scales in the two data sets mentioned above. The results show that audio is in general more efficient in predicting perceived mood than tags. Furthermore, we analytically demonstrate the benefit of using a combination of semantic tags and audio features in automatic mood annotation. © 2013 International Society for Music Information Retrieval.","","Forecasting; Information retrieval; Semantics; Audio features; Circumplex models; Continuous scale; Large scale data sets; Last.fm; Low dimensional; Semantic layer; Semantic tags; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Sarala P.; Murthy H.A.","Sarala, Padi (56448938200); Murthy, Hema A. (57200197348)","56448938200; 57200197348","Inter and intra item segmentation of continuous audio recordings of carnatic music for archival","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039931930&partnerID=40&md5=a7a54a295ecd1e48a2a286b100c950bc","Computer Science and Engineering, Indian Institute of Technology, Madras, India","Sarala P., Computer Science and Engineering, Indian Institute of Technology, Madras, India; Murthy H.A., Computer Science and Engineering, Indian Institute of Technology, Madras, India","The purpose of this paper is to segment carnatic music recordings into individual items for archival purposes using applauses. A concert in carnatic music is replete with applauses. These applauses may be inter-item or intra-item applauses. A property of an item in carnatic music, is that within every item, a small portion of the audio corresponds to the rendering of a composition which is rendered by the entire ensemble of lead performer and accompanying instruments. A concert is divided into segments using applauses and the location of the ensemble in every item is first obtained using Cent Filterbank Cepstral Coefficients (CFCC) combined with Gaussian Mixture Models (GMMs). Since constituent parts of an item are rendered in a single raga, raga information is used to merge adjacent segments belonging to the same item. Inter-item applauses are used to locate the end of an item in a concert. The results are evaluated for fifty live recordings with 990 applauses in total. The classification accuracy for inter and intra item applauses is 93%. Given a song list and the audio, the song list is mapped to the segmented audio of items, which are then stored in the database. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Cepstral coefficients; Classification accuracy; Gaussian mixture model (GMMs); Music recording; Audio recordings","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"De Man B.; Leonard B.; King R.; Reiss J.D.","De Man, Brecht (55843411700); Leonard, Brett (37079258200); King, Richard (57211593369); Reiss, Joshua D. (10140139100)","55843411700; 37079258200; 57211593369; 10140139100","An analysis and evaluation of audio features for multitrack music mixtures","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983096463&partnerID=40&md5=0db1bb5ba861aad9bb7fea8ee2733607","Centre for Digital Music, Queen Mary University of London, United Kingdom; Graduate Program in Sound Recording, Schulich School of Music, McGill University, Canada; Centre for Interdisciplinary Research in Music Media and Technology, Canada","De Man B., Centre for Digital Music, Queen Mary University of London, United Kingdom; Leonard B., Graduate Program in Sound Recording, Schulich School of Music, McGill University, Canada, Centre for Interdisciplinary Research in Music Media and Technology, Canada; King R., Graduate Program in Sound Recording, Schulich School of Music, McGill University, Canada, Centre for Interdisciplinary Research in Music Media and Technology, Canada; Reiss J.D., Centre for Digital Music, Queen Mary University of London, United Kingdom","Mixing multitrack music is an expert task where characteristics of the individual elements and their sum are manipulated in terms of balance, timbre and positioning, to resolve technical issues and to meet the creative vision of the artist or engineer. In this paper we conduct a mixing experiment where eight songs are each mixed by eight different engineers. We consider a range of features describing the dynamic, spatial and spectral characteristics of each track, and perform a multidimensional analysis of variance to assess whether the instrument, song and/or engineer is the determining factor that explains the resulting variance, trend, or consistency in mixing methodology. A number of assumed mixing rules from literature are discussed in the light of this data, and implications regarding the automation of various mixing processes are explored. Part of the data used in this work is published in a new online multitrack dataset through which public domain recordings, mixes, and mix settings (DAW projects) can be shared. © Brecht De Man, Brett Leonard, Richard King and Joshua D. Reiss.","","Engineers; Information retrieval; Mixing; Analysis and evaluation; Audio features; Mixing experiments; Mixing process; Mixing rules; Multi-dimensional analysis; Public domains; Spectral characteristics; Audio acoustics","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Burlet G.; Fujinaga I.","Burlet, Gregory (55582430200); Fujinaga, Ichiro (9038140900)","55582430200; 9038140900","Robotaba guitar tablature transcription framework","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013648374&partnerID=40&md5=d25e431320f305c06e3a671626e10952","Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","Burlet G., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","This paper presents Robotaba, a web-based guitar tablature transcription framework. The framework facilitates the creation of web applications in which polyphonic transcription and guitar tablature arrangement algorithms can be embedded. Such a web application is implemented, and consists of an existing polyphonic transcription algorithm and a new guitar tablature arrangement algorithm. The result is a unified system that is capable of transcribing guitar tablature from a digital audio recording and displaying the resulting tablature in the web browser. Additionally, two ground-truth datasets for polyphonic transcription and guitar tablature arrangement are compiled from manual transcriptions gathered from the tablature website ultimate-guitar.com. The implemented transcription web application is evaluated on the compiled ground-truth datasets using several metrics. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Musical instruments; Websites; Digital audio recordings; Ground truth; Polyphonic transcriptions; Unified system; WEB application; Web based; Transcription","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"van Herwaarden S.; Grachten M.; Bas de Haas W.","van Herwaarden, Sam (57210214022); Grachten, Maarten (8974600000); Bas de Haas, W. (51160955300)","57210214022; 8974600000; 51160955300","Predicting expressive dynamics in piano performances using neural networks","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989828176&partnerID=40&md5=ee92f1698110bbd3260abcefe4d1db4d","Austrian Research Institute for AI, Austria; Utrecht University, Netherlands","van Herwaarden S., Austrian Research Institute for AI, Austria; Grachten M., Austrian Research Institute for AI, Austria; Bas de Haas W., Utrecht University, Netherlands","This paper presents a model for predicting expressive accentuation in piano performances with neural networks. Using Restricted Boltzmann Machines (RBMs), features are learned from performance data, after which these features are used to predict performed loudness. During feature learning, data describing more than 6000 musical pieces is used; when training for prediction, two datasets are used, both recorded on a Bösendorfer piano (accurately measuring note on- and offset times and velocity values), but describing different compositions performed by different pianists. The resulting model is tested by predicting note velocity for unseen performances. Our approach differs from earlier work in a number of ways: (1) an additional input representation based on a local history of velocity values is used, (2) the RBMs are trained to result in a network with sparse activations, (3) network connectivity is increased by adding skip-connections, and (4) more data is used for training. These modifications result in a network performing better than the state-of-the-art on the same data and more descriptive features, which can be used for rendering performances, or for gaining insight into which aspects of a musical piece influence its performance. © S. van Herwaarden, M. Grachten, W.B. de Haas.","","Information retrieval; Musical instruments; Feature learning; Gaining insights; Musical pieces; Network connectivity; Performance data; Rendering performance; Restricted boltzmann machine; State of the art; Forecasting","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Kroher N.; Gómez E.; Guastavino C.; Gómez F.; Bonada J.","Kroher, N. (56407186200); Gómez, E. (14015483200); Guastavino, C. (8239911100); Gómez, F. (57196622778); Bonada, J. (16199826200)","56407186200; 14015483200; 8239911100; 57196622778; 16199826200","Computational models for perceived melodic similarity in a cappella flamenco singing","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971242167&partnerID=40&md5=1ac8c98ac4b5b67b95d3528deb097c35","Universitat Pompeu Fabra, Spain; McGill University, CIRMMT, Canada; Technical University of Madrid, Spain","Kroher N., Universitat Pompeu Fabra, Spain; Gómez E., Universitat Pompeu Fabra, Spain; Guastavino C., McGill University, CIRMMT, Canada; Gómez F., Technical University of Madrid, Spain; Bonada J., Universitat Pompeu Fabra, Spain","The present study investigates the mechanisms involved in the perception of melodic similarity in the context of a cappella flamenco singing performances. Flamenco songs belonging to the same style are characterized by a common melodic skeleton, which is subject to spontaneous improvisation containing strong prolongations and ornamen-tations. For our research we collected human similarity judgements from naïve and expert listeners who listened to audio recordings of a cappella flamenco performances as well as synthesized versions of the same songs. We furthermore calculated distances from manually extracted high-level descriptors defined by flamenco experts. The suitability of a set of computational melodic similarity measures was evaluated by analyzing the correlation between computed similarity and human ratings. We observed significant differences between listener groups and stimuli types. Furthermore, we observed a high correlation between human ratings and similarities computed from features from flamenco experts. We also observed that computational models based on temporal deviation, dynamics and ornamentation are better suited to model perceived similarity for this material than models based on chroma distance. © N. Kroher, E. Gómez, C. Guastavino, F. Gómez, J. Bonada.","","Computation theory; Computational methods; Information retrieval; Computational model; Descriptors; Melodic similarity; Similarity judgements; Singing performance; Audio recordings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sigtia S.; Benetos E.; Cherla S.; Weyde T.; d’Avila Garcez A.S.; Dixon S.","Sigtia, Siddharth (56304321500); Benetos, Emmanouil (16067946900); Cherla, Srikanth (24824198800); Weyde, Tillman (24476899500); d’Avila Garcez, Artur S. (57188767693); Dixon, Simon (7201479437)","56304321500; 16067946900; 24824198800; 24476899500; 57188767693; 7201479437","An RNN-based music language model for improving automatic music transcription","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946061003&partnerID=40&md5=4fb8f97d2836888b7572230f0139058a","Centre for Digital Music, Queen Mary University of London, United Kingdom; Department of Computer Science, City University, London, United Kingdom","Sigtia S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Benetos E., Department of Computer Science, City University, London, United Kingdom; Cherla S., Department of Computer Science, City University, London, United Kingdom; Weyde T., Department of Computer Science, City University, London, United Kingdom; d’Avila Garcez A.S., Department of Computer Science, City University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a significant 3% improvement in terms of F-measure, when compared to using an acoustic-only model. © S. Sigtia, E. Benetos, S. Cherla, T. Weyde, A. S. d’Avila Garcez, and S. Dixon.","","Computational linguistics; Information retrieval; Speech recognition; Statistical tests; Transcription; Automatic music transcription; Automatic speech recognition; Multiple instruments; Prior information; Prior probability; Probabilistic latent component analysis; Recurrent neural network (RNN); Temporal structures; Recurrent neural networks","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Nieto O.; Farbood M.M.","Nieto, Oriol (55583364500); Farbood, Morwaread M. (7801537706)","55583364500; 7801537706","Identifying polyphonic patterns from audio recordings using music segmentation techniques","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946037794&partnerID=40&md5=d497730b5959132238f190ed99af017a","Music and Audio Research Lab, New York University, United States","Nieto O., Music and Audio Research Lab, New York University, United States; Farbood M.M., Music and Audio Research Lab, New York University, United States","This paper presents a method for discovering patterns of note collections that repeatedly occur in a piece of music. We assume occurrences of these patterns must appear at least twice across a musical work and that they may contain slight differences in harmony, timbre, or rhythm. We describe an algorithm that makes use of techniques from the music information retrieval task of music segmentation, which exploits repetitive features in order to automatically identify polyphonic musical patterns from audio recordings. The novel algorithm is assessed using the recently published JKU Patterns Development Dataset, and we show how it obtains state-of-the-art results employing the standard evaluation metrics. © Oriol Nieto, Morwaread M. Farbood.","","Audio acoustics; Information retrieval; Music information retrieval; Music segmentations; Novel algorithm; Standard evaluations; State of the art; Audio recordings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Khadkevich M.; Omologo M.","Khadkevich, Maksim (49061151500); Omologo, Maurizio (6701724204)","49061151500; 6701724204","Large-scale cover song identification using chord profiles","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040692081&partnerID=40&md5=320134ee1642730f02b5bf184f44887e","Fondazione Bruno Kessler-irst, via Sommarive 18, Povo, 38050, Italy","Khadkevich M., Fondazione Bruno Kessler-irst, via Sommarive 18, Povo, 38050, Italy; Omologo M., Fondazione Bruno Kessler-irst, via Sommarive 18, Povo, 38050, Italy","This paper focuses on cover song identification among datasets potentially containing millions of songs. A compact representation of music contents plays an important role in large-scale analysis and retrieval. The proposed approach is based on high-level summarization of musical songs using chord profiles. Search is performed in two steps. In the first step, the Locality Sensitive Hashing (LHS) method is used to retrieve songs with similar chord profiles. On the resulting list of songs a second processing step is applied to progressively refine the ranking. Experiments conducted on both the Million Song Dataset (MSD) and a subset of the Second Hand Songs (SHS) dataset showed the effectiveness of the proposed solution, which provides state-of-the-art results. © 2013 International Society for Music Information Retrieval.","","Compact representation; Cover song identifications; Large-scale analysis; Locality sensitive hashing; Music contents; Processing steps; State of the art; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Böck S.; Widmer G.","Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","55413719000; 7004342843","Local group delay based vibrato and tremolo suppression for onset detection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069847676&partnerID=40&md5=dbe86df6074f82fc047bdc9f942e6484","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new vibrato and tremolo suppression technique for onset detection. It weights the differences of the magnitude spectrogram used for the calculation of the spectral flux onset detection function on the basis of the local group delay information. With this weighting technique applied, the onset detection function is able to reliably distinguish between genuine onsets and spectral energy peaks originating from vibrato or tremolo present in the signal and lowers the number of false positive detections considerably. Especially in cases of music with numerous vibratos and tremolos (e.g. opera singing or string performances) the number of false positive detections can be reduced by up to 50% without missing any additional events. Performance is evaluated and compared to current state-of-the-art algorithms using three different datasets comprising mixed audio material (25,927 onsets), violin recordings (7,677 onsets) and solo voice recordings of operas (1,448 onsets). © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Signal detection; False positive detection; Onset detection; Spectral energy; Spectral flux; Spectrograms; State-of-the-art algorithms; Suppression technique; Weighting techniques; Group delay","S. Böck; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: sebastian.boeck@jku.at","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Bonnin G.; Jannach D.","Bonnin, Geoffray (25927800000); Jannach, Dietmar (6602129680)","25927800000; 6602129680","Evaluating the quality of playlists based on hand-crafted samples","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032556444&partnerID=40&md5=9217cead3b186d7543bfa64e65c8e956","TU Dortmund, Germany","Bonnin G., TU Dortmund, Germany; Jannach D., TU Dortmund, Germany","The automated generation of playlists represents a particular type of the music recommendation problem with two special characteristics. First, the tracks of the list are usually consumed immediately at recommendation time; second, tracks are listened to mostly in consecutive order so that the sequence of the recommended tracks can be relevant. A number of different approaches for playlist generation have been proposed in the literature. In this paper, we review the existing core approaches to playlist generation, discuss aspects of appropriate offline evaluation designs and report the results of a comparative evaluation based on different data sets. Based on the insights from these experiments, we propose a comparably simple and computationally tractable new baseline algorithm for future comparisons, which is based on track popularity and artist information and is competitive with more sophisticated techniques in our evaluation settings. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Automated generation; Comparative evaluations; Music recommendation; Offline evaluation; Sound recording","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Song Y.; Dixon S.; Pearce M.; Halpern A.","Song, Yading (55582606800); Dixon, Simon (7201479437); Pearce, Marcus (8674558800); Halpern, Andrea (7103054771)","55582606800; 7201479437; 8674558800; 7103054771","Do online social tags predict perceived or induced emotional responses to music?","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015144404&partnerID=40&md5=09e8d083479720a21bf2b4ac2865da63","Centre for Digital Music, Queen Mary University of London, United Kingdom; Department of Psychology, Bucknell University, United States","Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Halpern A., Department of Psychology, Bucknell University, United States","Music provides a powerful means of communication and self-expression. A wealth of research has been performed on the study of music and emotion, including emotion modelling and emotion classification. The emergence of online social tags (OST) has provided highly relevant information for the study of mood, as well as an important impetus for using discrete emotion terms in the study of continuous models of affect. Yet, the extent to which human annotation reveals either perceived emotion or induced emotion remains unknown. 80 musical excerpts were randomly selected from a collection of 2904 songs labelled with the Last.fm tags “happy”, “sad”, “angry” and “relax”. Forty-seven participants provided emotion ratings on the two continuous dimensions of valence and arousal for both perceived and induced emotion. Analysis of variance did not reveal significant differences in ratings between perceived emotion and induced emotion. Moreover, the results indicated that, regardless of the discrete type of emotion experienced, listeners’ ratings of perceived and induced emotion were highly positively correlated. Finally, the emotion tags “happy”, “sad” and “angry” but not “relax” predicted the corresponding experimentally provided emotion categories. © 2013 International Society for Music Information Retrieval.","","Computer music; Information retrieval; Social networking (online); Continuous models; Emotion classification; Emotion modelling; Emotional response; Human annotations; Last.fm; Music and emotions; Social Tags; Behavioral research","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Pesek M.; Godec P.; Poredoš M.; Strle G.; Guna J.; Stojmenova E.; Pogačnik M.; Marolt M.","Pesek, Matevž (56258907000); Godec, Primož (56377091000); Poredoš, Mojca (56376897200); Strle, Gregor (35175418100); Guna, Jože (17345882300); Stojmenova, Emilija (54407530200); Pogačnik, Matevž (6603825626); Marolt, Matija (6603601816)","56258907000; 56377091000; 56376897200; 35175418100; 17345882300; 54407530200; 6603825626; 6603601816","Introducing a dataset of emotional and color responses to music","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946808365&partnerID=40&md5=18a5515197955b226b8a4eccb7225022","University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Scientific Research Centre of the Slovenian Academy of Sciences and Arts, Institute of Ethnomusicology, Slovenia; University of Ljubljana, Faculty of Electrotechnics, Slovenia","Pesek M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Godec P., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Poredoš M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia; Strle G., Scientific Research Centre of the Slovenian Academy of Sciences and Arts, Institute of Ethnomusicology, Slovenia; Guna J., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Stojmenova E., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Pogačnik M., University of Ljubljana, Faculty of Electrotechnics, Slovenia; Marolt M., University of Ljubljana, Faculty of Computer and Information Science, Slovenia","The paper presents a new dataset of mood-dependent and color responses to music. The methodology of gath-ering user responses is described along with two new inter-faces for capturing emotional states: the MoodGraph and MoodStripe. An evaluation study showed both inter-faces have significant advantage over more traditional methods in terms of intuitiveness, usability and time complexity. The preliminary analysis of current data (over 6.000 responses) gives an interesting insight into participants’ emotional states and color associations, as well as relationships between musically perceived and induced emotions. We believe the size of the dataset, in-terfaces and multi-modal approach (connecting emo-tional, visual and auditory aspects of human perception) give a valuable contribution to current research. © Matevž Pesek, Primož Godec, Mojca Poredoš, Gregor Strle, Jože Guna, Emilija Stojmenova, Matevž Pogačnik, Matija Marolt.","","Color; Information retrieval; AS relationships; Color association; Emotional state; Evaluation study; Human perception; Multi-modal approach; Preliminary analysis; Time complexity; Behavioral research","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Hamel P.; Davies M.E.P.; Yoshii K.; Goto M.","Hamel, Philippe (8402361900); Davies, Matthew E.P. (55349903900); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","8402361900; 55349903900; 7103400120; 7403505330","Transfer learning in MIR: Sharing learned latent representations for music audio classification and similarity","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069815617&partnerID=40&md5=edaea987d5938d17da4a489c70ed81a8","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Hamel P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Davies M.E.P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting, a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning, but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore, these datasets often contain relatively few songs. Thus, there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity. © 2013 International Society for Music Information Retrieval.","","Classification (of information); Information retrieval; Knowledge management; Machine learning; Semantics; Supervised learning; Audio classification; Classification accuracy; Improve performance; Music similarity; Musical concepts; Shared representations; Supervised machine learning; Transfer learning; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Madsen J.; Sand Jensen B.; Larsen J.","Madsen, Jens (57077066300); Sand Jensen, Bjørn (57210203627); Larsen, Jan (7402980719)","57077066300; 57210203627; 7402980719","Modeling temporal structure in music for emotion prediction using pairwise comparisons","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994018694&partnerID=40&md5=e7066f98c453cabfb03a41d88579d048","Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark","Madsen J., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark; Sand Jensen B., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark; Larsen J., Technical University of Denmark, Department of Applied Mathematics and Computer Science, Richard Petersens Plads, Building 321, Kongens Lyngby, 2800, Denmark","The temporal structure of music is essential for the cognitive processes related to the emotions expressed in music. However, such temporal information is often disregarded in typical Music Information Retrieval modeling tasks of predicting higher-level cognitive or semantic aspects of music such as emotions, genre, and similarity. This paper addresses the specific hypothesis whether temporal information is essential for predicting expressed emotions in music, as a prototypical example of a cognitive aspect of music. We propose to test this hypothesis using a novel processing pipeline: 1) Extracting audio features for each track resulting in a multivariate” feature time series”. 2) Using generative models to represent these time series (acquiring a complete track representation). Specifically, we explore the Gaussian Mixture model, Vector Quantization, Autore-gressive model, Markov and Hidden Markov models. 3) Utilizing the generative models in a discriminative setting by selecting the Probability Product Kernel as the natural kernel for all considered track representations. We evaluate the representations using a kernel based model specifically extended to support the robust two-alternative forced choice self-report paradigm, used for eliciting expressed emotions in music. The methods are evaluated using two data sets and show increased predictive performance using temporal information, thus supporting the overall hypothesis. © Jens Madsen, Bjørn Sand Jensen, Jan Larsen.","","Forecasting; Gaussian distribution; Hidden Markov models; Information retrieval; Semantics; Time series; Trellis codes; Alternative forced choice; Emotion predictions; Gaussian Mixture Model; Music information retrieval; Pair-wise comparison; Predictive performance; Temporal information; Temporal structures; Computer music","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Vigliensoni G.; Burgoyne J.A.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); Burgoyne, John Ashley (23007865600); Fujinaga, Ichiro (9038140900)","55217696900; 23007865600; 9038140900","MusicBrainz for the world: The Chilean experience","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014780043&partnerID=40&md5=ffcb821f670b670c53194a2a46de4360","CIRMMT, McGill University, Canada; ILLC, University of Amsterdam, Netherlands","Vigliensoni G., CIRMMT, McGill University, Canada; Burgoyne J.A., ILLC, University of Amsterdam, Netherlands; Fujinaga I., CIRMMT, McGill University, Canada","In this paper we present our research in gathering data from several semi-structured collections of cultural heritage—Chilean music-related websites—and uploading the data into an open-source music database, where the data can be easily searched, discovered, and interlinked. This paper also reviews the characteristics of four user-contributed, music metadatabases (MusicBrainz, Discogs, MusicMoz, and FreeDB), and explains why we chose MusicBrainz as the repository for our data. We also explain how we collected data from the five most important sources of Chilean music-related data, and we give details about the context, design, and results of an experiment for artist name comparison to verify which of the artists that we have in our database exist in the MusicBrainz database already. Although it represents a single case study, we believe this information will be of great help to other MIR researchers who are trying to design their own studies of world music. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Cultural heritages; Music database; Open sources; Semi-structured; Database systems","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"de Valk R.; Weyde T.; Benetos E.","de Valk, Reinier (55979450000); Weyde, Tillman (24476899500); Benetos, Emmanouil (16067946900)","55979450000; 24476899500; 16067946900","A machine learning approach to voice separation in lute tablature","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985018300&partnerID=40&md5=de3cb2b8d545f9a4ce0e9268d245a37d","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","de Valk R., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Benetos E., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","In this paper, we propose a machine learning model for voice separation in lute tablature. Lute tablature is a practical notation that reveals only very limited information about polyphonic structure. This has complicated research into the large surviving corpus of lute music, notated exclusively in tablature. A solution may be found in automatic transcription, of which voice separation is a necessary step. During the last decade, several methods for separating voices in symbolic polyphonic music formats have been developed. However, all but two of these methods adopt a rule-based approach; moreover, none of them is designed for tablature. Our method differs on both these points. First, rather than using fixed rules, we use a model that learns from data: a neural network that predicts voice assignments for notes. Second, our method is specifically designed for tablature—tablature information is included in the features used as input for the models—but it can also be applied to other music corpora. We have experimented on a dataset containing tablature pieces of different polyphonic textures, and compare the results against those obtained from a baseline hidden Markov model (HMM) model. Additionally, we have performed a preliminary comparison of the neural network model with several existing methods for voice separation on a small dataset. We have found that the neural network model performs clearly better than the baseline model, and competitively with the existing methods. © 2013 International Society for Music Information Retrieval.","","Hidden Markov models; Information retrieval; Machine learning; Textures; Automatic transcription; Limited information; Machine learning approaches; Machine learning models; Neural network model; Polyphonic texture; Rule-based approach; Voice separation; Separation","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Lee J.H.; Choi K.; Hu X.; Stephen Downie J.","Lee, Jin Ha (57190797465); Choi, Kahyun (56452038000); Hu, Xiao (55496358400); Stephen Downie, J. (7102932568)","57190797465; 56452038000; 55496358400; 7102932568","K-pop genres: A cross-cultural exploration","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053061979&partnerID=40&md5=e756ea6d947eedf6f973d9a218c12c22","University of Washington, United States; University of Illinois, United States; University of Hong Kong, Hong Kong","Lee J.H., University of Washington, United States; Choi K., University of Illinois, United States; Hu X., University of Hong Kong, Hong Kong; Stephen Downie J., University of Illinois, United States","Current music genre research tends to focus heavily on classical and popular music from Western cultures. Few studies discuss the particular challenges and issues related to non-Western music. The objective of this study is to improve our understanding of how genres are used and perceived in different cultures. In particular, this study attempts to fill gaps in our understanding by examining K-pop music genres used in Korea and comparing them with genres used in North America. We provide background information on K-pop genres by analyzing 602 genre-related labels collected from eight major music distribution websites in Korea. In addition, we report upon a user study in which American and Korean users annotated genre information for 1894 K-pop songs in order to understand how their perceptions might differ or agree. The results show higher consistency among Korean users than American users demonstrated by the difference in Fleiss’ Kappa values and proportion of agreed genre labels. Asymmetric disagreements between Americans and Koreans on specific genres reveal some interesting differences in the perception of genres. Our findings provide some insights into challenges developers may face in creating global music services. © 2013 International Society for Music Information Retrieval.","","Background information; Kappa values; Music distribution; Music genre; Popular music; User study; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Agarwal P.; Karnick H.; Raj B.","Agarwal, Parul (57210186272); Karnick, Harish (6603377058); Raj, Bhiksha (7102615577)","57210186272; 6603377058; 7102615577","A comparative study of indian and western music forms","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017535970&partnerID=40&md5=b255b833edf4f3f78222d9df5ca6a629","Indian Institute of Technology, Kanpur, India; Carnegie Mellon University, United States","Agarwal P., Indian Institute of Technology, Kanpur, India; Karnick H., Indian Institute of Technology, Kanpur, India; Raj B., Carnegie Mellon University, United States","Music in India has very ancient roots. Indian classical music is considered to be one of the oldest musical traditions in the world but compared to Western music very little work has been done in the areas of genre recognition, classification, automatic tagging, comparative studies etc. In this work, we investigate the structural differences between Indian and Western music forms and compare the two forms of music in terms of harmony, rhythm, micro-tones, timbre and other spectral features. To capture the temporal and static structure of the spectrogram, we form a set of global and local frame-wise features for 5- genres of each music form. We then apply Adaboost classification and GMM based Hidden Markov Models for four types of feature sets and observe that Indian Music performs better as compared to Western Music. We have achieved a best accuracy of 98.0% and 77.5% for Indian and Western musical genres respectively. Our comparative analysis indicates that features that work well with one form of music may not necessarily perform well with the other form. The results obtained on Indian Music Genres are better than the previous state-of-the-art. © 2013 International Society for Music Information Retrieval.","","Adaptive boosting; Information retrieval; Adaboost classifications; Automatic tagging; Comparative analysis; Comparative studies; Indian classical music; Spectral feature; Static structures; Structural differences; Hidden Markov models","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Porter A.; Sordo M.; Serra X.","Porter, Alastair (55582507300); Sordo, Mohamed (43462170300); Serra, Xavier (55892979900)","55582507300; 43462170300; 55892979900","Dunya: A system for browsing audio music collections exploiting cultural context","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008909252&partnerID=40&md5=52014f94d135c56a79a0ea17cce1f359","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Porter A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Music recommendation and discovery is an important MIR application with a strong impact in the music industry, but most music recommendation systems are still quite generic and without much musical knowledge. In this paper we present a web-based software application that lets users interact with an audio music collection through the use of musical concepts that are derived from a specific musical culture, in this case Carnatic music. The application includes a database containing information relevant to that music collection, such as audio recordings, editorial information, and metadata obtained from various sources. An analysis module extracts features from the audio recordings that are related to Carnatic music, which are then used to create musically meaningful relationships between all of the items in the database. The application displays the content of these items, allowing users to navigate through the collection by identifying and showing other information that is related to the currently viewed item, either by showing the relationships between them or by using culturally relevant similarity measures. The basic architecture and the design principles developed are reusable for other music collections with different characteristics. © 2013 International Society for Music Information Retrieval.","","Application programs; Audio acoustics; Computer software reusability; Recommender systems; Cultural context; Design Principles; Music collection; Music recommendation; Music Recommendation System; Musical concepts; Similarity measure; Web-based softwares; Audio recordings","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Burgoyne J.A.; Bountouridis D.; Van Balen J.; Honing H.","Burgoyne, John Ashley (23007865600); Bountouridis, Dimitrios (36607463900); Van Balen, Jan (55874307000); Honing, Henkjan (6603233047)","23007865600; 36607463900; 55874307000; 6603233047","Hooked: A game for discovering what makes music catchy","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016305231&partnerID=40&md5=fad07bee794580b098704a67d673ed4b","Music Cognition Group, University of Amsterdam, Netherlands; Department of Information and Computing Sciences, Utrecht University, Netherlands","Burgoyne J.A., Music Cognition Group, University of Amsterdam, Netherlands; Bountouridis D., Department of Information and Computing Sciences, Utrecht University, Netherlands; Van Balen J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Honing H., Music Cognition Group, University of Amsterdam, Netherlands","Although there has been some empirical research on earworms, songs that become caught and replayed in one’s memory over and over again, there has been surprisingly little empirical research on the more general concept of the musical hook, the most salient moment in a piece of music, or the even more general concept of what may make music ‘catchy’. Almost by definition, people like catchy music, and thus this question is a natural candidate for approaching with ‘gamification’. We present the design of Hooked, a game we are using to study musical catchiness, as well as the theories underlying its design and the results of a pilot study we undertook to check its scientific validity. We found significant differences in time to recall pieces of music across different segments, identified parameters for making recall tasks more or less challenging, and found that players are not as reliable as one might expect at predicting their own recall performance. © 2013 International Society for Music Information Retrieval.","","Empirical research; Gamification; Identified parameter; Pilot studies; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Prätzlich T.; Müller M.","Prätzlich, Thomas (55582692100); Müller, Meinard (7404689873)","55582692100; 7404689873","Freischütz digital: A case study for reference-based audio segmentation of operas","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069822684&partnerID=40&md5=5a9fc9a3a69bb63172c4274d1f3a918e","International Audio Laboratories Erlangen, Germany","Prätzlich T., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany","Music information retrieval has started to become more and more important in the humanities by providing tools for computer-assisted processing and analysis of music data. However, when applied to real-world scenarios, even established techniques, which are often developed and tested under lab conditions, reach their limits. In this paper, we illustrate some of these challenges by presenting a study on automated audio segmentation in the context of the interdisciplinary project “Freischütz Digital”. One basic task arising in this project is to automatically segment different recordings of the opera “Der Freischütz” according to a reference segmentation specified by a domain expert (musicologist). As it turns out, the task is more complex as one may think at first glance due to significant acoustic and structural variations across the various recordings. As our main contribution, we reveal and discuss these variations by systematically adapting segmentation procedures based on synchronization and matching techniques. © 2013 International Society for Music Information Retrieval.","","Audio signal processing; Audio systems; Computer aided analysis; Information retrieval; Audio segmentation; Computer assisted; Interdisciplinary project; Matching techniques; Music information retrieval; Real-world scenario; Segmentation procedure; Structural variations; Audio recordings","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Gao B.; Dellandréa E.; Chen L.","Gao, Boyang (56303302500); Dellandréa, Emmanuel (15844758600); Chen, Liming (35228518000)","56303302500; 15844758600; 35228518000","Sparse music decomposition onto a MIDI dictionary driven by statistical music knowledge","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069881011&partnerID=40&md5=4528d45d2b09ad717c4175b3def7f65b","Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France","Gao B., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France; Dellandréa E., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France; Chen L., Université de Lyon, CNRS, Ecole centrale de Lyon, LIRIS, UMR5205, F-69134, France","The general goal of music signal decomposition is to represent the music structure into a note level to provide valuable semantic features for further music analysis tasks. In this paper, we propose a new method to sparsely decompose the music signal onto a MIDI dictionary made of musical notes. Statistical music knowledge is further integrated into the whole sparse decomposition process. The proposed method is divided into a frame level sparse decomposition stage and a whole music level optimal note path searching. In the first stage note co-occurrence probabilities are embedded to generate a sparse multiple candidate graph while in the second stage note transition probabilities are incorporated into the optimal path searching. Experiments on real-world polyphonic music show that embedding music knowledge within the sparse decomposition achieves notable improvement in terms of note recognition precision and recall. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Semantics; Co-occurrence probability; Music decompositions; Music structures; Precision and recall; Semantic features; Sparse decomposition; Sparse multiples; Transition probabilities; Signal processing","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Benetos E.; Holzapfel A.","Benetos, Emmanouil (16067946900); Holzapfel, Andre (18041818000)","16067946900; 18041818000","Automatic transcription of Turkish makam music","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050217187&partnerID=40&md5=72585a0704ef22c129ebd539a333fd25","City University London, United Kingdom; Boğaziçi University, Turkey","Benetos E., City University London, United Kingdom; Holzapfel A., Boğaziçi University, Turkey","In this paper we propose an automatic system for transcribing makam music of Turkey. We document the specific traits of this music that deviate from properties that were targeted by transcription tools so far and we compile a dataset of makam recordings along with aligned microtonal ground-truth. An existing multi-pitch detection algorithm is adapted for transcribing music in 20 cent resolution, and the final transcription is centered around the tonic frequency of the recording. Evaluation metrics for transcribing microtonal music are utilized and results show that transcription of Turkish makam music in e.g. an interactive transcription software is feasible using the current state-of-the-art. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Transcription; Automatic systems; Automatic transcription; Evaluation metrics; Ground truth; Multi pitches; State of the art; Turkishs; Computer music","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Smith J.B.L.; Chew E.","Smith, Jordan B.L. (55582613300); Chew, Elaine (8706714000)","55582613300; 8706714000","A meta-analysis of the mirex structure segmentation task","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994518163&partnerID=40&md5=2dbec4d359876821d39d553f822a19c3","Queen Mary, University of London, United Kingdom","Smith J.B.L., Queen Mary, University of London, United Kingdom; Chew E., Queen Mary, University of London, United Kingdom","The Music Information Retrieval Evaluation eXchange (MIREX) serves an essential function in the MIR community, but researchers have noted that the anonymity of its datasets, while useful, has made it difficult to interpret the successes and failures of the algorithms. We use the results of the 2012 MIREX Structural Segmentation task, which was accompanied by anonymous ground truth, to conduct a meta-evaluation of the algorithms. We hope this demonstrates the benefits, to both the participants and evaluators of MIREX, of releasing more data in evaluation tasks. Our aim is to learn more about the performance of the algorithms by studying how their success relates to properties of the annotations and recordings. We find that some evaluation metrics are redundant, and that several algorithms do not adequately model the true number of segments in typical annotations We also use publicly available ground truth to identify many of the recordings in the MIREX test sets, allowing us to identify specific pieces on which algorithms generally performed poorly and to discover where the most improvement is needed. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Evaluation metrics; Ground truth; Meta analysis; Music information retrieval; Structure segmentation; Test sets; Insecticides","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Moreira J.; Roy P.; Pachet F.","Moreira, Julian (57205486315); Roy, Pierre (55506419000); Pachet, François (6701441655)","57205486315; 55506419000; 6701441655","VirtualBand: Interacting with stylistically consistent agents","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059163106&partnerID=40&md5=985db2aa4f73be6cb54bd33f281c2a37","Sony CSL, Japan","Moreira J., Sony CSL, Japan; Roy P., Sony CSL, Japan; Pachet F., Sony CSL, Japan","VirtualBand is a multi-agent system dedicated to live computer-enhanced music performances. VirtualBand enables one or several musicians to interact in real-time with stylistically plausible virtual agents. The problem addressed is the generation of virtual agents, each representing the style of a given musician, while reacting to human players. We propose a generation framework that relies on feature-based interaction. Virtual agents exploit a style database, which consists of audio signals from which a set of MIR features are extracted. Musical interactions are represented by directed connections between agents through these features. The connections are themselves specified as mappings and database filters. We claim that such a connection framework allows to implement meaningful musical interactions and to produce stylistically consistent musical output. We illustrate this concept through several examples in jazz improvisation, beatboxing and interactive mash-ups. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Virtual reality; Audio signal; Database filters; Feature-based; Human players; Music performance; Real time; Virtual agent; Multi agent systems","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Stober S.; Cameron D.J.; Grahn J.A.","Stober, Sebastian (14027561800); Cameron, Daniel J. (56373210600); Grahn, Jessica A. (13407386100)","14027561800; 56373210600; 13407386100","Classifying EEG recordings of rhythm perception","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937702257&partnerID=40&md5=adbe973692c5eb4848fad5fa588a2b74","Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Stober S., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Cameron D.J., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada; Grahn J.A., Brain and Mind Institute, Department of Psychology, Western University, London, ON, Canada","Electroencephalography (EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. In this paper, we present first classification results using deep learning techniques on EEG data recorded within a rhythm perception study in Kigali, Rwanda. We tested 13 adults, mean age 21, who performed three behavioral tasks using rhythmic tone sequences derived from either East African or Western music. For the EEG testing, 24 rhythms – half East African and half Western with identical tempo and based on a 2-bar 12/8 scheme – were each repeated for 32 seconds. During presentation, the participants’ brain waves were recorded via 14 EEG channels. We applied stacked denoising autoencoders and convolutional neural networks on the collected data to distinguish African and Western rhythms on a group and individual participant level. Furthermore, we investigated how far these techniques can be used to recognize the individual rhythms. © Sebastian Stober, Daniel J. Cameron and Jessica A. Grahn.","","Audio recordings; Deep learning; Electrophysiology; Information retrieval; Neural networks; Autoencoders; Classification results; Convolutional neural network; De-noising; East africans; EEG recording; Learning techniques; Rhythm perception; Electroencephalography","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Joren S.; Leman M.","Joren, Six (55585855700); Leman, Marc (6603703642)","55585855700; 6603703642","Panako - A scalable acoustic fingerprinting system handling time-scale and pitch modification","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942370708&partnerID=40&md5=fc4426050f5b70be5e05bae64c4ac61a","Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium","Joren S., Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium; Leman M., Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium","This paper presents a scalable granular acoustic fingerprinting system. An acoustic fingerprinting system uses condensed representation of audio signals, acoustic fingerprints, to identify short audio fragments in large audio databases. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The system presented here is designed to handle time-scale and pitch modifications. The open source implementation of the system is called Panako and is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30,000 songs. The results show that the system responds quickly and reliably on queries, while handling time-scale and pitch modifications of up to ten percent. The system is also shown to handle GSM-compression, several audio effects and band-pass filtering. After a query, the system returns the start time in the reference audio and how much the query has been pitch-shifted or time-stretched with respect to the reference audio. The design of the system that offers this combination of features is the main contribution of this paper. © Six Joren, Marc Leman.","","Information retrieval; Open systems; Query processing; Time measurement; Acoustic fingerprint; Audio database; Band pass filtering; Commodity hardware; Condensed representations; Open source implementation; Pitch modification; Reference database; Audio acoustics","S. Joren; Institute for Psychoacoustics and Electronic Music (IPEM), Department of Musicology, Ghent University, Ghent, Belgium; email: joren.six@ugent.be","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Rafii Z.; Germain F.G.; Sun D.L.; Mysore G.J.","Rafii, Zafar (44461970600); Germain, François G. (36845735700); Sun, Dennis L. (58831687000); Mysore, Gautham J. (24465525500)","44461970600; 36845735700; 58831687000; 24465525500","Combining modeling of singing voice and background music for automatic separation of musical mixtures","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057712454&partnerID=40&md5=0aa4fad1e4cf8f7a7e9e917f95a6c4d5","Northwestern University, Department of Electrical Engineering and Computer Science, United States; Stanford University, Center for Computer Research in Music and Acoustics, United States; Stanford University, Department of Statistics, United States; Adobe Research, United States","Rafii Z., Northwestern University, Department of Electrical Engineering and Computer Science, United States; Germain F.G., Stanford University, Center for Computer Research in Music and Acoustics, United States; Sun D.L., Stanford University, Center for Computer Research in Music and Acoustics, United States, Stanford University, Department of Statistics, United States; Mysore G.J., Adobe Research, United States","Musical mixtures can be modeled as being composed of two characteristic sources: singing voice and background music. Many music/voice separation techniques tend to focus on modeling one source; the residual is then used to explain the other source. In such cases, separation performance is often unsatisfactory for the source that has not been explicitly modeled. In this work, we propose to combine a method that explicitly models singing voice with a method that explicitly models background music, to address separation performance from the point of view of both sources. One method learns a singer-independent model of voice from singing examples using a Non-negative Matrix Factorization (NMF) based technique, while the other method derives a model of music by identifying and extracting repeating patterns using a similarity matrix and a median filter. Since the model of voice is singer-independent and the model of music does not require training data, the proposed method does not require training data from a user, once deployed. Evaluation on a data set of 1,000 song clips showed that combining modeling of both sources can improve separation performance, when compared with modeling only one of the sources, and also compared with two other state-of-the-art methods. © 2013 International Society for Music Information Retrieval.","","Factorization; Information retrieval; Matrix algebra; Median filters; Mixtures; Separation; Source separation; Automatic separations; Characteristic sources; Independent model; Music/voice separations; Nonnegative matrix factorization; Repeating patterns; Separation performance; State-of-the-art methods; Computer music","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Pesek M.; Leonardis A.; Marolt M.","Pesek, Matevž (56258907000); Leonardis, Aleš (57197724902); Marolt, Matija (6603601816)","56258907000; 57197724902; 6603601816","A compositional hierarchical model for music information retrieval","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008439280&partnerID=40&md5=48db903c5b1548b8bca9937e20c1e98a","University of Ljubljana, Faculty of computer and information science, Slovenia; Centre for Computational Neuroscience and Cognitive Robotics, School of Computer Science, University of Birmingham, United Kingdom","Pesek M., University of Ljubljana, Faculty of computer and information science, Slovenia; Leonardis A., Centre for Computational Neuroscience and Cognitive Robotics, School of Computer Science, University of Birmingham, United Kingdom; Marolt M., Centre for Computational Neuroscience and Cognitive Robotics, School of Computer Science, University of Birmingham, United Kingdom","This paper presents a biologically-inspired compositional hierarchical model for MIR. The model can be treated as a deep learning model, and poses an alternative to deep architectures based on neural networks. Its main features are generativeness and transparency that allow clear insight into concepts learned from the input music signals. The model consists of multiple layers, each is composed of a number of parts. The hierarchical nature of the model corresponds well with the hierarchical structures in music. Parts in lower layers correspond to low-level concepts (e.g. tone partials), while parts in higher layers combine lower-level representations into more complex concepts (tones, chords). The layers are unsupervisedly learned one-by-one from music signals. Parts in each layer are compositions of parts from previous layers based on statistical co-occurrences as the driving force of the learning process. We present the model’s structure and compare it to other deep architectures. A preliminary evaluation of the model’s usefulness for automated chord estimation and multiple fundamental frequency estimation tasks is provided. Additionally, we show how the model can be extended to event-based music processing, which is our final goal. © Matevž Pesek, Aleš Leonardis, Matija Marolt.","","Frequency estimation; Hierarchical systems; Information retrieval; Network architecture; Biologically inspired; Deep architectures; Hierarchical model; Hierarchical structures; Learning models; Learning process; Multiple fundamental frequency estimations; Music information retrieval; Deep learning","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Davies M.E.P.; Hamel P.; Yoshii K.; Goto M.","Davies, Matthew E.P. (55349903900); Hamel, Philippe (8402361900); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","55349903900; 8402361900; 7103400120; 7403505330","AutoMashUpper: An automatic multi-song mashup system","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057364410&partnerID=40&md5=e81e282a8cf209d3c8efd08c2d803373","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Davies M.E.P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Hamel P., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper describes AutoMashUpper, an interactive system for creating music mashups by automatically selecting and mixing multiple songs together. Given a user-specified input song, the system first identifies the phrase-level structure and then estimates the “mashability” between each phrase section of the input and songs in the user’s music collection. Mashability is calculated based on the harmonic similarity between beat synchronous chromagrams over a user-definable range of allowable key shifts and tempi. Once a match in the collection for a given section of the input song has been found, a pitch-shifting and time-stretching algorithm is used to harmonically and temporally align the sections, after which the loudness of the transformed section is modified to ensure a balanced mix. AutoMashUpper has a user interface to allow visualisation and manipulation of mashups. When creating a mashup, users can specify a list of songs to choose from, modify the mashability parameters and change the granularity of the phrase segmentation. Once created, users can also switch, add, or remove sections from the mashup to suit their taste. In this way, AutoMashUpper can assist users to actively create new music content by enabling and encouraging them to explore the mashup space. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Interactive system; Level structure; Music collection; Music contents; Phrase segmentations; Pitch shifting; Time stretching; Transformed sections; User interfaces","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Panteli M.; Purwins H.","Panteli, Maria (55915922100); Purwins, Hendrik (24336076900)","55915922100; 24336076900","A computational comparison of theory and practice of scale intonation in Byzantine chant","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069841913&partnerID=40&md5=4c7817643844767a5e5e9c317e91e40f","Department of Computer Science, University of Cyprus, Cyprus; Neurotechnology Group, EE and CS, Berlin Institute of Technology, Germany; Sound and Music Computing Group, Aalborg University, Copenhagen, Denmark","Panteli M., Department of Computer Science, University of Cyprus, Cyprus; Purwins H., Neurotechnology Group, EE and CS, Berlin Institute of Technology, Germany, Sound and Music Computing Group, Aalborg University, Copenhagen, Denmark","Byzantine Chant performance practice is quantitatively compared to the Chrysanthine theory. The intonation of scale degrees is quantified, based on pitch class profiles. An analysis procedure is introduced that consists of the following steps: 1) Pitch class histograms are calculated via non-parametric kernel smoothing. 2) Histogram peaks are detected. 3) Phrase ending analysis aids the finding of the tonic to align histogram peaks. 4) The theoretical scale degrees are mapped to the practical ones. 5) A schema of statistical tests detects significant deviations of theoretical scale tuning from the estimated ones in performance practice. The analysis of 94 echoi shows a tendency of the singer to level theoretic particularities of the echos that stand out of the general norm in the octoechos: theoretically extremely large scale steps are diminished in performance. © 2013 International Society for Music Information Retrieval.","","Graphic methods; Information retrieval; Computational comparisons; Kernel smoothing; Non-parametric; Theory and practice; Computation theory","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Kaneshiro B.; Kim H.-S.; Herrera J.; Oh J.; Berger J.; Slaney M.","Kaneshiro, Blair (56884405500); Kim, Hyung-Suk (57195929637); Herrera, Jorge (57212698361); Oh, Jieun (55871127300); Berger, Jonathan (7403413229); Slaney, Malcolm (6701855101)","56884405500; 57195929637; 57212698361; 55871127300; 7403413229; 6701855101","QBT-extended: An annotated dataset of melodically contoured tapped queries","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069804344&partnerID=40&md5=6f93c149df156ce8792558a29d752a1c","CCRMA, Stanford University, United States; Microsoft Research, CCRMA, United States","Kaneshiro B., CCRMA, Stanford University, United States; Kim H.-S., CCRMA, Stanford University, United States; Herrera J., CCRMA, Stanford University, United States; Oh J., CCRMA, Stanford University, United States; Berger J., CCRMA, Stanford University, United States; Slaney M., CCRMA, Stanford University, United States, Microsoft Research, CCRMA, United States","Query by tapping remains an intuitive yet underdeveloped form of content-based querying. Tapping databases suffer from small size and often lack useful annotations about users and query cues. More broadly, tapped representations of music are inherently lossy, as they lack pitch information. To address these issues, we publish QBT-Extended—an annotated dataset of over 3,300 tapped queries of pop song excerpts, along with a system for collecting them. The queries, collected from 60 users for 51 songs, contain both time stamps and pitch positions of tap events and are annotated with information about the user, such as musical training and familiarity with each excerpt. Queries were performed from both short-term and long-term memory, cued by lyrics alone or lyrics and audio. In the present paper, we characterize and evaluate the dataset and perform initial analyses, providing early insights into the added value of the novel information. While the current data were collected under controlled experimental conditions, the system is designed for large-scale, crowdsourced data collection, presenting an opportunity to expand upon this richer form of tapping data. © 2013 International Society for Music Information Retrieval.","","Added values; Content-based; Current data; Data collection; Experimental conditions; Long term memory; Novel information; Time stamps; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Han Y.; Lee K.","Han, Yoonchang (56096725400); Lee, Kyogu (8597995500)","56096725400; 8597995500","Hierarchical approach to detect common mistakes of beginner flute players","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955100773&partnerID=40&md5=70621b77fa3666a4036dc2b2ed272e9e","Music and Audio Research Group, Seoul National University, Seoul, South Korea","Han Y., Music and Audio Research Group, Seoul National University, Seoul, South Korea; Lee K., Music and Audio Research Group, Seoul National University, Seoul, South Korea","Music lessons are a repetitive process of giving feedback on a student’s performance techniques. The manner in which performance skills are improved depends on the particular instrument, and therefore, it is important to consider the unique characteristics of the target instrument. In this paper, we investigate the common mistakes of beginner flute players and propose a hierarchical approach to detect such mistakes. We first examine the structure and mechanism of the flute, and define several types of common mistakes that can be caused by incorrect assembly, poor blowing skills, or mis-fingering. We propose tailored algorithms for detecting each case by combining deterministic signal processing and deep learning, to quantify the quality of a flute sound. The system is structured hierarchically, as mis-fingering detection requires the input sound to be correctly assembled and blown to discriminate minor sound difference. Experimental results show that it is possible to identify different mistakes in flute performance using our proposed algorithms. © First author, Second author, Third author.","","Deep learning; Information retrieval; Deterministic signals; Hierarchical approach; Repetitive process; Signal processing","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Yen F.; Luo Y.-J.; Chi T.-S.","Yen, Frederick (57140019100); Luo, Yin-Jyun (57226047667); Chi, Tai-Shih (7005693586)","57140019100; 57226047667; 7005693586","Singing voice separation using spectro-temporal modulation features","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976221687&partnerID=40&md5=e27ca66b4355138c07c7e91339801ecd","Master Program of SMIT, National Chiao-Tung University, Taiwan; Dept. of Elec. and Comp. Engineering, National Chiao-Tung University, Taiwan","Yen F., Master Program of SMIT, National Chiao-Tung University, Taiwan; Luo Y.-J., Master Program of SMIT, National Chiao-Tung University, Taiwan; Chi T.-S., Dept. of Elec. and Comp. Engineering, National Chiao-Tung University, Taiwan","An auditory-perception inspired singing voice separation algorithm for monaural music recordings is proposed in this paper. Under the framework of computational auditory scene analysis (CASA), the music recordings are first transformed into auditory spectrograms. After extracting the spectral-temporal modulation contents of the time-frequency (T-F) units through a two-stage auditory model, we define modulation features pertaining to three categories in music audio signals: vocal, harmonic, and percussive. The T-F units are then clustered into three categories and the singing voice is synthesized from T-F units in the vocal category via time-frequency masking. The algorithm was tested using the MIR-1K dataset and demonstrated comparable results to other unsupervised masking approaches. Meanwhile, the set of novel features gives a possible explanation on how the auditory cortex analyzes and identifies singing voice in music audio mixtures. © Frederick Yen, Yin-Jyun Luo, Tai-Shih Chi.","","Audio acoustics; Audio recordings; Audition; Information retrieval; Auditory perception; Computational auditory scene analysis; Modulation Features; Singing voice separations; Spectro-temporal modulations; Temporal modulations; Time frequency(T F) units; Time-Frequency Masking; Modulation","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Grant M.; Ekanayake A.; Turnbull D.","Grant, Maurice (57210186643); Ekanayake, Adeesha (57210193821); Turnbull, Douglas (8380095700)","57210186643; 57210193821; 8380095700","Meuse: Recommending internet radio stations","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030992562&partnerID=40&md5=7ca8cfab5f82370d9062775bff48e32d","Ithaca College, United States","Grant M., Ithaca College, United States; Ekanayake A., Ithaca College, United States; Turnbull D., Ithaca College, United States","In this paper, we describe a novel Internet radio recommendation system called MeUse. We use the Shoutcast API to collect historical data about the artists that are played on a large set of Internet radio stations. This data is used to populate an artist-station index that is similar to the term-document matrix of a traditional text-based information retrieval system. When a user wants to find stations for a given seed artist, we check the index to determine a set of stations that are either currently playing or have recently played that artist. These stations are grouped into three clusters and one representative station is selected from each cluster. This promotes diversity among the stations that are returned to the user. In addition, we provide additional information such as relevant tags (e.g., genres, emotions) and similar artists to give the user more contextual information about the recommended stations. Finally, we describe a web-based user interface that provides an interactive experience that is more like a personalized Internet radio player (e.g., Pandora) and less like a search engine for Internet radio stations (e.g., Shoutcast). A small-scale user study suggests that the majority of users enjoyed using MeUse but that providing additional contextual information may be needed to help with recommendation transparency. © 2013 International Society for Music Information Retrieval.","","Radio stations; Recommender systems; User interfaces; Contextual information; Document matrices; Historical data; Internet radio; Small scale; Text-based information; User study; Web based; Search engines","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Bogdanov D.; Wack N.; Gómez E.; Gulati S.; Herrera P.; Mayor O.; Roma G.; Salamon J.; Zapata J.; Serra X.","Bogdanov, Dmitry (35748642000); Wack, Nicolas (8373003100); Gómez, Emilia (14015483200); Gulati, Sankalp (37087243200); Herrera, Perfecto (24824250300); Mayor, Oscar (35799867600); Roma, Gerard (57191952463); Salamon, Justin (55184866100); Zapata, José (55349861800); Serra, Xavier (55892979900)","35748642000; 8373003100; 14015483200; 37087243200; 24824250300; 35799867600; 57191952463; 55184866100; 55349861800; 55892979900","Essentia: An audio analysis library for music information retrieval","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","262","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019537868&partnerID=40&md5=b9ece58c08d191f5921f6b4fc48c3553","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Wack N., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Mayor O., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Roma G., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Zapata J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications. © 2013 International Society for Music Information Retrieval.","","C++ (programming language); Computer operating systems; Digital signal processing; Industrial research; Information retrieval; Audio analysis; Computational costs; Fast prototyping; Music information retrieval; Re-usable algorithms; Signal processing algorithms; Statistical characterization; Windows system; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Korzeniowski F.; Böck S.; Widmer G.","Korzeniowski, Filip (56328099700); Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","56328099700; 55413719000; 7004342843","Probabilistic extraction of beat positions from a beat activation function","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003466090&partnerID=40&md5=7c66f23e97c75c972a879403b329b033","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Korzeniowski F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker currently uses for this purpose. Our experiments show improvement upon the current method for a variety of data sets and quality measures, as well as better results compared to other state-of-the-art algorithms. © Filip Korzeniowski, Sebastian Böck, Gerhard Widmer.","","Information retrieval; Activation functions; Greedy search; Quality measures; State-of-the-art algorithms; Chemical activation","F. Korzeniowski; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: filip.korzeniowski@jku.at","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"McFee B.; Ellis D.P.W.","McFee, Brian (34875379700); Ellis, Daniel P.W. (13609089200)","34875379700; 13609089200","Analyzing song structure with spectral clustering","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","51","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946067288&partnerID=40&md5=eb5b8e66f2b6b15ba0db2242f4bf192e","Center for Jazz Studies, Columbia University, United States; LabROSA, Columbia University, United States","McFee B., Center for Jazz Studies, Columbia University, United States; Ellis D.P.W., LabROSA, Columbia University, United States","Many approaches to analyzing the structure of a musical recording involve detecting sequential patterns within a self-similarity matrix derived from time-series features. Such patterns ideally capture repeated sequences, which then form the building blocks of large-scale structure. In this work, techniques from spectral graph theory are applied to analyze repeated patterns in musical recordings. The proposed method produces a low-dimensional encoding of repetition structure, and exposes the hierarchical relationships among structural components at differing levels of granularity. Finally, we demonstrate how to apply the proposed method to the task of music segmentation. © Brian McFee, Daniel P.W. Ellis.","","Clustering algorithms; Graph theory; Information retrieval; Large scale structures; Music segmentations; Self-similarity matrix; Sequential patterns; Spectral clustering; Spectral graph theory; Structural component; Time series features; Audio recordings","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Koerich A.L.","Koerich, Alessandro L. (6602596292)","6602596292","Improving the reliability of music genre classification using rejection and verification","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032463965&partnerID=40&md5=2d96007906abbb5c71aa65ce599b48b5","Pontifical Catholic University of Paraná (PUCPR), Federal University of Paraná (UFPR), Brazil","Koerich A.L., Pontifical Catholic University of Paraná (PUCPR), Federal University of Paraná (UFPR), Brazil","This paper presents a novel approach for post-processing the music genre hypotheses generated by a baseline classifier. Given a music piece, the baseline classifier produces a ranked list of the N best hypotheses consisting of music genre labels and recognition scores. A rejection strategy is then applied to either reject or accept the output of the baseline classifier. Some of the rejected instances are handled by a verification stage which extracts visual features from the spectrogram of the music signal and employs binary support vector machine classifiers to disambiguate between confusing classes. The rejection and verification approach has improved the reliability in classifying music genres. Our approach is described in detail and the experimental results on a benchmark dataset are presented. © 2013 International Society for Music Information Retrieval.","","Support vector machines; Benchmark datasets; Binary support vector machines; Music genre classification; Music signals; N-best hypothesis; Post processing; Spectrograms; Visual feature; Information retrieval","A.L. Koerich; Pontifical Catholic University of Paraná (PUCPR), Federal University of Paraná (UFPR), Brazil; email: alekoe@computer.org","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"van Kranenburg P.; Karsdorp F.","van Kranenburg, Peter (35108158000); Karsdorp, Folgert (56297352000)","35108158000; 56297352000","Cadence detection in western traditional stanzaic songs using melodic and textual features","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956838999&partnerID=40&md5=b8ccb551c230dce322546397b6f735ac","Meertens Institute, Amsterdam, Netherlands","van Kranenburg P., Meertens Institute, Amsterdam, Netherlands; Karsdorp F., Meertens Institute, Amsterdam, Netherlands","Many Western songs are hierarchically structured in stanzas and phrases. The melody of the song is repeated for each stanza, while the lyrics vary. Each stanza is subdivided into phrases. It is to be expected that melodic and textual formulas at the end of the phrases offer intrinsic clues of closure to a listener or singer. In the current paper we aim at a method to detect such cadences in symbolically encoded folk songs. We take a trigram approach in which we classify trigrams of notes and pitches as cadential or as non-cadential. We use pitch, contour, rhythmic, textual, and contextual features, and a group of features based on the conditions of closure as stated by Narmour [11]. We employ a random forest classification algorithm. The precision of the classifier is considerably improved by taking the class labels of adjacent trigrams into account. An ablation study shows that none of the kinds of features is sufficient to account for good classification, while some of the groups perform moderately well on their own. © Peter van Kranenburg, Folgert Karsdorp.","","Decision trees; Class labels; Contextual feature; Folk songs; Random forest classification; Textual features; Tri grams; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sidorov K.; Jones A.; Marshall D.","Sidorov, Kirill (35180427500); Jones, Andrew (56900015400); Marshall, David (57203049547)","35180427500; 56900015400; 57203049547","Music analysis as a smallest grammar problem","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956743603&partnerID=40&md5=1810a62254128d354d937917ced1c664","Cardiff University, United Kingdom","Sidorov K., Cardiff University, United Kingdom; Jones A., Cardiff University, United Kingdom; Marshall D., Cardiff University, United Kingdom","In this paper we present a novel approach to music analysis, in which a grammar is automatically generated explaining a musical work’s structure. The proposed method is predicated on the hypothesis that the shortest possible grammar provides a model of the musical structure which is a good representation of the composer’s intent. The effectiveness of our approach is demonstrated by comparison of the results with previously-published expert analysis; our automated approach produces results comparable to human annotation. We also illustrate the power of our approach by showing that it is able to locate errors in scores, such as introduced by OMR or human transcription. Further, our approach provides a novel mechanism for intuitive high-level editing and creative transformation of music. A wide range of other possible applications exists, including automatic summarization and simplification; estimation of musical complexity and similarity, and plagiarism detection. © Kirill Sidorov, Andrew Jones, David Marshall.","","Automated approach; Automatic summarization; Automatically generated; Expert analysis; Human annotations; Musical structures; Plagiarism detection; Smallest grammar problem; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Hamanaka M.; Hirata K.; Tojo S.","Hamanaka, Masatoshi (35253968400); Hirata, Keiji (55730620800); Tojo, Satoshi (7103319884)","35253968400; 55730620800; 7103319884","Musical structural analysis database based on GTTM","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956763221&partnerID=40&md5=3853cbed46c71bd912dbd62bc14d848f","Kyoto University, Japan; Future University Hakodate, Japan; JAIST, Japan","Hamanaka M., Kyoto University, Japan; Hirata K., Future University Hakodate, Japan; Tojo S., JAIST, Japan","This paper, we present the publication of our analysis data and analyzing tool based on the generative theory of tonal music (GTTM). Musical databases such as score databases, instrument sound databases, and musical pieces with standard MIDI files and annotated data are key to advancements in the field of music information technology. We started implementing the GTTM on a computer in 2004 and ever since have collected and publicized test data by musicologists in a step-by-step manner. In our efforts to further advance the research on musical structure analysis, we are now publicizing 300 pieces of analysis data as well as the analyzer. Experiments showed that for 267 of 300 pieces the analysis results obtained by a new musicologist were almost the same as the original results in the GTTM database and that the other 33 pieces had different interpretations. © Masatoshi Hamanaka, Keiji Hirata, Satoshi Tojo.","","Information retrieval; MIDI files; Music information technology; Musical database; Musical pieces; Musical structures; Sound database; Test data; Tonal music; Database systems","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Hu Y.; Li D.; Mitsunori O.","Hu, Yajie (55364877000); Li, Dingding (57210191877); Mitsunori, Ogihara (54420747900)","55364877000; 57210191877; 54420747900","Evaluation on feature importance for favorite song detection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069879439&partnerID=40&md5=1eac4a3749d07a46aaaebb8ede1ad820","Department of Computer Science, University of Miami, United States","Hu Y., Department of Computer Science, University of Miami, United States; Li D., Department of Computer Science, University of Miami, United States; Mitsunori O., Department of Computer Science, University of Miami, United States","Detecting whether a song is favorite for a user is an important but also challenging task in music recommendation. One of critical steps to do this task is to select important features for the detection. This paper presents two methods to evaluate feature importance, in which we compared nine available features based on a large user log in the real world. The set of features includes song metadata, acoustic feature, and user preference used by Collaborative Filtering techniques. The evaluation methods are designed from two views: i) the correlation between the estimated scores by song similarity in respect of a feature and the scores estimated by real play count, ii) feature selection methods over a binary classification problem, i.e., “like” or “dislike”. The experimental results show the user preference is the most important feature and artist similarity is of the second importance among these nine features. © 2013 International Society for Music Information Retrieval.","","Collaborative filtering; Information retrieval; Acoustic features; Artist similarities; Binary classification problems; Collaborative filtering techniques; Evaluation methods; Feature selection methods; Important features; Music recommendation; Feature extraction","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cartwright M.; Pardo B.","Cartwright, Mark (53979459000); Pardo, Bryan (10242155400)","53979459000; 10242155400","Social-EQ: Crowdsourcing an equalization descriptor map","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054226848&partnerID=40&md5=66d11e5dcc51bf6a97ef6284c4718996","Northwestern University, EECS Department, United States","Cartwright M., Northwestern University, EECS Department, United States; Pardo B., Northwestern University, EECS Department, United States","We seek to simplify audio production interfaces (such as those for equalization) by letting users communicate their audio production objectives with descriptive language (e.g. “Make the violin sound ‘warmer.’”). To achieve this goal, a system must be able to tell whether the stated goal is appropriate for the selected tool (e.g. making the violin “warmer” with a panning tool does not make sense). If the goal is appropriate for the tool, it must know what actions need to be taken. Further, the tool should not impose a vocabulary on users, but rather understand the vocabulary users prefer. In this work, we describe SocialEQ, a web-based project for learning a vocabulary of actionable audio equalization descriptors. Since deployment, SocialEQ has learned 324 distinct words in 731 learning sessions. Data on these terms is made available for download. We examine terms users have provided, exploring which ones map well to equalization, which ones have broadly-agreed upon meaning, which term have meanings specific small groups, and which terms are synonymous. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Crowdsourcing; Information retrieval; Descriptors; Learning sessions; Web based projects; Equalizers","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Polfreman R., Dr","Polfreman, Richard (36608775200)","36608775200","Comparing onset detection & perceptual attack time","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052763440&partnerID=40&md5=29b8af59d9e40c12076ac79f046d5779","University of Southampton, United Kingdom","Polfreman R., Dr, University of Southampton, United Kingdom","Accurate performance timing is associated with the perceptual attack time (PAT) of notes, rather than their physical or perceptual onsets (PhOT, POT). Since manual annotation of PAT for analysis is both time-consuming and impractical for real-time applications, automatic transcription is desirable. However, computational methods for onset detection in audio signals are conventionally measured against PhOT or POT data. This paper describes a comparison between PAT and onset detection data to assess whether in some circumstances they are similar enough to be equivalent, or whether additional models for PAT-PhOT difference are always necessary. Eight published onset algorithms, and one commercial system, were tested with five onset types in short monophonic sequences. Ground truth was established by multiple human transcription of the audio for PATs using rhythm adjustment with synchronous presentation, and parameters for each detection algorithm manually adjusted to produce the maximum agreement with the ground truth. Results indicate that for percussive attacks, a number of algorithms produce data close to or within the limits of human agreement and therefore may be substituted for PATs, while for non-percussive sounds corrective measures are necessary to match detector outputs to human estimates. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Accurate performance; Automatic transcription; Commercial systems; Corrective measures; Detection algorithm; Manual annotation; Onset detection; Real-time application; Transcription","R. Polfreman; University of Southampton, United Kingdom; email: r.polfreman@soton.ac.uk","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Masuda T.; Yoshii K.; Goto M.; Morishima S.","Masuda, Taro (56315351700); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330); Morishima, Shigeo (7005317462)","56315351700; 7103400120; 7403505330; 7005317462","Spotting a query phrase from polyphonic music audio signals based on semi-supervised nonnegative matrix factorization","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973290458&partnerID=40&md5=e3c601d4a57149411c017bd588b5f221","Waseda University, Japan; Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Masuda T., Waseda University, Japan; Yoshii K., Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Morishima S., Waseda University, Japan","This paper proposes a query-by-audio system that aims to detect temporal locations where a musical phrase given as a query is played in musical pieces. The “phrase” in this paper means a short audio excerpt that is not limited to a main melody (singing part) and is usually played by a single musical instrument. A main problem of this task is that the query is often buried in mixture signals consisting of various instruments. To solve this problem, we propose a method that can appropriately calculate the distance between a query and partial components of a musical piece. More specifically, gamma process nonnegative matrix factorization (GaP-NMF) is used for decomposing the spectrogram of the query into an appropriate number of basis spectra and their activation patterns. Semi-supervised GaP-NMF is then used for estimating activation patterns of the learned basis spectra in the musical piece by presuming the piece to partially consist of those spectra. This enables distance calculation based on activation patterns. The experimental results showed that our method outperformed conventional matching methods. © Taro Masuda, Kazuyoshi Yoshii, Masataka Goto, Shigeo Morishima.","","Audio acoustics; Chemical activation; Factorization; Gallium phosphide; III-V semiconductors; Information retrieval; Activation patterns; Distance calculation; Matching methods; Mixture signals; Musical phrase; Nonnegative matrix factorization; Polyphonic music; Semi-supervised; Matrix algebra","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Xing Z.; Wang X.; Wang Y.","Xing, Zhe (57210204745); Wang, Xinxi (36731900500); Wang, Ye (36103845200)","57210204745; 36731900500; 36103845200","Enhancing collaborative filtering music recommendation by balancing exploration and exploitation","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011109065&partnerID=40&md5=5b788e2d5a4fa210ecedbe72c49e4cde","School of Computing, National University of Singapore, Singapore","Xing Z., School of Computing, National University of Singapore, Singapore; Wang X., School of Computing, National University of Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore","Collaborative filtering (CF) techniques have shown great success in music recommendation applications. However, traditional collaborative-filtering music recommendation algorithms work in a greedy way, invariably recommending songs with the highest predicted user ratings. Such a purely exploitative strategy may result in suboptimal performance over the long term. Using a novel reinforcement learning approach, we introduce exploration into CF and try to balance between exploration and exploitation. In order to learn users’ musical tastes, we use a Bayesian graphical model that takes account of both CF latent factors and recommendation novelty. Moreover, we designed a Bayesian inference algorithm to efficiently estimate the posterior rating distributions. In music recommendation, this is the first attempt to remedy the greedy nature of CF approaches. Results from both simulation experiments and user study show that our proposed approach significantly improves recommendation performance. © Zhe Xing, Xinxi Wang, Ye Wang.","","Bayesian networks; Inference engines; Information retrieval; Reinforcement learning; Balancing exploration and exploitations; Bayesian graphical models; Bayesian inference; Exploration and exploitation; Music recommendation; Recommendation performance; Reinforcement learning approach; Sub-optimal performance; Collaborative filtering","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Sturm B.L.; Collins N.","Sturm, Bob L. (14014190500); Collins, Nick (24478398900)","14014190500; 24478398900","The Kiki-Bouba challenge: Algorithmic composition for content-based MIR research & development","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003624914&partnerID=40&md5=f46c12c7ea3015679acd20521874788e","Audio Analysis Lab, Aalborg University, Denmark; Dept. Music, Durham University, United Kingdom","Sturm B.L., Audio Analysis Lab, Aalborg University, Denmark; Collins N., Dept. Music, Durham University, United Kingdom","We propose the “Kiki-Bouba Challenge” (KBC) for the research and development of content-based music information retrieval (MIR) systems. This challenge is unencumbered by several problems typically encountered in MIR research: insufficient data, restrictive copyrights, imperfect ground truth, a lack of specific criteria for classes (e.g., genre), a lack of explicit problem definition, and irrepro-ducibility. KBC provides a limitless amount of free data, a perfect ground truth, and well-specifiable and meaningful characteristics defining each class. These ideal conditions are made possible by open source algorithmic composition — a hitherto under-exploited resource for MIR. © Bob L. Sturm, Nick Collins.","","Information retrieval; Algorithmic compositions; Content-based; Ground truth; Imperfect ground; Music information retrieval; Open sources; Problem definition; Research and development; Search engines","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Vera B.; Chew E.","Vera, Bogdan (56448833500); Chew, Elaine (8706714000)","56448833500; 8706714000","Towards seamless network music performance: Predicting an ensemble’s expressive decisions for distributed performance","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009192202&partnerID=40&md5=68346923908b0b55a0985d6f5f15820a","Queen Mary University of London, Centre for Digital Music, United Kingdom","Vera B., Queen Mary University of London, Centre for Digital Music, United Kingdom; Chew E., Queen Mary University of London, Centre for Digital Music, United Kingdom","Internet performance faces the challenge of network latency. One proposed solution is music prediction, wherein musical events are predicted in advance and transmitted to distributed musicians ahead of the network delay. We present a context-aware music prediction system focusing on expressive timing: a Bayesian network that incorporates stylistic model selection and linear conditional gaussian distributions on variables representing proportional tempo change. The system can be trained using rehearsals of distributed or co-located ensembles. We evaluate the model by comparing its prediction accuracy to two others: one employing only linear conditional dependencies between expressive timing nodes but no stylistic clustering, and one using only independent distributions for timing changes. The three models are tested on performances of a custom-composed piece that is played ten times, each in one of two styles. The results are promising, with the proposed system outperforming the other two. In predictable parts of the performance, the system with conditional dependencies and stylistic clustering achieves errors of 15ms; in more difficult sections, the errors rise to 100ms; and, in unpredictable sections, the error is too great for seamless timing emulation. Finally, we discuss avenues for further research and propose the use of predictive timing cues using our system. © Bogdan Vera, Elaine Chew.","","Bayesian networks; Errors; Information retrieval; Conditional Gaussian distribution; Distributed performance; Internet performance; Model Selection; Music prediction; Network latencies; Network music performance; Prediction accuracy; Forecasting","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Yang P.-K.; Hsu C.-C.; Chien J.-T.","Yang, Po-Kai (56413459100); Hsu, Chung-Chien (36959626300); Chien, Jen-Tzung (7202435221)","56413459100; 36959626300; 7202435221","Bayesian singing-voice separation","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990833868&partnerID=40&md5=dc24ea0005559c48d0e8d16ccf43ba31","Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","Yang P.-K., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Hsu C.-C., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan; Chien J.-T., Department of Electrical and Computer Engineering, National Chiao Tung University, Taiwan","This paper presents a Bayesian nonnegative matrix factorization (NMF) approach to extract singing voice from background music accompaniment. Using this approach, the likelihood function based on NMF is represented by a Poisson distribution and the NMF parameters, consisting of basis and weight matrices, are characterized by the exponential priors. A variational Bayesian expectation-maximization algorithm is developed to learn variational parameters and model parameters for monaural source separation. A clustering algorithm is performed to establish two groups of bases: one is for singing voice and the other is for background music. Model complexity is controlled by adaptively selecting the number of bases for different mixed signals according to the variational lower bound. Model regularization is tackled through the uncertainty modeling via variational inference based on marginal likelihood. The experimental results on MIR-1K database show that the proposed method performs better than various unsupervised separation algorithms in terms of the global normalized source to distortion ratio. © Po-Kai Yang, Chung-Chien Hsu and Jen-Tzung Chien.","","Factorization; Image segmentation; Information retrieval; Matrix algebra; Maximum principle; Poisson distribution; Separation; Source separation; Uncertainty analysis; Likelihood functions; Nonnegative matrix factorization; Separation algorithms; Singing voice separations; Uncertainty modeling; Variational bayesian; Variational inference; Variational parameters; Clustering algorithms","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Davies M.E.P.; Böck S.","Davies, Mathew E.P. (55349903900); Böck, Sebastian (55413719000)","55349903900; 55413719000","Evaluating the evaluation measures for beat tracking","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956832444&partnerID=40&md5=93f5601ec3d3eafd3e814ea9d1193cb6","Sound and Music Computing Group, INESC TEC, Porto, Portugal; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria","The evaluation of audio beat tracking systems is normally addressed in one of two ways. One approach is for human listeners to judge performance by listening to beat times mixed as clicks with music signals. The more common alternative is to compare beat times against ground truth annotations via one or more of the many objective evaluation measures. However, despite a large body of work in audio beat tracking, there is currently no consensus over which evaluation measure(s) to use, meaning multiple accuracy scores are typically reported. In this paper, we seek to evaluate the evaluation measures by examining the relationship between objective accuracy scores and human judgements of beat tracking performance. First, we present the raw correlation between objective scores and subjective ratings, and show that evaluation measures which allow alternative metrical levels appear more correlated than those which do not. Second, we explore the effect of parameterisation of objective evaluation measures, and demonstrate that correlation is maximised for smaller tolerance windows than those currently used. Our analysis suggests that true beat tracking performance is currently being overestimated via objective evaluation. © Mathew E. P. Davies, Sebastian Böck.","","Information retrieval; Audio beat tracking; Beat tracking; Evaluation measures; Ground truth; Human listeners; Music signals; Objective evaluation; Subjective rating; Audio systems","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Lin Y.; Chen X.; Yang D.","Lin, Yi (57210196602); Chen, Xiaoou (11639519200); Yang, Deshun (24485594500)","57210196602; 11639519200; 24485594500","Exploration of music emotion recognition based on MIDI","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046891653&partnerID=40&md5=15743094a75a3135347822bbca36731c","Institute of Computer Science and Technology, Peking University, China","Lin Y., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China","Audio and lyric features are commonly considered in the research of music emotion recognition, whereas MIDI features are rarely used. Some research revealed that among the features employed in music emotion recognition, lyric has the best performance on valence, MIDI takes the second place, and audio is the worst. However, lyric cannot be found in some music types, such as instrumental music. In this case, MIDI features can be considered as a choice for music emotion recognition on valence dimension. In this presented work, we systematically explored the effect and value of using MIDI features for music emotion recognition. Emotion recognition was treated as a regression problem in this paper. We also discussed the emotion regression performance of three aspects of music in terms of edited MIDI: chorus, melody, and accompaniment. We found that the MIDI features performed better than audio features on valence. And under the realistic conditions, converted MIDI performed better than edited MIDI on valence. We found that melody was more important to valence regression than accompaniment, which was in contrary to arousal. We also found that the chorus part of an edited MIDI might contain as sufficient information as the entire edited MIDI for valence regression. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Regression analysis; Speech recognition; Audio features; Emotion recognition; Music emotions; Realistic conditions; Regression problem; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Vera B.; Chew E.; Healey P.G.T.","Vera, Bogdan (56448833500); Chew, Elaine (8706714000); Healey, Patrick G.T. (7004324758)","56448833500; 8706714000; 7004324758","A study of ensemble synchronisation under restricted line of sight","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044856869&partnerID=40&md5=94a820b7cd0c541e9ae82cd4e00d38ff","Queen Mary University of London, Centre for Digital Music, United Kingdom; Queen Mary University of London, Cognitive Science Research Group, United Kingdom","Vera B., Queen Mary University of London, Centre for Digital Music, United Kingdom; Chew E., Queen Mary University of London, Centre for Digital Music, United Kingdom; Healey P.G.T., Queen Mary University of London, Cognitive Science Research Group, United Kingdom","This paper presents a quantitative study of musician synchronisation in ensemble performance under restricted line of sight, an inherent condition in scenarios like distributed music performance. The study focuses on the relevance of gestural (e.g. visual, breath) cues in achieving note onset synchrony in a violin and cello duo, in which musicians must fulfill a mutual conducting role. The musicians performed two pieces – one with long notes separated by long pauses, another with long notes but no pauses – under direct, partial (silhouettes), and no line of sight. Analysis of the musicians’ note synchrony shows that visual contact significantly impacts synchronization in the first piece, but not significantly in the second piece, leading to the hypothesis that opportunities to shape notes may provide further cues for synchronization. The results also show that breath cues are important, and that the relative positions of these cues impact note asynchrony at the ends of pauses; thus, the advance timing information provided by breath cues could form a basis for generating virtual cues in distributed performance, where network latency delays sonic and visual cues. This study demonstrates the need to account for structure (e.g. pauses, long notes) and prosodic gestures in ensemble synchronisation. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Distributed performance; Ensemble performance; Music performance; Network latencies; Quantitative study; Relative positions; Timing information; Visual contacts; Synchronization","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Prockup M.; Schmidt E.M.; Scott J.; Kim Y.E.","Prockup, Matthew (37089471200); Schmidt, Erik M. (36053813000); Scott, Jeffrey (35312262000); Kim, Youngmoo E. (24724623000)","37089471200; 36053813000; 35312262000; 24724623000","Toward understanding expressive percussion through content based analysis","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047961160&partnerID=40&md5=f1e4b7663d35340ba7f7b40ce7b2e45a","Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Prockup M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Scott J., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Musical expression is the creative nuance through which a musician conveys emotion and connects with a listener. In un-pitched percussion instruments, these nuances are a very important component of performance. In this work, we present a system that seeks to classify different expressive articulation techniques independent of percussion instrument. One use of this system is to enhance the organization of large percussion sample libraries, which can be cumbersome and daunting to navigate. This work is also a necessary first step towards understanding musical expression as it relates to percussion performance. The ability to classify expressive techniques can lead to the development of models that learn the the functionality of articulations in patterns, as well as how certain performers use them to communicate their ideas and define their musical style. Additionally, in working towards understanding expressive percussion, we introduce a publicly available dataset of articulations recorded from a standard four piece drum kit that captures the instrument’s expressive range. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Content-based analysis; Musical expression; Percussion instruments; Sample libraries; Musical instruments","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Peeters G.; Bisot V.","Peeters, Geoffroy (22433836000); Bisot, Victor (57188863282)","22433836000; 57188863282","Improving music structure segmentation using lag-priors","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476589&partnerID=40&md5=a17666ce67322dd78694020d762ed03a","STMS IRCAM-CNRS-UPMC, France","Peeters G., STMS IRCAM-CNRS-UPMC, France; Bisot V., STMS IRCAM-CNRS-UPMC, France","Methods for music structure discovery usually process a music track by first detecting segments and then labeling them. Depending on the assumptions made on the signal content (repetition, homogeneity or novelty), different methods are used for these two steps. In this paper, we deal with the segmentation in the case of repetitive content. In this field, segments are usually identified by looking for sub-diagonals in a Self-Similarity-Matrix (SSM). In order to make this identification more robust, Goto proposed in 2003 to cumulate the values of the SSM over constant-lag and search only for segments in the SSM when this sum is large. Since the various repetitions of a segment start simultaneously in a self-similarity-matrix, Serra et al. proposed in 2012 to cumulate these simultaneous values (using a so-called structure feature) to enhance the novelty of the starting and ending time of a segment. In this work, we propose to combine both approaches by using Goto method locally as a prior to the lag-dimensions of Serra et al. structure features used to compute the novelty curve. Through a large experiment on RWC and Isophonics test-sets and using MIREX segmentation evaluation measure, we show that this simple combination allows a large improvement of the segmentation results. © Geoffroy Peeters, Victor Bisot.","","Information retrieval; Music structures; Segmentation evaluation; Segmentation results; Self-similarity matrix; Signal content; Structure features; Test sets; Image segmentation","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Kuuskankare M.; Sapp C.S.","Kuuskankare, Mika (56301483200); Sapp, Craig Stuart (14032002500)","56301483200; 14032002500","Visual humdrum-library for PWGL","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069822629&partnerID=40&md5=d9fb419a5d2408893a09e49f15bdf5dc","DocMus, Sibelius Academy, United Kingdom; CCARH, Stanford University, United Kingdom","Kuuskankare M., DocMus, Sibelius Academy, United Kingdom; Sapp C.S., CCARH, Stanford University, United Kingdom","We introduce a PWGL Humdrum interface that integrates command-line unix tools for music analysis into a visual programming environment. This symbiosis allows users access to the strengths of each system—algorithmic composition and visual programming components of PWGL along with computational analysis and data processing features of Humdrum tools. Our novel interface for Humdrum graphical programming allows non-programmers better access to Humdrum analysis tools, particularly with the built-in music notation display capabilities of PWGL. ENP (Expressive Notation Package) data from PWGL can be exported as Humdrum data. Humdrum files in turn can be converted back into ENP data, allowing bi-directional communication between the two software systems. © 2013 International Society for Music Information Retrieval.","","Computer graphics; Computer programming; Information retrieval; Algorithmic compositions; Bi-directional communication; Computational analysis; Graphical programming; Music analysis; Software systems; Visual programming; Visual programming environments; Data handling","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Marques A.; Andrade N.; Balby L.","Marques, Andryw (57210194966); Andrade, Nazareno (7003429497); Balby, Leandro (23393414800)","57210194966; 7003429497; 23393414800","Exploring the relation between novelty aspects and preferences in music listening","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044180467&partnerID=40&md5=c1f98ede71e923939c203e84744c4dce","Universidade Federal de Campina Grande, Brazil","Marques A., Universidade Federal de Campina Grande, Brazil; Andrade N., Universidade Federal de Campina Grande, Brazil; Balby L., Universidade Federal de Campina Grande, Brazil","The discovery of new music, e.g. song tracks and artists, is a central aspect of music consumption. In order to assist users in this task, several mechanisms have been proposed to incorporate novelty awareness into music recommender systems. In this paper, we complement these efforts by investigating how the music preferences of users are affected by two different aspects of novel artists, namely familiarity and mainstreamness. We collected historical data from Last.fm users, a popular online music discovery service, to investigate how these aspects of novel artists relate to the preferences of music listeners for novel artists. The results of this analysis suggests that the users tend to cluster according to their novelty related preferences. We then conducted a comprehensive study on these groups, from where we derive implications and useful insights for developers of music retrieval services. © 2013 International Society for Music Information Retrieval.","","Historical data; Last.fm; Music preferences; Music recommender systems; Music retrieval; Online music; Recommender systems","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Boland D.; Murray-Smith R.","Boland, Daniel (39360893500); Murray-Smith, Roderick (6602499837)","39360893500; 6602499837","Information-theoretic measures of music listening behaviour","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959414335&partnerID=40&md5=0a9753d0eab2957d567901d269851a42","School of Computing Science, University of Glasgow, United Kingdom","Boland D., School of Computing Science, University of Glasgow, United Kingdom; Murray-Smith R., School of Computing Science, University of Glasgow, United Kingdom","We present an information-theoretic approach to the measurement of users’ music listening behaviour and selection of music features. Existing ethnographic studies of music use have guided the design of music retrieval systems however are typically qualitative and exploratory in nature. We introduce the SPUD dataset, comprising 10, 000 handmade playlists, with user and audio stream metadata. With this, we illustrate the use of entropy for analysing music listening behaviour, e.g. identifying when a user changed music retrieval system. We then develop an approach to identifying music features that reflect users’ criteria for playlist curation, rejecting features that are independent of user behaviour. The dataset and the code used to produce it are made available. The techniques described support a quantitative yet user-centred approach to the evaluation of music features and retrieval systems, without assuming objective ground truth labels. © Daniel Boland, Roderick Murray-Smith.","","Information retrieval; Information theory; Ethnographic study; Ground truth; Information theoretic measure; Information-theoretic approach; Music retrieval systems; Retrieval systems; User behaviour; User-centred; Behavioral research","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Köküer M.; Jančovič P.; Ali-MacLachlan I.; Athwal C.","Köküer, Münevver (6508139174); Jančovič, Peter (7801590038); Ali-MacLachlan, Islah (57190986595); Athwal, Cham (35617523100)","6508139174; 7801590038; 57190986595; 35617523100","Automated detection of single- and multi-note ornaments in Irish traditional flute playing","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985028423&partnerID=40&md5=b973dc2008e9938a8cbc49de7266cacc","DMT Lab, Birmingham City University, United Kingdom; School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom","Köküer M., DMT Lab, Birmingham City University, United Kingdom, School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Jančovič P., School of Electronic, Electrical and Systems Engineering, University of Birmingham, United Kingdom; Ali-MacLachlan I., DMT Lab, Birmingham City University, United Kingdom; Athwal C., DMT Lab, Birmingham City University, United Kingdom","This paper presents an automatic system for the detection of single- and multi-note ornaments in Irish traditional flute playing. This is a challenging problem because ornaments are notes of a very short duration. The presented ornament detection system is based on first detecting onsets and then exploiting the knowledge of musical ornamentation. We employed onset detection methods based on signal envelope and fundamental frequency and customised their parameters to the detection of soft onsets of possibly short duration. Single-note ornaments are detected based on the duration and pitch of segments, determined by adjacent onsets. Multi-note ornaments are detected based on analysing the sequence of segments. Experimental evaluations are performed on monophonic flute recordings from Grey Larsen’s CD, which was manually annotated by an experienced flute player. The onset and single- and multi-note ornament detection performance is presented in terms of the precision, recall and F-measure. © Münevver Köküer, Peter Jančovič, Islah Ali-MacLachlan, Cham Athwal.","","Automated detection; Automatic systems; Detection performance; Detection system; Experimental evaluation; Fundamental frequencies; Onset detection; Signal envelope; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Benetos E.; Badeau R.; Weyde T.; Richard G.","Benetos, Emmanouil (16067946900); Badeau, Roland (11939840000); Weyde, Tillman (24476899500); Richard, Gaël (57195915952)","16067946900; 11939840000; 24476899500; 57195915952","Template adaptation for improving automatic music transcription","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978078861&partnerID=40&md5=f169fb17f913bb8edf031967397cb55f","Department of Computer Science, City University, London, United Kingdom; Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, France","Benetos E., Department of Computer Science, City University, London, United Kingdom; Badeau R., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, France; Weyde T., Department of Computer Science, City University, London, United Kingdom; Richard G., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, France","In this work, we propose a system for automatic music transcription which adapts dictionary templates so that they closely match the spectral shape of the instrument sources present in each recording. Current dictionary-based automatic transcription systems keep the input dictionary fixed, thus the spectral shape of the dictionary components might not match the shape of the test instrument sources. By performing a conservative transcription pre-processing step, the spectral shape of detected notes can be extracted and utilized in order to adapt the template dictionary. We propose two variants for adaptive transcription, namely for single-instrument transcription and for multiple-instrument transcription. Experiments are carried out using the MAPS and Bach10 databases. Results in terms of multi-pitch detection and instrument assignment show that there is a clear and consistent improvement when adapting the dictionary in contrast with keeping the dictionary fixed. © E. Benetos, R. Badeau, T. Weyde, and G. Richard.","","Information retrieval; Automatic music transcription; Automatic transcription; Multi pitches; Multiple instruments; Pre-processing step; Spectral shapes; Template adaptations; Test instruments; Transcription","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Gonzalez Thomas N.; Pasquier P.; Eigenfeldt A.; Maxwell J.B.","Gonzalez Thomas, Nicolas (57210182899); Pasquier, Philippe (8850202000); Eigenfeldt, Arne (24724145100); Maxwell, James B. (55582782500)","57210182899; 8850202000; 24724145100; 55582782500","A methodology for the comparison of melodic generation models using meta-melo","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066842012&partnerID=40&md5=688e75985baa65169caa058411b46281","MAMAS Lab, Simon Fraser University, Canada","Gonzalez Thomas N., MAMAS Lab, Simon Fraser University, Canada; Pasquier P., MAMAS Lab, Simon Fraser University, Canada; Eigenfeldt A., MAMAS Lab, Simon Fraser University, Canada; Maxwell J.B., MAMAS Lab, Simon Fraser University, Canada","We investigate Musical Metacreation algorithms by applying Music Information Retrieval techniques for comparing the output of three off-line, corpus-based style imitation models. The first is Variable Order Markov Chains, a statistical model; second is the Factor Oracle, a pattern matcher; and third, MusiCOG, a novel graphical model based on perceptual and cognitive processes. Our focus is on discovering which musical biases are introduced by the models, that is, the characteristics of the output which are shaped directly by the formalism of the models and not by the corpus itself. We describe META-MELO, a system that implements the three models, along with a methodology for the quantitative analysis of model output, when trained on a corpus of melodies in symbolic form. Results show that the models’ output are indeed different and suggest that the cognitive approach is more successful at the tasks, although none of them encompass the full creative space of the corpus. We conclude that this methodology is promising for aiding in the informed application and development of generative models for music composition problems. © 2013 International Society for Music Information Retrieval.","","Markov processes; Cognitive approaches; Cognitive process; Generative model; Imitation models; Music composition; Music information retrieval; Statistical modeling; Variable-order Markov chains; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Tian M.; Fazekas G.; Black D.A.A.; Sandler M.","Tian, Mi (55977248100); Fazekas, György (37107520200); Black, Dawn A.A. (55257515800); Sandler, Mark (7202740804)","55977248100; 37107520200; 55257515800; 7202740804","Design and evaluation of onset detectors using different fusion policies","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939862343&partnerID=40&md5=2bff3eef001242c15599066434a26df0","Centre for Digital Music, Queen Mary University of London, United Kingdom","Tian M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Black D.A.A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Note onset detection is one of the most investigated tasks in Music Information Retrieval (MIR) and various detection methods have been proposed in previous research. The primary aim of this paper is to investigate different fusion policies to combine existing onset detectors, thus achieving better results. Existing algorithms are fused using three strategies, first by combining different algorithms, second, by using the linear combination of detection functions, and third, by using a late decision fusion approach. Large scale evaluation was carried out on two published datasets and a new percussion database composed of Chinese traditional instrument samples. An exhaustive search through the parameter space was used enabling a systematic analysis of the impact of each parameter, as well as reporting the most generally applicable parameter settings for the onset detectors and the fusion. We demonstrate improved results attributed to both fusion and the optimised parameter settings. © Mi Tian, György Fazekas, Dawn A. A. Black, Mark Sandler.","","Large dataset; Design and evaluations; Detection functions; Detection methods; Linear combinations; Music information retrieval; Note onset detections; Parameter setting; Systematic analysis; Information retrieval","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Driedger J.; Müller M.; Disch S.","Driedger, Jonathan (55582371700); Müller, Meinard (7404689873); Disch, Sascha (24450382800)","55582371700; 7404689873; 24450382800","Extending harmonic-percussive separation of audio signals","2014","Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR 2014","68","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946012233&partnerID=40&md5=bc7af7482188c47fcf588a68540dc1be","International Audio Laboratories Erlangen, Germany; Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","Driedger J., International Audio Laboratories Erlangen, Germany; Müller M., International Audio Laboratories Erlangen, Germany; Disch S., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","In recent years, methods to decompose an audio signal into a harmonic and a percussive component have received a lot of interest and are frequently applied as a processing step in a variety of scenarios. One problem is that the computed components are often not of purely harmonic or percussive nature but also contain noise-like sounds that are neither clearly harmonic nor percussive. Furthermore, depending on the parameter settings, one often can observe a leakage of harmonic sounds into the percussive component and vice versa. In this paper we present two extensions to a state-of-the-art harmonic-percussive separation procedure to target these problems. First, we introduce a separation factor parameter into the decomposition process that allows for tightening separation results and for enforcing the components to be clearly harmonic or percussive. As second contribution, inspired by the classical sines+transients+noise (STN) audio model, this novel concept is exploited to add a third residual component to the decomposition which captures the sounds that lie in between the clearly harmonic and percussive sounds of the audio signal. © Jonathan Driedger, Meinard Müller, and Sascha Disch.","","Acoustic noise; Audio acoustics; Harmonic analysis; Information retrieval; Separation; Audio modeling; Decomposition process; Novel concept; Parameter setting; Processing steps; Residual components; Separation factors; State of the art; Audio signal processing","","","15th International Society for Music Information Retrieval Conference, ISMIR 2014","27 October 2014 through 31 October 2014","Taipei","149353"
"Schedl M.; Flexer A.","Schedl, Markus (8684865900); Flexer, Arthur (7004555682)","8684865900; 7004555682","Putting the user in the center of music information retrieval","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476088&partnerID=40&md5=9e2be1471e00759ed3ade60118c08ef8","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Personalized and context-aware music retrieval and recommendation algorithms ideally provide music that perfectly fits the individual listener in each imaginable situation and for each of her information or entertainment need. Although first steps towards such systems have recently been presented at ISMIR and similar venues, this vision is still far away from being a reality. In this paper, we investigate and discuss literature on the topic of user-centric music retrieval and reflect on why the breakthrough in this field has not been achieved yet. Given the different expertises of the authors, we shed light on why this topic is a particularly challenging one, taking a psychological and a computer science view. Whereas the psychological point of view is mainly concerned with proper experimental design, the computer science aspect centers on modeling and machine learning problems. We further present our ideas on aspects vital to consider when elaborating user-aware music retrieval systems, and we also describe promising evaluation methodologies, since accurately evaluating personalized systems is a notably challenging task. © 2012 International Society for Music Information Retrieval.","","Computer science; Context-Aware; Evaluation methodologies; Machine learning problem; Music information retrieval; Music retrieval; Music retrieval systems; Recommendation algorithms; User-centric; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Jure L.; Lopez E.; Rocamora M.; Cancela P.; Sponton H.; Irigaray I.","Jure, Luis (55583332100); Lopez, Ernesto (55566520900); Rocamora, Martín (55347707700); Cancela, Pablo (6508290965); Sponton, Haldo (55582030700); Irigaray, Ignacio (25825024500)","55583332100; 55566520900; 55347707700; 6508290965; 55582030700; 25825024500","Pitch content visualization tools for music performance analysis","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448692&partnerID=40&md5=509c7f500bf2e4a965ec3854b17dad50","School of Music, Universidad de la República, Uruguay; Faculty of Engineering, Universidad de la República, Uruguay","Jure L., School of Music, Universidad de la República, Uruguay; Lopez E., Faculty of Engineering, Universidad de la República, Uruguay; Rocamora M., School of Music, Universidad de la República, Uruguay, Faculty of Engineering, Universidad de la República, Uruguay; Cancela P., Faculty of Engineering, Universidad de la República, Uruguay; Sponton H., Faculty of Engineering, Universidad de la República, Uruguay; Irigaray I., Faculty of Engineering, Universidad de la República, Uruguay","This work deals with pitch content visualization tools for the analysis of music performance from audio recordings. An existing computational method for the representation of pitch contours is briefly reviewed. Its application to music analysis is exemplified with two pieces of non-notated music: a field recording of a folkloric form of polyphonic singing and a commercial recording by a noted blues musician. Both examples have vocal parts exhibiting complex pitch evolution, difficult to analyze and notate with precision using Western common music notation. By using novel time-frequency analysis techniques that improve the location of the components of a harmonic sound, the melodic content representation implemented here allows a detailed study of aspects related to pitch intonation and tuning. This in turn permits an objective measurement of essential musical characteristics that are difficult or impossible to properly evaluate by subjective perception alone, and which are often not accounted for in traditional mu-sicological analysis. Two software tools are released that allow the practical use of the described methods. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Visualization; Content representation; Field recording; Music analysis; Music notation; Music performance; Objective measurement; Pitch contours; Pitch evolutions; Time frequency analysis; Visualization tools; Audio recordings","L. Jure; School of Music, Universidad de la República, Uruguay; email: lj@eumus.edu.uy","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Coviello E.; Vaizman Y.; Chan A.B.; Lanckriet G.R.G.","Coviello, Emanuele (35147534800); Vaizman, Yonatan (55583001400); Chan, Antoni B. (14015159100); Lanckriet, Gert R.G. (7801431767)","35147534800; 55583001400; 14015159100; 7801431767","Multivariate autoregressive mixture models for music auto-tagging","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447086&partnerID=40&md5=c4a79dc65f994e202191d26535eebf01","University of California, San Diego, United States; City University of Hong Kong, Hong Kong","Coviello E., University of California, San Diego, United States; Vaizman Y., University of California, San Diego, United States; Chan A.B., City University of Hong Kong, Hong Kong; Lanckriet G.R.G., University of California, San Diego, United States","We propose the multivariate autoregressive model for content based music auto-tagging. At the song level our approach leverages the multivariate autoregressive mixture (ARM) model, a generative time-series model for audio, which assumes each feature vector in an audio fragment is a linear function of previous feature vectors. To tackle tagmodel estimation, we propose an efficient hierarchical EM algorithm for ARMs (HEM-ARM), which summarizes the acoustic information common to the ARMs modeling the individual songs associated with a tag. We compare the ARM model with the recently proposed dynamic texture mixture (DTM) model. We hence investigate the relative merits of different modeling choices for music time-series: i) the flexibility of selecting higher memory order in ARM, ii) the capability of DTM to learn specific frequency basis for each particular tag and iii) the effect of the hidden layer of the DT versus the time efficiency of learning and inference with fully observable AR components. Finally, we experiment with a support vector machine (SVM) approach that classifies songs based on a kernel calculated on the frequency responses of the corresponding song ARMs. We show that the proposed approach outperforms SVMs trained on a different kernel function, based on a competing generative model. © 2012 International Society for Music Information Retrieval.","","Algorithms; Computer simulation; Frequency response; Image retrieval; Mixtures; Support vector machines; Temperature control; Acoustic information; Content-based; Dynamic textures; EM algorithms; Feature vectors; Generative model; Hidden layers; Kernel function; Linear functions; Mixture model; Multivariate autoregressive; Multivariate autoregressive models; Specific frequencies; Time efficiencies; Time series models; Computer music","E. Coviello; University of California, San Diego, United States; email: ecoviell@ucsd.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Schmidt E.M.; Kim Y.E.","Schmidt, Erik M. (36053813000); Kim, Youngmoo E. (24724623000)","36053813000; 24724623000","Learning rhythm and melody features with deep belief networks","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905407267&partnerID=40&md5=287eb50d64e9d1181f749a0f06b1529e","Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab) Electrical and Computer Engineering, Drexel University, United States","Deep learning techniques provide powerful methods for the development of deep structured projections connecting multiple domains of data. But the fine-tuning of such networks for supervised problems is challenging, and many current approaches are therefore heavily reliant on pre-training, which consists of unsupervised processing on the input observation data. In previous work, we have investigated using magnitude spectra as the network observations, finding reasonable improvements over standard acoustic representations. However, in necessarily supervised problems such as music emotion recognition, there is no guarantee that the starting points for optimization are anywhere near optimal, as emotion is unlikely to be the most dominant aspect of the data. In this new work, we develop input representations using harmonic/percussive source separation designed to inform rhythm and melodic contour. These representations are beat synchronous, providing an event-driven representation, and potentially the ability to learn emotion informative representations from pre-training alone. In order to provide a large dataset for our pre-training experiments, we select a subset of 50,000 songs from the Million Song Dataset, and employ their 30-60 second preview clips from 7digital to compute our custom feature representations. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Large dataset; Source separation; Deep belief networks; Feature representation; Learning techniques; Magnitude spectrum; Multiple domains; Music emotions; Network observation; Observation data; Deep learning","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Collins N.","Collins, Nick (24478398900)","24478398900","Influence in early electronic dance music: An audio content analysis investigation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469306&partnerID=40&md5=a3007a955dac11ac58729720438d412d","University of Sussex, United Kingdom","Collins N., University of Sussex, United Kingdom","Audio content analysis can assist investigation of musical influence, given a corpus of date-annotated works. We study a number of techniques which illuminate musicological questions on genre and creative influence. By applying machine learning tests and statistical analysis to a database of early EDM tracks, we examine how distinct putatively different musical genres really are, the retrospectively labelled Detroit techno and Chicago house being the core case study. Further, by building predictive models based on works from earlier years, both by a priori assumed genre groups and by individual tracks, we examine questions of influence, and whether Detroit techno really is a sort of electronic future funk, and Chicago house an electronic extension of disco. We discuss the implications and prospects for modeling musical influence. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Audio content analysis; Chicago; Detroit; Musical genre; Predictive models; Electronic musical instruments","N. Collins; University of Sussex, United Kingdom; email: N.Collins@sussex.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Wülfing J.; Riedmiller M.","Wülfing, Jan (48362111400); Riedmiller, Martin (6603794300)","48362111400; 6603794300","Unsupervised learning of local features for music classification","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465585&partnerID=40&md5=a1ab7025bce34f99d14bb270ebe83919","University of Freiburg, Germany","Wülfing J., University of Freiburg, Germany; Riedmiller M., University of Freiburg, Germany","In this work we investigate the applicability of unsuper-vised feature learning methods to the task of automatic genre prediction of music pieces. More specifically we evaluate a framework that recently has been successfully used to recognize objects in images. We first extract local patches from the time-frequency transformed audio signal, which are then pre-processed and used for unsupervised learning of an overcomplete dictionary of local features. For learning we either use a bootstrapped k-means clustering approach or select features randomly. We further extract feature responses in a convolutional manner and train a linear SVM for classification. We extensively evaluate the approach on the GTZAN dataset, emphasizing the influence of important design choices such as dimensionality reduction, pooling and patch dimension on the classification accuracy. We show that convolutional extraction of local feature responses is crucial to reach high performance. Furthermore we find that using this approach, simple and fast learning techniques such as k-means or randomly selected features are competitive with previously published results which also learn features from audio signals. © 2012 International Society for Music Information Retrieval.","","Convolution; Data processing; Information retrieval; Unsupervised learning; Audio signal; Classification accuracy; Data sets; Dimensionality reduction; Fast learning; Feature learning; K-means; K-means clustering; Linear SVM; Local feature; Music classification; Overcomplete dictionaries; Patch dimensions; Time frequency; Classification (of information)","J. Wülfing; University of Freiburg, Germany; email: wuelfj@tf.uni-freiburg.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Joder C.; Schuller B.","Joder, Cyril (35107208400); Schuller, Björn (6603767415)","35107208400; 6603767415","Score-informed leading voice separation from monaural audio","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444645&partnerID=40&md5=f6c7123964c36026d51922c5a987f84a","Institute for Human-Machine Communication, Technische Universität, München, Germany","Joder C., Institute for Human-Machine Communication, Technische Universität, München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität, München, Germany","Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a difficult problem for automatic systems, in particular in the blind case, where no information is known about the signal. However, in the case where a musical score is available, one can take advantage of this additional information. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporally-aligned MIDI Score. The model used is based on Nonnegative Matrix Factorization (NMF), whose solo part is represented by a source-filter model. We exploit the score information by constraining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popular songs show that the use of these constraints can significantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Automatic systems; Human ear; MIDI files; Musical score; Nonnegative matrix factorization; Novel applications; Perceptual evaluation; Popular song; Voice separation; Separation","C. Joder; Institute for Human-Machine Communication, Technische Universität, München, Germany; email: cyril.joder@tum.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Bryan N.J.; Mysore G.J.; Wang G.","Bryan, Nicholas J. (57196969500); Mysore, Gautham J. (24465525500); Wang, Ge (56144195500)","57196969500; 24465525500; 56144195500","Source separation of polyphonic music with interactive user-feedback on a piano roll display","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946069010&partnerID=40&md5=5a022d7853c9c71867b3aa698e26ab42","CCRMA, Stanford University, United States; Adobe Research, United States","Bryan N.J., CCRMA, Stanford University, United States; Mysore G.J., Adobe Research, United States; Wang G., CCRMA, Stanford University, United States","The task of separating a single recording of a polyphonic instrument (e.g. piano, guitar, etc.) into distinctive pitch tracks is challenging. One promising class of methods to accomplish this task is based on non-negative matrix factorization (NMF). Such methods, however, are still far from perfect. Distinct pitches from a single instrument have similar timbre, similar note attacks, and contain overlapping harmonics that all make separation difficult. In an attempt to overcome these issues, we use a database of synthesized piano and guitar recordings to learn the harmonic structure of distinct pitches, perform NMF-based separation, and then extend the method to allow an end-user to interactively correct for errors in the output separation estimates by drawing on a piano roll display of the separated tracks. The user-annotations are mapped to linear grouping regularization parameters within a modified NMF-based algorithm and are then used to refine the separation estimates in an iterative manner. For evaluation, a prototype user-interface was built and used to separate several polyphonic guitar and piano recordings. Initial results show that the method of interactive feedback can significantly increase the separation quality and produce high-quality separation results. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Factorization; Information retrieval; Iterative methods; Matrix algebra; Musical instruments; Separation; User interfaces; Class of methods; Harmonic structures; High-quality separation; Interactive feedback; Nonnegative matrix factorization; Polyphonic music; Regularization parameters; User annotations; Source separation","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Hu X.; Lee J.H.","Hu, Xiao (55496358400); Lee, Jin Ha (57190797465)","55496358400; 57190797465","A Cross-cultural study of music mood perception between American and Chinese listeners","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451903&partnerID=40&md5=70ce9dd3402bb05813077555a2d095e3","Faculty of Education, University of Hong Kong, Hong Kong; Information School, University of Washington, United States","Hu X., Faculty of Education, University of Hong Kong, Hong Kong; Lee J.H., Information School, University of Washington, United States","Music mood has been recognized as an important access point for music and many online music services support browsing by mood. However, how people judge music mood has not been well studied in the Music Information Retrieval (MIR) domain. In particular, people's cultural background is often assumed to be an important factor in music mood perception, but this assumption has not been verified by empirical studies. This paper reports on a study comparing mood judgments on a set of 30 songs by American and Chinese people. Results show that mood judgments do indeed differ between American and Chinese respondents. Furthermore, respondents' mood judgments tended to agree more with other respondents from the same culture than those from the other group. Both the song characteristics (e.g., genre, lyrical or instrumental) and the non-cultural background of the respondents (e.g., age, gender, familiarity with the songs) were analyzed to further examine the difference in mood judgments. Findings of this study help further our understanding on how cultural background affects mood perception. Also discussed in this paper are implications for designing MIR systems for cross-cultural music mood classification and recommendation. © 2012 International Society for Music Information Retrieval.","","Access points; Chinese people; Cross-cultural study; Cultural backgrounds; Empirical studies; Music information retrieval; Online music services; Information retrieval","X. Hu; Faculty of Education, University of Hong Kong, Hong Kong; email: xiaoxhu@hku.hk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Mayer R.; Rauber A.","Mayer, Rudolf (23397787500); Rauber, Andreas (57074846700)","23397787500; 57074846700","Towards time-resilient MIR processes","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873474989&partnerID=40&md5=347a2414cbe08ae2864ff94c07070b87","Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria","Mayer R., Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria; Rauber A., Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria","In experimental sciences, under which we may likely subsume most research areas in MIR, repeatability is one of the key cornerstones of validating research and measuring progress. Yet, due to the complexity of typical MIR experiments, ensuring the capability of re-running any experiment, achieving exactly identical outputs is challenging at best. Performance differences observed may be attributed to incomplete documentation of the process, slight variations in data (preprocessing) or software libraries used, and others. Digital preservation aims at keeping digital objects authentically accessible and usable over long time spans. While traditionally focussed on individual objects, research is now moving towards the preservation of entire processes. In this paper we present the challenges of preserving a classical MIR process, i.e. music genre classifications, discuss the kinds of context information to be captured, as well as means to validate the re-execution of a preserved process. © 2012 International Society for Music Information Retrieval.","","Digital storage; Information retrieval; Context information; Digital Objects; Digital preservation; Experimental science; Individual objects; Music genre classification; Re-execution; Software libraries; Time span; Experiments","R. Mayer; Secure Business Austria, 1040 Vienna, Favoritenstrasse 16, Austria; email: mayer@sba-research.at","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Proutskva P.; Rhodes C.; Wiggins G.; Crawford T.","Proutskva, Polina (55586627700); Rhodes, Christophe (57196565939); Wiggins, Geraint (14032393700); Crawford, Tim (15054056900)","55586627700; 57196565939; 14032393700; 15054056900","Breathy or resonant - A controlled and curated dataset for phonation mode detection in singing","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448726&partnerID=40&md5=cfb6073ec7503e7fa00cbfe7f15f1b16","Goldsmiths, University of London, United Kingdom; Queen Mary, University of London, United Kingdom","Proutskva P., Goldsmiths, University of London, United Kingdom; Rhodes C., Goldsmiths, University of London, United Kingdom; Wiggins G., Queen Mary, University of London, United Kingdom; Crawford T., Goldsmiths, University of London, United Kingdom","This paper presents a new reference dataset of sustained, sung vowels with attached labels indicating the phonation mode. The dataset is intended for training computational models for automated phonation mode detection. Four phonation modes are distinguished by Johan Sundberg [15]: breathy, neutral, flow (or resonant) and pressed. The presented dataset consists of ca. 700 recordings of nine vowels from several languages, sung at various pitches in various phonation modes. The recorded sounds were produced by one female singer under controlled conditions, following recommendations by voice acoustics researchers. While datasets on phonation modes in speech exist, such resources for singing are not available. Our dataset closes this gap and offers researchers in various disciplines a reference and a training set. It will be made available online under Creative Commons license. Also, the format of the dataset is extensible. Further content additions and future support for the dataset are planned. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Linguistics; Computational model; Controlled conditions; Creative Commons; Data sets; Mode detection; Training sets; Speech","P. Proutskva; Goldsmiths, University of London, United Kingdom; email: proutskova@googlemail.com","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Mauch M.; Dixon S.","Mauch, Matthias (36461512900); Dixon, Simon (7201479437)","36461512900; 7201479437","A corpus-based study of rhythm patterns","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451988&partnerID=40&md5=c72946f851a5e906f3ce72f958257941","Centre for Digital Music, Queen Mary University of London, United Kingdom","Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present a corpus-based study of musical rhythm, based on a collection of 4.8 million bar-length drum patterns extracted from 48, 176 pieces of symbolic music. Approaches to the analysis of rhythm in music information retrieval to date have focussed on low-level features for retrieval or on the detection of tempo, beats and drums in audio recordings. Musicological approaches are usually concerned with the description or implementation of man-made music theories. In this paper, we present a quantitative bottom-up approach to the study of rhythm that relies upon well-understood statistical methods from natural language processing. We adapt these methods to our corpus of music, based on the realisation that - unlike words - bar-length drum patterns can be systematically decomposed into sub-patterns both in time and by instrument. We show that, in some respects, our rhythm corpus behaves like natural language corpora, particularly in the sparsity of vocabulary. The same methods that detect word collocations allow us to quantify and rank idiomatic combinations of drum patterns. In other respects, our corpus has properties absent from language corpora, in particular, the high amount of repetition and strong mutual information rates between drum instruments. Our findings may be of direct interest to musicians and musicologists, and can inform the design of ground truth corpora and computational models of musical rhythm. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Natural language processing systems; Bottom up approach; Computational model; Ground truth; Low-level features; Music information retrieval; Music theory; Musical rhythm; Mutual informations; NAtural language processing; Natural languages; Sub-patterns; Word collocations; Linguistics","M. Mauch; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: matthias.mauch@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Volk A.; Bas de Haas W.","Volk, Anja (30567849900); Bas de Haas, W. (51160955300)","30567849900; 51160955300","A corpus-based study on ragtime syncopation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956796322&partnerID=40&md5=cba038fe2208516e290c21527e495c4d","Utrecht University, Netherlands","Volk A., Utrecht University, Netherlands; Bas de Haas W., Utrecht University, Netherlands","This paper presents a corpus-based study on syncopation patterns in ragtime. We discuss open questions on the ragtime genre and the potential of computational tools in addressing these questions, contributing to the fields of Musicology and Music Information Retrieval (MIR), and giving back to the ragtime enthusiasts community. We introduce the RAG-collection of around 11000 ragtime MIDI files collected, organised, and distributed by many ragtime lovers around the world. The collection is accompanied by a compendium, providing useful metadata on ragtime compositions. Using this collection and the compendium, we investigate syncopation patterns in ragtime melodies, for which we tailored a melody extraction algorithm. We test and confirm musicological hypotheses about the occurrence of syncopation patterns that are considered typical for ragtime on the extracted melodies. Thus, the paper presents a first step towards modelling typical characteristics of the ragtime genre, which is an important means for enabling automatic genre classification. © 2013 International Society for Music Information Retrieval.","","Automatic genre classification; Computational tools; Corpus-based; Melody extractions; MIDI files; Music information retrieval; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Yoshii K.; Tomioka R.; Mochihashi D.; Goto M.","Yoshii, Kazuyoshi (7103400120); Tomioka, Ryota (18937831000); Mochihashi, Daichi (15045398700); Goto, Masataka (7403505330)","7103400120; 18937831000; 15045398700; 7403505330","Beyond NMF: Time-domain audio source separation without phase reconstruction","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937854467&partnerID=40&md5=8519f297cf20d3e0dcd6dbc1dfb6e37b","National Institute of Advanced Industrial Science and Technology (AIST), Japan; University of Tokyo, Japan; Institute of Statistical Mathematics (ISM), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Tomioka R., University of Tokyo, Japan; Mochihashi D., Institute of Statistical Mathematics (ISM), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a new fundamental technique for source separation of single-channel audio signals. Although nonnegative matrix factorization (NMF) has recently become very popular for music source separation, it deals only with the amplitude or power of the spectrogram of a given mixture signal and completely discards the phase. The component spectrograms are typically estimated using a Wiener filter that reuses the phase of the mixture spectrogram, but such rough phase reconstruction makes it hard to recover high-quality source signals because the estimated spectrograms are inconsistent, i.e., they do not correspond to any real time-domain signals. To avoid the frequency-domain phase reconstruction, we use positive semidefinite tensor factorization (PSDTF) for directly estimating source signals from the mixture signal in the time domain. Since PSDTF is a natural extension of NMF, an efficient multiplicative update algorithm for PSDTF can be derived. Experimental results show that PSDTF outperforms conventional NMF variants in terms of source separation quality. © 2013 International Society for Music Information Retrieval.","","Factorization; Frequency domain analysis; Information retrieval; Matrix algebra; Mixtures; Separation; Spectrographs; Audio source separation; High quality source; Multiplicative updates; Music source separations; Natural extension; Nonnegative matrix factorization; Phase reconstruction; Positive semidefinite tensors; Source separation","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Bay M.; Ehmann A.F.; Beauchamp J.W.; Smaragdis P.; Stephen Downie J.","Bay, Mert (56259607500); Ehmann, Andreas F. (8988651500); Beauchamp, James W. (7103273878); Smaragdis, Paris (6507529311); Stephen Downie, J. (7102932568)","56259607500; 8988651500; 7103273878; 6507529311; 7102932568","Second fiddle is important too: Pitch tracking individual voices in polyphonic music","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461145&partnerID=40&md5=b206d528253cd6c251a0b739564410be","Department of Computer Science, University of Illinois, Urbana-Champaign, United States; Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Bay M., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Ehmann A.F., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Beauchamp J.W., Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Smaragdis P., Department of Computer Science, University of Illinois, Urbana-Champaign, United States, Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; Stephen Downie J., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Recently, there has been much interest in automatic pitch estimation and note tracking of polyphonic music. To date, however, most techniques produce a representation where pitch estimates are not associated with any particular instrument or voice. Therefore, the actual tracks for each instrument are not readily accessible. Access to individual tracks is needed for more complete music transcription and additionally will provide a window to the analysis of higher constructs such as counterpoint and instrument theme imitation during a composition. In this paper, we present a method for tracking the pitches (F0s) of individual instruments in polyphonic music. The system uses a pre-learned dictionary of spectral basis vectors for each note for a variety of musical instruments. The method then formulates the tracking of pitches of individual voices in a probabilistic manner by attempting to explain the input spectrum as the most likely combination of musical instruments and notes drawn from the dictionary. The method has been evaluated on a subset of the MIREX multiple-F0 estimation test dataset, showing promising results. © 2012 International Society for Music Information Retrieval.","","Musical instruments; Statistical tests; Basis vector; Data sets; Music transcription; Pitch estimation; Pitch-tracking; Polyphonic music; System use; Information retrieval","M. Bay; Department of Electrical and Computer Eng., University of Illinois, Urbana-Champaign, United States; email: mertbay@illinois.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Ross J.C.; Vinutha T.P.; Rao P.","Ross, Joe Cheri (55321796400); Vinutha, T.P. (56628761100); Rao, Preeti (35180193500)","55321796400; 56628761100; 35180193500","Detecting melodic motifs from audio for Hindustani Classical Music","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455677&partnerID=40&md5=292d8fcf6125dee3ba202293ed1beacc","Department of Computer Science and Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India","Ross J.C., Department of Computer Science and Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; Vinutha T.P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; Rao P., Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India","Melodic motifs form essential building blocks in Indian Classical music. The motifs, or key phrases, provide strong cues to the identity of the underlying raga in both Hindustani and Carnatic styles of Indian music. Thus the automatic detection of such recurring basic melodic shapes from audio is of relevance in music information retrieval. The extraction of melodic attributes from polyphonic audio and the variability inherent in the performance, which does not follow a predefined score, make the task particularly challenging. In this work, we consider the segmentation of selected melodic motifs from audio signals by computing similarity measures on time series of automatically detected pitch values. The methods are investigated in the context of detecting the signature phrase of Hindustani vocal music compositions (bandish) within and across performances. © 2012 International Society for Music Information Retrieval.","","Audio signal processing; Information retrieval; Audio signal; Automatic Detection; Building blockes; Indian classical music; Key-phrase; Music information retrieval; Pitch values; Similarity measure; Vocal music; Audio acoustics","J.C. Ross; Department of Computer Science and Engineering, Indian Institute of Technology Bombay, Mumbai 400076, India; email: joe@cse.iitb.ac.in","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Wu D.","Wu, Dekai (12793699800)","12793699800","Simultaneous unsupervised learning of flamenco metrical structure, hypermetrical structure, and multipart structural relations","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973884765&partnerID=40&md5=0b5e640cfd87a618433f257ed6dc2717","HKUST, Human Language Technology Center, Department of CSE, Hong Kong","Wu D., HKUST, Human Language Technology Center, Department of CSE, Hong Kong","We show how a new unsupervised approach to learning musical relationships can exploit Bayesian MAP induction of stochastic transduction grammars to overcome the challenges of learning complex relationships between multiple rhythmic parts that previously lay outside the scope of general computational approaches to music structure learning. A good illustrative genre is flamenco, which employs not only regular but also irregular hypermetrical structures that rapidly switch between 3/4 and 6/8 mediocompas blocks. Moreover, typical flamenco idioms employ heavy syncopation and sudden, misleading off-beat accents and patterns, while often elliding the downbeat accents that humans as well as existing meter-finding algorithms rely on, thus creating a high degree of listener “surprise” that makes not only the structural relations, but even the metrical structure itself, ellusive to learn. Flamenco musicians rely on both complex regular hypermetrical knowledge as well as irregular real-time clues to recognize when to switch meters and patterns. Our new approach envisions this as an integrated problem of learning a bilingual transduction, i.e., a structural relation between two languages—where there are different musical languages of, say, flamenco percussion versus zapateado footwork or palmas hand clapping. We apply minimum description length criteria to induce transduction grammars that simultaneously learn (1) the multiple metrical structures, (2) the hypermetrical structure that stochastically governs meter switching, and (3) the probabilistic transduction relationship between patterns of different rhythmic languages that enables musicians to predict when to switch meters and how to select patterns depending on what fellow musicians are generating. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Machine learning; Stochastic systems; Complex relationships; Computational approach; Finding algorithm; Hand-clapping; Minimum description length criteria; Music structures; New approaches; Unsupervised approaches; Bacteriophages","D. Wu; HKUST, Human Language Technology Center, Department of CSE, Hong Kong; email: dekai@cs.ust.hk","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Humphrey E.J.; Bello J.P.; LeCun Y.","Humphrey, Eric J. (55060792500); Bello, Juan Pablo (7102889110); LeCun, Yann (55666793600)","55060792500; 7102889110; 55666793600","Moving beyond feature design: Deep architectures and automatic feature learning in music informatics","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","105","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453413&partnerID=40&md5=600c912ce2d2d2e2fcaf37c5277b700f","Music and Audio Research Lab., NYU, United States; Courant School of Computer Science, NYU, United States","Humphrey E.J., Music and Audio Research Lab., NYU, United States; Bello J.P., Music and Audio Research Lab., NYU, United States; LeCun Y., Courant School of Computer Science, NYU, United States","The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Information science; Alternative approach; Content-based; De facto standard; Deep learning; Feature learning; Informatics; Informatics research; Processing architectures; Specific tasks; Design","E.J. Humphrey; Music and Audio Research Lab., NYU, United States; email: ejhumphrey@nyu.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Sarroff A.M.; Casey M.","Sarroff, Andy M. (36731459200); Casey, Michael (15080769900)","36731459200; 15080769900","Groove kernels as rhythmic-acoustic motif descriptors","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977121228&partnerID=40&md5=5d4ce3cb60cead76990699c961c3fc61","Dartmouth College, Department of Computer Science, United States","Sarroff A.M., Dartmouth College, Department of Computer Science, United States; Casey M., Dartmouth College, Department of Computer Science, United States","The “groove” of a song correlates with enjoyment and bodily movement. Recent work has shown that humans often agree whether a song does or does not have groove and how much groove a song has. It is therefore useful to develop algorithms that characterize the quality of groove across songs. We evaluate three unsupervised tempo-invariant models for measuring pairwise musical groove similarity: A temporal model, a timbre-temporal model, and a pitch-timbre-temporal model. The temporal model uses a rhythm similarity metric proposed by Holzapfel and Stylianou, while the timbre-inclusive models are built on shift invariant probabilistic latent component analysis. We evaluate the models using a dataset of over 8000 real-world musical recordings spanning approximately 10 genres, several decades, multiple meters, a large range of tempos, and Western and non-Western localities. A blind perceptual study is conducted: given a random music query, humans rate the groove similarity of the top three retrievals chosen by each of the models, as well as three random retrievals. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Bodily movement; Descriptors; Probabilistic latent component analysis; Real-world; Shift invariant; Similarity metrics; Temporal modeling; Large dataset","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Grachten M.; Gasser M.; Arzt A.; Widmer G.","Grachten, Maarten (8974600000); Gasser, Martin (18037419600); Arzt, Andreas (36681791200); Widmer, Gerhard (7004342843)","8974600000; 18037419600; 36681791200; 7004342843","Automatic alignment of music performances with structural differences","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973344156&partnerID=40&md5=b46fc8aff98534dbbb82d75d7155195c","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","Grachten M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Arzt A., Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Dept. of Computational Perception, Johannes Kepler Universität, Linz, Austria","Both in interactive music listening, and in music performance research, there is a need for automatic alignment of different recordings of the same musical piece. This task is challenging, because musical pieces often contain parts that may or may not be repeated by the performer, possibly leading to structural differences between performances (or between performance and score). The most common alignment method, dynamic time warping (DTW), cannot handle structural differences adequately, and existing approaches to deal with structural differences explicitly rely on the annotation of “break points” in one of the sequences. We propose a simple extension of the Needleman-Wunsch algorithm to deal effectively with structural differences, without relying on annotations. We evaluate several audio features for alignment, and show how an optimal value can be found for the cost-parameter of the alignment algorithm. A single cost value is demonstrated to be valid across different types of music. We demonstrate that our approach yields roughly equal alignment accuracies compared to DTW in the absence of structural differences, and superior accuracies when structural differences occur. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Alignment accuracy; Alignment algorithms; Automatic alignment; Dynamic time warping; Interactive music; Music performance; Needleman-Wunsch algorithm; Structural differences; Alignment","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cunningham S.J.; Lee J.H.","Cunningham, Sally Jo (7201937110); Lee, Jin Ha (57190797465)","7201937110; 57190797465","Influences of ISMIR and mirex research on technology patents","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907053036&partnerID=40&md5=02fd7217223e9c5188bedc4b68685821","Department of Computer Science, University of Waikato, New Zealand; Information School, University of Washington, United States","Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand; Lee J.H., Information School, University of Washington, United States","Much of the current Music Information Retrieval (MIR) research aims to contribute to the field by creating practical music applications or algorithms that can be used as part of such applications. Understanding how academic research results influence and translate to commercial products can be useful for MIR researchers, especially when we try to measure the impact of our research. This study aims to improve our understanding of the commercial influence of academic MIR research by analyzing the patents citing publications from ISMIR (International Society for Music Information Retrieval) Conference proceedings and its associated MIREX (Music Information Retrieval Evaluation eXchange) MIR algorithm trials. In this paper, we provide our preliminary analyses of the relevant patents as well as the ISMIR publications that are referenced in those patents. © 2013 International Society for Music Information Retrieval.","","Insecticides; Patents and inventions; Academic research; Commercial products; International society; Music applications; Music information retrieval; Preliminary analysis; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Grosche P.; Serrà J.; Müller M.; Arcos J.L.","Grosche, Peter (55413290700); Serrà, Joan (35749172500); Müller, Meinard (7404689873); Arcos, Josep Ll. (56131974300)","55413290700; 35749172500; 7404689873; 56131974300","Structure-based audio fingerprinting for music retrieval","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451912&partnerID=40&md5=5ade038283f724198ad1a763140b7d7f","Saarland University, Germany; Bonn University, Germany; MPI Informatik, Germany; Artificial Intelligence Research Institute (IIIA-CSIC), Spain","Grosche P., Saarland University, Germany, MPI Informatik, Germany; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Spain; Müller M., Bonn University, Germany, MPI Informatik, Germany; Arcos J.L., Artificial Intelligence Research Institute (IIIA-CSIC), Spain","Content-based approaches to music retrieval are of great relevance as they do not require any kind of manually generated annotations. In this paper, we introduce the concept of structure fingerprints, which are compact descriptors of the musical structure of an audio recording. Given a recorded music performance, structure fingerprints facilitate the retrieval of other performances sharing the same underlying structure. Avoiding any explicit determination of musical structure, our fingerprints can be thought of as a probability density function derived from a self-similarity matrix. We show that the proposed fingerprints can be compared by using simple Euclidean distances without using any kind of complex warping operations required in previous approaches. Experiments on a collection of Chopin Mazurkas reveal that structure fingerprints facilitate robust and efficient content-based music retrieval. Furthermore, we give a musically informed discussion that also deepens the understanding of this popular Mazurka dataset. © 2012 International Society for Music Information Retrieval.","","Audio recordings; Probability density function; Audio fingerprinting; Content-based approach; Content-based music retrieval; Data sets; Descriptors; Euclidean distance; Music performance; Music retrieval; Musical structures; Self-similarity matrix; Structure-based; Information retrieval","P. Grosche; Saarland University, Germany; email: pgrosche@mpi-inf.mpg.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Liang D.; Hoffman M.D.; Ellis D.P.W.","Liang, Dawen (55586031200); Hoffman, Matthew D. (17434675700); Ellis, Daniel P.W. (13609089200)","55586031200; 17434675700; 13609089200","Beta process sparse nonnegative matrix factorization for music","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973372047&partnerID=40&md5=7fc7fa4de889b2e7b90fd2c541579599","LabROSA, EE Dept., Columbia University, United States; Adobe Research, Adobe Systems Incorporated, United States","Liang D., LabROSA, EE Dept., Columbia University, United States; Hoffman M.D., Adobe Research, Adobe Systems Incorporated, United States; Ellis D.P.W., Adobe Research, Adobe Systems Incorporated, United States","Nonnegative matrix factorization (NMF) has been widely used for discovering physically meaningful latent components in audio signals to facilitate source separation. Most of the existing NMF algorithms require that the number of latent components is provided a priori, which is not always possible. In this paper, we leverage developments from the Bayesian nonparametrics and compressive sensing literature to propose a probabilistic Beta Process Sparse NMF (BP-NMF) model, which can automatically infer the proper number of latent components based on the data. Unlike previous models, BP-NMF explicitly assumes that these latent components are often completely silent. We derive a novel mean-field variational inference algorithm for this nonconjugate model and evaluate it on both synthetic data and real recordings on various tasks. © 2013 International Society for Music Information Retrieval.","","Factorization; Inference engines; Information retrieval; Source separation; Audio signal; Bayesian nonparametrics; Beta process; Compressive sensing; Nonnegative matrix factorization; Sparse non-negative matrix factorizations; Synthetic data; Variational inference; Matrix algebra","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Barthet M.; Marston D.; Baume C.; Fazekas G.; Sandler M.","Barthet, Mathieu (24723525000); Marston, David (22634832500); Baume, Chris (55354443400); Fazekas, György (37107520200); Sandler, Mark (7202740804)","24723525000; 22634832500; 55354443400; 37107520200; 7202740804","Design and evaluation of semantic mood models for music recommendation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904287489&partnerID=40&md5=685f4546ab8913864c96431a0aa7bd5d","Centre for Digital Music, Queen Mary University of London, United Kingdom; BBC R and D London, United Kingdom","Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Marston D., BBC R and D London, United Kingdom; Baume C., BBC R and D London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","In this paper we present and evaluate two semantic music mood models relying on metadata extracted from over 180,000 production music tracks sourced from I Like Music (ILM)’s collection. We performed non-metric multidimensional scaling (MDS) analyses of mood stem dissimilarity matrices (1 to 13 dimensions) and devised five different mood tag summarisation methods to map tracks in the dimensional mood spaces. We then conducted a listening test to assess the ability of the proposed models to match tracks by mood in a recommendation task. The models were compared against a classic audio content-based similarity model relying on Mel Frequency Cepstral Coefficients (MFCCs). The best performance (60% of correct match, on average) was yielded by coupling the five-dimensional MDS model with the term-frequency weighted tag centroid method to map tracks in the mood space. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Centroid method; Design and evaluations; Listening tests; Mel-frequency cepstral coefficients; Music recommendation; Non-metric multidimensional scaling; Similarity models; Term Frequency; Semantics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Scott J.; Kim Y.E.","Scott, Jeffrey (35312262000); Kim, Youngmoo E. (24724623000)","35312262000; 24724623000","Instrument identification informed multi-track mixing","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983139327&partnerID=40&md5=cc0b064a7582bb5527151467c0f84651","Music and Entertainment Technology Laboratory (MET-lab), Electrical and Computer Engineering, Drexel University, United States","Scott J., Music and Entertainment Technology Laboratory (MET-lab), Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Electrical and Computer Engineering, Drexel University, United States","Although digital music production technology has become more accessible over the years, the tools are complex and often difficult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efficacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Basic principles; Digital music; Exploratory analysis; Hierarchical approach; Instrument identification; Learning curves; Mixing models; Processing technique; Mixing","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Gärtner D.","Gärtner, Daniel (18041853400)","18041853400","Tempo detection of urban music using tatum grid non-negative matrix factorization","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904331538&partnerID=40&md5=1719006c91a2a55b3db450063328f3b9","Fraunhofer Institute for Media Technology IDMT, Germany","Gärtner D., Fraunhofer Institute for Media Technology IDMT, Germany","High tempo detection accuracies have been reported for the analysis of percussive, constant-tempo, Western music audio signals. As a consequence, active research in the tempo detection domain has been shifted to yet open tasks like tempo analysis of non-percussive, expressive, or non-western music. Also, tempo detection is included in a large range of music-related software. In DJ software, features like beat-synching or tempo-synchronized sound effects are widely accepted in the DJ community, and their users rely on correct tempo hypothesis as their basis. In this paper, we are evaluating both academic and commercial tempo detection systems on a typical dataset of an urban club music DJ. Based on this evaluation, we identify octave errors as a problem that has not yet been solved. Further, an approach based on non-negative matrix factorization is presented. In its current state it can compete with the state of the art. It further provides a foundation to tackle the octave error issue in future research. © 2013 International Society for Music Information Retrieval.","","Factorization; Information retrieval; Matrix algebra; Audio signal; Detection accuracy; Detection system; Nonnegative matrix factorization; Sound effects; State of the art; Tempo analysis; Audio acoustics","D. Gärtner; Fraunhofer Institute for Media Technology IDMT, Germany; email: daniel.gaertner@idmt.fraunhofer.de","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cabredo R.; Legaspi R.; Inventado P.S.; Numao M.","Cabredo, Rafael (35274548200); Legaspi, Roberto (35610218300); Inventado, Paul Salvador (36573634500); Numao, Masayuki (7004090356)","35274548200; 35610218300; 36573634500; 7004090356","An emotion model for music using brain waves","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452532&partnerID=40&md5=8bf8ba79afe59a8e512ce8aa75f58b86","Institute of Scientific and Industrial Research, Osaka University, Japan; Center for Empathic Human-Computer Interactions, De La Salle University, Philippines","Cabredo R., Institute of Scientific and Industrial Research, Osaka University, Japan, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines; Legaspi R., Institute of Scientific and Industrial Research, Osaka University, Japan; Inventado P.S., Institute of Scientific and Industrial Research, Osaka University, Japan, Center for Empathic Human-Computer Interactions, De La Salle University, Philippines; Numao M., Institute of Scientific and Industrial Research, Osaka University, Japan","Every person reacts differently to music. The task then is to identify a specific set of music features that have a significant effect on emotion for an individual. Previous research have used self-reported emotions or tags to annotate short segments of music using discrete labels. Our approach uses an electroencephalograph to record the subject's reaction to music. Emotion spectrum analysis method is used to analyse the electric potentials and provide continuous-valued annotations of four emotional states for different segments of the music. Music features are obtained by processing music information from the MIDI files which are separated into several segments using a windowing technique. The music features extracted are used in two separate supervised classification algorithms to build the emotion models. Classifiers have a minimum error rate of 5% predicting the emotion labels. © 2012 International Society for Music Information Retrieval.","","Electric potential; Information retrieval; Separation; Spectrum analysis; Brain wave; Emotion models; Emotional state; MIDI files; Minimum error rate; Music information; Short segments; Supervised classification; Windowing techniques; Computer music","R. Cabredo; Institute of Scientific and Industrial Research, Osaka University, Japan; email: cabredo@ai.sanken.osaka-u.ac.jp","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Böck S.; Krebs F.; Schedl M.","Böck, Sebastian (55413719000); Krebs, Florian (7006192702); Schedl, Markus (8684865900)","55413719000; 7006192702; 8684865900","Evaluating the online capabilities of onset detection methods","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","85","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873481777&partnerID=40&md5=519212661037d48cff2abb69c8f11de2","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper, we evaluate various onset detection algorithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27, 774 annotated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detection method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with of various volume levels. © 2012 International Society for Music Information Retrieval.","","Achievable performance; Choice of parameters; Data sets; Off-line algorithm; Offline; On-line applications; Onset detection; Peak detection; Peak-picking method; Spectral flux; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Mauch M.; Ewert S.","Mauch, Matthias (36461512900); Ewert, Sebastian (32667575400)","36461512900; 32667575400","The audio degradation toolbox and its application to robustness evaluation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","48","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905267530&partnerID=40&md5=7fcc826576d3181014bdcc7133d80440","Queen Mary University of London, Centre for Digital Music, United Kingdom","Mauch M., Queen Mary University of London, Centre for Digital Music, United Kingdom; Ewert S., Queen Mary University of London, Centre for Digital Music, United Kingdom","We introduce the Audio Degradation Toolbox (ADT) for the controlled degradation of audio signals, and propose its usage as a means of evaluating and comparing the robustness of audio processing algorithms. Music recordings encountered in practical applications are subject to varied, sometimes unpredictable degradation. For example, audio is degraded by low-quality microphones, noisy recording environments, MP3 compression, dynamic compression in broadcasting or vinyl decay. In spite of this, no standard software for the degradation of audio exists, and music processing methods are usually evaluated against clean data. The ADT fills this gap by providing Matlab scripts that emulate a wide range of degradation types. We describe 14 degradation units, and how they can be chained to create more complex, ‘real-world’ degradations. The ADT also provides functionality to adjust existing ground-truth, correcting for temporal distortions introduced by degradation. Using four different music informatics tasks, we show that performance strongly depends on the combination of method and degradation applied. We demonstrate that specific degradations can reduce or even reverse the performance difference between two competing methods. ADT source code, sounds, impulse responses and definitions are freely available for download. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Audio recordings; Information retrieval; MATLAB; Processing; Controlled degradation; Dynamic compression; Music Informatics; Noisy recordings; Processing method; Robustness evaluation; Standard software; Temporal distortions; Audio signal processing","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Meredith D.","Meredith, David (57125703100)","57125703100","A geometric language for representing structure in polyphonic music","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873457544&partnerID=40&md5=80c16bd89ce283d7188c718d88ba1ffc","Aalborg University, Denmark","Meredith D., Aalborg University, Denmark","In 1981, Deutsch and Feroe proposed a formal language for representing melodic pitch structure that employed the powerful concept of hierarchically-related pitch alphabets. However, neither rhythmic structure nor pitch structure in polyphonic music can be adequately represented using this language. A new language is proposed here that incorporates certain features of Deutsch and Feroe's model but extends and generalises it to allow for the representation of both rhythm and pitch structure in polyphonic music. The new language adopts a geometric approach in which a passage of polyphonic music is represented as a set of multidimensional points, generated by performing transformations on component patterns. The language introduces the concept of a periodic mask, a generalisation of Deutsch and Feroe's notion of a pitch alphabet, that can be applied to any dimension of a geometric representation, allowing for both rhythms and pitch collections to be represented parsimoniously in a uniform way. © 2012 International Society for Music Information Retrieval.","","Formal languages; Geometry; Information retrieval; Generalisation; Geometric approaches; Geometric languages; Geometric representation; Polyphonic music; Rhythmic structures; Mathematical transformations","D. Meredith; Aalborg University, Denmark; email: dave@create.aau.dk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Terrell M.J.; Fazekas G.; Simpson A.J.R.; Smith J.; Dixon S.","Terrell, Michael J. (7004302211); Fazekas, György (37107520200); Simpson, Andrew J.R. (55354839100); Smith, Jordan (55582613300); Dixon, Simon (7201479437)","7004302211; 37107520200; 55354839100; 55582613300; 7201479437","Listening level changes music similarity","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452097&partnerID=40&md5=13e64419ca525780eecc9dd319624722","Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom","Terrell M.J., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Simpson A.J.R., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Smith J., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom","We examine the effect of listening level, i.e. the absolute sound pressure level at which sounds are reproduced, on music similarity, and in particular, on playlist generation. Current methods commonly use similarity metrics based on Mel-frequency cepstral coefficients (MFCCs), which are derived from the objective frequency spectrum of a sound. We follow this approach, but use the level-dependent auditory spectrum, evaluated using the loudness models of Glasberg and Moore, at three listening levels, to produce auditory spectrum cepstral coefficients (ASCCs). The AS-CCs are used to generate sets of playlists at each listening level, using a typical method, and these playlists were found to differ greatly. From this we conclude that music recommendation systems could be made more perceptually relevant if listening level information were included. We discuss the findings in relation to other fields within MIR where inclusion of listening level might also be of benefit. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Cepstral coefficients; Frequency spectra; Listening levels; Loudness models; Mel-frequency cepstral coefficients; Music Recommendation System; Music similarity; Similarity metrics; Sound pressure level; Acoustic variables measurement","M.J. Terrell; Centre for Digital Music, Queen Mary University of London, London, E1 4NS, Mile End Road, United Kingdom; email: michael.terrell@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Müller M.; Prätzlich T.; Driedger J.","Müller, Meinard (7404689873); Prätzlich, Thomas (55582692100); Driedger, Jonathan (55582371700)","7404689873; 55582692100; 55582371700","A Cross-version approach for stabilizing tempo-based novelty detection","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444305&partnerID=40&md5=97466c215cfed0af4c00e30068e2948c","Bonn University, Germany; Saarland University, Germany; MPI Informatik, Germany","Müller M., Bonn University, Germany, MPI Informatik, Germany; Prätzlich T., Saarland University, Germany, MPI Informatik, Germany; Driedger J., Bonn University, Germany, Saarland University, Germany","The task of novelty detection with the objective of detecting changes regarding musical properties such as harmony, dynamics, timbre, or tempo is of fundamental importance when analyzing structural properties of music recordings. But for a specific audio version of a given piece of music, the novelty detection result may also crucially depend on the individual performance style of the musician. This particularly holds true for tempo-related properties, which may vary significantly across different performances of the same piece of music. In this paper, we show that tempo-based novelty detection can be stabilized and improved by simultaneously analyzing a set of different performances. We first warp the version-dependent novelty curves onto a common musical time axis, and then combine the individual curves to produce a single fusion curve. Our hypothesis is that musically relevant points of novelty tend to be consistent across different performances. This hypothesis is supported by our experiments in the context of music structure analysis, where the cross-version fusion curves yield, on average, better results than the novelty curves obtained from individual recordings. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Individual performance; Music recording; Music structure analysis; Novelty detection; Time axis; Audio recordings","M. Müller; Bonn University, Germany; email: meinard@mpi-inf.mpg.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Sébastien V.; Ralambondrainy H.; Sébastien O.; Conruyt N.","Sébastien, Véronique (36167211300); Ralambondrainy, Henri (6602216602); Sébastien, Olivier (36680583700); Conruyt, Noël (24765910100)","36167211300; 6602216602; 36680583700; 24765910100","Score Analyzer: Automatically determining scores difficulty level for instrumental e-learning","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462290&partnerID=40&md5=8ced487224c8f3fdcac2671ed2fd4ddf","IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France","Sébastien V., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Ralambondrainy H., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Sébastien O., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; Conruyt N., IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France","Nowadays, huge sheet music collections exist on the Web, allowing people to access public domain scores for free. However, beginners may be lost in finding a score appropriate to their instrument level, and should often rely on themselves to start out on the chosen piece. In this instrumental e-Learning context, we propose a Score Analyzer prototype in order to automatically extract the difficulty level of a MusicXML piece and suggest advice thanks to a Musical Sign Base (MSB). To do so, we first review methods related to score performance information retrieval. We then identify seven criteria to characterize technical instrumental difficulties and propose methods to extract them from a MusicXML score. The relevance of these criteria is then evaluated through a Principal Components Analysis and compared to human estimations. Lastly we discuss the integration of this work to @-MUSE, a collaborative score annotation platform based on multimedia contents indexation. © 2012 International Society for Music Information Retrieval.","","E-learning; Information retrieval; Principal component analysis; Analyzer prototype; Multimedia contents; Music collection; Principal components analysis; Public domains; Instruments","V. Sébastien; IREMIA - Laboratoire d'Informatique et de Mathématiques, EA2525 University of Reunion Island, Saint-Denis, Reunion, France; email: veronique.sebastien@univ-reunion.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Koduri G.K.; Serrà J.; Serra X.","Koduri, Gopala K. (36721381900); Serrà, Joan (35749172500); Serra, Xavier (55892979900)","36721381900; 35749172500; 55892979900","Characterization of intonation in Carnatic music by parametrizing pitch histograms","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449435&partnerID=40&md5=576af0e98824246419a65e423b0b6b15","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain","Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Intonation is an important concept in Carnatic music that is characteristic of a raaga, and intrinsic to the musical expression of a performer. In this paper we approach the description of intonation from a computational perspective, obtaining a compact representation of the pitch track of a recording. First, we extract pitch contours from automatically selected voice segments. Then, we obtain a a pitch histogram of its full pitch-range, normalized by the tonic frequency, from which each prominent peak is automatically labelled and parametrized. We validate such parame-trization by considering an explorative classification task: three raagas are disambiguated using the characterization of a single peak (a task that would seriously challenge a more naïve parametrization). Results show consistent improvements for this particular task. Furthermore, we perform a qualitative assessment on a larger collection of raa-gas, showing the discriminative power of the entire representation. The proposed generic parametrization of the intonation histogram should be useful for musically relevant tasks such as performer and instrument characterization. © 2012 International Society for Music Information Retrieval.","","Characterization; Graphic methods; Information retrieval; Classification tasks; Compact representation; Instrument characterization; Musical expression; Parametrizations; Pitch contours; Qualitative assessments; Single peak; Voice segments; Statistical methods","G.K. Koduri; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: gopala.koduri@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Rosão C.; Ribeiro R.; De Matos D.M.","Rosão, Carlos (49561665500); Ribeiro, Ricardo (13609216700); De Matos, David Martins (54959008500)","49561665500; 13609216700; 54959008500","Influence of Peak Selection methods on onset detection","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873458919&partnerID=40&md5=97920f6191a90e5706679868134cacbc","ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; IST/UTL, L2F/INESC-ID, Lisboa, Portugal","Rosão C., ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; Ribeiro R., ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; De Matos D.M., IST/UTL, L2F/INESC-ID, Lisboa, Portugal","Finding the starting time of musical notes in an audio signal, that is, to perform onset detection, is an important task as this information can be used as the basis for high-level musical processing tasks. Many different methods exist to perform onset detection. However their results depend on a Peak Selection step that makes the decision whether an onset is present at some point in time. In this paper we review a number of different Peak Selection methods and compare their influence in the performance of different onset detection methods and on 4 distinct onset classes. Our results show that the post-processing method used deeply influences both positively and negatively the results obtained. © 2012 International Society for Music Information Retrieval.","","Audio signal; Musical notes; Onset detection; Postprocessing methods; Selection methods; Information retrieval","C. Rosão; ISCTE-IUL L2F, INESC-ID, Lisboa, Portugal; email: rosao@l2f.inesc-id.pt","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Stober S.; Low T.; Gossen T.; Nürnberger A.","Stober, Sebastian (14027561800); Low, Thomas (48761661000); Gossen, Tatiana (48761315500); Nürnberger, Andreas (14027288100)","14027561800; 48761661000; 48761315500; 14027288100","Incremental visualization of growing music collections","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911368782&partnerID=40&md5=684f9929b871dbabb22816d18bcd247a","Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany","Stober S., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Low T., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Gossen T., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany; Nürnberger A., Data and Knowledge Engineering Group, Faculty of Computer Science, University of Magdeburg, DE, Germany","Map-based visualizations – sometimes also called projections – are a popular means for exploring music collections. But how useful are they if the collection is not static but grows over time? Ideally, a map that a user is already familiar with should be altered as little as possible and only as much as necessary to reflect the changes of the underlying collection. This paper demonstrates to what extent existing approaches are able to incrementally integrate new songs into existing maps and discusses their technical limitations. To this end, Growing Self-Organizing Maps, (Landmark) Multidimensional Scaling, Stochastic Neighbor Embedding, and the Neighbor Retrieval Visualizer are considered. The different algorithms are experimentally compared based on objective quality measurements as well as in a user study with an interactive user interface. In the experiments, the well-known Beatles corpus comprising the 180 songs from the twelve official albums is used – adding one album at a time to the collection. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Conformal mapping; Information retrieval; Stochastic systems; User interfaces; Visualization; Growing music; Growing Self Organizing Map; Interactive user interfaces; Multi-dimensional scaling; Music collection; Objective qualities; Stochastic neighbor embedding; Technical limitations; Self organizing maps","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Hockman J.A.; Davies M.E.P.; Fujinaga I.","Hockman, Jason A. (36730968100); Davies, Matthew E.P. (55349903900); Fujinaga, Ichiro (9038140900)","36730968100; 55349903900; 9038140900","One in the jungle: Downbeat detection in hardcore, jungle, and drum and bass","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469004&partnerID=40&md5=0416b45c3cfed1bf35864061d88cd320","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada; Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada; Sound and Music Computing Group, INESC TEC, Porto, Portugal","Hockman J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada, Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada; Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada, Distributed Digital Archives and Libraries (DDMAL), McGill University, Montreal, Canada","Hardcore, jungle, and drum and bass (HJDB) are fast-paced electronic dance music genres that often employ resequenced breakbeats or drum samples from jazz and funk percussionist solos. We present a style-specific method for downbeat detection specifically designed for HJDB. The presented method combines three forms of metrical information in the prediction of downbeats: low-level onset event information; periodicity information from beat tracking; and high-level information from a regression model trained with classic breakbeats. In an evaluation using 206 HJDB pieces, we demonstrate superior accuracy of our style specific method over four general downbeat detection algorithms. We present this result to motivate the need for style-specific knowledge and techniques for improved downbeat detection. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Regression analysis; Beat tracking; Detection algorithm; High-level information; Music genre; Regression model; Electronic musical instruments","J.A. Hockman; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montreal, Canada; email: jason.hockman@mail.mcgill.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Benetos E.; Dixon S.; Giannoulis D.; Kirchhoff H.; Klapuri A.","Benetos, Emmanouil (16067946900); Dixon, Simon (7201479437); Giannoulis, Dimitrios (54683886100); Kirchhoff, Holger (37072578300); Klapuri, Anssi (6602945099)","16067946900; 7201479437; 54683886100; 37072578300; 6602945099","Automatic music transcription: Breaking the glass ceiling","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447133&partnerID=40&md5=2933b8e9db00a73e86939a87bc176aa6","Centre for Digital Music, Queen Mary University of London, United Kingdom","Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Giannoulis D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Kirchhoff H., Centre for Digital Music, Queen Mary University of London, United Kingdom; Klapuri A., Centre for Digital Music, Queen Mary University of London, United Kingdom","Automatic music transcription is considered by many to be the Holy Grail in the field of music signal analysis. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. In order to overcome the limited performance of transcription systems, algorithms have to be tailored to specific use-cases. Semiautomatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information across different methods and musical aspects. © 2012 International Society for Music Information Retrieval.","","Audio signal processing; Information retrieval; Transcription; Audio data; Automatic music transcription; General purpose; Glass ceiling; Human expert; Music signal analysis; Music signals; Musical score; Potential sources; Training data; Transcription methods; Audio acoustics","E. Benetos; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: emmanouilb@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Orio N.; Piva R.","Orio, Nicola (6507928255); Piva, Roberto (57210195429)","6507928255; 57210195429","Combining timbric and rhythmic features for semantic music tagging","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951791111&partnerID=40&md5=6d83f696fd6899d3ab7732dffbd0b8d6","Department of Cultural Heritage, University of Padua, Italy; Department of Information Engineering, University of Padua, Italy","Orio N., Department of Cultural Heritage, University of Padua, Italy; Piva R., Department of Information Engineering, University of Padua, Italy","In this paper we propose a novel approach to music tagging. The approach uses a statistical framework to model two acoustic features: timbre and rhythm. A collection of tagged music is thus represented as a graph where the states correspond to the songs and the models probabilities are related to the timbric and rhythmic similarity. Under the assumption that acoustically similar songs have similar tags, we infer the tags of a new song by adding it to the graph structure and observing the tags visited in acoustically meaningful random walks. The approach has been tested using the CAL500 dataset, with encouraging results in terms of precision. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Acoustic features; Graph structures; Random Walk; Rhythmic features; Statistical framework; Semantics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Şentürk S.; Gulati S.; Serra X.","Şentürk, Sertan (43461595100); Gulati, Sankalp (37087243200); Serra, Xavier (55892979900)","43461595100; 37087243200; 55892979900","Score informed tonic identification for makam music of Turkey","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926667499&partnerID=40&md5=6480ce699460454705a08dc657cb2ee8","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Şentürk S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Tonic is a fundamental concept in many music traditions and its automatic identification should be relevant for establishing the reference pitch when we analyse the melodic content of the music. In this paper, we present two methodologies for the identification of the tonic in audio recordings of makam music of Turkey, both taking advantage of some score information. First, we compute a prominent pitch and a audio kernel-density pitch class distribution (KPCD) from the audio recording. The peaks in the KPCD are selected as tonic candidates. The first method computes a score KPCD from the monophonic melody extracted from the score. Then, the audio KPCD is circular-shifted with respect to each tonic candidate and compared with the score KPCD. The best matching shift indicates the estimated tonic. The second method extracts the monophonic melody of the most repetitive section of the score. Normalising the audio prominent pitch with respect to each tonic candidate, the method attempts to link the repetitive structural element given in the score with the respective time-intervals in the audio recording. The result producing the most confident links marks the estimated tonic. We have tested the methods on a dataset of makam music of Turkey, achieving a very high accuracy (94.9%) with the first method, and almost perfect identification (99.6%) with the second method. We conclude that score informed tonic identification can be a useful first step in the computational analysis (e.g. expressive analysis, intonation analysis, audio-score alignment) of music collections involving melody-dominant content. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Automation; Information retrieval; Automatic identification; Class distributions; Computational analysis; Fundamental concepts; Intonation analysis; Kernel density; Music collection; Structural elements; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Kosta K.; Marchini M.; Purwins H.","Kosta, Katerina (55582193200); Marchini, Marco (51161739000); Purwins, Hendrik (24336076900)","55582193200; 51161739000; 24336076900","Unsupervised chord-sequence generation from an audio example","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468434&partnerID=40&md5=2c149440c7f57b8b6327d495bd7fbdd2","Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom; Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain; Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany","Kosta K., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom, Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain; Marchini M., Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany; Purwins H., Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Spain, Neurotechnology Group, Berlin Institute of Technology, 10587 Berlin, Germany","A system is presented that generates a sound sequence from an original audio chord sequence, having the following characteristics: The generation can be arbitrarily long, preserves certain musical characteristics of the original and has a reasonable degree of interestingness. The procedure comprises the following steps: 1) chord segmentation by onset detection, 2) representation as Constant Q Profiles, 3) multi-level clustering, 4) cluster level selection, 5) metrical analysis, 6) building of a suffix tree, 7) generation heuristics. The system can be seen as a computational model of the cognition of harmony consisting of an unsupervised formation of harmonic categories (via multilevel clustering) and a sequence learning module (via suffix trees) which in turn controls the harmonic categorization in a top-down manner (via a measure of regularity). In the final synthesis, the system recombines the audio material derived from the sample itself and it is able to learn various harmonic styles. The system is applied to various musical styles and is then evaluated subjectively by musicians and non-musicians, showing that it is capable of producing sequences that maintain certain musical characteristics of the original. © 2012 International Society for Music Information Retrieval.","","Harmonic analysis; Information retrieval; Chord sequence; Computational model; Interestingness; Level selection; Multilevel clustering; Onset detection; Q profiles; Sequence learning; Suffix-trees; Top-down manner; Audio acoustics","K. Kosta; Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom; email: marco.marchini@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Yang Y.-H.","Yang, Yi-Hsuan (55218558400)","55218558400","Low-rank representation of both singing voice and music accompaniment via learned dictionaries","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946061883&partnerID=40&md5=5056c663295f7e43e37abc424e1b0aaa","Research Center for IT Innovation, Academia Sinica, Taiwan","Yang Y.-H., Research Center for IT Innovation, Academia Sinica, Taiwan","Recent research work has shown that the magnitude spectrogram of a song can be considered as a superposition of a low-rank component and a sparse component, which appear to correspond to the instrumental part and the vocal part of the song, respectively. Based on this observation, one can separate singing voice from the background music. However, the quality of such separation might be limited, because the vocal part of a song can sometimes be low-rank as well. Therefore, we propose to learn the subspace structures of vocal and instrumental sounds from a collection of clean signals first, and then compute the low-rank representations of both the vocal and instrumental parts of a song based on the learned subspaces. Specifically, we use online dictionary learning to learn the subspaces, and propose a new algorithm called multiple low-rank representation (MLRR) to decompose a magnitude spectrogram into two low-rank matrices. Our approach is flexible in that the subspaces of singing voice and music accompaniment are both learned from data. Evaluation on the MIR-1K dataset shows that the approach improves the source-to-distortion ratio (SDR) and the source-to-interference ratio (SIR), but not the source-to-artifact ratio (SAR). © 2013 International Society for Music Information Retrieval.","","Information retrieval; Background musics; Interference ratio; Learned dictionaries; Low-rank matrices; Low-rank representations; Music accompaniments; Online dictionary learning; Recent researches; Spectrographs","Y.-H. Yang; Research Center for IT Innovation, Academia Sinica, Taiwan; email: yang@citi.sinica.edu.tw","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Grohganz H.; Clausen M.; Jiang N.; Müller M.","Grohganz, Harald (57209036668); Clausen, Michael (56225233200); Jiang, Nanzhu (55552898300); Müller, Meinard (7404689873)","57209036668; 56225233200; 55552898300; 7404689873","Converting path structures into block structures using eigenvalue decompositions of self-similarity matrices","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905219825&partnerID=40&md5=a9b565743b0ffb5903098c75a61b8c76","Bonn University, Germany; International Audio Laboratories, Erlangen, Germany","Grohganz H., Bonn University, Germany; Clausen M., Bonn University, Germany; Jiang N., International Audio Laboratories, Erlangen, Germany; Müller M., International Audio Laboratories, Erlangen, Germany","In music structure analysis the two principles of repetition and homogeneity are fundamental for partitioning a given audio recording into musically meaningful structural elements. When converting the audio recording into a suitable self-similarity matrix (SSM), repetitions typically lead to path structures, whereas homogeneous regions yield block structures. In previous research, handling both structural elements at the same time has turned out to be a challenging task. In this paper, we introduce a novel procedure for converting path structures into block structures by applying an eigenvalue decomposition of the SSM in combination with suitable clustering techniques. We demonstrate the effectiveness of our conversion approach by showing that algorithms previously designed for homogeneity-based structure analysis can now be applied for repetition-based structure analysis. Thus, our conversion may open up novel ways for handling both principles within a unified structure analysis framework. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Eigenvalues and eigenfunctions; Information retrieval; Clustering techniques; Eigenvalue decomposition; Homogeneous regions; Music structure analysis; Self-similarities; Self-similarity matrix; Structural elements; Structure analysis; Audio recordings","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Jin R.; Raphael C.","Jin, Rong (55582166300); Raphael, Christopher (7004214964)","55582166300; 7004214964","Interpreting rhythm in optical music recognition","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449670&partnerID=40&md5=784d578b0addf1624cd974e3765a1d53","School of Informatics and Computing, Indiana University, Bloomington, United States","Jin R., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","We present a method for understanding the rhythmic content of a collection of identified symbols in optical music recognition, designed for polyphonic music. Our object of study is a measure of music symbols. Our model explains the symbols as a collection of voices, while the number of voices is variable throughout a measure. We introduce a dynamic programming framework that identifies the best-scoring interpretation subject to the constraint that each voice accounts for the musical time indicated by the known time signature. Our approach applies as well to the situation in which their are multiple possible hypotheses for each symbol, and thus combines interpretation with recognition in a top-down manner. We present experiments demonstrating a nearly 4-fold decrease in the number of false positive symbols with monophonic music, identify missing tuplets, and show preliminary results with polyphonic music. © 2012 International Society for Music Information Retrieval.","","False positive; Monophonic music; Music recognition; Polyphonic music; Programming framework; Top-down manner; Information retrieval","R. Jin; School of Informatics and Computing, Indiana University, Bloomington, United States; email: rongjin@imail.iu.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Bertin-Mahieux T.; Ellis D.P.W.","Bertin-Mahieux, Thierry (49060926500); Ellis, Daniel P.W. (13609089200)","49060926500; 13609089200","Large-scale cover song recognition using the 2D Fourier transform magnitude","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","66","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468293&partnerID=40&md5=95fe816f8cfdf9cc0268281e8dce1137","Columbia University, LabROSA, EE Dept., United States","Bertin-Mahieux T., Columbia University, LabROSA, EE Dept., United States; Ellis D.P.W., Columbia University, LabROSA, EE Dept., United States","Large-scale cover song recognition involves calculating item-to-item similarities that can accommodate differences in timing and tempo, rendering simple Euclidean measures unsuitable. Expensive solutions such as dynamic time warping do not scale to million of instances, making them inappropriate for commercial-scale applications. In this work, we transform a beat-synchronous chroma matrix with a 2D Fourier transform and show that the resulting representation has properties that fit the cover song recognition task. We can also apply PCA to efficiently scale comparisons. We report the best results to date on the largest available dataset of around 18, 000 cover songs amid one million tracks, giving a mean average precision of 3.0%. © 2012 International Society for Music Information Retrieval.","","Data sets; Dynamic time warping; Euclidean; Information retrieval","T. Bertin-Mahieux; Columbia University, LabROSA, EE Dept., United States; email: tb2332@columbia.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Pugin L.; Crawford T.","Pugin, Laurent (23009752900); Crawford, Tim (15054056900)","23009752900; 15054056900","Evaluating OMR on the early music online collection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981550093&partnerID=40&md5=80e0528a43245e0e2bb4045395ddd667","Swiss RISM Office, Switzerland; Goldsmiths College, University of London, United Kingdom","Pugin L., Swiss RISM Office, Switzerland; Crawford T., Goldsmiths College, University of London, United Kingdom","The Early Music Online (EMO) collection consists of about 300 printed music books of the sixteenth century held at the British Library. They were recently digitized from microfilms and made available online. In total, about 35,000 pages were digitized. This paper presents an optical music recognition (OMR) evaluation on the EMO collection. Firstly, the content of the collection is reviewed, looking at the type of music notation and the type of printing technique. Secondly, for the books for which it is possible (260 books), an OMR evaluation performed using the Aruspix OMR software application is presented. For each book, one randomly selected page of music was processed and the recognition rate was computed using a corrected transcription of the page. This evaluation shows very promising results for large-scale OMR on the EMO or similar collections. The paper also highlights critical points that should be taken into account in such an enterprise. © 2013 International Society for Music Information Retrieval.","","Information retrieval; British Library; Early musics; Music notation; Online collection; Optical music recognition; Printing techniques; Software applications; Application programs","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Moore J.L.; Chen S.; Joachims T.; Turnbull D.","Moore, Joshua L. (54995503400); Chen, Shuo (55355054100); Joachims, Thorsten (6602804136); Turnbull, Douglas (8380095700)","54995503400; 55355054100; 6602804136; 8380095700","Taste over time: The temporal dynamics of user preferences","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956813216&partnerID=40&md5=bd93cd5864a558a51c41b881e09907c3","Cornell University, Dept. of Computer Science, United States; Ithaca College, Dept. of Computer Science, United States","Moore J.L., Cornell University, Dept. of Computer Science, United States; Chen S., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","We develop temporal embedding models for exploring how listening preferences of a population develop over time. In particular, we propose time-dynamic probabilistic embedding models that incorporate users and songs in a joint Eu-clidian space in which they gradually change position over time. Using large-scale Scrobbler data from Last.fm spanning a period of 8 years, our models generate trajectories of how user tastes changed over time, how artists developed, and how songs move in the embedded space. This ability to visualize and quantify listening preferences of a large population of people over a multi-year time period provides exciting opportunities for data-driven exploration of musicological trends and patterns. © 2013 International Society for Music Information Retrieval.","","Embeddings; Information retrieval; Data driven; Large population; Last.fm; Temporal dynamics; Time dynamic; Time-periods; Population statistics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Dieleman S.; Schrauwen B.","Dieleman, Sander (55418865400); Schrauwen, Benjamin (22941905500)","55418865400; 22941905500","Multiscale approaches to music audio feature learning","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","60","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947089220&partnerID=40&md5=a7b73e69140e46cefbd9032f1767f240","Electronics and Information Systems department, Ghent University, Belgium","Dieleman S., Electronics and Information Systems department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems department, Ghent University, Belgium","Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classifier. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Classification (of information); Information retrieval; K-means clustering; Learning algorithms; Automatic tagging; Multi-scale approaches; Multiple timescales; Multiscale representations; Music information retrieval; Restricted boltzmann machine; Similarity metric learning; Two stage approach; Machine learning","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Dighe P.; Karnick H.; Raj B.","Dighe, Pranay (56783171700); Karnick, Harish (6603377058); Raj, Bhiksha (7102615577)","56783171700; 6603377058; 7102615577","Swara histogram based structural analysis and identification of Indian classical ragas","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973313819&partnerID=40&md5=b7c57d2050d8f9464a2b92ae6e3201d4","Indian Institute of Technology, Kanpur, India; Carnegie Mellon University, Pittsburgh, PA, United States","Dighe P., Indian Institute of Technology, Kanpur, India; Karnick H., Indian Institute of Technology, Kanpur, India; Raj B., Carnegie Mellon University, Pittsburgh, PA, United States","This work is an attempt towards robust automated analysis of Indian classical ragas through machine learning and signal processing tools and techniques. Indian classical music has a definite heirarchical structure where macro level concepts like thaats and raga are defined in terms of micro entities like swaras and shrutis. Swaras or notes in Indian music are defined only in terms of their relation to one another (akin to the movable do-re-mi-fa system), and an inference must be made from patterns of sounds, rather than their absolute frequency structure. We have developed methods to perform scale-independent raga identification using a random forest classifier on swara histograms and achieved state-of-the-art results for the same. The approach is robust as it directly works on partly noisy raga recordings from Youtube videos without knowledge of the scale used, whereas previous work in this direction often use audios generated in a controlled environment with the desired scale. The current work demonstrates the approach for 8 ragas namely Darbari, Khamaj, Malhar, Sohini, Bahar, Basant, Bhairavi and Yaman and we have achieved an average identification accuracy of 94.28% through the framework. © 2013 International Society for Music Information Retrieval.","","Decision trees; Graphic methods; Information retrieval; Absolute frequency; Automated analysis; Controlled environment; Identification accuracy; Indian classical music; Random forest classifier; State of the art; YouTube; Signal processing","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Hillewaere R.; Manderick B.; Conklin D.","Hillewaere, Ruben (36720698100); Manderick, Bernard (6602920805); Conklin, Darrell (57220096325)","36720698100; 6602920805; 57220096325","String methods for folk tune genre classification","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456470&partnerID=40&md5=b7c9c11c87b6f6fd66caf140e067321f","Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; Department of Computer Science and AI, Universidad del País Vasco UPV/EHU, San Sebastián, Spain; IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","Hillewaere R., Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Department of Computer Science and AI, Universidad del País Vasco UPV/EHU, San Sebastián, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","In folk song research, string methods have been widely used to retrieve highly similar tunes or to perform tune family classification. In this study, we investigate how various string methods perform on a fundamentally different classification task, which is to classify folk tunes into genres, the genres being the dance types of the tunes. A new data set Dance-9 is therefore introduced. The different string method classification accuracies are compared with each other and also with n-gram models and global feature models which have been proven to be useful in previous folk song research. They are shown to yield similar results to the global feature models, but are outperformed by the n-gram models. © 2012 International Society for Music Information Retrieval.","","Classification accuracy; Classification tasks; Data sets; Folk songs; Genre classification; Global feature; N-gram models; String methods; Information retrieval","R. Hillewaere; Computational Modeling Lab., Department of Computing, Vrije Universiteit Brussel, Brussels, Belgium; email: rhillewa@vub.ac.be","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Miryala S.S.; Bali K.; Bhagwan R.; Choudhury M.","Miryala, Sai Sumanth (57188984643); Bali, Kalika (16052071200); Bhagwan, Ranjita (6506809871); Choudhury, Monojit (55841605200)","57188984643; 16052071200; 6506809871; 55841605200","Automatically identifying vocal expressions for music transcription","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905228891&partnerID=40&md5=ddd7b7142a6b0e04dc7f48d6a361a088","Microsoft Research, India","Miryala S.S., Microsoft Research, India; Bali K., Microsoft Research, India; Bhagwan R., Microsoft Research, India; Choudhury M., Microsoft Research, India","Music transcription has many uses ranging from music information retrieval to better education tools. An important component of automated transcription is the identification and labeling of different kinds of vocal expressions such as vibrato, glides, and riffs. In Indian Classical Music such expressions are particularly important since a raga is often established and identified by the correct use of these expressions. It is not only important to classify what the expression is, but also when it starts and ends in a vocal rendition. Some examples of such expressions that are key to Indian music are Meend (vocal glides) and Andolan (very slow vibrato). In this paper, we present an algorithm for the automatic transcription and expression identification of vocal renditions with specific application to North Indian Classical Music. Using expert human annotation as the ground truth, we evaluate this algorithm and compare it with two machine-learning approaches. Our results show that we correctly identify the expressions and transcribe vocal music with 85% accuracy. As a part of this effort, we have created a corpus of 35 voice recordings, of which 12 recordings are annotated by experts. The corpus is available for download1. 1 © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Machine learning; Transcription; Automatic transcription; Education tool; Ground truth; Human annotations; Indian classical music; Music information retrieval; Music transcription; Vocal expression; Computer music","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Ranjani H.G.; Sreenivas T.V.","Ranjani, H.G. (19933973300); Sreenivas, T.V. (7003644773)","19933973300; 7003644773","Hierarchical classification of carnatic music forms","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982275116&partnerID=40&md5=126572665cbb1fbe3faa68509f54312c","Dept. of Electrical Communication Engineering, Indian Institute of Science, Bangalore - 12, India","Ranjani H.G., Dept. of Electrical Communication Engineering, Indian Institute of Science, Bangalore - 12, India; Sreenivas T.V., Dept. of Electrical Communication Engineering, Indian Institute of Science, Bangalore - 12, India","We address the problem of classifying a given piece of Carnatic art music into one of its several forms recognized pedagogically. We propose a hierarchical approach for classification of these forms as different combinations of rhythm, percussion and repetitive syllabic structures. The proposed 3-level hierarchy is based on various signal processing measures and classifiers. Features derived from short term energy contours, along with formant information are used to obtain discriminative features. The statistics of the features are used to design simple classifiers at each level of the hierarchy. The method is validated on a subset of IIT-M Carnatic concert music database, comprising of more than 20 hours of music. Using 10 s audio clips, we get an average f-ratio performance of 0.62 for the classification of the following six typesof Carnatic art music: /AlApana/, /viruttam/, /thillAna/, /krithi/, /thani-Avarthanam/ and /thAnam/. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Signal processing; 3 levels; Audio clips; Discriminative features; Energy contours; Hierarchical approach; Hierarchical classification; Music database; Short term; Classification (of information)","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Ishwar V.; Dutta S.; Bellur A.; Murthy H.A.","Ishwar, Vignesh (56095536200); Dutta, Shrey (55002622000); Bellur, Ashwin (36023240100); Murthy, Hema A. (57200197348)","56095536200; 55002622000; 36023240100; 57200197348","Motif spotting in an alapana in carnatic music","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901439496&partnerID=40&md5=02c5746249d424d9c822de645fcf0c0e","Dept. of Computer Sci. and Engg, IIT, Madras, India; Dept. of Electrical Engg, IIT, Madras, India","Ishwar V., Dept. of Computer Sci. and Engg, IIT, Madras, India; Dutta S., Dept. of Computer Sci. and Engg, IIT, Madras, India; Bellur A., Dept. of Electrical Engg, IIT, Madras, India; Murthy H.A., Dept. of Computer Sci. and Engg, IIT, Madras, India","This work addresses the problem of melodic motif spotting, given a query, in Carnatic music. Melody in Carnatic music is based on the concept of raga. Melodic motifs are signature phrases which give a raga its identity. They are also the fundamental units that enable extempore elaborations of a raga. In this paper, an attempt is made to spot typical melodic motifs of a raga queried in a musical piece using a two pass dynamic programming approach, with pitch as the basic feature. In the first pass, the rough longest common subsequence (RLCS) matching is performed between the saddle points of the pitch contours of the reference motif and the musical piece. These saddle points corresponding to quasi-stationary points of the motifs, are relevant entities of the raga. Multiple sequences are identified in this step, not all of which correspond to the the motif that is queried. To reduce the false alarms, in the second pass a fine search using RLCS is performed between the continuous pitch contours of the reference motif and the subsequences obtained in the first pass. The proposed methodology is validated by testing on Alapanas of 20 different musicians. © 2013 International Society for Music Information Retrieval.","","Information retrieval; False alarms; Fundamental units; Longest common subsequences; Multiple sequences; Musical pieces; Pitch contours; Quasi-stationary; Saddle point; Dynamic programming","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Sakaue D.; Otsuka T.; Itoyama K.; Okuno H.G.","Sakaue, Daichi (55413127400); Otsuka, Takuma (35410110900); Itoyama, Katsutoshi (18042499100); Okuno, Hiroshi G. (7102397930)","55413127400; 35410110900; 18042499100; 7102397930","Bayesian nonnegative harmonic-temporal factorization and its application to multipitch analysis","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445984&partnerID=40&md5=e6d05932f27d0138984bd4e4e6aa086c","Graduate School of Informatics, Kyoto University, Japan","Sakaue D., Graduate School of Informatics, Kyoto University, Japan; Otsuka T., Graduate School of Informatics, Kyoto University, Japan; Itoyama K., Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan","Since important musical features are mutually dependent, their relations should be analyzed simultaneously. Their Bayesian analysis is particularly important to reveal their statistical relation. As the first step for a unified music content analyzer, we focus on the harmonic and temporal structures of the wavelet spectrogram obtained from harmonic sounds. In this paper, we present a new Bayesian multipitch analyzer, called Bayesian non-negative harmonic-temporal factorization (BNHTF). BN-HTF models the harmonic and temporal structures separately based on Gaussian mixture model. The input signal is assumed to contain a finite number of harmonic sounds. Each harmonic sound is assumed to emit a large number of sound quanta over the time-log-frequency domain. The observation probability is expressed as the product of two Gaussian mixtures. The number of quanta is calculated in the e-neighborhood of each grid point on the spectrogram. BNHTF integrates latent harmonic allocation (LHA) and nonnegative matrix factorization (NMF) to estimate both the observation probability and the number of quanta. The model is optimized by newly designed deterministic procedures with several approximations for the variational Bayesian inference. Results of experiments on multipitch estimation with 40 musical pieces showed that BNHTF outperforms the conventional method by 0.018 in terms of F-measure on average. © 2012 International Society for Music Information Retrieval.","","Bayesian networks; Factorization; Inference engines; Information retrieval; Spectrographs; Bayesian Analysis; Conventional methods; F-measure; Finite number; Gaussian Mixture Model; Gaussian mixtures; Grid points; Music contents; Musical features; Musical pieces; Nonnegative matrix factorization; Spectrograms; Statistical relations; Temporal structures; Variational bayesian; Harmonic analysis","D. Sakaue; Graduate School of Informatics, Kyoto University, Japan; email: dsakaue@kuis.kyoto-u.ac.jp","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"McFee B.; Lanckriet G.","McFee, Brian (34875379700); Lanckriet, Gert (7801431767)","34875379700; 7801431767","Hypergraph models of playlist dialects","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","69","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479796&partnerID=40&md5=a28c5d533027b0c80622f6fc1717e23c","Computer Science and Engineering, University of California, San Diego, United States; Electrical and Computer Engineering, University of California, San Diego, United States","McFee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","Playlist generation is an important task in music information retrieval. While previous work has treated a playlist collection as an undifferentiated whole, we propose to build playlist models which are tuned to specific categories or dialects of playlists. Toward this end, we develop a general class of flexible and scalable playlist models based upon hypergraph random walks. To evaluate the proposed models, we present a large corpus of categorically annotated, user-generated playlists. Experimental results indicate that category-specific models can provide substantial improvements in accuracy over global playlist models. © 2012 International Society for Music Information Retrieval.","","General class; Hypergraph; Hypergraph model; Music information retrieval; Random Walk; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Gu Y.; Raphael C.","Gu, Yupeng (55583184600); Raphael, Christopher (7004214964)","55583184600; 7004214964","Modeling piano interpretation using switching Kalman filter","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473337&partnerID=40&md5=4cc477f5fb150ca14ce1ef05de2fe6a7","Indiana University, School of Informatics and Computing, United States","Gu Y., Indiana University, School of Informatics and Computing, United States; Raphael C., Indiana University, School of Informatics and Computing, United States","An approach of parsing piano music interpretation is presented. We focus mainly on quantifying expressive timing activities. A small number of different expressive timing behaviors (constant, slowing down, speeding up, accent) are defined in order to explain the tempo discretely. Given a MIDI performance of a piano music, we simultaneously estimate both discrete variables that corresponds to the behaviors and continuous variables that describe tempo. A graphical model is introduced to represent the evolution of the discrete behaviors and tempo progression. We demonstrate a computational method that acquires the approximate most likely configuration of the discrete behaviors and the hidden continuous variable tempo. This configuration represent a ""smoothed"" version of the performance which greatly reduces parametrization while retaining most of its musicality. Experiments are presented on several MIDI piano music performed on a digital piano. An user study is performed to evaluate our method. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Continuous variables; Discrete variables; GraphicaL model; Parametrizations; Piano music; User study; Musical instruments","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Chuan C.-H.; Chew E.","Chuan, Ching-Hua (15050129500); Chew, Elaine (8706714000)","15050129500; 8706714000","Creating ground truth for audio key finding: When the title key may not be the key","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455583&partnerID=40&md5=ca8487093a2d35f29f22c63c4e751bac","University of North Florida, School of Computing, United States; Queen Mary, University of London, Centre for Digital Music, United Kingdom","Chuan C.-H., University of North Florida, School of Computing, United States; Chew E., Queen Mary, University of London, Centre for Digital Music, United Kingdom","In this paper, we present an effective and efficient way to create an accurately labeled dataset to advance audio key finding research. The MIREX audio key finding contest has been held twice using classical compositions for which the key is designated in the title. The problem with this accepted practice is that the title key may not be the perceived key in the audio excerpt. To reduce manual annotation, which is costly, we use a confusion index generated by existing audio key finding algorithms to determine if an audio excerpt requires manual annotation. We collected 3224 excerpts and identified 727 excerpts requiring manual annotation. We evaluate the algorithms' performance on these challenging cases using the title keys, and the re-labeled keys. The musicians who aurally identify the key also provide comments on the reasons for their choice. The relabeling process reveals the mismatch between title and perceived keys to be caused by tuning practices (in 471 of the 727 excerpts, 64.79%), and other factors (188 excerpts, 25.86%) including key modulation and intonation choices. The remaining 68 challenging cases provide useful information for algorithm design. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Algorithm design; Audio key finding; Data sets; Ground truth; Manual annotation; Relabeling; Data processing","C.-H. Chuan; University of North Florida, School of Computing, United States; email: c.chuan@unf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Fenet S.; Grenier Y.; Richard G.","Fenet, Sébastien (54977223000); Grenier, Yves (55496021000); Richard, Gaël (57195915952)","54977223000; 55496021000; 57195915952","An extended audio-fingerprint method with capabilities for similar music detection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971227413&partnerID=40&md5=68eb703969a6c0e37273db4212c8a0fc","Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France","Fenet S., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France; Grenier Y., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France; Richard G., Institut Mines-Télécom, Télécom ParisTech, CNRS LTCI, 37 rue Dareau, Paris, 75014, France","Content-based Audio Identification consists of retrieving the meta-data (i.e. title, artist, album) associated with an unknown audio excerpt. Audio fingerprint techniques are amongst the most efficient for this goal: following the extraction of a fingerprint from the unknown signal, the closest fingerprint in a reference database is sought in order to perform the identification. While being able to manage large scale databases, the recent developments in fingerprint methods have mostly focused on the improvement of robustness to post-processing distortions (equalization, amplitude compression, pitch-shifting,...). In this work, we describe a novel fingerprint model that is robust not only to the classical set of distortions handled by most methods but also to the variations that occur when a title is re-recorded (live vs studio version in particular). As a result our fingerprint method is able to identify any signal that is an excerpt of one of the references from the database or that is similar to one of the references. The issue that we cover thus lies at the intersection of audio fingerprint and cover song detection, meaning that the functional perimeter of our method is substantially larger than the classical audio fingerprint approaches. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Database systems; Information retrieval; Amplitude compression; Audio fingerprint; Audio identification; Fingerprint method; Large-scale database; Pitch shifting; Post processing; Reference database; Palmprint recognition","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Urbano J.; McFee B.; Stephen Downie J.; Schedl M.","Urbano, Julián (36118414700); McFee, Brian (34875379700); Stephen Downie, J. (7102932568); Schedl, Markus (8684865900)","36118414700; 34875379700; 7102932568; 8684865900","How significant is statistically significant? The case of audio music similarity and retrieval","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460939&partnerID=40&md5=30dabb1804ac0f76a60f1ff7b18334b2","University Carlos III of Madrid, Spain; University of California, San Diego, United States; University of Illinois, Urbana-Champaign, United States; Johannes Kepler University, Linz, Austria","Urbano J., University Carlos III of Madrid, Spain; McFee B., University of California, San Diego, United States; Stephen Downie J., University of Illinois, Urbana-Champaign, United States; Schedl M., Johannes Kepler University, Linz, Austria","The principal goal of the annual Music Information Retrieval Evaluation eXchange (MIREX) experiments is to determine which systems perform well and which systems perform poorly on a range of MIR tasks. However, there has been no systematic analysis regarding how well these evaluation results translate into real-world user satisfaction. For most researchers, reaching statistical significance in the evaluation results is usually the most important goal, but in this paper we show that indicators of statistical significance (i.e., small p-value) are eventually of secondary importance. Researchers who want to predict the real-world implications of formal evaluations should properly report upon practical significance (i.e., large effect-size). Using data from the 18 systems submitted to the MIREX 2011 Audio Music Similarity and Retrieval task, we ran an experiment with 100 real-world users that allows us to explicitly map system performance onto user satisfaction. Based upon 2, 200 judgments, the results show that absolute system performance needs to be quite large for users to be satisfied, and differences between systems have to be very large for users to actually prefer the supposedly better system. The results also suggest a practical upper bound of 80% on user satisfaction with the current definition of the task. Reflecting upon these findings, we make some recommendations for future evaluation experiments and the reporting and interpretation of results in peer-reviewing. © 2012 International Society for Music Information Retrieval.","","Experiments; Information retrieval; Insecticides; Audio music; Current definition; Evaluation experiments; Evaluation results; MAP systems; Music information retrieval; P-values; Statistical significance; Systematic analysis; Upper Bound; User satisfaction; Search engines","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Salamon J.; Gulati S.; Serra X.","Salamon, Justin (55184866100); Gulati, Sankalp (37087243200); Serra, Xavier (55892979900)","55184866100; 37087243200; 55892979900","A multipitch approach to tonic identification in Indian classical music","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475875&partnerID=40&md5=e7ef3879d89edb01efc9dadff1a5e776","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Gulati S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The tonic is a fundamental concept in Indian classical music since it constitutes the base pitch from which a lead performer constructs the melodies, and accompanying instruments use it for tuning. This makes tonic identification an essential first step for most automatic analyses of Indian classical music, such as intonation and melodic analysis, and raga recognition. In this paper we address the task of automatic tonic identification. Unlike approaches that identify the tonic from a single predominant pitch track, here we propose a method based on a multipitch analysis of the audio. We use a multipitch representation to construct a pitch histogram of the audio excerpt, out of which the tonic is identified. Rather than manually define a template, we employ a classification approach to automatically learn a set of rules for selecting the tonic. The proposed method returns not only the pitch class of the tonic but also the precise octave in which it is played. We evaluate the approach on a large collection of Carnatic and Hindustani music, obtaining an identification accuracy of 93%. We also discuss the types of errors made by our proposed method, as well as the challenges in generating ground truth annotations. © 2012 International Society for Music Information Retrieval.","","Automatic analysis; Classification approach; Fundamental concepts; Ground truth; Identification accuracy; Indian classical music; Set of rules; Information retrieval","J. Salamon; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: justin.salamon@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Yang Y.-H.; Hu X.","Yang, Yi-Hsuan (55218558400); Hu, Xiao (55496358400)","55218558400; 55496358400","Cross-cultural music mood classification: A comparison on English and Chinese songs","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873451540&partnerID=40&md5=cb6d21b681218665007ae2d4a96b8600","Academia Sinica, Taiwan; University of Denver, United States","Yang Y.-H., Academia Sinica, Taiwan; Hu X., University of Denver, United States","Most existing studies on music mood classification have been focusing on Western music while little research has investigated whether mood categories, audio features, and classification models developed from Western music are applicable to non-Western music. This paper attempts to answer this question through a comparative study on English and Chinese songs. Specifically, a set of Chinese pop songs were annotated using an existing mood taxonomy developed for English songs. Six sets of audio features commonly used on Western music (e.g., timbre, rhythm) were extracted from both Chinese and English songs, and mood classification performances based on these feature sets were compared. In addition, experiments were conducted to test the generalizability of classification models across English and Chinese songs. Results of this study shed light on cross-cultural applicability of research results on music mood classification. © 2012 International Society for Music Information Retrieval.","","Audio features; Classification models; Classification performance; Comparative studies; Feature sets; Mood taxonomy; Research results; Information retrieval","Y.-H. Yang; Academia Sinica, Taiwan; email: yang@citi.sinica.edu.tw","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Elowsson A.; Friberg A.; Madison G.; Paulin J.","Elowsson, Anders (56381573300); Friberg, Anders (7006743539); Madison, Guy (6602540007); Paulin, Johan (37061558700)","56381573300; 7006743539; 6602540007; 37061558700","Modelling the speed of music using features from harmonic/percussive separated audio","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907866137&partnerID=40&md5=48808b46c8f7f53462a4adf44cc322bb","KTH Royal Institute of Technology, CSC, Dept. of Speech, Music and Hearing, Sweden; Department of Psychology, Umeå University, Sweden","Elowsson A., KTH Royal Institute of Technology, CSC, Dept. of Speech, Music and Hearing, Sweden; Friberg A., KTH Royal Institute of Technology, CSC, Dept. of Speech, Music and Hearing, Sweden; Madison G., Department of Psychology, Umeå University, Sweden; Paulin J., Department of Psychology, Umeå University, Sweden","One of the major parameters in music is the overall speed of a musical performance. In this study, a computational model of speed in music audio has been developed using a custom set of rhythmic features. Speed is often associated with tempo, but as shown in this study, factors such as note density (onsets per second) and spectral flux are important as well. The original audio was first separated into a harmonic part and a percussive part and the features were extracted separately from the different layers. In previous studies, listeners had rated the speed of 136 songs, and the ratings were used in a regression to evaluate the validity of the model as well as to find appropriate features. The final models, consisting of 5 or 8 features, were able to explain about 90% of the variation in the training set, with little or no degradation for the test set. © 2013 International Society for Music Information Retrieval.","","Computer music; Information retrieval; Speed; Computational model; Different layers; Musical performance; Rhythmic features; Spectral flux; Test sets; Training sets; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Nam J.; Herrera J.; Slaney M.; Smith J.","Nam, Juhan (35812266500); Herrera, Jorge (57212698361); Slaney, Malcolm (6701855101); Smith, Julius (7410167523)","35812266500; 57212698361; 6701855101; 7410167523","Learning sparse feature representations for music annotation and retrieval","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","68","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444848&partnerID=40&md5=dc09f3098ed46b842c9fb00f06e3a374","CCRMA, Stanford University, United States; Yahoo, Research Stanford University, United States","Nam J., CCRMA, Stanford University, United States; Herrera J., CCRMA, Stanford University, United States; Slaney M., Yahoo, Research Stanford University, United States; Smith J., CCRMA, Stanford University, United States","We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems. © 2012 International Society for Music Information Retrieval.","","Data handling; Information retrieval; Learning algorithms; Natural language processing systems; Content-based; Data sets; Feature learning; Feature representation; High-dimensional; Linear classifiers; Music data; Retrieval systems; Systemic approach; Unsupervised algorithms; Audio acoustics","J. Nam; CCRMA, Stanford University, United States; email: juhan@ccrma.stanford.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Schindler A.; Mayer R.; Rauber A.","Schindler, Alexander (54891737000); Mayer, Rudolf (23397787500); Rauber, Andreas (57074846700)","54891737000; 23397787500; 57074846700","Facilitating comprehensive benchmarking experiments on the million song dataset","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","51","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459791&partnerID=40&md5=95d12c16e08094ac427f832fd2b506c1","Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Schindler A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Mayer R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","The Million Song Dataset (MSD), a collection of one million music pieces, enables a new era of research of Music Information Retrieval methods for large-scale applications. It comes as a collection of meta-data such as the song names, artists and albums, together with a set of features extracted with the The Echo Nest services, such as loudness, tempo, and MFCC-like features. There is, however, no easily obtainable download for the audio files. Furthermore, labels for supervised machine learning tasks are missing. Researchers thus are currently restricted on working solely with these features provided, limiting the usefulness of MSD. We therefore present in this paper a more comprehensive set of data based on the MSD, allowing its broader use as benchmark collection. Specifically, we provide a wide and growing collection of other well-known features in the MIR domain, as well as ground truth data with a set of recommended training/test splits. We obtained these features from audio samples provided by 7digital.com, and metadata from the All Music Guide. While copyright prevents re-distribution of the audio snippets per se, the features as well as metadata are publicly available on our website for benchmarking evaluations. In this paper we describe the pre-processing and cleansing steps applied, as well as feature sets and tools made available, together with first baseline classification results. © 2012 International Society for Music Information Retrieval.","","Benchmarking; Data processing; Information retrieval; Metadata; Audio files; Audio samples; Classification results; Data sets; Feature sets; Ground truth data; Large-scale applications; Music information retrieval; Pre-processing; Re-distribution; Supervised machine learning; Audio acoustics","A. Schindler; Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; email: schindler@ifs.tuwien.ac.at","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Fukayama S.; Yoshii K.; Goto M.","Fukayama, Satoru (56407004300); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","56407004300; 7103400120; 7403505330","Chord-sequence-factory: A chord arrangement system modifying factorized chord sequence probabilities","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963746662&partnerID=40&md5=89f72489b947871146e997c483df3ca3","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Fukayama S., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a system named ChordSequenceFactory for automatically generating chord arrangements. A key element of musical composition is the arrangement of chord sequences because good chord arrangements have the potential to enrich the listening experience and create a pleasant feeling of surprise by borrowing elements from different musical styles in unexpected ways. While chord sequences have conventionally been modeled by using N-grams, generative grammars, or music theoretic rules, our system decomposes a matrix consisting of chord transition probabilities by using nonnegative matrix factorization. This enables us to not only generate chord sequences from scratch but also transfer characteristic transition patterns from one chord sequence to another. ChordSequenceFactory can assist users to edit chord sequences by modifying factorized chord transition probabilities and then automatically re-arranging them. By leveraging knowledge from chord sequences of over 2000 songs, our system can help users generate a wide range of musically interesting and entertaining chord arrangements. © 2013 International Society for Music Information Retrieval.","","Factorization; Information retrieval; Knowledge management; Matrix algebra; Chord sequence; Key elements; Musical composition; N-grams; Nonnegative matrix factorization; Transfer characteristics; Transition patterns; Transition probabilities; Probability","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Dupont S.; Ravet T.","Dupont, Stéphane (7007183914); Ravet, Thierry (29068044400)","7007183914; 29068044400","Improved audio classification using a novel non-linear dimensionality reduction ensemble approach","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935847385&partnerID=40&md5=f38beba8a0b68ca30640ba174773ac01","University of Mons, Belgium","Dupont S., University of Mons, Belgium; Ravet T., University of Mons, Belgium","Two important categories of machine learning methodologies have recently attracted much interest in classification research and its applications. On one side, unsupervised and semi-supervised learning allow to benefit from the availability of larger sets of training data, even if not fully annotated with class labels, and of larger sets of diverse feature representations, through novel dimensionality reduction schemes. On the other side, ensemble methods allow to benefit from more diversity in base learners though larger data and feature sets. In this paper, we propose a novel ensemble learning approach making use of recent non-linear dimensionality reduction methods. More precisely, we apply t-SNE (t-distributed Stochastic Neighbor Embedding) to a large feature set to come up with embeddings of various dimensionality. A k-NN classifier is then obtained for each embedding, leading to an ensemble whose estimates can then be combined, making use of various ensemble combination rules from the literature. The rationale of this approach resides in its potential capacity to better handle manifolds of different dimensionality in different regions of the feature space. We evaluate the approach on a transductive audio classification task, where only part of the whole data set is labeled. We confirm that dimensionality reduction by itself can improve performance (by 40% relative), and that creating an ensemble through the proposed approach further reduces classification error rate by about 10% relative. © 2013 International Society for Music Information Retrieval.","","Audio acoustics; Embeddings; Information retrieval; Learning algorithms; Machine learning; Nearest neighbor search; Stochastic systems; Supervised learning; Audio classification; Classification error rate; Dimensionality reduction; Ensemble learning approach; Improve performance; Nonlinear dimensionality reduction; Semi- supervised learning; Stochastic neighbor embedding; Classification (of information)","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Bimbot F.; Deruty E.; Sargent G.; Vincent E.","Bimbot, Frédéric (6701567957); Deruty, Emmanuel (35503270700); Sargent, Gabriel (55583665800); Vincent, Emmanuel (14010158800)","6701567957; 35503270700; 55583665800; 14010158800","Semiotic structure labeling of music pieces: Concepts, methods and annotation conventions","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480534&partnerID=40&md5=08efa44e892832ef0c9a8ecf1204da8c","IRISA/METISS, CNRS - UMR 6074, France; INRIA/METISS, Rennes, France; IRISA/METISS, Université Rennes 1, France","Bimbot F., IRISA/METISS, CNRS - UMR 6074, France; Deruty E., INRIA/METISS, Rennes, France; Sargent G., IRISA/METISS, Université Rennes 1, France; Vincent E., INRIA/METISS, Rennes, France","Music structure description, i.e. the task of representing the high-level organization of music pieces in a concise, generic and reproducible way, is currently a scientific challenge both algorithmically and conceptually. In this paper, we focus on semiotic structure, i.e. the description of similarities and internal relationships within a music piece, as a low-rate stream of arbitrary symbols from a limited alphabet and we address methodological questions related to annotation. We formulate the labeling task as a blind demodulation problem, whose goal is to identify a minimal set of semiotic codewords, whose realizations within the music piece are subject to a number of connotative variations viewed as modulations. The determination of labels is achieved by combining morphological, paradigmatic and syntagmatic considerations relying respectively on (i) a morphological model of semiotic blocks in order to define their individual properties, (ii) the support of prototypical structural patterns to guide the comparison between blocks and (iii) a methodology for the determination of distinctive features across semiotic classes. Specific notations are introduced to account for unresolvable semiotic ambiguities, which are occasional but must be considered as inherent to the music matter itself. A set of 500 music pieces labeled in accordance with the proposed concepts and annotation conventions is being released with this article. © 2012 International Society for Music Information Retrieval.","","Computer hardware description languages; Information retrieval; Arbitrary symbols; Code-words; Morphological model; Music structures; Structural pattern; Semiotics","F. Bimbot; IRISA/METISS, CNRS - UMR 6074, France; email: frederic.bimbot@irisa.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Özaslan T.H.; Serra X.; Arcos J.L.","Özaslan, Tan Hakan (51161878200); Serra, Xavier (55892979900); Arcos, Josep Lluis (56131974300)","51161878200; 55892979900; 56131974300","Characterization of embellishments in ney performances of makam music in Turkey","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453734&partnerID=40&md5=53d268add37ec389405990729063ca66","Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Özaslan T.H., Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Arcos J.L., Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain","The embellishments of makam music in Turkey are an inherent characteristic of the music rather than a separate expressive resource, thus their understanding is essential to characterize this music. We do a computational study, in which we analyze audio recordings of 8 widely acknowledged Turkish ney players covering the period from the year 1920 to 2000. From the extracted fundamental frequency, we manually segment and identify 327 separate embellishments of the types vibrato and kaydirma. We analyze them and characterize the behavior of two features that help us differentiate performance styles, namely vibrato rate change and pitch bump. Also we compare these embellishments with the ones used in Western classical music. With our approach, we have an explicit and formalized way to understand ney embellishments, which is a step towards the automatic characterization of makam music in Turkey. © 2012 International Society for Music Information Retrieval.","","Separation; Computational studies; Fundamental frequencies; Inherent characteristics; Turkishs; Vibrato rate; Information retrieval","T.H. Özaslan; Artificial Intelligence Research Institute - CSIC, Bellaterra, Spain; email: tan@iiia.csic.es","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Lefèvre A.; Bach F.; Févotte C.","Lefèvre, Augustin (49061248200); Bach, Francis (7202286449); Févotte, Cédric (14031507800)","49061248200; 7202286449; 14031507800","Semi-supervised NMF with time-frequency annotations for single-channel source separation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873466806&partnerID=40&md5=8577f5908b3c355836c17b0461c2775a","INRIA Team SIERRA, France; LTCI, Telecom ParisTech, France","Lefèvre A., INRIA Team SIERRA, France; Bach F., INRIA Team SIERRA, France; Févotte C., LTCI, Telecom ParisTech, France","We formulate a novel extension of nonnegative matrix factorization (NMF) to take into account partial information on source-specific activity in the spectrogram. This information comes in the form of masking coefficients, such as those found in an ideal binary mask. We show that state-of-the-art results in source separation may be achieved with only a limited amount of correct annotation, and furthermore our algorithm is robust to incorrect annotations. Since in practice ideal annotations are not observed, we propose several supervision scenarios to estimate the ideal masking coefficients. First, manual annotations by a trained user on a dedicated graphical user interface are shown to provide satisfactory performance although they are prone to errors. Second, we investigate simple learning strategies to predict the Wiener coefficients based on local information around a given time-frequency bin of the spectrogram. Results on single-channel source separation show that time-frequency annotations allow to disambiguate the source separation problem, and learned annotations open the way for a completely unsupervised learning procedure for source separation with no human intervention. © 2012 International Society for Music Information Retrieval.","","Graphical user interfaces; Spectrographs; Human intervention; Ideal binary mask; Learning strategy; Local information; Manual annotation; Nonnegative matrix factorization; Partial information; Semi-supervised; Separation problems; Single-channel; Spectrograms; Time frequency; Information retrieval","A. Lefèvre; INRIA Team SIERRA, France; email: augustin.lefevre@inria.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Ni Y.; Mcvicar M.; Santos-Rodríguez R.; De Bie T.","Ni, Yizhao (25823257400); Mcvicar, Matt (36721968600); Santos-Rodríguez, Raul (27968154800); De Bie, Tijl (57203775071)","25823257400; 36721968600; 27968154800; 57203775071","Using hyper-genre training to explore genre information for automatic chord estimation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453177&partnerID=40&md5=d197821b21140af95a86f8a5725273b4","Intelligent Systems Laboratory, University of Bristol, United Kingdom","Ni Y., Intelligent Systems Laboratory, University of Bristol, United Kingdom; Mcvicar M., Intelligent Systems Laboratory, University of Bristol, United Kingdom; Santos-Rodríguez R., Intelligent Systems Laboratory, University of Bristol, United Kingdom; De Bie T., Intelligent Systems Laboratory, University of Bristol, United Kingdom","Recently a large amount of new chord annotations have been made available. This raises hopes for further development in automatic chord estimation. While more data seems to imply better performance, a major challenge however, is the wide variety of genres covered by these new data. As a result, the genre-independent training scheme as is common today is bound to fail. In this paper we investigate various options for exploring genre information for chord estimation, while also maximally exploiting the full dataset. More specifically, we propose a hyper-genre training scheme in which each genre cluster has its own parameters, tied together by hyper parameters as a Bayesian prior. The results are promising, showing significant improvements over other prevailing training schemes. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Data sets; Hyper-parameter; Training schemes; Estimation","Y. Ni; Intelligent Systems Laboratory, University of Bristol, United Kingdom; email: enxyn@bristol.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Arora V.; Behera L.","Arora, Vipul (57210249491); Behera, Laxmidhar (6701522952)","57210249491; 6701522952","Semi-supervised polyphonic source identification using PLCA based graph clustering","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901433498&partnerID=40&md5=ac71a95e49b572a6a79675407312addd","Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","Arora V., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India; Behera L., Department of Electrical Engineering, Indian Institute of Technology, Kanpur, India","For identifying instruments or singers in the polyphonic audio, supervised probabilistic latent component analysis (PLCA) is a popular tool. But in many cases individual source audio is not available for training. To address this problem, this paper proposes a novel scheme using semi-supervised PLCA with probabilistic graph clustering, which does not require individual sources for training. The PLCA is based on source-filter approach which models the spectral envelope as a weighted sum of elementary band-pass filters. The novel graph based approach, embedded in the PLCA framework, takes into account various perceptual cues for characterizing a source. These cues include temporal cues like the evolution of F0 contours as well as the acoustic cues like mel-frequency cepstral coefficients. The proposed scheme shows better results in identifying vocal sources than a state of the art unsupervised scheme. In addition, the proposed framework can be used to incorporate perceptual cues so as to enhance the performance of supervised schemes too. © 2013 International Society for Music Information Retrieval.","","Graphic methods; Information retrieval; Graph clustering; Mel frequency cepstral co-efficient; Probabilistic graphs; Probabilistic latent component analysis; Semi-supervised; Source identification; Spectral envelopes; State of the art; Bandpass filters","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Scheeren F.M.; Pimenta M.S.; Keller D.; Lazzarini V.","Scheeren, Felipe M. (57210187445); Pimenta, Marcelo S. (22433533400); Keller, Damián (36720882700); Lazzarini, Victor (24577088500)","57210187445; 22433533400; 36720882700; 24577088500","Coupling social network services and support for online communities in codes environment","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969930477&partnerID=40&md5=388e96060b518497d4f14e424b57cd4d","Institute of Informatics UFRGS, Brazil; Amazon Center for Music Research UFAC, Brazil; National Univ. of Ireland, Maynooth, Ireland","Scheeren F.M., Institute of Informatics UFRGS, Brazil; Pimenta M.S., Institute of Informatics UFRGS, Brazil; Keller D., Amazon Center for Music Research UFAC, Brazil; Lazzarini V., National Univ. of Ireland, Maynooth, Ireland","In recent years, our research group has been investigating the use of computing technology to support novice-oriented computer-based musical activities. CODES (Cooperative Music Prototyping Design) is a Web-based environment designed to allow novice users to create musical prototypes through combining basic sound patterns. This paper shows how CODES has been changed to provide support to some concepts originally from of Social Networks and also to Online Communities having Music Creation as intrinsic motivation. © 2013 International Society for Music Information Retrieval.","","Codes (symbols); Information retrieval; Network coding; Online systems; Computing technology; Intrinsic motivation; Music creation; On-line communities; Prototyping designs; Research groups; Social network services; Web-based environment; Social networking (online)","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Lehner B.; Sonnleitner R.; Widmer G.","Lehner, Bernhard (7003282869); Sonnleitner, Reinhard (55566668400); Widmer, Gerhard (7004342843)","7003282869; 55566668400; 7004342843","Towards light-weight, real-time-capable singing voice detection","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905250130&partnerID=40&md5=3098d60da8ced894d5fdd8f69a28ccc1","Department of Computational Perception, Johannes Kepler University of Linz, Austria","Lehner B., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Sonnleitner R., Department of Computational Perception, Johannes Kepler University of Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University of Linz, Austria","We present a study that indicates that singing voice detection – the problem of identifying those parts of a polyphonic audio recording where one or several persons sing(s) – can be realised with substantially fewer (and less expensive) features than used in current state-of-the-art methods. Essentially, we show that MFCCs alone, if appropriately optimised and used with a suitable classifier, are sufficient to achieve detection results that seem on par with the state of the art – at least as far as this can be ascertained by direct, fair comparisons to existing systems. To make this comparison, we select three relevant publications from the literature where publicly accessible training/test data were used, and where the experimental setup is described in enough detail for us to perform fair comparison experiments. The result of the experiments is that with our simple, optimised MFCC-based classifier we achieve at least comparable identification results, but with (in some cases much) less computational effort, and without any need for extensive lookahead, thus paving the way to on-line, real-time voice detection applications. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Computational effort; Existing systems; Light weight; Publicly accessible; Real-time voice; Singing voice detection; State of the art; State-of-the-art methods; Speech recognition","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Mechtley B.; Cook P.; Spanias A.","Mechtley, Brandon (24462317400); Cook, Perry (57203105418); Spanias, Andreas (7006177932)","24462317400; 57203105418; 7006177932","Shortest path techniques for annotation and retrieval of environmental sounds","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461191&partnerID=40&md5=da02c4b6fa74315b58f626132f3d88b6","Arizona State University, Computer Science (SCIDSE), United States; Princeton University, Computer Science and Music, United States; Arizona State University, Electrical Engineering (SECEE), United States","Mechtley B., Arizona State University, Computer Science (SCIDSE), United States; Cook P., Princeton University, Computer Science and Music, United States; Spanias A., Arizona State University, Electrical Engineering (SECEE), United States","Many techniques for text-based retrieval and automatic annotation of music and sound effects rely on learning with explicit generalization, training individual classifiers for each tag. Non-parametric approaches, where queries are individually compared to training instances, can provide added flexibility, both in terms of robustness to shifts in database content and support for foreign queries, such as concepts not yet included in the database. In this paper, we build upon prior work in designing an ontological framework for annotation and retrieval of environmental sounds, where shortest paths are used to navigate a network containing edges that represent content-based similarity, semantic similarity, and user tagging data. We evaluate novel techniques for ordering query results using weights of both shortest paths and minimum cost paths of specified lengths, pruning outbound edges by nodes' K nearest neighbors, and adjusting edge weights depending on type (acoustic, semantic, or user tagging). We evaluate these methods both through traditional cross-validation and through simulation of live systems containing a complete collection of sounds and tags but incomplete tagging data. © 2012 International Society for Music Information Retrieval.","","Graph theory; Information retrieval; Query languages; Query processing; Semantics; Automatic annotation; Content-based; Cross validation; Database contents; Edge weights; Environmental sounds; Individual classifiers; K-nearest neighbors; Minimum cost; Nonparametric approaches; Novel techniques; Ontological frameworks; Query results; Semantic similarity; Shortest path; Sound effects; Text-based retrieval; User interfaces","B. Mechtley; Arizona State University, Computer Science (SCIDSE), United States; email: bmechtley@asu.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Wilmering T.; Fazekas G.; Sandler M.B.","Wilmering, Thomas (36997647000); Fazekas, György (37107520200); Sandler, Mark B. (7202740804)","36997647000; 37107520200; 7202740804","The audio effects ontology","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896452212&partnerID=40&md5=7c4e297022b4350d2743a6178a3a0cda","Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom","Wilmering T., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Fazekas G., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; Sandler M.B., Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom","In this paper we present the Audio Effects Ontology for the ontological representation of audio effects in music production workflows. Designed as an extension to the Studio Ontology, its aim is to provide a framework for the detailed description and sharing of information about audio effects, their implementations, and how they are applied in real-world production scenarios. The ontology enables capturing and structuring data about the use of audio effects and thus facilitates reproducibility of audio effect application, as well as the detailed analysis of music production practices. Furthermore, the ontology may inform the creation of metadata standards for adaptive audio effects that map high-level semantic descriptors to control parameter values. The ontology is using Semantic Web technologies that enable knowledge representation and sharing, and is based on modular ontology design methodologies. It is evaluated by examining how it fulfils requirements in a number of production and retrieval use cases. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Knowledge representation; Ontology; Control parameters; High level semantics; Metadata Standards; Modular ontologies; Music production; Ontological representation; Reproducibilities; Semantic Web technology; Audio acoustics","T. Wilmering; Centre for Digital Music (C4DM), Queen Mary University of London, United Kingdom; email: thomas.wilmering@eecs.qmul.ac.uk","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Pikrakis A.; Báñez J.M.D.; Mora J.; Escobar F.; Gomez F.; Oramas S.; Gomez E.; Salamon J.","Pikrakis, A. (6507232714); Báñez, J.M.D. (8847635500); Mora, J. (55582857600); Escobar, F. (53563105800); Gomez, F. (57196622778); Oramas, S. (55582112200); Gomez, E. (14015483200); Salamon, J. (55184866100)","6507232714; 8847635500; 55582857600; 53563105800; 57196622778; 55582112200; 14015483200; 55184866100","Tracking melodic patterns in flamenco singing by analyzing polyphonic music recordings","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465150&partnerID=40&md5=d5d9bde3f07988809f580cc2449d841e","University of Piraeus, Greece; University of Sevilla, Spain; Polytechnic University, Madrid, Spain; Universitat Pompeu Fabra, Spain","Pikrakis A., University of Piraeus, Greece; Báñez J.M.D., University of Sevilla, Spain; Mora J., University of Sevilla, Spain; Escobar F., University of Sevilla, Spain; Gomez F., Polytechnic University, Madrid, Spain; Oramas S., Polytechnic University, Madrid, Spain; Gomez E., Universitat Pompeu Fabra, Spain; Salamon J., Universitat Pompeu Fabra, Spain","The purpose of this paper is to present an algorithmic pipeline for melodic pattern detection in audio files. Our method follows a two-stage approach: first, vocal pitch sequences are extracted from the audio recordings by means of a predominant fundamental frequency estimation technique; second, instances of the patterns are detected directly in the pitch sequences by means of a dynamic programming algorithm which is robust to pitch estimation errors. In order to test the proposed method, an analysis of characteristic melodic patterns in the context of the flamenco fandango style was performed. To this end, a number of such patterns were defined in symbolic format by flamenco experts and were later detected in music corpora, which were composed of un-segmented audio recordings taken from two fandango styles, namely Valverde fandangos and Huelva capital fandangos. These two styles are representative of the fandango tradition and also differ with respect to their musical characteristics. Finally, the strategy in the evaluation of the algorithm performance was discussed by flamenco experts and their conclusions are presented in this paper. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Algorithm performance; Audio files; Dynamic programming algorithm; Fundamental frequency estimation; Pattern detection; Pitch estimation; Polyphonic music; Vocal pitches; Audio recordings","A. Pikrakis; University of Piraeus, Greece; email: pikrakis@unipi.gr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Gerber T.; Dutasta M.; Girin L.; Févotte C.","Gerber, Timothée (56319538100); Dutasta, Martin (55582022900); Girin, Laurent (6602388299); Févotte, Cédric (14031507800)","56319538100; 55582022900; 6602388299; 14031507800","Professionally-produced music separation guided by covers","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448116&partnerID=40&md5=e41974a2585be448e6814b3d581d90b6","Grenoble-INP, GIPSA-Lab., France; TELECOM ParisTech, CNRS LTCI, France","Gerber T., Grenoble-INP, GIPSA-Lab., France; Dutasta M., Grenoble-INP, GIPSA-Lab., France; Girin L., Grenoble-INP, GIPSA-Lab., France; Févotte C., TELECOM ParisTech, CNRS LTCI, France","This paper addresses the problem of demixing professionally produced music, i.e., recovering the musical source signals that compose a (2-channel stereo) commercial mix signal. Inspired by previous studies using MIDI synthesized or hummed signals as external references, we propose to use the multitrack signals of a cover interpretation to guide the separation process with a relevant initialization. This process is carried out within the framework of the multichannel convolutive NMF model and associated EM/MU estimation algorithms. Although subject to the limitations of the convolutive assumption, our experiments confirm the potential of using multitrack cover signals for source separation of commercial music. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Separation; De-mixing; Estimation algorithm; Multi-channel; Separation process; Source signals; Source separation","T. Gerber; Grenoble-INP, GIPSA-Lab., France; email: Gerber.Timothee@gipsa-lab.grenoble-inp.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Benetos E.; Weyde T.","Benetos, Emmanouil (16067946900); Weyde, Tillman (24476899500)","16067946900; 24476899500","Explicit duration hidden Markov models for multiple-instrument polyphonic music transcription","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963983592&partnerID=40&md5=067519ed92566ae66ac8f41f9efe0381","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","Benetos E., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom","In this paper, a method for multiple-instrument automatic music transcription is proposed that models the temporal evolution and duration of tones. The proposed model supports the use of spectral templates per pitch and instrument which correspond to sound states such as attack, sustain, and decay. Pitch-wise explicit duration hidden Markov models (EDHMMs) are integrated into a convolutive probabilistic framework for modelling the temporal evolution and duration of the sound states. A two-stage transcription procedure integrating note tracking information is performed in order to provide more robust pitch estimates. The proposed system is evaluated on multi-pitch detection and instrument assignment using various publicly available datasets. Results show that the proposed system outperforms a hidden Markov model-based transcription system using the same framework, as well as several state-of-the-art automatic music transcription systems. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Transcription; Trellis codes; Automatic music transcription; Explicit duration; Multi pitches; Multiple instruments; Polyphonic music; Probabilistic framework; State of the art; Temporal evolution; Hidden Markov models","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Aucouturier J.-J.; Bigand E.","Aucouturier, Jean-Julien (9943558100); Bigand, Emmanuel (7004338526)","9943558100; 7004338526","Mel Cepstrum & Ann Ova: The difficult dialog between MIR and music cognition","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445003&partnerID=40&md5=b95b67a48ffcec99f045b9cc937f7c4c","LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France","Aucouturier J.-J., LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France; Bigand E., LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France","Mel is a MIR researcher (the audio type) who's always been convinced that his field of research had something to contribute to the study of music cognition. His feeling, however, hasn't been much shared by the reviewers of the many psychology journals he tried submitting his views to. Their critics, rejecting his data as irrelevant, have frustrated him - the more he tried to rebut, the more defensive both sides of the debate became. He was close to give up his hopes of interdisciplinary dialog when, in one final and desperate rejection letter, he sensed an unusual touch of interest in the editor's response. She, a cognitive psychologist named Ann, was clearly open to discussion. This was the opportunity that Mel had always hoped for: clarifying what psychologists really think of audio MIR, correcting misconceptions that he himself made about cognition, and maybe, developing a vision of how both fields could work together. The following is the imaginary dialog that ensued. Meet Dr Mel Cepstrum, the MIR researcher, and Prof. Ann Ova, the psychologist. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Mel cepstrum; Research","J.-J. Aucouturier; LEAD/CNRS UMR 5022, University of Burgundy, Dijon, France; email: aucouturier@gmail.com","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Devaney J.; Mandel M.; Fujinaga I.","Devaney, Johanna (35766484200); Mandel, Michael (14060460000); Fujinaga, Ichiro (9038140900)","35766484200; 14060460000; 9038140900","A study of intonation in three-part singing using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476660&partnerID=40&md5=d5547773d66078baea308d3058d0550a","CNMAT, UC Berkeley, School of Music, United States; Audience Inc., College of Engineering, Ohio State University, United States; CIRMMT, Schulich School of Music, McGill University, Canada","Devaney J., CNMAT, UC Berkeley, School of Music, United States; Mandel M., Audience Inc., College of Engineering, Ohio State University, United States; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","This paper introduces the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT), is a MATLAB toolkit for accurately aligning monophonic audio to MIDI scores as well as extracting and analyzing timing-, pitch-, and dynamics-related performance data from the aligned recordings. This paper also presents the results of an analysis performed with AMPACT on an experiment studying intonation in three-part singing. The experiment examines the interval size and drift in four ensembles' performances of a short exercise by Benedetti, which was designed to highlight the conflict between Just Intonation tuning and pitch drift. © 2012 International Society for Music Information Retrieval.","","Experiments; Information retrieval; Interval size; MATLAB toolkit; Music performance; Performance data; Audio acoustics","J. Devaney; CNMAT, UC Berkeley, School of Music, United States; email: j@devaney.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Font F.; Serrà J.; Serra X.","Font, Frederic (35182806000); Serrà, Joan (35749172500); Serra, Xavier (55892979900)","35182806000; 35749172500; 55892979900","Folksonomy-based tag recommendation for online audio clip sharing","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460515&partnerID=40&md5=b10ea431818a860622a7f614e2f23a88","Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain; Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain","Font F., Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Serra X., Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain","Collaborative tagging has emerged as an efficient way to semantically describe online resources shared by a community of users. However, tag descriptions present some drawbacks such as tag scarcity or concept inconsistencies. In these situations, tag recommendation strategies can help users in adding meaningful tags to the resources being described. Freesound is an online audio clip sharing site that uses collaborative tagging to describe a collection of more than 140, 000 sound samples. In this paper we propose four algorithm variants for tag recommendation based on tag co-occurrence in the Freesound folksonomy. On the basis of removing a number of tags that have to be later predicted by the algorithms, we find that using ranks instead of raw tag similarities produces statistically significant improvements. Moreover, we show how specific strategies for selecting the appropriate number of tags to be recommended can significantly improve algorithms' performance. These two aspects provide insight into some of the most basic components of tag recommendation systems, and we plan to exploit them in future real-world deployments. © 2012 International Society for Music Information Retrieval.","","Algorithms; Audio recordings; Information retrieval; Audio clips; Co-occurrence; Collaborative tagging; Folksonomies; Online resources; Sound sample; Tag recommendations; Tag similarity; User interfaces","F. Font; Music Technology Gorup, Universitat Pompeu Fabra, Barcelona, Spain; email: frederic.font@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Page K.R.; Fields B.; De Roure D.; Crawford T.; Stephen Downie J.","Page, Kevin R. (8104972900); Fields, Ben (35733701400); De Roure, David (6701509117); Crawford, Tim (15054056900); Stephen Downie, J. (7102932568)","8104972900; 35733701400; 6701509117; 15054056900; 7102932568","Reuse, remix, repeat: The workflows of MIR","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472741&partnerID=40&md5=5f9c240f9d9e93fcd9db22dd7282acf0","Oxford e-Research Centre, University of Oxford, United Kingdom; Musicmetric (Semetric Ltd.), United Kingdom; Department of Computing, Goldsmiths, University of London, United Kingdom; Graduate School of Library and Information Sciences, University of Illinois, United States","Page K.R., Oxford e-Research Centre, University of Oxford, United Kingdom; Fields B., Musicmetric (Semetric Ltd.), United Kingdom, Department of Computing, Goldsmiths, University of London, United Kingdom; De Roure D., Oxford e-Research Centre, University of Oxford, United Kingdom; Crawford T., Department of Computing, Goldsmiths, University of London, United Kingdom; Stephen Downie J., Graduate School of Library and Information Sciences, University of Illinois, United States","Many solutions for the reuse and remixing of MIR methods and the tools implementing them have been introduced over recent years. Proposals for achieving the necessary interoperability have ranged from shared software libraries and interfaces, through common frameworks and portals, to standardised file formats and metadata. Each proposal shares the desire to reuse and combine repurposable components into assemblies (or ""workflows"") that can be used in novel and possibly more ambitious ways. Reuse and remixing also have great implications for the process of MIR research. The encapsulation of any algorithm and its operation - including inputs, parameters, and outputs - is fundamental to the repeatability and reproducibility of any experiment. This is desirable both for the open and reliable evaluation of algorithms (e.g. in MIREX) and for the advancement of MIR by building more effectively upon prior research. At present there is no clear best practice widely adopted throughout the community. Should this be considered a failure? Are there limits to interoperability unique to MIR, and how might they be overcome? In this paper we assess contemporary MIR solutions to these issues, aligning them with the emerging notion of Research Objects for reproducible research in other domains, and propose their adoption as a route to reuse in MIR. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Interoperability; Metadata; File formats; Reproducibilities; Reproducible research; Research object; Software libraries; Work-flows; Research","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Su L.; Yang Y.-H.","Su, Li (55966919100); Yang, Yi-Hsuan (55218558400)","55966919100; 55218558400","Sparse modeling for artist identification: Exploiting phase information and vocal separation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963947798&partnerID=40&md5=4f007d08229736274ce6d2161696191b","Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","Su L., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Yang Y.-H., Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan","As artist identification deals with the vocal part of music, techniques such as vocal sound separation and speech feature extraction has been found relevant. In this paper, we argue that the phase information, which is usually overlooked in the literature, is also informative in modeling the voice timbre of a singer, given the necessary processing techniques. Specifically, instead of directly using the raw phase spectrum as features, we show that significantly better performance can be obtained by learning sparse features from the negative derivative of phase with respect to frequency (i.e., group delay function) using unsupervised feature learning algorithms. Moreover, better performance is achieved by using singing voice separation as a pre-processing step, and then learning features from both the magnitude spectrum and the group delay function. The proposed system achieves 66% accuracy in identifying 20 artists from the artist20 dataset, which is better than a prior art by 7%. © 2013 International Society for Music Information Retrieval.","","Group delay; Information retrieval; Separation; Group delay functions; Magnitude spectrum; Phase information; Pre-processing step; Processing technique; Singing voice separations; Sound separation; Unsupervised feature learning; Learning algorithms","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Hauger D.; Schedl M.; Košir A.; Tkalčič M.","Hauger, David (42961319900); Schedl, Markus (8684865900); Košir, Andrej (6603413691); Tkalčič, Marko (24438438300)","42961319900; 8684865900; 6603413691; 24438438300","The million musical tweets dataset: What can we learn from microblogs","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","64","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897638496&partnerID=40&md5=c6009bb0e03f5e921b984332ad7f73e4","Johannes Kepler University Linz, Austria; University of Ljubljana, Slovenia","Hauger D., Johannes Kepler University Linz, Austria; Schedl M., Johannes Kepler University Linz, Austria; Košir A., University of Ljubljana, Slovenia; Tkalčič M., Johannes Kepler University Linz, Austria","Microblogs and Social Media applications are continuously growing in spread and importance. Users of Twitter, the currently most popular platform for microblogging, create more than a billion posts (called tweets) every week. Among all the different types of information being shared, some people post their music listening behavior, which is why Twitter became interesting for the Music Information Retrieval (MIR) community. Depending on the device and personal settings, some users provide geographic coordinates for their microposts. Having continuously crawled and analyzed tweets for more than 500 days (17 months) we can now present the “Million Musical Tweet Dataset” (MMTD) – the biggest publicly available source of microblog-based music listening histories that includes geographic, temporal, and other contextual information. These extended information makes the MMTD outstanding from other datasets providing music listening histories. We introduce the dataset, give basic statistics about its composition, and show how this dataset allows to detect new contextual music listening patterns by performing a comprehensive statistical investigation with respect to correlation between music taste and day of the week, hour of day, and country. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Contextual information; Geographic coordinates; Listening history; Micro-blog; Microblogging; Music information retrieval; Popular platform; Social media; Social networking (online)","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Gkiokas A.; Katsouros V.; Carayannis G.","Gkiokas, Aggelos (14035320300); Katsouros, Vassilis (6507518296); Carayannis, George (7003979360)","14035320300; 6507518296; 7003979360","Reducing tempo octave errors by periodicity vector coding and SVM learning","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456653&partnerID=40&md5=ae239812cfed7fd8d32bd3c57c772bfe","Institute for Language and Speech Processing, R.C. Athena, Greece; National Technical University of Athens, Greece","Gkiokas A., Institute for Language and Speech Processing, R.C. Athena, Greece, National Technical University of Athens, Greece; Katsouros V., Institute for Language and Speech Processing, R.C. Athena, Greece; Carayannis G., National Technical University of Athens, Greece","In this paper we present a method for learning tempo classes in order to reduce tempo octave errors. There are two main contributions of this paper in the rhythm analysis field. Firstly, a novel technique is proposed to code the rhythm periodicity functions of a music signal. Target tempi range is divided into overlapping ""tempo bands"" and the periodicity function is filtered by triangular masks aligned to those tempo bands, in order to calculate the respective saliencies, followed by the application of the DCT transform on band strengths. The second contribution is the adoption of Support Vector Machines to learn broad tempo classes from the coded periodicity vectors. Training instances are assigned a tempo class according to annotated tempo. The classes are assumed to correspond to ""music speed"". At classification phase, each target excerpt is assigned a tempo class label by the SVM. Target periodicity vector is masked by the predicted tempo class range, and tempo is estimated by peak picking in the reduced periodicity vector. The proposed method was evaluated on the benchmark ISMIR 2004 Tempo Induction Evaluation Exchange Dataset for both tempo class and tempo value estimation tasks. Results indicate that the proposed approach provides an efficient framework to tackle the tempo estimation task. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Support vector machines; Vectors; Class labels; Data sets; DCT transform; Music signals; Novel techniques; Peak picking; Periodicity vectors; Value estimation; Coding errors","A. Gkiokas; Institute for Language and Speech Processing, R.C. Athena, Greece; email: agkiokas@ilsp.gr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Giraud M.; Groult R.; Levé F.","Giraud, Mathieu (8700367400); Groult, Richard (6507884031); Levé, Florence (55893852300)","8700367400; 6507884031; 55893852300","Detecting episodes with harmonic sequences for fugue analysis","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873454232&partnerID=40&md5=2a8e31c02f6ad830211aa6dce583905e","LIFL, CNRS, Université Lille 1 INRIA, Lille, France; MIS, Université Picardie Jules Verne, Amiens, France","Giraud M., LIFL, CNRS, Université Lille 1 INRIA, Lille, France; Groult R., MIS, Université Picardie Jules Verne, Amiens, France; Levé F., MIS, Université Picardie Jules Verne, Amiens, France","Fugues alternate between instances of the subject and of other patterns, such as the counter-subject, and modula-tory sections called episodes. The episodes play an important role in the overall design of a fugue: detecting them may help the analysis of the fugue, in complement to a subject and a counter-subject detection. We propose an algorithm to retrieve episodes in the fugues of the first book of Bach's Well-Tempered Clavier, starting from a symbolic score which is already track-separated. The algorithm does not use any information on subject or counter-subject occurrences, but tries to detect partial harmonic sequences, that is similar pitch contour in at least two voices. For this, it uses a substitution function considering ""quantized partially overlapping intervals"" © 2012 International Society for Music Information Retrieval. [14] and a strict length matching for all notes, except for the first and the last one. On half of the tested fugues, the algorithm has correct or good results, enabling to sketch the design of the fugue. © 2012 International Society for Music Information Retrieval.","","Algorithms; Modula (programming language); Overall design; Pitch contours; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Izmirli O.; Sharma G.","Izmirli, Özgür (6507754258); Sharma, Gyanendra (57653240600)","6507754258; 57653240600","Bridging printed music and audio through alignment using a mid-level score representation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456894&partnerID=40&md5=637f8b2bded314a607d157b437c11386","Center for Arts and Technology, Computer Science Department, Connecticut College, United States","Izmirli O., Center for Arts and Technology, Computer Science Department, Connecticut College, United States; Sharma G., Center for Arts and Technology, Computer Science Department, Connecticut College, United States","We present a system that utilizes a mid-level score representation for aligning printed music to its audio rendition. The mid-level representation is designed to capture an approximation to the musical events present in the printed score. It consists of a template based note detection frontend that seeks to detect notes without regard to musical duration, accidentals or the key signature. The presented method is designed for the commonly used grand staff and the approach is extendable to other types of scores. The image processing consists of page segmentation into lines followed by multiple stages that optimally orient the lines and establish a reference grid to be used in the note identification stage. Both the audio and the printed score are converted into compatible frequency representations. Alignment is performed using dynamic time warping with a specially designed distance measure. The insufficient pitch resolution due to the reductive nature of the mid-level representation is compensated by this pitch tolerant distance measure. Evaluation is carried out at the beat level using annotated scores and audio. The results demonstrate that the approach provides an efficient and practical alternative to methods that rely on symbolic MIDI-like information through OMR methods for alignment. © 2012 International Society for Music Information Retrieval.","","Alignment; Image processing; Information retrieval; Distance measure; Dynamic time warping; Mid-level representation; Musical events; Page segmentation; Reference grids; Template-based; Audio acoustics","O. Izmirli; Center for Arts and Technology, Computer Science Department, Connecticut College, United States; email: oizm@conncoll.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Cai Z.; Ellis R.J.; Duan Z.; Lu H.; Wang Y.","Cai, Zhuohong (55440711200); Ellis, Robert J. (57089639000); Duan, Zhiyan (55925524500); Lu, Hong (55729839600); Wang, Ye (36103845200)","55440711200; 57089639000; 55925524500; 55729839600; 36103845200","Basic evaluation of auditory temporal stability (BEATS): A novel rationale and implementation","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916223613&partnerID=40&md5=bc9717679ac1ad9127fe85de478137d4","School of Computing National University of Singapore, Singapore; School of Computer Science, Fudan University, China","Cai Z., School of Computing National University of Singapore, Singapore; Ellis R.J., School of Computing National University of Singapore, Singapore; Duan Z., School of Computing National University of Singapore, Singapore; Lu H., School of Computer Science, Fudan University, China; Wang Y., School of Computing National University of Singapore, Singapore","The accurate detection of pulse-level temporal stability has important practical applications; for example, the creation of fixed-tempo playlists for recreational exercise (e.g., jogging), rehabilitation therapy (e.g., rhythmic gait training), or disc jockeying (e.g., dance mixes). Although there are numerous software algorithms which return simple point estimate statistics of “overall” tempo, none has operationalized the beat-to-beat stability of an inter-beat interval series. We propose such a method here, along with several novel summary statistics. We illustrate this approach using a public data set (the 10,000-item subset of the Million Song Dataset) and outline a series of future steps for this project. © 2013 International Society for Music Information Retrieval.","","Gait training; Point estimate; Public data; Rehabilitation therapy; Software algorithms; Summary statistic; Temporal stability; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Pachet F.; Suzda J.; Martín D.","Pachet, François (6701441655); Suzda, Jeff (57192866079); Martín, Daniel (56754481200)","6701441655; 57192866079; 56754481200","A comprehensive online database of machine-readable leadsheets for jazz standards","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923211959&partnerID=40&md5=101af6554b24d87bff712c578631f8f2","Sony CSL, Japan","Pachet F., Sony CSL, Japan; Suzda J., Sony CSL, Japan; Martín D., Sony CSL, Japan","Jazz standards are songs representative of a body of musical knowledge shared by most professional jazz musicians. As such, the corpus of jazz standards constitutes a unique opportunity to study a musical genre with a “closed-world” approach, since most jazz composers are no longer in activity today. Although many scores for jazz standards can be found on the Internet, no effort, to our knowledge, has been dedicated so far to building a comprehensive database of machine-readable scores for jazz standards. This paper reports on the rationale, design and population of such a database, containing harmonic (chord progressions) as well as melodic and structural information. The database can be used to feed both analysis and generation systems. We report on preliminary results in this vein. We get around the tricky and often unclear copyright issues imposed by the publishing industry, by providing only statistical information about songs. The completeness of such a database should benefit many research experiments in MIR and opens up novel and exciting applications in music generation exploiting symbolic information, notably in style modeling. © 2013 International Society for Music Information Retrieval.","","Computer music; Information retrieval; Publishing; Generation systems; Jazz standards; Musical genre; Online database; Publishing industry; Statistical information; Structural information; Database systems","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Krebs F.; Böck S.; Widmer G.","Krebs, Florian (7006192702); Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","7006192702; 55413719000; 7004342843","Rhythmic pattern modeling for beat and downbeat tracking in musical audio","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","63","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911929737&partnerID=40&md5=24ec924b8dff3b8ad441912b6a16f4cc","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Krebs F., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Rhythmic patterns are an important structural element in music. This paper investigates the use of rhythmic pattern modeling to infer metrical structure in musical audio recordings. We present a Hidden Markov Model (HMM) based system that simultaneously extracts beats, downbeats, tempo, meter, and rhythmic patterns. Our model builds upon the basic structure proposed by Whiteley et. al [20], which we further modified by introducing a new observation model: rhythmic patterns are learned directly from data, which makes the model adaptable to the rhythmical structure of any kind of music. For learning rhythmic patterns and evaluating beat and downbeat tracking, 697 ballroom dance pieces were annotated with beat and measure information. The results showed that explicitly modeling rhythmic patterns of dance styles drastically reduces octave errors (detection of half or double tempo) and substantially improves downbeat tracking. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Hidden Markov models; Information retrieval; Basic structure; Musical audio; Observation model; Rhythmic patterns; Structural elements; Audio acoustics","F. Krebs; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: florian.krebs@jku.at","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Robertson A.","Robertson, Andrew (56301105400)","56301105400","Decoding tempo and timing variations in music recordings from beat annotations","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873446242&partnerID=40&md5=df0b9a770b5ba9c6f2843ac58edfe77e","School of Electronic Engineering and Computer Science, United Kingdom","Robertson A., School of Electronic Engineering and Computer Science, United Kingdom","This paper addresses the problem of determining tempo and timing data from a list of beat annotations. Whilst an approximation to the tempo can be calculated from the inter-beat interval, the annotations also include timing variations due to expressively timed events, phase shifts and errors in the annotation times. These deviations tend to propagate into the tempo graph and so tempo analysis methods tend to average over recent inter-beat intervals. However, whilst this minimises the effect such timing deviations have on the local tempo estimate, it also obscures the expressive timing devices used by the performer. Here we propose a more formal method for calculation of the optimal tempo path through use of an appropriate cost function that incorporates tempo change, phase shift and expressive timing. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Music recording; Tempo analysis; Timing variations; Phase shift","A. Robertson; School of Electronic Engineering and Computer Science, United Kingdom; email: andrew.robertson@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Fohl W.; Turkalj I.; Meisel A.","Fohl, Wolfgang (55561923300); Turkalj, Ivan (55919338900); Meisel, Andreas (57197170877)","55561923300; 55919338900; 57197170877","A feature relevance study for guitar tone classification","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475561&partnerID=40&md5=283371e23b410343dd68d9cc15df3028","HAW Hamburg University of Applied Sciences, Germany","Fohl W., HAW Hamburg University of Applied Sciences, Germany; Turkalj I., HAW Hamburg University of Applied Sciences, Germany; Meisel A., HAW Hamburg University of Applied Sciences, Germany","A series of experiments on the automatic classification of classical guitar sounds with support vector machines has been carried out to investigate the relevance of the features and to minimise the feature set for successful classification. Features used for classification were the time series of the partial tone amplitudes, and of the MFCCs, and the energy distribution of the nontonal percussive sound that is produced in the attack phase of the tone. Furthermore the influence of sound parameters as timbre, player, fret position and string number on the recognition rate is investigated. Finally, several nonlinear kernels are compared in their classification performance. It turns out, that a selection of 505 features out of the full feature set of 1155 elements does only reduce the recognition rate of a linear SVM from 82% to 78%. With the use of a polynomial instead of a linear kernel the recognition rate with the reduced feature set can even be increased to 84%. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Support vector machines; Automatic classification; Classical guitar; Classification performance; Energy distributions; Feature relevance; Feature sets; Linear kernel; Linear SVM; Nonlinear kernels; Recognition rates; String numbers; Classification (of information)","W. Fohl; HAW Hamburg University of Applied Sciences, Germany; email: wolfgang.fohl@haw-hamburg.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Ünal E.; Bozkurt B.; Kemal Karaosmanoǧlu M.","Ünal, Erdem (8300824800); Bozkurt, Bariş (23476648700); Kemal Karaosmanoǧlu, M. (35102480700)","8300824800; 23476648700; 35102480700","N-gram based statistical makam detection on makam music in Turkey using symbolic data","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455699&partnerID=40&md5=5338980d1a745c021e80bea812b28552","TÜB IT AK-B ILGEM, Turkey; Bahçeşehir University, Turkey; Yildiz Technical University, Turkey","Ünal E., TÜB IT AK-B ILGEM, Turkey; Bozkurt B., Bahçeşehir University, Turkey; Kemal Karaosmanoǧlu M., Yildiz Technical University, Turkey","This work studies the effect of different score representations and the potential of n-grams in makam classification for traditional makam music in Turkey. While makams are defined with various characteristics including a distinct set of pitches, pitch hierarchy, melodic direction, typical phrases and typical makam transitions, such characteristics result in certain n-gram distributions which can be used for makam detection effectively. 13 popular makams, some of which are very similar to each other, are used in this study. Using the leave-one-out strategy, makam models are created statistically and tested against the left out music piece. Tests indicate that n-gram based statistical modeling and perplexity based similarity metric can be effectively used for makam detection. However the main dimension that cannot be captured is the overall progression which is the most unique feature for classification of close makams that uses the same scale notes as well as the same tonic. © 2012 International Society for Music Information Retrieval.","","Leave-one-out; N-grams; Similarity metrics; Statistical modeling; Symbolic data; Unique features; Work study; Information retrieval","E. Ünal; TÜB IT AK-B ILGEM, Turkey; email: unal@uekae.tubitak.gov.tr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"De Haas W.B.; Magalhães J.P.; Wiering F.","De Haas, W. Bas (51160955300); Magalhães, José Pedro (35793830900); Wiering, Frans (8976178100)","51160955300; 35793830900; 8976178100","Improving audio chord transcription by exploiting harmonic and metric knowledge","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462428&partnerID=40&md5=a32daf37d2b61f54202459bf899d89bb","Utrecht University, Netherlands; University of Oxford, United Kingdom","De Haas W.B., Utrecht University, Netherlands; Magalhães J.P., University of Oxford, United Kingdom; Wiering F., Utrecht University, Netherlands","We present a new system for chord transcription from polyphonic musical audio that uses domain-specific knowledge about tonal harmony and metrical position to improve chord transcription performance. Low-level pulse and spectral features are extracted from an audio source using the Vamp plugin architecture. Subsequently, for each beat-synchronised chromagram we compute a list of chord candidates matching that chromagram, together with the confidence in each candidate. When one particular chord candidate matches the chromagram significantly better than all others, this chord is selected to represent the segment. However, when multiple chords match the chromagram similarly well, we use a formal music theoretical model of tonal harmony to select the chord candidate that best matches the sequence based on the surrounding chords. In an experiment we show that exploiting metrical and harmonic knowledge yields statistically significant chord transcription improvements on a corpus of 217 Beatles, Queen, and Zweieck songs. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Transcription; Audio sources; Best match; Domain-specific knowledge; Musical audio; Plugin architecture; Spectral feature; Theoretical models; Audio acoustics","W.B. De Haas; Utrecht University, Netherlands; email: W.B.deHaas@uu.nl","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Collins T.; Arzt A.; Flossmann S.; Widmer G.","Collins, Tom (37088212600); Arzt, Andreas (36681791200); Flossmann, Sebastian (36472901400); Widmer, Gerhard (7004342843)","37088212600; 36681791200; 36472901400; 7004342843","SIARCT-CFP: Improving precision and the discovery of inexact musical patterns in point-set representations","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904316629&partnerID=40&md5=774a380f3306aba20627bff73ba912cb","Department of Computational Perception, Johannes Kepler University Linz, Austria","Collins T., Department of Computational Perception, Johannes Kepler University Linz, Austria; Arzt A., Department of Computational Perception, Johannes Kepler University Linz, Austria; Flossmann S., Department of Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, Austria","The geometric approach to intra-opus pattern discovery (in which notes are represented as points in pitch-time space in order to discover repeated patterns within a piece of music) shows promise particularly for polyphonic music, but has attracted some criticism because: (1) the approach extends to a limited number of inexact repetition types only; (2) typically geometric pattern discovery algorithms have poor precision, returning many false positives. This paper describes and evaluates a solution to the inexactness problem where algorithms for pattern discovery and inexact pattern matching are integrated for the first time. Two complementary solutions are proposed and assessed for the precision problem, one involving categorisation (hence reduction) of output patterns, and the second involving a new algorithm that calculates the difference between consecutive point pairs, rather than all point pairs. © 2013 International Society for Music Information Retrieval.","","Pattern matching; False positive; Geometric approaches; Geometric patterns; Pattern discovery; Point set; Polyphonic music; Repeated patterns; Time-space; Information retrieval","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Arjannikov T.; Sanden C.; Zhang J.Z.","Arjannikov, Tom (57189356387); Sanden, Chris (25723672800); Zhang, John Z. (16551442100)","57189356387; 25723672800; 16551442100","Verifying tag annotations through association analysis","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960885181&partnerID=40&md5=9a329dfbfaeabe1e7b26ef501d02ab50","University of Lethbridge, Canada","Arjannikov T., University of Lethbridge, Canada; Sanden C., University of Lethbridge, Canada; Zhang J.Z., University of Lethbridge, Canada","Music tags provide descriptive and rich information about a music piece, including its genre, artist, emotion, instrument, etc. While many work on automating it, at present, tag annotation is largely a manual process. It often involves judgements and opinions from people of different background and level of musical expertise. Therefore, the resulting tags are usually subjective, ambiguous, and error-prone. To deal with this situation, we seek automatic methods to verify and monitor this process. Furthermore, because multiple tags can annotate each music piece, our task lends itself to multi-label methods which capture the inherent associations among annotations in a given music repository. In this paper, we propose a novel approach to verify the quality of music tag annotations via association analysis. We demonstrate the effectiveness of our approach through a series of simulations using four publicly available music datasets. To our knowledge, our work is among the initial efforts in verifying music tag annotations. © 2013 International Society for Music Information Retrieval.","","Information retrieval; Association analysis; Automatic method; Error prones; Manual process; Multi-label; Quality control","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cherla S.; Weyde T.; d’Avila Garcez A.; Pearce M.","Cherla, Srikanth (24824198800); Weyde, Tillman (24476899500); d’Avila Garcez, Artur (57188767693); Pearce, Marcus (8674558800)","24824198800; 24476899500; 57188767693; 8674558800","A distributed model for multiple-viewpoint melodic prediction","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905383620&partnerID=40&md5=6a460d4d52683a9d29656b1b06fffea7","Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom; Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Centre for Digital Music, Queen Mary University of London, United Kingdom","Cherla S., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Weyde T., Music Informatics Research Group, Department of Computer Science, City University London, United Kingdom, Machine Learning Group, Department of Computer Science, City University London, United Kingdom; d’Avila Garcez A., Machine Learning Group, Department of Computer Science, City University London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom","The analysis of sequences is important for extracting information from music owing to its fundamentally temporal nature. In this paper, we present a distributed model based on the Restricted Boltzmann Machine (RBM) for melodic sequences. The model is similar to a previous successful neural network model for natural language [2]. It is first trained to predict the next pitch in a given pitch sequence, and then extended to also make use of information in sequences of note-durations in monophonic melodies on the same task. In doing so, we also propose an efficient way of representing this additional information that takes advantage of the RBM’s structure. In our evaluation, this RBM-based prediction model performs slightly better than previously evaluated n-gram models in most cases. Results on a corpus of chorale and folk melodies showed that it is able to make use of information present in longer contexts more effectively than n-gram models, while scaling linearly in the number of free parameters required. © 2013 International Society for Music Information Retrieval.","","Computational linguistics; Forecasting; Information retrieval; Distributed modeling; Extracting information; Free parameters; Multiple viewpoints; Natural languages; Neural network model; Prediction model; Restricted boltzmann machine; Information use","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Cheng T.; Dixon S.; Mauch M.","Cheng, Tian (56939676900); Dixon, Simon (7201479437); Mauch, Matthias (36461512900)","56939676900; 7201479437; 36461512900","A deterministic annealing EM algorithm for automatic music transcription","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963975554&partnerID=40&md5=a7754762153ed0befc952b48f83c962f","Centre for Digital Music, Queen Mary University of London, United Kingdom","Cheng T., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom","In the past decade, non-negative matrix factorisation (NMF) and probabilistic latent component analysis (PLCA) have been used widely in automatic music transcription. Despite their successes, these methods only guarantee that the decomposition converges to a local minimum in the cost function. In order to find better local minima, we propose to extend an existing PLCA-based transcription method with the deterministic annealing EM (DAEM) algorithm. The PLCA update rules are modified by introducing a “temperature” parameter. At higher temperatures, general areas of the search space containing good solutions are found. As the temperature is gradually decreased, distinctions in the data are sharpened, resulting in a more fine-grained optimisation at each successive temperature. This process reduces the dependence on the initialisation, which is otherwise a limitation of NMF and PLCA approaches. The method was tested on two standard multi-instrument transcription data sets (MIREX and Bach10). Experimental results show that the proposed method significantly outperforms a state-of-the-art reference method, according to both frame-based and note-based metrics. An additional analysis of instrument assignment results shows that instrument spectra are typically modelled as mixtures of templates from several instruments. © 2013 International Society for Music Information Retrieval.","","Cost functions; Information retrieval; Rapid thermal annealing; Transcription; Automatic music transcription; Deterministic Annealing EM algorithms; Local minimums; Non-negative matrix factorisation; Probabilistic latent component analysis; Reference method; State of the art; Transcription methods; Computer music","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Silva D.F.; Papadopoulos H.; Batista G.E.A.P.A.; Ellis D.P.W.","Silva, Diego F. (55585876200); Papadopoulos, Hélène (24462317300); Batista, Gustavo E.A.P.A. (55062789000); Ellis, Daniel P.W. (13609089200)","55585876200; 24462317300; 55062789000; 13609089200","A video compression-based approach to measure music structure similarity","2013","Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928159114&partnerID=40&md5=828ede74b8f055a09392269202d4981a","Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Laboratoire des Signaux et Systèmes (L2S), CNRS UMR, 8506, France; Department of Electrical Engineering, Columbia University, United States","Silva D.F., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Papadopoulos H., Laboratoire des Signaux et Systèmes (L2S), CNRS UMR, 8506, France; Batista G.E.A.P.A., Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, Brazil; Ellis D.P.W., Department of Electrical Engineering, Columbia University, United States","The choice of the distance measure between time-series representations can be decisive to achieve good classification results in many content-based information retrieval applications. In the field of Music Information Retrieval, two-dimensional representations of the music signal are ubiquitous. Such representations are useful to display patterns of evidence that are not clearly revealed directly in the time domain. Among these representations, self-similarity matrices have become common representations for visualizing the time structure of an audio signal. In the context of organizing recordings, recent work has shown that, given a collection of recordings, it is possible to to group performances of the same musical work based on the pairwise similarity between structural representations of the audio signal. In this work, we introduce the use of the Campana-Keogh distance, a video compression-based measure, to compare musical items based on their structure. Through extensive experiments, we show that the use of this distance measure outperforms the results of previous work using similar approaches but other distance measures. Along with quantitative results, detailed examples are provided to to illustrate the benefits of using the newly proposed distance measure. © 2013 International Society for Music Information Retrieval.","","Audio recordings; Classification (of information); Clustering algorithms; Image compression; Information retrieval; Classification results; Content-based information retrieval; Group performance; Music information retrieval; Music structures; Quantitative result; Self-similarities; Structural representation; Audio acoustics","","","14th International Society for Music Information Retrieval Conference, ISMIR 2013","4 November 2013 through 8 November 2013","Curitiba","149352"
"Watson D.; Mandryk R.L.","Watson, Diane (55583015100); Mandryk, Regan L. (6506898492)","55583015100; 6506898492","Modeling musical mood from audio features and listening context on an in-situ data set","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449069&partnerID=40&md5=4860f350164f46c6b4b5f53856b40ddb","University of Saskatchewan, Canada","Watson D., University of Saskatchewan, Canada; Mandryk R.L., University of Saskatchewan, Canada","Real-life listening experiences contain a wide range of music types and genres. We create the first model of musical mood using a data set gathered in-situ during a user's daily life. We show that while audio features, song lyrics and socially created tags can be used to successfully model musical mood with classification accuracies greater than chance, adding contextual information such as the listener's affective state or listening context can improve classification accuracy. We successfully classify musical arousal with a classification accuracy of 67% and musical valence with an accuracy of 75% when using both musical features and listening context. © 2012 International Society for Music Information Retrieval.","","Affective state; Audio features; Classification accuracy; Contextual information; Daily lives; Data sets; In-situ data; Musical features; Information retrieval","D. Watson; University of Saskatchewan, Canada; email: diane.watson@usask.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Martin B.; Hanna P.; Ta V.-T.; Ferraro P.; Desainte-Catherine M.","Martin, Benjamin (57198809463); Hanna, Pierre (23134483000); Ta, Vinh-Thong (24825743200); Ferraro, Pascal (23134820600); Desainte-Catherine, Myriam (55925160500)","57198809463; 23134483000; 24825743200; 23134820600; 55925160500","Exemplar-based assignment of large missing audio parts using string matching on tonal features","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591527&partnerID=40&md5=0a9d22c68a835cfd4e29ec6f1ed87afe","LaBRI, Universite de Bordeaux, France","Martin B., LaBRI, Universite de Bordeaux, France; Hanna P., LaBRI, Universite de Bordeaux, France; Ta V.-T., LaBRI, Universite de Bordeaux, France; Ferraro P., LaBRI, Universite de Bordeaux, France; Desainte-Catherine M., LaBRI, Universite de Bordeaux, France","We propose a new approach for assigning audio data in large missing audio parts (from 1 to 16 seconds). Inspired by image inpainting approaches, the proposed method uses the repetitive aspect of music pieces on musical features to recover missing segments via an exemplar-based reconstruction. Tonal features combined with a string matching technique allows locating repeated segments accurately. The evaluation consists in performing on both musician and nonmusician subjects listening tests of randomly reconstructed audio excerpts, and experiments highlight good results in assigning musically relevant parts. The contribution of this paper is twofold: bringing musical features to solve a signal processing problem in the case of large missing audio parts, and successfully applying exemplar-based techniques on musical signals while keeping a musical consistency on audio pieces. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Signal processing; Audio data; Exemplar-based; Image Inpainting; Listening tests; Musical features; Musical signals; New approaches; Signal processing problems; String matching; Audio acoustics","B. Martin; LaBRI, Universite de Bordeaux, France; email: benjamin.martin@labri.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Abeßer J.; Lartillot O.","Abeßer, Jakob (36607532300); Lartillot, Olivier (6507137446)","36607532300; 6507137446","Modelling musical attributes to characterize two-track recordings with bass and drums","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604006&partnerID=40&md5=4c63af9d3e1242e5bc96c9809fd06a3e","Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvä skylä, Finland","Abeßer J., Semantic Music Technologies Group, Fraunhofer IDMT, Ilmenau, Germany; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyvä skylä, Finland","In this publication, we present a method to characterize twotrack audio recordings (bass and drum instruments) based on musical attributes. These attributes are modelled using different regression algorithms. All regression models are trained based on score-based audio features computed from given scores and human annotations of the attributes. We compare five regression model configurations that predict values of different attributes. The regression models are trained based on manual annotations from 11 participants for a data-set of 70 double-track recordings. The average estimation errors within a cross-validation scenario are computed as evaluation measure. Models based on Partial Least Squares Regression (PLSR) with preceding Principal Component Analysis (PCA) and on Support Vector Regression (SVR) performed best. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Principal component analysis; Audio features; Cross validation; Estimation errors; Evaluation measures; Human annotations; Manual annotation; Partial least squares regressions (PLSR); Regression algorithms; Regression model; Support vector regression (SVR); Regression analysis","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Vaizman Y.; Granot R.Y.; Lanckriet G.","Vaizman, Yonatan (55583001400); Granot, Roni Y. (13607677100); Lanckriet, Gert (7801431767)","55583001400; 13607677100; 7801431767","Modeling dynamic patterns for emotional content in music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598676&partnerID=40&md5=2b2f7ebfec03456c248442696b939a80","Edmond and Lily Safra Center for Brain Sciences, ICNC, Hebrew University, Jerusalem, Israel; Musicology Dept., Hebrew University, Jerusalem, Israel; Electrical and Computer Engineering Dept., University of California, San Diego, United States","Vaizman Y., Edmond and Lily Safra Center for Brain Sciences, ICNC, Hebrew University, Jerusalem, Israel; Granot R.Y., Musicology Dept., Hebrew University, Jerusalem, Israel; Lanckriet G., Electrical and Computer Engineering Dept., University of California, San Diego, United States","Emotional content is a major component in music. It has long been a research topic of interest to discover the acoustic patterns in the music that carry that emotional information, and enable performers to communicate emotional messages to listeners. Previous works looked in the audio signal for local cues, most of which assume monophonic music, and their statistics over time. Here, we used generic audio features, that can be calculated for any audio signal, and focused on the progression of these features through time, investigating how informative the dynamics of the audio is for emotional content. Our data is comprised of piano and vocal improvisations of musically trained performers, instructed to convey 4 categorical emotions. We applied Dynamic Texture Mixture (DTM), that models both the instantaneous sound qualities and their dynamics, and demonstrated the strength of the model. We further showed that once taking the dynamics into account even highly reduced versions of the generic audio features carry a substantial amount of information about the emotional content. Finally, we demonstrate how interpreting the parameters of the trained models can yield interesting cognitive suggestions. © 2011 International Society for Music Information Retrieval.","","Dynamics; Information retrieval; Temperature control; Amount of information; Audio features; Audio signal; Dynamic patterns; Dynamic textures; Emotional information; Monophonic music; Research topics; Sound Quality; Computer music","Y. Vaizman; Edmond and Lily Safra Center for Brain Sciences, ICNC, Hebrew University, Jerusalem, Israel; email: yonatan.vaizman@mail.huji.ac.il","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Chuan C.-H.","Chuan, Ching-Hua (15050129500)","15050129500","A comparison of statistical and rule-based models for style-specific harmonization","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592691&partnerID=40&md5=9fd73fd83d26a83fc03cd4b43559ba13","School of Computing, University of North Florida, Jacksonville, FL, United States","Chuan C.-H., School of Computing, University of North Florida, Jacksonville, FL, United States","The process of generating chords for harmonizing a melody with the goal of mimicking an artist's style is investigated in this paper. We compared and tested three different approaches, including a rule-based model, a statistical model, and a hybrid system of the two, for such tasks. Experiments were conducted using songs from seven stylistically identifiable pop/rock bands, and the chords generated by the systems were compared to the ones in the artists' original work. Evaluations were performed on multiple aspects, including calculating the average percentage of chords that were the same and those that were related, studying the manner in which the size of the training set affects the output harmonization, and examining a system's behaviors in terms of the ability of generating unseen chords and the number of unique chords produced per song. We observed that the rule-based system performs comparably well while the result of the system with learning capability varies as the training set grows. © 2011 International Society for Music Information Retrieval.","","Hybrid systems; Learning capabilities; Rule-based models; Statistical models; Training sets; Information retrieval","C.-H. Chuan; School of Computing, University of North Florida, Jacksonville, FL, United States; email: c.chuan@unf.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Martin B.; Brown D.G.; Hanna P.; Ferraro P.","Martin, Benjamin (57198809463); Brown, Daniel G. (55738804200); Hanna, Pierre (23134483000); Ferraro, Pascal (23134820600)","57198809463; 55738804200; 23134483000; 23134820600","Blast for audio sequences alignment: A fast scalable cover identification tool","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873425329&partnerID=40&md5=d7ae6bbda38de4f904d07f3c09436ae8","Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; University of Waterloo, Cheriton School of Computer Science, Canada","Martin B., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; Brown D.G., University of Waterloo, Cheriton School of Computer Science, Canada; Hanna P., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; Ferraro P., Université de Bordeaux, CNRS, LaBRI, UMR 5800, France","Searching for similarities in large musical databases is common for applications such as cover song identification. These methods typically use dynamic programming to align the shared musical motifs between subparts of two recordings. Such music local alignment methods are slow, as are the bioinformatics algorithms they are closely related to. We have adapted the ideas of the Basic Local Alignment Search Tool (BLAST) for biosequence alignment to the domain of aligning sequences of chroma features. Our tool allows local music sequence alignment in near-linear time. It identifies small regions of exact match between sequences, called seeds, and builds local alignments that include these seeds. Seed determination is a key issue for the accuracy of the method and closely depends on the database, the representation and the application. We introduce a particular seeding approach for cover detection, and evaluate it on both a 2000-piece training set and the million song dataset (MSD). We show that the heuristic alignment drastically improves time computation for cover song detection. Alignment sensitivity is still very high on the small database, but is dramatically weakened on the MSD, due to differences in chroma features. We discuss the impact of different choices of these features on alignment of musical pieces. © 2012 International Society for Music Information Retrieval.","","Bioinformatics; Data processing; Information retrieval; Basic local alignment search tools; Chroma features; Data sets; Identification tools; Local alignment; Local alignment method; Musical database; Musical pieces; Near-linear time; Sequence alignments; Small region; Training sets; Alignment","B. Martin; Université de Bordeaux, CNRS, LaBRI, UMR 5800, France; email: bmartin@labri.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Woelfer J.P.; Lee J.H.","Woelfer, Jill Palzkill (34978008800); Lee, Jin Ha (57190797465)","34978008800; 57190797465","The role of music in the lives of homeless young people: A preliminary report","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422734&partnerID=40&md5=64fc3e9b3e2df60067c45713b8700766","Information School, University of Washington, Seattle, WA 98195, United States","Woelfer J.P., Information School, University of Washington, Seattle, WA 98195, United States; Lee J.H., Information School, University of Washington, Seattle, WA 98195, United States","This paper is a preliminary report of findings in an ongoing study of the role of music in the lives of homeless young people which is taking place in Vancouver, British Columbia and Seattle, WA. One hundred homeless young people in Vancouver took part in online surveys, 20 of these young people participated in interviews and 64 completed design activities. Surveys included demographic and music questions. Interviews consisted of questions about music listening and preferences. In the design activities, participants envisioned a music device and provided a drawing and a scenario. Since the study is on-going, findings are limited to descriptive analysis of survey data supplemented with interview data. These findings provide initial insights into music listening behaviors, social aspects of shared music interests, and preferred music genres, bands and artists, and moods. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Social aspects; Descriptive analysis; Design activity; Music genre; Online surveys; Seattle; Survey data; Vancouver , British Columbia; Surveys","J.P. Woelfer; Information School, University of Washington, Seattle, WA 98195, United States; email: woelfj@uw.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Kamalzadeh M.; Baur D.; Möller T.","Kamalzadeh, Mohsen (36681511400); Baur, Dominikus (24477437400); Möller, Torsten (7103010114)","36681511400; 24477437400; 7103010114","A survey on music listening and management behaviours","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","44","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873417428&partnerID=40&md5=c1c3000e855de5fc6edb61ac6dfcaa10","Simon Fraser University, Canada; University of Calgary, Canada","Kamalzadeh M., Simon Fraser University, Canada; Baur D., University of Calgary, Canada; Möller T., Simon Fraser University, Canada","We report the results of a survey on music listening and management behaviours. The survey was conducted online with 222 participants with mostly technical backgrounds drawn from a college age population. The median size of offline music collections was found to be roughly 2540 songs (sum of physical media and digital files). The major findings of our survey show that elements such as familiarity of songs, how distracting they are, how much they match the listener's mood, and the desire of changing the mood within one listening session, are all affected by the activity during which music is listened to. While people want to have options for manipulating the above elements to control their experience, they prefer a minimal amount of interaction in general. Current music players lack such flexibility in their controls. Finally, online recommender systems have not gained much popularity thus far. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Amount of interaction; Digital files; Music collection; Music players; Offline; Online recommender systems; Technical background; Surveys","M. Kamalzadeh; Simon Fraser University, Canada; email: mkamalza@sfu.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Andén J.; Mallat S.","Andén, Joakim (55566570700); Mallat, Stéphane (7006576695)","55566570700; 7006576695","Multiscale scattering for audio classification","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","114","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597627&partnerID=40&md5=d8af0a041cf0cd29c70fa75a434f6be0","CMAP, Ecole Polytechnique, 91128 Palaiseau, France","Andén J., CMAP, Ecole Polytechnique, 91128 Palaiseau, France; Mallat S., CMAP, Ecole Polytechnique, 91128 Palaiseau, France","Mel-frequency cepstral coefficients (MFCCs) are efficient audio descriptors providing spectral energy measurements over short time windows of length 23 ms. These measurements, however, lose non-stationary spectral information such as transients or time-varying structures. It is shown that this information can be recovered as spectral co-occurrence coefficients. Scattering operators compute these coefficients with a cascade of wavelet filter banks and modulus rectifiers. The signal can be reconstructed from scattering coefficients by inverting these wavelet modulus operators. An application to genre classification shows that second-order cooccurrence coefficients improve results obtained by MFCC and Delta-MFCC descriptors. © 2011 International Society for Music Information Retrieval.","","Audio acoustics; Filter banks; Information retrieval; Speech recognition; Audio classification; Co-occurrence; Descriptors; Genre classification; Mel-frequency cepstral coefficients; Multiscales; Nonstationary; Scattering co-efficient; Scattering operators; Second orders; Short time windows; Spectral information; Time-varying structure; Wavelet filter banks; Scattering","J. Andén; CMAP, Ecole Polytechnique, 91128 Palaiseau, France; email: anden@cmap.polytechnique.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Ewert S.; Müller M.","Ewert, Sebastian (32667575400); Müller, Meinard (7404689873)","32667575400; 7404689873","Score-informed voice separation for piano recordings","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873590791&partnerID=40&md5=35c68943022e842dc689852b8bd03f91","Computer Science III, University of Bonn, Germany; MPI Informatik, Saarland University, Germany","Ewert S., Computer Science III, University of Bonn, Germany; Müller M., MPI Informatik, Saarland University, Germany","The decomposition of a monaural audio recording into musicallymeaningful sound sources or voices constitutes a fundamental problem in music information retrieval. In this paper, we consider the task of separating a monaural piano recording into two sound sources (or voices) that correspond to the left hand and the right hand. Since in this scenario the two sources share many physical properties, sound separation approaches identifying sources based on their spectral envelope are hardly applicable. Instead, we propose a score-informed approach, where explicit note events specified by the score are used to parameterize the spectrogram of a given piano recording. This parameterization then allows for constructing two spectrograms considering only the notes of the left hand and the right hand, respectively. Finally, inversion of the two spectrograms yields the separation result. First experiments show that our approach, which involves high-resolution music synchronization and parametric modeling techniques, yields good results for realworld non-synthetic piano recordings. © 2011 International Society for Music Information Retrieval.","","Computer music; Information retrieval; Musical instruments; Separation; Source separation; Spectrographs; High resolution; Music information retrieval; Music synchronizations; Parametric modeling; Real-world; Sound separation; Sound source; Spectral envelopes; Spectrograms; Two sources; Voice separation; Audio recordings","S. Ewert; Computer Science III, University of Bonn, Germany; email: ewerts@iai.uni-bonn.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Gómez E.; Cañadas F.; Salamon J.; Bonada J.; Vera P.; Cabañas P.","Gómez, E. (14015483200); Cañadas, F. (16174333600); Salamon, J. (55184866100); Bonada, J. (16199826200); Vera, P. (57216710350); Cabañas, P. (36015005400)","14015483200; 16174333600; 55184866100; 16199826200; 57216710350; 36015005400","Predominant fundamental frequency Estimation vs Singing voice separation for the automatic transcription of accompanied flamenco singing","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424095&partnerID=40&md5=b6ee60a7980a6912e1f74830f5ef3161","Music Technology Group, Universitat Pompeu Fabra, Spain; Telecommunication Engineering Department, University of Jaen, Spain","Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Cañadas F., Telecommunication Engineering Department, University of Jaen, Spain; Salamon J., Music Technology Group, Universitat Pompeu Fabra, Spain; Bonada J., Music Technology Group, Universitat Pompeu Fabra, Spain; Vera P., Telecommunication Engineering Department, University of Jaen, Spain; Cabañas P., Telecommunication Engineering Department, University of Jaen, Spain","This work evaluates two strategies for predominant fundamental frequency (f0) estimation in the context of melodic transcription from flamenco singing with guitar accompaniment. The first strategy extracts the f 0 from salient pitch contours computed from the mixed spectrum; the second separates the voice from the guitar and then performs mono-phonic f 0 estimation. We integrate both approaches with an automatic transcription system, which first estimates the tuning frequency and then implements an iterative strategy for note segmentation and labeling. We evaluate them on a flamenco music collection, including a wide range of singers and recording conditions. Both strategies achieve satisfying results. The separation-based approach yields a good overall accuracy (76.81%), although instrumental segments have to be manually located. The predominant f0 estimator yields slightly higher accuracy (79.72%) but does not require any manual annotation. Furthermore, its accuracy increases (84.68%) if we adapt some algorithm parameters to each analyzed excerpt. Most transcription errors are due to incorrect f0 estimations (typically octave and voicing errors in strong presence of guitar) and incorrect note segmentation in highly ornamented sections. Our study confirms the difficulty of transcribing flamenco singing and the need for repertoire-specific and assisted algorithms for improving state-of-the-art methods. © 2012 International Society for Music Information Retrieval.","","Algorithms; Audio recordings; Errors; Information retrieval; Iterative methods; Musical instruments; Natural frequencies; Separation; Transcription; Algorithm parameters; Automatic transcription; Fundamental frequencies; Fundamental frequency estimation; Iterative strategy; Manual annotation; Mixed spectrum; Music collection; Note segmentation; Pitch contours; State-of-the-art methods; Tuning frequency; Voice separation; Estimation","E. Gómez; Music Technology Group, Universitat Pompeu Fabra, Spain; email: emilia.gomez@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Chien Y.-R.; Wang H.-M.; Jeng S.-K.","Chien, Yu-Ren (36891439000); Wang, Hsin-Min (8297293300); Jeng, Shyh-Kang (26023218700)","36891439000; 8297293300; 26023218700","An acoustic-phonetic approach to vocal melody extraction","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588865&partnerID=40&md5=5a7088884f2452f88276a795f64f84c0","Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; Institute of Information Science, Academia Sinica, Taiwan; Department of Electrical Engineering, National Taiwan University, Taiwan","Chien Y.-R., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan, Institute of Information Science, Academia Sinica, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taiwan; Jeng S.-K., Graduate Institute of Communication Engineering, National Taiwan University, Taiwan, Department of Electrical Engineering, National Taiwan University, Taiwan","This paper addresses the problemof extracting vocalmelodies from polyphonic audio. In short-term processing, a timbral distance between each pitch contour and the space of human voice is measured, so as to isolate any vocal pitch contour. Computation of the timbral distance is based on an acousticphonetic parametrization of human voiced sound. Longterm processing organizes short-term procedures in such a manner that relatively reliable melody segments are determined first. Tested on vocal excerpts from the ADC 2004 dataset, the proposed system achieves an overall transcription accuracy of 77%. © 2011 International Society for Music Information Retrieval.","","Acoustic-phonetic approach; Human voice; Melody extractions; Parametrizations; Pitch contours; Vocal pitches; Voiced sounds; Information retrieval","Y.-R. Chien; Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; email: yrchien@ntu.edu.tw","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Marques G.; Domingues M.A.; Langlois T.; Gouyon F.","Marques, Gonçalo (23995645000); Domingues, Marcos Aurélio (13906857500); Langlois, Thibault (6602432323); Gouyon, Fabien (8373002800)","23995645000; 13906857500; 6602432323; 8373002800","Three current issues in music autotagging","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607627&partnerID=40&md5=b25bd8015e7c4adfa7abbfebdfbd5499","DEETC-ISEL Lisboa, Portugal; INESC Porto, Portugal; DI-FCUL Lisboa, Portugal","Marques G., DEETC-ISEL Lisboa, Portugal; Domingues M.A., INESC Porto, Portugal; Langlois T., DI-FCUL Lisboa, Portugal; Gouyon F., INESC Porto, Portugal","The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Current performance; Different origins; Direct mapping; Evaluation metrics; Improve performance; Performance level; Systematic experiment; Tag models; Experiments","G. Marques; DEETC-ISEL Lisboa, Portugal; email: gmarques@isel.pt","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Koduri G.K.; Miron M.; Serrà J.; Serra X.","Koduri, Gopala K. (36721381900); Miron, Marius (55585881400); Serrà, Joan (35749172500); Serra, Xavier (55892979900)","36721381900; 55585881400; 35749172500; 55892979900","Computational approaches for the understanding of melody in carnatic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604998&partnerID=40&md5=eb8e4e40631de142f7c591bcc43bde58","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The classical music traditions of the Indian subcontinent, Hindustani and Carnatic, offer an excellent ground on which to test the limitations of current music information research approaches. At the same time, studies based on these music traditions can shed light on how to solve new and complex music modeling problems. Both traditions have very distinct characteristics, specially compared with western ones: they have developed unique instruments, musical forms, performance practices, social uses and context. In this article, we focus on the Carnatic music tradition of south India, especially on its melodic characteristics. We overview the theoretical aspects that are relevant for music information research and discuss the scarce computational approaches developed so far. We put emphasis on the limitations of the current methodologies and we present open issues that have not yet been addressed and that we believe are important to be worked on. © 2011 International Society for Music Information Retrieval.","","Computational methods; Information retrieval; Classical musics; Computational approach; Indian subcontinents; Modeling problems; Music information; South India; Theoretical aspects; Computer music","G.K. Koduri; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: gopala.koduri@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Ness S.; Trail S.; Driessen P.; Schloss A.; Tzanetakis G.","Ness, Steven (35076349900); Trail, Shawn (54080640800); Driessen, Peter (7006761168); Schloss, Andrew (36125610900); Tzanetakis, George (6602262192)","35076349900; 54080640800; 7006761168; 36125610900; 6602262192","Music information robotics: Coping strategies for musically challenged robots","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602299&partnerID=40&md5=587d1842b0f5df5802a200a7fd3af296","University of Victoria, Canada","Ness S., University of Victoria, Canada; Trail S., University of Victoria, Canada; Driessen P., University of Victoria, Canada; Schloss A., University of Victoria, Canada; Tzanetakis G., University of Victoria, Canada","In the past few years there has been a growing interest in music robotics. Robotic instruments that generate sound acoustically using actuators have been increasingly developed and used in performances and compositions over the past 10 years. Although such devices can be very sophisticated mechanically, in most cases they are passive devices that directly respond to control messages from a computer. In the few cases where more sophisticated control and feedback is employed it is in the form of simple mappings with little musical understanding. Several techniques for extracting musical information have been proposed in the field of music information retrieval. In most cases the focus has been the batch processing of large audio collections rather than real time performance understanding. In this paper we describe how such techniques can be adapted to deal with some of the practical problems we have experienced in our own work with music robotics. Of particular importance is the idea of self-awareness or proprioception in which the robot(s) adapt their behavior based on understanding the connection between their actions and sound generation through listening. More specifically we describe techniques for solving the following problems: 1) controller mapping 2) velocity calibration, and 3) gesture recognition. © 2011 International Society for Music Information Retrieval.","","Batch data processing; Gesture recognition; Information retrieval; Behavior-based; Control messages; Coping strategies; Following problem; Music information; Music information retrieval; Musical information; Passive devices; Practical problems; Real time performance; Sound generation; Velocity calibration; Robotics","S. Ness; University of Victoria, Canada; email: sness@sness.net","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hamel P.; Bengio Y.; Eck D.","Hamel, Philippe (8402361900); Bengio, Yoshua (7003958245); Eck, Douglas (12141444300)","8402361900; 7003958245; 12141444300","Building musically-relevant audio features through multiple timescale representations","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419188&partnerID=40&md5=a2841e1b2c67870d49a0ebcb65c01a85","DIRO, Université de Montréal, Montréal, QC, Canada; Google Inc., Mountain View, CA, United States","Hamel P., DIRO, Université de Montréal, Montréal, QC, Canada; Bengio Y., DIRO, Université de Montréal, Montréal, QC, Canada; Eck D., Google Inc., Mountain View, CA, United States","Low-level aspects of music audio such as timbre, loud-ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and require a better representation of time dynamics. For various music information retrieval tasks, one would benefit from modelling both low and high level aspects in a unified feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale features. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for automatic tag annotation. © 2012 International Society for Music Information Retrieval.","","Feature extraction; Adaptive features; Audio features; High level aspects; Multi-scale features; Music information retrieval; Time dynamic; Time-scale features; Time-scales; Information retrieval","P. Hamel; DIRO, Université de Montréal, Montréal, QC, Canada; email: hamelphi@iro.umontreal.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Daido R.; Hahm S.-J.; Ito M.; Makino S.; Ito A.","Daido, Ryunosuke (55327122700); Hahm, Seong-Jun (36447379400); Ito, Masashi (55190504900); Makino, Shozo (7403067655); Ito, Akinori (7403722531)","55327122700; 36447379400; 55190504900; 7403067655; 7403722531","A system for evaluating singing enthusiasm for karaoke","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607349&partnerID=40&md5=f2a90a3ba8df1ae2fb0002f17702d7bd","Graduate School of Engineering, Tohoku University, Japan; Tohoku Institute of Technology, Japan; Tohoku Bunka Gakuen University, Japan","Daido R., Graduate School of Engineering, Tohoku University, Japan; Hahm S.-J., Graduate School of Engineering, Tohoku University, Japan; Ito M., Tohoku Institute of Technology, Japan; Makino S., Tohoku Bunka Gakuen University, Japan; Ito A., Graduate School of Engineering, Tohoku University, Japan","Evaluation of singing skill is a popular function of karaoke machines. Here, we introduce a different aspect of evaluating the singing voice of an amateur singer: ""enthusiasm"". First, we investigated whether human listeners can evaluate enthusiasm consistently and whether the listener's perception matches the singer's enthusiasm. We then identified three acoustic features relevant to the perception of enthusiasm: A-weighted power, ""fall-down"", and vibrato extent. Finally, we developed a system for evaluating singing enthusiasm using these features, and obtained a correlation coefficient of 0.65 between the system output and human evaluation. © 2011 International Society for Music Information Retrieval.","","Acoustic features; Correlation coefficient; Human evaluation; Human listeners; Karaoke; Singing voices; System output; Information retrieval","R. Daido; Graduate School of Engineering, Tohoku University, Japan; email: ryunosuke@spcom.ecei.tohoku.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Fuhrmann F.; Herrera P.","Fuhrmann, Ferdinand (36642587400); Herrera, Perfecto (24824250300)","36642587400; 24824250300","Quantifying the relevance of locally extracted information for musical instrument recognition from entire pieces of music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598303&partnerID=40&md5=12496e40e298cd8af5f869414575b55a","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Fuhrmann F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this work we study the problem of automatic musical instrument recognition from entire pieces of music. In particular, we present and evaluate 4 different methods to select, from an unknown piece of music, relevant excerpts in terms of instrumentation, on top of which instrument recognition techniques are applied to infer the labels. Since the desired information is assumed to be redundant (we may extract just a few labels from a thousands of audio frames) we examine the recognition performance, the amount of data used for processing, and their possible correlation. Experimental results on a collection ofWestern music pieces reveal state-ofthe- art performance in instrument recognition together with a great reduction of the required input data. However, we also observe a performance ceiling with the currently applied instrument recognition method. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio frames; Input datas; Instrument recognition; Musical instrument recognition; Recognition performance; Musical instruments","F. Fuhrmann; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: ferdinand.fuhrmann@upf.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Macrae R.; Dixon S.","Macrae, Robert (36608414000); Dixon, Simon (7201479437)","36608414000; 7201479437","Ranking lyrics for online search","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439450&partnerID=40&md5=93775d6bf117b043f2350151190675fc","Centre for Digital Music, Queen Mary University of London, United Kingdom","Macrae R., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","When someone wishes to find the lyrics for a song they typically go online and use a search engine. There are a large number of lyrics available on the internet as the effort required to transcribe and post lyrics is minimal. These lyrics are promptly returned to the user with customary search engine page ranking formula deciding the ordering of these results based on links, views, clicks, etc. However the content, and specifically, the accuracy of the lyrics in question are not analysed or used in any way to determine the rank of the lyrics, despite this being of concern to the searcher. In this work, we show that online lyrics are often inaccurate and the ranking methods used by search engines do not distinguish the more accurate annotations. We present an alternative method for ranking lyrics based purely on the collection of lyrics themselves using the Lyrics Concurrence. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Alternative methods; Online search; Page ranking; Ranking methods; Search engines","R. Macrae; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: robert.macrae@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Juhász Z.","Juhász, Zoltán (56935971200)","56935971200","Low dimensional visualisation of folk music systems using the self organising cloud","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604482&partnerID=40&md5=bb8e3abb90d025fe013a2c39e2e3b5ce","Res Inst. For Technical Physics and Materials Sciences, H-1525 Budapest, P.O.B. 49, Hungary","Juhász Z., Res Inst. For Technical Physics and Materials Sciences, H-1525 Budapest, P.O.B. 49, Hungary","We describe a computational method derived from self organizing mapping and multidimensional scaling algorithms for automatic classification and visual clustering of large vector databases. Testing the method on a large corpus of folksongs we have found that the performance of the classification and topological clustering was significantly improved compared to current techniques. Applying the method to an analysis of the connections of 31 Eurasian and North-American folk music cultures, a clearly interpretable system of musical connections was revealed. The results show the relevance of the musical language groups in the oral tradition of the humanity. © 2011 International Society for Music Information Retrieval.","","Automatic classification; Large corpora; Low dimensional; Multi-dimensional scaling; Oral tradition; Self-organising; Self-organizing mapping; Topological clustering; Visual clustering; Information retrieval","Z. Juhász; Res Inst. For Technical Physics and Materials Sciences, H-1525 Budapest, P.O.B. 49, Hungary; email: juhasz@mfa.kfki.hu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hu Y.; Ogihara M.","Hu, Yajie (55364877000); Ogihara, Mitsunori (54420747900)","55364877000; 54420747900","Nextone player: A music recommendation system based on user behavior","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","55","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588899&partnerID=40&md5=36cef38f05794c7dfd3969ec2d263925","Department of Computer Science, University of Miami, United States","Hu Y., Department of Computer Science, University of Miami, United States; Ogihara M., Department of Computer Science, University of Miami, United States","We present a new approach to recommend suitable tracks from a collection of songs to the user. The goal of the system is to recommend songs that are favored by the user, are fresh to the user's ear, and fit the user's listening pattern. We use ""Forgetting Curve"" to assess freshness of a song and evaluate ""favoredness"" using user log. We analyze user's listening pattern to estimate the level of interest of the user in the next song. Also, we treat user behavior on the song being played as feedback to adjust the recommendation strategy for the next one. We develop an application to evaluate our approach in the real world. The user logs of trial volunteers show good performance of the proposed method. © 2011 International Society for Music Information Retrieval. © 2011 International Society for Music Information Retrieval.","","Image recording; Information retrieval; Forgetting curve; Level Of Interest; Music Recommendation System; New approaches; Nextone; Recommendation strategies; User behaviors; User log; Behavioral research","Y. Hu; Department of Computer Science, University of Miami, United States; email: yajie.hu@umail.miami.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Davies S.; Allen P.; Mann M.; Cox T.","Davies, Sam (57198502574); Allen, Penelope (55421609100); Mann, Mark (55586106800); Cox, Trevor (7203000240)","57198502574; 55421609100; 55586106800; 7203000240","Musical moods: A mass participation experiment for affective classification of music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594935&partnerID=40&md5=ef18b68c768e9390f2797c065688bf2e","BBC Research and Development, United Kingdom; University of Salford, United Kingdom","Davies S., BBC Research and Development, United Kingdom; Allen P., BBC Research and Development, United Kingdom; Mann M., BBC Research and Development, United Kingdom; Cox T., University of Salford, United Kingdom","In this paper we present our mass participation experiment, Musical Moods. This experiment placed 144 theme tunes online, taken from TV and radio programmes from the last 60 years of the British Broadcasting Corporations (BBC) output. Members of the public were then invited to audition then rate these according to a set of semantic differentials based on the affective categories of evaluation, potency and activity. Participants were also asked to rate their familiarity of the theme tune and how much they liked the theme tune. A final question asked participants to identify the genre of the TV programme with which they associated the tune. The purpose of this is to aid in the affective classification of large-scale TV archives, such as those possessed by the BBC. We find correlations between evaluation and potency, potency and activity but none between activity and evaluation but no clear correlation between affect and genre. This paper presents our key findings from an analysis of the results along with our plans for further analysis. The initial results from this experiment are based on an analyses of over 51,000 answers from over 13,000 participants. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Radio broadcasting; Semantics; British broadcasting corporations; Radio programmes; Semantic differential; Experiments","S. Davies; BBC Research and Development, United Kingdom; email: sam.davies@bbc.co.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Peeters G.; Cornu F.; Charbuillet Ch.; Tardieu D.; Burred J.J.; Vian M.; Botherel V.; Rault J.-B.; Cabanal J.-Ph.","Peeters, G. (22433836000); Cornu, F. (55582304000); Charbuillet, Ch. (16067900500); Tardieu, D. (24725404000); Burred, J.J. (16201737900); Vian, M. (55583140800); Botherel, V. (6505595998); Rault, J.-B. (7005804024); Cabanal, J.-Ph. (57213108062)","22433836000; 55582304000; 16067900500; 24725404000; 16201737900; 55583140800; 6505595998; 7005804024; 57213108062","A Multimedia search and navigation prototype, including music and video-clips","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427658&partnerID=40&md5=359eaf43c6c69d7012f8832c90678898","STMS, IRCAM-CNRS-UPMC, France; Bertin Technologies, France; Orange-Labs, France","Peeters G., STMS, IRCAM-CNRS-UPMC, France; Cornu F., STMS, IRCAM-CNRS-UPMC, France; Charbuillet Ch., STMS, IRCAM-CNRS-UPMC, France; Tardieu D., STMS, IRCAM-CNRS-UPMC, France; Burred J.J., STMS, IRCAM-CNRS-UPMC, France; Vian M., Bertin Technologies, France; Botherel V., Orange-Labs, France; Rault J.-B., Orange-Labs, France; Cabanal J.-Ph., Orange-Labs, France","Moving music indexing technologies developed in a research lab to their integration and use in the context of a third-party search and navigation engine that indexes music files, archives of TV music programs and videoclips, involves a set of choices and works that we relate here. First one has to choose technologies that perform well, which are scalable (in terms of computation time of extraction and item comparison for search-by-similarity), and which are not sensitive to media quality (being able to process equally music files or audio tracks from video archives). These technologies must be applied to estimate tags chosen to be understandable and useful for users (the specific genre and mood tags or other content-descriptions). For training the related technologies, relevant and reliable annotated corpus must be created. For using them, relevant user-scenarios must be created and friendly Graphical User-Interface designed. In this paper, we share the experience we had in a recent project on integrating six state-of-the-art music-indexing technologies in a multimedia search and navigation prototype. © 2012 International Society for Music Information Retrieval.","","Indexing (of information); Information retrieval; Navigation; Technology; User interfaces; Audio track; Computation time; Media quality; Multimedia search; Music files; Music indexing; Music program; Navigation Engine; Video archives; Video-clips; Audio acoustics","G. Peeters; STMS, IRCAM-CNRS-UPMC, France; email: peeters@ircam.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Boulanger-Lewandowski N.; Bengio Y.; Vincent P.","Boulanger-Lewandowski, Nicolas (45560980800); Bengio, Yoshua (7003958245); Vincent, Pascal (57203214842)","45560980800; 7003958245; 57203214842","Discriminative non-negative matrix factorization for multiple pitch estimation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429839&partnerID=40&md5=cd0c49e5b38033674b1fd9e048c31a45","Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada","Boulanger-Lewandowski N., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada; Bengio Y., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada; Vincent P., Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada","In this paper, we present a supervised method to improve the multiple pitch estimation accuracy of the non-negative matrix factorization (NMF) algorithm. The idea is to extend the sparse NMF framework by incorporating pitch information present in time-aligned musical scores in order to extract features that enforce the separability between pitch labels. We introduce two discriminative criteria that maximize inter-class scatter and quantify the predictive potential of a given decomposition using logistic regressors. Those criteria are applied to both the latent variable and the deterministic autoencoder views of NMF, and we devise efficient update rules for each. We evaluate our method on three polyphonic datasets of piano recordings and orchestral instrument mixes. Both models greatly enhance the quality of the basis spectra learned by NMF and the accuracy of multiple pitch estimation. © 2012 International Society for Music Information Retrieval.","","Acoustic variables measurement; Data processing; Information retrieval; Autoencoders; Data sets; Instrument mix; Latent variable; Musical score; Non-negative matrix factorization algorithms; Nonnegative matrix factorization; Pitch estimation; Factorization","N. Boulanger-Lewandowski; Dept. IRO, Université de Montréal, Montréal, QC H3C 3J7, Canada; email: boulanni@iro.umontreal.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Panagakis Y.; Kotropoulos C.","Panagakis, Yannis (35503932300); Kotropoulos, Constantine (35563688200)","35503932300; 35563688200","Music structure analysis by ridge regression of beat-synchronous audio features","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420616&partnerID=40&md5=955288cfe54c00575aadfcaf20dafe89","Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece","Panagakis Y., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece","A novel unsupervised method for automatic music structure analysis is proposed. Three types of audio features, namely the mel-frequency cepstral coefficients, the chroma features, and the auditory temporal modulations are employed in order to form beat-synchronous feature sequences modeling the audio signal. Assume that the feature vectors from each segment lie in a subspace and the song as a whole occupies the union of several subspaces. Then any feature vector can be represented as a linear combination of the feature vectors stemming from the same subspace. The coefficients of such a linear combination are found by solving an appropriate ridge regression problem, resulting to the ridge representation (RR) of the audio features. The RR yields an affinity matrix with nonzero within-subspace affinities and zero between-subspace ones, revealing the structure of the music recording. The segmentation of the feature sequence into music segments is found by applying the normalized cuts algorithm to the RR-based affinity matrix. In the same context, the combination of multiple audio features is investigated as well. The proposed method is referred to as ridge regression-based music structure analysis (RRMSA). State-of-the-art performance is reported for the RRMSA by conducting experiments on the manually annotated Beatles benchmark dataset. © 2012 International Society for Music Information Retrieval.","","Benchmarking; Information retrieval; Matrix algebra; Regression analysis; Affinity matrix; Audio features; Audio signal; Benchmark datasets; Chroma features; Feature sequence; Feature vectors; Linear combinations; Mel-frequency cepstral coefficients; Music recording; Music segments; Music structure analysis; Normalized cuts; Ridge regression; State-of-the-art performance; Temporal modulations; Unsupervised method; Vectors","Y. Panagakis; Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; email: panagakis@aiia.csd.auth.gr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"O'Hara T.; Schüler N.; Lu Y.; Tamir D.E.","O'Hara, Tom (58459540400); Schüler, Nico (55583423200); Lu, Yijuan (14054322300); Tamir, Dan E. (23096310900)","58459540400; 55583423200; 14054322300; 23096310900","Inferring chord sequence meanings via lyrics: Process and evaluation","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873437186&partnerID=40&md5=1aab5fb8e6a92fc7505cecd77718d869","Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; School of Music, Texas State University, San Marcos, TX 78666, United States","O'Hara T., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; Schüler N., School of Music, Texas State University, San Marcos, TX 78666, United States; Lu Y., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; Tamir D.E., Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States","We improve upon our simple approach for learning the ""associational meaning"" of chord sequences from lyrics based on contingency statistics induced over a set of lyrics with chord annotations. Specifically, we refine this process by using word alignment tools developed for statistical machine translation, and we also use a much larger set of chord annotations. In addition, objective evaluation measures are included. Thus, this work validates a novel application of lexicon induction techniques over parallel corpora to a domain outside of natural language learning. To confirm the associations commonly attributed to major versus minor chords (i.e., happy and sad, respectively), we compare the inferred word associations against synonyms reflecting this dichotomy. To evaluate meanings associated with chord sequences, we check how often tagged chords occur in songs labeled with the same overall meaning. © 2012 International Society for Music Information Retrieval.","","Chord sequence; Natural language learning; Novel applications; Objective evaluation; Parallel corpora; Simple approach; Statistical machine translation; Word alignment; Word association; Information retrieval","T. O'Hara; Dept. of Computer Science, Texas State University, San Marcos, TX 78666, United States; email: tomohara@txstate.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Liem C.C.S.; Hanjalic A.","Liem, Cynthia C.S. (21733247100); Hanjalic, Alan (6701775211)","21733247100; 6701775211","Expressive timing from cross-performance and audio-based alignment patterns: An extended case study","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606376&partnerID=40&md5=d08feb0c87890c1327003042fd28df01","Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands","Liem C.C.S., Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands; Hanjalic A., Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands","Audio recordings of classical music pieces reflect the artistic interpretation of the piece as seen by the recorded performing musician. With many recordings being typically available for the same music piece, multiple expressive rendition variations of this piece are obtained, many of which are induced by the underlying musical content. In earlier work, we focused on timing as a means of expressivity, and proposed a light-weight, unsupervised and audio-based method to study timing deviations among different performances through alignment patterns. By using the standard deviation of alignment patterns as a measure for the display of individuality in a recording, structural and interpretational aspects of a music piece turned out to be highlighted in a qualitative case study on five Chopin mazurkas. In this paper, we propose an entropy-based deviation measure as an alternative to the existing standard deviation measure. The obtained results for multiple short-time window resolutions, both from a quantitative and qualitative perspective, strengthen our earlier finding that the found patterns are musically informative and confirm that entropy is a good alternative measure for highlighting expressive timing deviations in recordings. © 2011 International Society for Music Information Retrieval.","","Entropy; Information retrieval; Motion compensation; Statistics; Alignment patterns; Audio-based; Classical musics; Deviation measures; Entropy-based; Light weight; Qualitative case studies; Standard deviation; Audio recordings","C.C.S. Liem; Multimedia Information Retrieval Lab, Delft University of Technology, Netherlands; email: c.c.s.liem@tudelft.nl","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bragg J.; Chew E.; Shieber S.","Bragg, Jonathan (57225384052); Chew, Elaine (8706714000); Shieber, Stuart (6603019922)","57225384052; 8706714000; 6603019922","Neo-riemannian cycle detection with weighted finite-state transducers","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607621&partnerID=40&md5=56d9a6b22e1989ca0835af25d85818fa","Harvard University, United States; Queen Mary University of London, United Kingdom","Bragg J., Harvard University, United States; Chew E., Queen Mary University of London, United Kingdom; Shieber S., Harvard University, United States","This paper proposes a finite-state model for detecting harmonic cycles as described by neo-Riemannian theorists. Given a string of triads representing a harmonic analysis of a piece, the task is to identify and label all substrings corresponding to these cycles with high accuracy. The solution method uses a noisy channel model implemented with weighted finitestate transducers. On a dataset of four works by Franz Schubert, our model predicted cycles in the same regions as cycles in the ground truth with a precision of 0.18 and a recall of 1.0. The recalled cycles had an average edit distance of 3.2 insertions or deletions from the ground truth cycles, which average 6.4 labeled triads in length. We suggest ways in which our model could be used to contribute to current work in music theory, and be generalized to other music pattern-finding applications. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Cycle detection; Edit distance; Finite-state models; Ground truth; Music theory; Noisy channel models; Solution methods; Sub-strings; Weighted finite-state transducers; Transducers","J. Bragg; Harvard University, United States; email: jbragg@post.harvard.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Wang D.; Ogihara M.","Wang, Dingding (23037039100); Ogihara, Mitsunori (54420747900)","23037039100; 54420747900","Potential relationship discovery in tag-aware music style clustering and artist social networks","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598855&partnerID=40&md5=4bd8dd8031c496aa0caa08dae46b9495","Center for Computational Science, University of Miami, Coral Gables, FL, United States","Wang D., Center for Computational Science, University of Miami, Coral Gables, FL, United States; Ogihara M., Center for Computational Science, University of Miami, Coral Gables, FL, United States","With the rapid growth of music information and data in today's ever changing world, exploring and analyzing music style has become more and more difficult. Traditional content-based methods for music style analysis and newly emerged tag-based methods usually assume music items are independent of each other. However, in real world applications, do there exist some relationships among them. In this paper, we construct the social relation graph among different music artists by extracting the friendship information from social media such as Twitter, and incorporate the generated social networking graph into tag-based music style clustering. Experiments on real data show the effectiveness of this novel integration of different information sources. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Content-based methods; Information sources; Music information; Rapid growth; Real-world application; Social media; Social Networks; Social relations; Tag-based; Social networking (online)","D. Wang; Center for Computational Science, University of Miami, Coral Gables, FL, United States; email: d.wang1@miami.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bennett C.; McNeer R.; Leider C.","Bennett, Christopher (54950758300); McNeer, Richard (6506269265); Leider, Colby (24450681900)","54950758300; 6506269265; 24450681900","Urgency analysis of audible alarms in the operating room","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603265&partnerID=40&md5=26e0e2e0f8251dd01d265725d791880f","Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; Department of Music Engineering, Frost School of Music, University of Miami, United States","Bennett C., Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; McNeer R., Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; Leider C., Department of Music Engineering, Frost School of Music, University of Miami, United States","Recent studies by researchers, governmental agencies, and safety organizations have recognized a deficiency in the performance of medically related audible alarms [1-4]. In the clinical setting, care providers can suffer from alarm fatigue, a condition in which audible alarms in an operating room are perceived as a nuisance. In this study, we explore the auditory features associated with current audible alarms using tools from the music information retrieval community, and then we examine how those auditory features correlate to listeners' perception of urgency. The results show that aperiodic changes in the auditory spectrum over time are the most salient contributor to the perception of urgency in sound. These results could inform the development of a novel standard regarding the composition of medical audible alarms. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Operating rooms; Auditory feature; Clinical settings; Governmental agency; Music information retrieval; Safety organizations; Alarm systems","C. Bennett; Department of Anesthesiology, Miller School of Medicine, University of Miami, United States; email: cbennett4@med.miami.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hankinson A.; Porter A.; Burgoyne J.A.; Thompson J.; Vigliensoni G.; Liu W.; Chiu R.; Fujinaga I.","Hankinson, Andrew (54970482500); Porter, Alastair (55582507300); Burgoyne, John Ashley (23007865600); Thompson, Jessica (55582942400); Vigliensoni, Gabriel (55217696900); Liu, Wendy (55418470400); Chiu, Remi (55553515000); Fujinaga, Ichiro (9038140900)","54970482500; 55582507300; 23007865600; 55582942400; 55217696900; 55418470400; 55553515000; 9038140900","Digital document image retrieval using optical music recognition","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418537&partnerID=40&md5=ba7e4951a47c55d4f1c125b8fe62b180","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada","Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Porter A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Thompson J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Liu W., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Chiu R., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada","Optical music recognition (OMR) and optical character recognition (OCR) have traditionally been used for document transcription - that is, extracting text or symbolic music from page images for use in an editor while discarding all spatial relationships between the transcribed notation and the original image. In this paper we discuss how OCR has shifted fundamentally from a transcription tool to an indexing tool for document image collections resulting from large digitization efforts. OMR tools and procedures, in contrast, are still focused on small-scale modes of operation. We argue that a shift in OMR development towards document image indexing would present new opportunities for searching, browsing, and analyzing large musical document collections. We present a prototype system we built to evaluate the tools and to develop practices needed to process print and manuscript sources. © 2012 International Society for Music Information Retrieval.","","Optical character recognition; Transcription; Digital Documents; Document collection; Document images; Modes of operation; Music recognition; Original images; Prototype system; Spatial relationships; Image retrieval","A. Hankinson; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montréal, QC, Canada; email: andrew.hankinson@mail.mcgill.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Sordo M.; Serrà J.; Koduri G.K.; Serra X.","Sordo, Mohamed (43462170300); Serrà, Joan (35749172500); Koduri, Gopala K. (36721381900); Serra, Xavier (55892979900)","43462170300; 35749172500; 36721381900; 55892979900","Extracting semantic information from an online Carnatic music forum","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426831&partnerID=40&md5=90603c0e3fda24bd524eb1e9d7ce8e2c","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain","Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serrà J., Artificial Intelligence Research Institute (IIIA-CSIC), Bellaterra, Barcelona, Spain; Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","By mining user-generated text content we can obtain music-related information that could not otherwise be extracted from audio signals or symbolic score representations. In this paper we propose a methodology for extracting music-related semantic information from an online discussion forum, rasikas.org, dedicated to the Carnatic music tradition. We first define a dictionary of relevant terms within categories such as raagas, taalas, performers, composers, and instruments, and create a complex network representation by matching such dictionary against the forum posts. This network representation is used to identify popular terms within the forum, as well as relevant co-occurrences and semantic relationships. This way, for instance, we are able to learn the instrument played by a performer with 95% accuracy, to discover the confusion between two raagas with different naming conventions, or to infer semantic relationships regarding lineage or musical influence. This contribution is a first step towards the automatic creation of ontologies for specific musical cultures. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Audio signal; Automatic creations; Complex networks; Network representation; Online discussions; Semantic information; Semantic relationships; Text content; Semantics","M. Sordo; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: mohamed.sordo@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Pugin L.; Kepper J.; Roland P.; Hartwig M.; Hankinson A.","Pugin, Laurent (23009752900); Kepper, Johannes (55583240700); Roland, Perry (55583223600); Hartwig, Maja (55582243100); Hankinson, Andrew (54970482500)","23009752900; 55583240700; 55583223600; 55582243100; 54970482500","Separating presentation and content in MEI","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424411&partnerID=40&md5=16d3667a0fe3ab6681967fd923fe93ac","Swiss RISM, Fribourg University, Germany; Edirom, Germany; University of Virginia, United States; McGill University, Schulich School of Music, Canada","Pugin L., Swiss RISM, Fribourg University, Germany; Kepper J., Edirom, Germany; Roland P., University of Virginia, United States; Hartwig M., Edirom, Germany; Hankinson A., McGill University, Schulich School of Music, Canada","Common Western music notation is traditionally organized on staves that can be grouped into systems. When multiple systems appear on a page, they are arranged from the top to the bottom of the page, similar to lines of words in a text document. Encoding music notation documents for printing requires this arrangement to be captured. However, in the music notation model proposed by the Music Encoding Initiative (MEI), the hierarchy of the XML sub-tree representing the music emphasizes the content rather than the layout. Since systems and pages do not coincide with the musical content, they are encoded in a secondary hierarchy that contains very limited information. In this paper, we present a complementary solution for augmenting the level of detail of the layout of musical documents; that is, the layout information can be encoded in a separate sub-tree with cross-references to other elements holding the musical content. The major advantage of the proposed solution is that it enables multiple layout descriptions, each describing a different visual instantiation of the same musical content. © 2012 International Society for Music Information Retrieval.","","Forestry; Information Retrieval; Symbols; Encoding (symbols); Forestry; Layout information; Level of detail; Limited information; Multiple systems; Music notation; Text document; Information retrieval","L. Pugin; Swiss RISM, Fribourg University, Germany; email: laurent.pugin@rism-ch.org","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Bugge E.P.; Juncher K.L.; Mathiasen B.S.; Simonsen J.G.","Bugge, Esben Paul (55586601900); Juncher, Kim Lundsteen (55586099400); Mathiasen, Brian Søborg (55586151800); Simonsen, Jakob Grue (8976163500)","55586601900; 55586099400; 55586151800; 8976163500","Using sequence alignment and voting to improve optical music recognition from multiple recognizers","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596832&partnerID=40&md5=e18e72776f4a84c958ff3ac102c0c757","Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark","Bugge E.P., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Juncher K.L., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Mathiasen B.S., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; Simonsen J.G., Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark","Digitalizing sheet music using Optical Music Recognition (OMR) is error-prone, especially when using noisy images created from scanned prints. Inspired by DNA-sequence alignment, we devise a method to use multiple sequence alignment to automatically compare output from multiple third party OMR tools and perform automatic error-correction of pitch and duration of notes. We perform tests on a corpus of 49 one-page scores of varying quality. Our method on average reduces the amount of errors from an ensemble of 4 commercial OMR tools. The method achieves, on average, fewer errors than each recognizer by itself, but statistical tests show that it is significantly better than only 2 of the 4 commercial recognizers. The results suggest that recognizers may be improved somewhat by sequence alignment and voting, but that more elaborate methods may be needed to obtain substantial improvements. All software, scanned music data used for testing, and experiment protocols are open source and available at: http://code.google.com/p/omr-errorcorrection/. © 2011 International Society for Music Information Retrieval.","","Errors; Information retrieval; Open systems; Statistical tests; Error prones; Multiple sequence alignments; Music data; Noisy image; Open sources; Optical music recognition; Sequence alignments; Third parties; Software testing","E.P. Bugge; Department of Computer Science, University of Copenhagen, DIKU, 2300 Copenhagen S, Njalsgade 126-128, Denmark; email: ebugge@diku.dk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Song Y.; Dixon S.; Pearce M.","Song, Yading (55582606800); Dixon, Simon (7201479437); Pearce, Marcus (8674558800)","55582606800; 7201479437; 8674558800","Evaluation of musical features for emotion classification","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","66","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423683&partnerID=40&md5=56a721c74268089d9533d3c221d192f0","Centre for Digital Music, Queen Mary University of London, United Kingdom","Song Y., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Pearce M., Centre for Digital Music, Queen Mary University of London, United Kingdom","Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recognition. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only limited success has been obtained in learning automatic classifiers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words ""happy"", ""sad"", ""angry"" and ""relaxed"", on the Last. FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio features are extracted using standard algorithms. Two classifiers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dynamics, and, to a lesser extent, harmony. We also find that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classification. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Automatic classifiers; Cross validation; Emotion classification; Emotion recognition; Feature sets; Ground truth data; Musical features; Polynomial kernels; Radial basis functions; Spectral feature; Standard algorithms; Radial basis function networks","Y. Song; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: yading.song@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Arzt A.; Böck S.; Widmer G.","Arzt, Andreas (36681791200); Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","36681791200; 55413719000; 7004342843","Fast identification of piece and score position via symbolic fingerprinting","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421780&partnerID=40&md5=ced21e465b08ee2429520423e6650e3d","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Arzt A., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In this paper we present a novel algorithm that, given a short snippet of an audio performance (piano music, for the time being), identifies the piece and the score position. Instead of using audio matching methods we propose a combination of a state-of-the-art music transcription algorithm and a new symbolic fingerprinting method. The resulting system is usable in both on-line and off-line scenarios and thus may be of use in many application areas. As the evaluation shows the system operates with only minimal lag and achieves high precision even with very short queries. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Application area; High precision; Matching methods; Music transcription; Novel algorithm; Piano music; Audio acoustics","A. Arzt; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: andreas.arzt@jku.at","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Lagrange M.; Ozerov A.; Vincent E.","Lagrange, Mathieu (18042165200); Ozerov, Alexey (24725182100); Vincent, Emmanuel (14010158800)","18042165200; 24725182100; 14010158800","Robust singer identification in polyphonic music using melody enhancement and uncertainty-based learning","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421020&partnerID=40&md5=1acde08fa51d82611bdc7ffceaa95732","STMS, IRCAM - CNRS, UPMC, France; Technicolor Research and Innovation, France; INRIA, Centre de Rennes - Bretagne Atlantique, France","Lagrange M., STMS, IRCAM - CNRS, UPMC, France; Ozerov A., Technicolor Research and Innovation, France; Vincent E., INRIA, Centre de Rennes - Bretagne Atlantique, France","Enhancing specific parts of a polyphonic music signal is believed to be a promising way of breaking the glass ceiling that most Music Information Retrieval (MIR) systems are now facing. The use of signal enhancement as a pre-processing step has led to limited improvement though, because distortions inevitably remain in the enhanced signals that may propagate to the subsequent feature extraction and classification stages. Previous studies attempting to reduce the impact of these distortions have relied on the use of feature weighting or missing feature theory. Based on advances in the field of noise-robust speech recognition, we represent the uncertainty about the enhanced signals via a Gaussian distribution instead that is subsequently propagated to the features and to the classifier. We introduce new methods to estimate the uncertainty from the signal in a fully automatic manner and to learn the classifier directly from polyphonic data. We illustrate the results by considering the task of identifying, from a given set of singers, which one is singing at a given time in a given song. Experimental results demonstrate the relevance of our approach. © 2012 International Society for Music Information Retrieval.","","Feature extraction; Information retrieval; Speech recognition; Feature extraction and classification; Feature weighting; Glass ceiling; Missing feature theories; Music information retrieval; Noise robust speech recognition; Polyphonic music; Pre-processing step; Signal enhancement; Uncertainty analysis","M. Lagrange; STMS, IRCAM - CNRS, UPMC, France; email: mathieu.lagrange@ircam.fr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Wang X.; Chen X.; Yang D.; Wu Y.","Wang, Xing (55586910500); Chen, Xiaoou (11639519200); Yang, Deshun (24485594500); Wu, Yuqian (55938761100)","55586910500; 11639519200; 24485594500; 55938761100","Music emotion classification of chinese songs based on lyrics using TF*IDF and rhyme","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873590604&partnerID=40&md5=845ff0a2133848682f54555c21c24bd3","Institute of Computer Science and Technology, Peking University, China","Wang X., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China; Wu Y., Institute of Computer Science and Technology, Peking University, China","This paper presents the outcomes of research into an automatic classification system based on the lingual part of music. Two novel kinds of short features are extracted from lyrics using tf*idf and rhyme. Meta-learning algorithm is adapted to combine these two sets of features. Results show that our features promote the accuracy of classification and meta-learning algorithm is effective in fusing the two features. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Accuracy of classifications; Automatic classification systems; Metalearning; Music emotion classifications; Sets of features; Learning algorithms","X. Wang; Institute of Computer Science and Technology, Peking University, China; email: wangxing@icst.pku.edu.cn","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Thoshkahna B.; Ramakrishnan K.R.","Thoshkahna, Balaji (15051329900); Ramakrishnan, K.R. (7101600367)","15051329900; 7101600367","A postprocessing technique for improved harmonic / percussion separation for polyphonic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600899&partnerID=40&md5=1b25bffc2070d114212d0583e03c3880","Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India","Thoshkahna B., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; Ramakrishnan K.R., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India","In this paper we propose a postprocessing technique for a spectrogram diffusion based harmonic/percussion decomposition algorithm. The proposed technique removes harmonic instrument leakages in the percussion enhanced outputs of the baseline algorithm. The technique uses median filtering and an adaptive detection of percussive segments in subbands followed by piecewise signal reconstruction using envelope properties to ensure that percussion is enhanced while harmonic leakages are suppressed. A new binarymask is created for the percussion signal which upon applying on the original signal improves harmonic versus percussion separation. We compare our algorithm with two recent techniques and show that on a database of polyphonic Indian music, the postprocessing algorithm improves the harmonic versus percussion decomposition significantly. © 2011 International Society for Music Information Retrieval.","","Algorithms; Harmonic analysis; Information retrieval; Median filters; Separation; Adaptive detection; Decomposition algorithm; Harmonic leakage; Median filtering; Original signal; Piece-wise; Polyphonic music; Post-processing techniques; Postprocessing algorithms; Spectrograms; Subbands; Musical instruments","B. Thoshkahna; Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; email: balajitn@ee.iisc.ernet.in","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Tryfou G.; Härmä A.; Mouchtaris A.","Tryfou, Georgina (55418606700); Härmä, Aki (6701367190); Mouchtaris, Athanasios (6602611014)","55418606700; 6701367190; 6602611014","Tempo estimation based on linear prediction and perceptual modelling","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592508&partnerID=40&md5=00555eb3fa4cabbca25e2772b0dfa6ed","Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece; Department of Computer Science, University of Crete, Heraklion, Crete, Greece; Philips Research, Eindhoven, Netherlands","Tryfou G., Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece, Department of Computer Science, University of Crete, Heraklion, Crete, Greece; Härmä A., Philips Research, Eindhoven, Netherlands; Mouchtaris A., Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece, Department of Computer Science, University of Crete, Heraklion, Crete, Greece","Many applications demand the automatic induction of the tempo of a musical excerpt. The tempo estimation systems follow a general scheme that consists of two main steps: the creation of a feature list and the detection of periodicities on this list. In this study, we propose a new method for the implementation of the first step, along with the addition of a final step that will enhance the tempo estimation procedure. The proposed method for the extraction of the feature list is based on Gammatone subspace analysis and Linear Prediction Error Filters (LPEFs). As a final step on the system, the application of a model that approximates the tempo perception by human listeners is proposed. The results of the evaluation indicate the proposed method compares favourably with other, state-of-the-art tempo estimation methods, using only one frame of the musical experts when most of the literature methods demand the processing of the whole piece. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Automatic induction; Human listeners; Linear prediction; Linear prediction error filter; Perceptual modelling; Subspace analysis; Tempo estimations; Estimation","G. Tryfou; Foundation for Research and Technology - Hellas (FORTH-ICS), Institute of Computer Science, Heraklion, Crete, Greece; email: tryfou@ics.forth.gr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Ellis K.; Coviello E.; Lanckriet G.R.G.","Ellis, Katherine (55586314400); Coviello, Emanuele (35147534800); Lanckriet, Gert R.G. (7801431767)","55586314400; 35147534800; 7801431767","Semantic annotation and retrieval of music using a bag of systems representation","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592866&partnerID=40&md5=087ed42489720dc227288ece892d7170","University of California, San Diego, United States","Ellis K., University of California, San Diego, United States; Coviello E., University of California, San Diego, United States; Lanckriet G.R.G., University of California, San Diego, United States","We present a content-based auto-tagger that leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. This leads to a higher-level, concise ""Bag of Systems"" (BoS) representation of the characteristics of a musical piece. Once songs are represented as a BoS histogram over codewords, traditional algorithms for text document retrieval can be leveraged for music autotagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different classes of generative models at various time resolutions through the selection of the BoS codewords. Experiments show that this enriches the audio representation and leads to superior auto-tagging performance. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio representation; Code-words; Codeword; Content-based; Different class; Generative model; Musical pieces; Semantic annotations; Systems representation; Temporal characteristics; Text document; Time resolution; Semantics","K. Ellis; University of California, San Diego, United States; email: kellis@ucsd.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Flexer A.; Schnitzer D.; Schlüter J.","Flexer, Arthur (7004555682); Schnitzer, Dominik (23996271700); Schlüter, Jan (55063593300)","7004555682; 23996271700; 55063593300","A MIREX meta-analysis of hubness in audio music similarity","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435945&partnerID=40&md5=f7e501b8ad3ecf3f34cec34a31428116","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schlüter J., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We use results from the 2011 MIREX ""Audio Music Similarity and Retrieval"" task for a meta analysis of the hub phenomenon. Hub songs appear similar to an undesirably high number of other songs due to a problem of measuring distances in high dimensional spaces. Comparing 17 algorithms we are able to confirm that different algorithms produce very different degrees of hubness. We also show that hub songs exhibit less perceptual similarity to the songs they are close to, according to an audio similarity function, than non-hub songs. Application of the recently introduced method of ""mutual proximity"" is able to decisively improve this situation. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Audio music; High dimensional spaces; Measuring distances; Meta analysis; Perceptual similarity; Similarity functions; Insecticides","A. Flexer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: arthur.flexer@ofai.at","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Bandera C.D.L.; Barbancho A.M.; Tardón L.J.; Sammartino S.; Barbancho I.","Bandera, Cristina De La (55586504000); Barbancho, Ana M. (6602362530); Tardón, Lorenzo J. (6602405058); Sammartino, Simone (55204516100); Barbancho, Isabel (6602638932)","55586504000; 6602362530; 6602405058; 55204516100; 6602638932","Humming method for content-based music information retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592626&partnerID=40&md5=02c76ddc2ed0fe9152323868ea5f9824","Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain","Bandera C.D.L., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Barbancho A.M., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Tardón L.J., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Sammartino S., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; Barbancho I., Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain","In this paper a humming method for music information retrieval is presented. The system uses a database with real songs and does not need another type of symbolic representation of them. The system employs an original fingerprint based on chroma vectors to characterize the humming and the references songs. With this fingerprint, it is possible to get the hummed songs without needed of transcription of the notes of the humming or of the songs. The system showed a good performance on Pop/Rock and Spanish folk music. © 2011 International Society for Music Information Retrieval.","","Content-based; Music information retrieval; Symbolic representation; System use; Information retrieval","C.D.L. Bandera; Dept. Ingeniería de Comunicaciones, Campus Universitario de Teatinos S/n, Universidad de Ḿalaga, 29071, Ḿalaga, Spain; email: cdelabandera@ic.uma.es","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Kemal Karaosmanoǧlu M.","Kemal Karaosmanoǧlu, M. (35102480700)","35102480700","A Turkish makam music symbolic database for music information retrieval: SymbTr","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432678&partnerID=40&md5=48117b1831fe09437c7d0594aea4574a","Yildiz Technical University, Turkey","Kemal Karaosmanoǧlu M., Yildiz Technical University, Turkey","Turkish makam music needs a comprehensive database for public consumption, to be used in MIR. This article introduces SymbTr, a Turkish Makam Music Symbolic Representation Database, aimed at filling this void. SymbTr consists of musical information in text, PDF, and MIDI formats. Raw data, drawn from reliable sources, and consisting of 1, 700 musical pieces in Turkish art and folk music was processed featuring distinct examples in 155 diverse makams, 100 usuls and 48 forms. Special care was devoted to selection of works that scatter across a broad historical time span and were among those still performed today. Total number of musical notes in these pieces was 630, 000, corresponding to a nominal playback time of 72 hours. Synthesized sounds particular to Turkish makam music were used in MIDI playback, and transcription/playback errors were corrected by input from experts. Symbolic representation data, open to the public, is output from a computer program developed exclusively for Turkish makam music. SymbTr was designed as a wholesome representation of aforementioned distinct auditory and visual features that distinguish Turkish makam music from other music genres. This article explains the database format in detail, and also provides, through examples, statistical information on pitch/interval allocation and distribution. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Music genre; Music information retrieval; Musical information; Musical notes; Musical pieces; Statistical information; Symbolic database; Symbolic representation; Synthesized sounds; Time span; Turkishs; Visual feature; Database systems","M. Kemal Karaosmanoǧlu; Yildiz Technical University, Turkey; email: kkara@yildiz.edu.tr","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Hu X.; Yu B.","Hu, Xiao (55496358400); Yu, Bei (7402092787)","55496358400; 7402092787","Exploring the relationship between mood and creativity in rock lyrics","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604977&partnerID=40&md5=aaa50420072369c2a9809ce71f8f00af","Library and Information Science Program, Morgridge College of Education, University of Denver, United States; School of Information Studies, Syracuse University, United States","Hu X., Library and Information Science Program, Morgridge College of Education, University of Denver, United States; Yu B., School of Information Studies, Syracuse University, United States","The relationship between mood and creativity has been widely studied in psychology, however, no conclusion is reached in terms of which mood triggers high creativity, positive or negative. This paper provides new insights to this on-going argument by examining the relationship between lyrics creativity and music mood. We use three computational measures to gauge lyrics creativity: Type-to- Token Ratio, word norms fraction, and WordNet similarity. We then test three hypotheses regarding differences in lyrics creativity between music with different moods on 2715 U.S. rock songs. The three measures led to consistent findings that lyrics of negative and sad songs demonstrate higher linguistic creativity than those of positive and happy songs. Our findings support previous studies in psycholinguistics that people write more creatively when the text conveys sad or negative sentiment, and contradict previous research that positive mood triggers more unusual word associations. The result also indicates that different measures capture different aspects of lyrics creativity. © 2011 International Society for Music Information Retrieval.","","Linguistics; Negative sentiments; Rock song; Word association; Wordnet; Information retrieval","X. Hu; Library and Information Science Program, Morgridge College of Education, University of Denver, United States; email: xiao.hu@du.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"García-Díez S.; Saerens M.; Senelle M.; Fouss F.","García-Díez, Silvia (36674907400); Saerens, Marco (6603603729); Senelle, Mathieu (55585901400); Fouss, François (8685481800)","36674907400; 6603603729; 55585901400; 8685481800","A simple-cycles weighted kernel based on harmony structure for similarity retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602397&partnerID=40&md5=a4ed57b0ad73d0e31ec8de05c370ac13","Université catholique de Louvain, Belgium; Université catholique de Louvain - Site de Mons, Belgium","García-Díez S., Université catholique de Louvain, Belgium; Saerens M., Université catholique de Louvain, Belgium; Senelle M., Université catholique de Louvain - Site de Mons, Belgium; Fouss F., Université catholique de Louvain - Site de Mons, Belgium","This paper introduces a novel methodology for music similarity retrieval based on chord progressions. From each chord progression, a directed labeled graph containing the interval transitions is extracted. This graph will be used as input for a graph comparison method based on simple cycles - cycles where the only repeated nodes are the first and the last one. In music, simple cycles represent the repetitive sub-structures of, e.g., modern pop/rock music. By means of a kernel function [10] whose feature space is spanned by these simple cycles, we obtain a kernel matrix (similarity matrix) which can then be used in music similarity retrieval tasks. The resulting algorithm has a time complexity of O(n+m(c+1)), where n is the number of vertices, m is the number of edges, and c is the number of simple cycles. The performance of our method is tested on both an idiom retrieval task, and a cover song retrieval task. Empirical results show the improved accuracy of our method in comparison with other string-matching, and graph-comparison methods used as baseline.","","Comparison methods; Cover songs; Feature space; Interval transition; Kernel function; Kernel matrices; Labeled graphs; Music similarity; Novel methodology; Similarity matrix; Similarity retrieval; String matching; Sub-structures; Time complexity; Information retrieval","S. García-Díez; Université catholique de Louvain, Belgium; email: silvia.garciadiez@ucLouvain.be","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Lee J.H.; Waterman N.M.","Lee, Jin Ha (57190797465); Waterman, Nichole Maiman (55582577300)","57190797465; 55582577300","Understanding user requirements for music information services","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415163&partnerID=40&md5=d2ab9ec869aef1b0d316592fee1702f8","Information School, University of Washington, United States","Lee J.H., Information School, University of Washington, United States; Waterman N.M., Information School, University of Washington, United States","User studies in the music information retrieval and music digital library fields have been gradually increasing in recent years, but large-scale studies that can help detect common user behaviors are still lacking. We have conducted a large-scale user survey in which we asked numerous questions related to users' music needs, uses, seeking, and management behaviors. In this paper, we present our preliminary findings, specifically focusing on the responses to questions of users' favorite music related websites/applications and the reasons why they like them. We provide a list of popular music services, as well as an analysis of how these services are used, and what qualities are valued. Our findings suggest several trends in the types of music services people like: an increase in the popularity of music streaming and mobile music consumption, the emergence of new functionality, such as music identification and cloud music services, an appreciation of music videos, serendipitous discovery of music, and customizability, as well as users' changing expectations of particular types of music information. © 2012 International Society for Music Information Retrieval.","","Digital libraries; Information retrieval; Information services; Customizability; Management behavior; Mobile music; Music digital libraries; Music identification; Music information; Music information retrieval; Music streaming; Music video; Popular music; Serendipitous discovery; User behaviors; User requirements; User study; User surveys; Behavioral research","J.H. Lee; Information School, University of Washington, United States; email: jinhalee@uw.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Peeters G.; Fort K.","Peeters, Geoffroy (22433836000); Fort, Karen (35236644800)","22433836000; 35236644800","Towards a (better) definition of the description of annotated MIR corpora","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436630&partnerID=40&md5=9427252be8f0f4beeddc69bac02af262","STMS, IRCAM-CNRS-UPMC, Paris, France; INIST-CNRS and Université Paris 13, Sorbonne Paris Cité, LIPN, Nancy, France","Peeters G., STMS, IRCAM-CNRS-UPMC, Paris, France; Fort K., INIST-CNRS and Université Paris 13, Sorbonne Paris Cité, LIPN, Nancy, France","Today, annotated MIR corpora are provided by various research labs or companies, each one using its own annotation methodology, concept definitions, and formats. This is not an issue as such. However, the lack of descriptions of the methodology used - how the corpus was actually annotated, and by whom - and of the annotated concepts, i.e. what is actually described, is a problem with respect to the sustainability, usability, and sharing of the corpora. Experience shows that it is essential to define precisely how annotations are supplied and described. We propose here a survey and consolidation report on the nature of the annotated corpora used and shared in MIR, with proposals for the axis against which corpora can be described so to enable effective comparison and the inherent influence this has on tasks performed using them. © 2012 International Society for Music Information Retrieval.","","Concept definitions; Research labs; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Kirchhoff H.; Dixon S.; Klapuri A.","Kirchhoff, Holger (37072578300); Dixon, Simon (7201479437); Klapuri, Anssi (6602945099)","37072578300; 7201479437; 6602945099","Multi-template shift-variant non-negative matrix deconvolution for semi-automatic music transcription","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431582&partnerID=40&md5=8053bd835b7ab286740753e8f1ee964e","Queen Mary University of London, Centre for Digital Music, United Kingdom","Kirchhoff H., Queen Mary University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary University of London, Centre for Digital Music, United Kingdom; Klapuri A., Queen Mary University of London, Centre for Digital Music, United Kingdom","For the task of semi-automatic music transcription, we extended our framework for shift-variant non-negative matrix deconvolution (svNMD) to work with multiple templates per instrument and pitch. A k-means clustering based learning algorithm is proposed that infers the templates from the data based on the provided user information. We experimentally explored the maximum achievable transcription accuracy of the algorithm and evaluated the prospective performance in a realistic setting. The results showed a clear superiority of the Itakura-Saito divergence over the Kullback-Leibler divergence and a consistent improvement of the maximum achievable accuracy when each pitch is represented by more than one spectral template. © 2012 International Society for Music Information Retrieval.","","Learning algorithms; K-means clustering; Kullback Leibler divergence; Music transcription; Non-negative matrix; Semi-automatics; Shift-variant; User information; Information retrieval","H. Kirchhoff; Queen Mary University of London, Centre for Digital Music, United Kingdom; email: holger.kirchhoff@eecs.qmul.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Speck J.A.; Schmidt E.M.; Morton B.G.; Kim Y.E.","Speck, Jacquelin A. (36457008100); Schmidt, Erik M. (36053813000); Morton, Brandon G. (36457242900); Kim, Youngmoo E. (24724623000)","36457008100; 36053813000; 36457242900; 24724623000","A comparative study of collaborative vs. Traditional musical mood annotation","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597750&partnerID=40&md5=64b8e8e590ba204bc79ab542b43f7328","Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States","Speck J.A., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Schmidt E.M., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Morton B.G., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States","Organizing music by emotional association is a natural process for humans, but the ambiguous nature of emotion makes it a difficult task for machines. Automatic systems for music emotion recognition rely on ground truth data collected from humans, and more effective methods for collecting such data are being continuously developed. In previous work, we developed MoodSwings, an online collaborative game for crowdsourcing dynamic (persecond) mood ratings from multiple players within the twodimensional arousal-valence (A-V) representation of emotion. MoodSwings has proven effective for data collection, but potential data effects caused by collaborative labeling have not yet been analyzed. In this work, we compare the effectiveness of MoodSwings to that of a more traditional data collection method, where annotation is performed by single, paid annotators. We implement a simplified labeling task to run on Amazon's crowdsourcing engine, Mechanical Turk (MTurk), and analyze the labels collected with each method. A statistical comparison shows consistencies between MoodSwings and MTurk data, and we produce similar results using each as training data for automatic emotion production via supervised machine learning. Furthermore the new dataset collected via MTurk has been made available to the Music Information Retrieval community. © 2011 International Society for Music Information Retrieval.","","Data acquisition; Automatic systems; Collaborative games; Comparative studies; Crowdsourcing; Data collection; Data collection method; Ground truth data; Mechanical turks; Music emotions; Music information retrieval; Natural process; Statistical comparisons; Supervised machine learning; Training data; Information retrieval","J.A. Speck; Department of Electrical and Computer Engineering, Music and Entertainment Technology Laboratory (MET-Lab), Drexel University, United States; email: jspeck@drexel.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Moore J.L.; Chen S.; Joachims T.; Turnbull D.","Moore, Joshua L. (54995503400); Chen, Shuo (55355054100); Joachims, Thorsten (6602804136); Turnbull, Douglas (8380095700)","54995503400; 55355054100; 6602804136; 8380095700","Learning to embed songs and tags for playlist prediction","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416522&partnerID=40&md5=1e4d231997ed60838d7312317d483758","Cornell University, Dept. of Computer Science, United States; Ithaca College, Dept. of Computer Science, United States","Moore J.L., Cornell University, Dept. of Computer Science, United States; Chen S., Cornell University, Dept. of Computer Science, United States; Joachims T., Cornell University, Dept. of Computer Science, United States; Turnbull D., Ithaca College, Dept. of Computer Science, United States","Automatically generated playlists have become an important medium for accessing and exploring large collections of music. In this paper, we present a probabilistic model for generating coherent playlists by embedding songs and social tags in a unified metric space. We show how the embedding can be learned from example playlists, providing the metric space with a probabilistic meaning for song/song, song/tag, and tag/tag distances. This enables at least three types of inference. First, our models can generate new playlists, outperforming conventional n-gram models in terms of predictive likelihood by orders of magnitude. Second, the learned tag embeddings provide a generalizing representation for embedding new songs, allowing it to create playlists even for songs it has never observed in training. Third, we show that the embedding space provides an effective metric for matching songs to natural-language queries, even if tags for a large fraction of the songs are missing. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Set theory; Automatically generated; Embeddings; Metric spaces; N-gram models; Orders of magnitude; Probabilistic models; Topology","J.L. Moore; Cornell University, Dept. of Computer Science, United States; email: jlmo@cs.cornell.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Xie B.; Bian W.; Tao D.; Chordia P.","Xie, Bo (55586632000); Bian, Wei (57205527163); Tao, Dacheng (7102600334); Chordia, Parag (24723695700)","55586632000; 57205527163; 7102600334; 24723695700","Music tagging with regularized logistic regression","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873608388&partnerID=40&md5=49b4845ba72f073a6655b966fe0be3e1","GTCMT, Georgia Tech, Atlanta, GA, United States; QCIS, Univ of Tech Sydney, Sydney, NSW, Australia","Xie B., GTCMT, Georgia Tech, Atlanta, GA, United States; Bian W., QCIS, Univ of Tech Sydney, Sydney, NSW, Australia; Tao D., QCIS, Univ of Tech Sydney, Sydney, NSW, Australia; Chordia P., GTCMT, Georgia Tech, Atlanta, GA, United States","In this paper, we present a set of simple and efficient regularized logistic regression algorithms to predict tags of music. We first vector-quantize the delta MFCC features using k-means and construct ""bag-of-words"" representation for each song. We then learn the parameters of these logistic regression algorithms from the ""bag-of- words"" vectors and ground truth labels in the training set. At test time, the prediction confidence by the linear classifiers can be used to rank the songs for music annotation and retrieval tasks. Thanks to the convex property of the objective functions, we adopt an efficient and scalable generalized gradient method to learn the parameters, with global optimum guaranteed. And we show that these efficient algorithms achieve stateof- the-art performance in annotation and retrieval tasks evaluated on CAL-500. © 2011 International Society for Music Information Retrieval.","","Algorithms; Gradient methods; Information retrieval; Logistics; Bag of words; Generalized gradients; Global optimum; Ground truth; K-means; Linear classifiers; Logistic regression algorithms; Logistic regressions; Objective functions; Prediction confidence; Test time; Training sets; Regression analysis","B. Xie; GTCMT, Georgia Tech, Atlanta, GA, United States; email: bo.xie@gatech.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Papadopoulos H.; Kowalski M.","Papadopoulos, Hélène (24462317300); Kowalski, Matthieu (55127695300)","24462317300; 55127695300","Sparse signal decomposition on hybrid dictionaries using musical priors","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607213&partnerID=40&md5=bc4a699377266649bda15bdc92847f76","UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France","Papadopoulos H., UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France; Kowalski M., UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France","This paper investigates the use of musical priors for sparse expansion of audio signals of music on overcomplete dictionaries taken from the union of two orthonormal bases. More specifically, chord information is used to build a structured model that takes into account dependencies between coefficients of the decomposition. Evaluation on various music signals shows that our approach provides results whose quality measured by the signal-to-noise ratio corresponds to state-of-the-art approaches, and shows that our model is relevant to represent audio signals ofWestern tonal music and opens new perspectives. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio signal; Music signals; Orthonormal basis; Over-complete dictionaries; Sparse expansions; Sparse signals; State-of-the-art approach; Structured model; Tonal music; Audio signal processing","H. Papadopoulos; UMR 8506, Laboratoire des Signaux et Systèmes, CNRS-SUPELEC-Univ Paris-Sud, 91172 Gif-sur-Yvette cedex, France; email: helene.papadopoulos@lss.supelec.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Rafii Z.; Pardo B.","Rafii, Zafar (44461970600); Pardo, Bryan (10242155400)","44461970600; 10242155400","Music/voice separation using the similarity matrix","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","78","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416007&partnerID=40&md5=82ba03a053dd97b16de6d5346e9dee30","Northwestern University, EECS Department, Evanston, IL, United States","Rafii Z., Northwestern University, EECS Department, Evanston, IL, United States; Pardo B., Northwestern University, EECS Department, Evanston, IL, United States","Repetition is a fundamental element in generating and perceiving structure in music. Recent work has applied this principle to separate the musical background from the vocal foreground in a mixture, by simply extracting the underlying repeating structure. While existing methods are effective, they depend on an assumption of periodically repeating patterns. In this work, we generalize the repetition-based source separation approach to handle cases where repetitions also happen intermittently or without a fixed period, thus allowing the processing of music pieces with fast-varying repeating structures and isolated repeating elements. Instead of looking for periodicities, the proposed method uses a similarity matrix to identify the repeating elements. It then calculates a repeating spectrogram model using the median and extracts the repeating patterns using a time-frequency masking. Evaluation on a data set of 14 full-track real-world pop songs showed that use of a similarity matrix can overall improve on the separation performance compared with a previous repetition-based source separation method, and a recent competitive music/voice separation method, while still being computationally efficient. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Separation; Computationally efficient; Data sets; Fixed period; Separation methods; Separation performance; Similarity matrix; Spectrograms; Time frequency; Source separation","Z. Rafii; Northwestern University, EECS Department, Evanston, IL, United States; email: zafarrafii@u.northwestern.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Fenet S.; Richard G.; Grenier Y.","Fenet, Sébastien (54977223000); Richard, Gäel (57195915952); Grenier, Yves (55496021000)","54977223000; 57195915952; 55496021000","A scalable audio fingerprint method with robustness to pitch-shifting","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588095&partnerID=40&md5=1d81d343be5d25580d1ac6c513269a94","Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France","Fenet S., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France; Grenier Y., Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France","Audio fingerprint techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing. Although most of nowadays techniques are robust to the majority of them, the quasi-systematic use of a spectral representation makes them possibly sensitive to pitch-shifting. This distortion indeed induces a modification of the spectral content of the signal. In this paper, we propose a novel fingerprint technique, relying on a hashing technique coupled with a CQT-based fingerprint, with a strong robustness to pitch-shifting. Furthermore, we have associated this method with an efficient post-processing for the removal of false alarms. We also present the adaptation of a database pruning technique to our specific context. We have evaluated our approach on a real-life broadcast monitoring scenario. The analyzed data consisted of 120 hours of real radio broadcast (thus containing all the distortions that would be found in an industrial context). The reference database consisted of 30.000 songs. Our method, thanks to its increased robustness to pitch-shifting, shows an excellent detection score. © 2011 International Society for Music Information Retrieval.","","Radio broadcasting; Audio fingerprint; Broadcast monitoring; Database pruning; False alarms; Fingerprint techniques; Hashing techniques; Industrial context; Noisy transmission channels; Post processing; Radio broadcasts; Reference database; Sound processing; Spectral content; Spectral representations; Strong robustness; Information retrieval","S. Fenet; Institut TELECOM, TELECOM ParisTech, CNRS-LTCI, 75014 Paris, 37 rue Dareau, France; email: sebastien.fenet@telecom-paristech.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Montecchio N.; Cont A.","Montecchio, Nicola (25929429200); Cont, Arshia (12344985300)","25929429200; 12344985300","Accelerating the mixing phase in studio recording productions by automatic audio alignment","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592861&partnerID=40&md5=b8df7e061c7fbf710708083ba38cd782","Department of Information Engineering, University of Padova, Italy; Institut de Recherche et Coordination Acoustique/Musique (IRCAM), France","Montecchio N., Department of Information Engineering, University of Padova, Italy; Cont A., Institut de Recherche et Coordination Acoustique/Musique (IRCAM), France","We propose a system for accelerating the mixing phase in a recording production, by making use of audio alignment techniques to automatically align multiple takes of excerpts of a music piece against a performance of the whole work. We extend the approach of our previous work, based on sequential Montecarlo inference techniques, that was targeted at real-time alignment for score/audio following. The proposed approach is capable of producing partial alignments as well as identifying relevant regions in the partial results with regards to the reference, for better integration within a studio mix workflow. The approach is evaluated using data obtained from two recording sessions of classical music pieces, and we discuss its effectiveness for reducing manual work in a production chain. © 2011 International Society for Music Information Retrieval.","","Alignment; Audio acoustics; Information retrieval; Mixing; Studios; Audio alignments; Classical musics; Inference techniques; Manual work; Partial alignment; Partial results; Production chain; Sequential Monte-Carlo; Studio recordings; Audio recordings","N. Montecchio; Department of Information Engineering, University of Padova, Italy; email: nicola.montecchio@dei.unipd.it","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Topel S.S.; Casey M.A.","Topel, Spencer S. (56563051900); Casey, Michael A. (15080769900)","56563051900; 15080769900","Elementary sources: Latent component analysis for music composition","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594489&partnerID=40&md5=4c5f02b45d65a495c12e8a791d5779aa","Bregman Music Audio Research Studio, Dartmouth College, United States","Topel S.S., Bregman Music Audio Research Studio, Dartmouth College, United States; Casey M.A., Bregman Music Audio Research Studio, Dartmouth College, United States","Complexity of music audio signals creates an access problemto specificmusical objects or structures within the source samples. Instead of employing more commonly used audio analysis or production techniques to access features, we describe extraction of sub-mixtures from real-world audio using a Probabilistic Latent Component Analysis-based decomposition tool for music composition. This is highlighted with the presentation of a prior relevant compositional approach named Spectral Music along with a discussion of five compositions extending these principles using methods more commonly associated with source separation research. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Access features; Audio analysis; Audio signal; Elementary sources; Latent component analysis; Music composition; Production techniques; Real-world; Signal analysis","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Niedermayer B.; Böck S.; Widmer G.","Niedermayer, Bernhard (37113938000); Böck, Sebastian (55413719000); Widmer, Gerhard (7004342843)","37113938000; 55413719000; 7004342843","On the importance of ""Real"" audio data for mir algorithm evaluation at the note-level - A comparative study","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599014&partnerID=40&md5=812b577c066fd27a8ecd0f11b048e61c","Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Niedermayer B., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Böck S., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","A considerable number of MIR tasks requires annotations at the note-level for the purpose of in-depth evaluation. A common means of obtaining accurately annotated data corpora is to start with a symbolic representation of a piece and generate corresponding audio data. This study investigates the effect of audio quality and source on the performance of two representative MIR algorithms - Onset Detection and Audio Alignment. Three kinds of audio material are compared: piano pieces generated using a freely available software synthesizer with its default instrument patches; a commercial high-quality sample library; and audio recordings made on a real (computer-controlled) grand piano. Also, the effect of varying richness of artistic changes in tempo and dynamics or natural asynchronies is examined. We show that the algorithms' performance on the different datasets varies considerably, but synthesized audio, does not necessarily yield better results. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Musical instruments; Algorithm evaluation; Audio alignments; Audio data; Audio quality; Comparative studies; Freely available software; Grand piano; High quality; Onset detection; Sample libraries; Symbolic representation; Algorithms","B. Niedermayer; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; email: music@jku.at","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Wang J.-C.; Lee H.-S.; Wang H.-M.; Jeng S.-K.","Wang, Ju-Chiang (36609288900); Lee, Hung-Shin (24341193500); Wang, Hsin-Min (8297293300); Jeng, Shyh-Kang (26023218700)","36609288900; 24341193500; 8297293300; 26023218700","Learning the similarity of audio music in bag-offrames representation from tagged music data","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594763&partnerID=40&md5=e0029ac5c06eadd21740e4710a8fe51b","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","Wang J.-C., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan, Institute of Information Science, Academia Sinica, Taipei, Taiwan; Lee H.-S., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan, Institute of Information Science, Academia Sinica, Taipei, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Jeng S.-K., Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Due to the cold-start problem, measuring the similarity between two pieces of audio music based on their low-level acoustic features is critical to many Music Information Retrieval (MIR) systems. In this paper, we apply the bag-offrames (BOF) approach to represent low-level acoustic features of a song and exploit music tags to help improve the performance of the audio-based music similarity computation. We first introduce a Gaussian mixture model (GMM) as the encoding reference for BOF modeling, then we propose a novel learning algorithm to minimize the similarity gap between low-level acoustic features and music tags with respect to the prior weights of the pre-trained GMM. The results of audio-based query-by-example MIR experiments on the MajorMiner and Magnatagatune datasets demonstrate the effectiveness of the proposed method, which gives a potential to guide MIR systems that employ BOF modeling. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Learning algorithms; Motion compensation; Acoustic features; Audio music; Audio-based; Cold start problems; Gaussian Mixture Model; Music data; Music information retrieval; Music similarity; Query-by-example; Computer music","J.-C. Wang; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; email: asriver@iis.sinica.edu.tw","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Ehmann A.F.; Bay M.; Downie J.S.; Fujinaga I.; De Roure D.","Ehmann, Andreas F. (8988651500); Bay, Mert (56259607500); Downie, J. Stephen (7102932568); Fujinaga, Ichiro (9038140900); De Roure, David (6701509117)","8988651500; 56259607500; 7102932568; 9038140900; 6701509117","Music structure segmentation algorithm evaluation: Expanding on MIREX 2010 analyses and datasets","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605778&partnerID=40&md5=977ea797b9635142a92725d2011720f8","GSLIS, University of Illinois, Urbana-Champaign, United States; Schulich School of Music, McGill University, Canada; Oxford E-Research Centre, University of Oxford, United Kingdom","Ehmann A.F., GSLIS, University of Illinois, Urbana-Champaign, United States; Bay M., GSLIS, University of Illinois, Urbana-Champaign, United States; Downie J.S., GSLIS, University of Illinois, Urbana-Champaign, United States; Fujinaga I., Schulich School of Music, McGill University, Canada; De Roure D., Oxford E-Research Centre, University of Oxford, United Kingdom","Music audio structure segmentation has been a task in the Music Information Retrieval Evaluation eXchange (MIREX) since 2009. In 2010, five algorithms were evaluated against two datasets (297 and 100 songs) with an almost exclusive focus on western popular music. A new annotated dataset significantly larger in size and with a more diverse range of musical styles became available in 2011. This new dataset comprises over 1,300 songs spanning pop, jazz, classical, and world music styles. The algorithms from the 2010 iteration of MIREX are re-evaluated against this new dataset. This paper presents a detailed analysis of these evaluation results in order to gain a better understanding of the current state-of-the-art in automatic structure segmentation. These expanded analyses focus on the interaction of algorithm performance and rankings with datasets, musical styles, and annotation level. Because the new dataset contains multiple annotations for each song, we also introduce a baseline for expected human performance for this task. © 2011 International Society for Music Information Retrieval.","","Algorithms; Image segmentation; Information retrieval; Insecticides; Iterative methods; Algorithm performance; Automatic structures; Diverse range; Evaluation results; Human performance; Music information retrieval; Music structures; Popular music; Structure segmentation; Audio acoustics","A.F. Ehmann; GSLIS, University of Illinois, Urbana-Champaign, United States; email: aehmann@illinois.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Cunningham S.J.; Bainbridge D.; Stephen Downie J.","Cunningham, Sally Jo (7201937110); Bainbridge, David (8756864800); Stephen Downie, J. (7102932568)","7201937110; 8756864800; 7102932568","The impact of MIREX on scholarly research (2005 - 2010)","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428806&partnerID=40&md5=857cd892682fdf38ff263882c9c8e3e9","University of Waikato, Hamilton, New Zealand; University of Illinois, Urbana-Champaign, United States","Cunningham S.J., University of Waikato, Hamilton, New Zealand; Bainbridge D., University of Waikato, Hamilton, New Zealand; Stephen Downie J., University of Illinois, Urbana-Champaign, United States","This paper explores the impact of the MIREX (Music Information Retrieval Evaluation eXchange) evaluation initiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended abstracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication venues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Research; Bibliometric; Citation analysis; Data sets; Extended abstracts; Geographic distribution; Music information retrieval; Scholarly research; Insecticides","S.J. Cunningham; University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Kameoka H.; Ochiai K.; Nakano M.; Tsuchiya M.; Sagayama S.","Kameoka, Hirokazu (7006771405); Ochiai, Kazuki (55414280200); Nakano, Masahiro (55701875500); Tsuchiya, Masato (55582707700); Sagayama, Shigeki (7004859104)","7006771405; 55414280200; 55701875500; 55582707700; 7004859104","Context-free 2D tree structure model of musical notes for Bayesian modeling of polyphonic spectrograms","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436781&partnerID=40&md5=f73ac291a2d1777adb0d6cc556fc425d","Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan; NTT Communication Science Laboratories, NTT Corporation Morinosato, Atsugi, Kanagawa, 243-0198, Wakamiya 3-1, Japan","Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan, NTT Communication Science Laboratories, NTT Corporation Morinosato, Atsugi, Kanagawa, 243-0198, Wakamiya 3-1, Japan; Ochiai K., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan; Nakano M., NTT Communication Science Laboratories, NTT Corporation Morinosato, Atsugi, Kanagawa, 243-0198, Wakamiya 3-1, Japan; Tsuchiya M., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Tokyo, 113-8656, Hongo 7-3-1, Bunkyo, Japan","This paper proposes a Bayesian model for automatic music transcription. Automatic music transcription involves several subproblems that are interdependent of each other: multiple fundamental frequency estimation, onset detection, and rhythm/tempo recognition. In general, simultaneous estimation is preferable when several estimation problems have chicken-and-egg relationships. This paper proposes modeling the generative process of an entire music spectrogram by combining the sub-process by which a musically natural tempo curve is generated, the sub-process by which a set of note onset positions is generated based on a 2-dimensional tree structure representation of music, and the sub-process by which a music spectrogram is generated according to the tempo curve and the note onset positions. Most conventional approaches to music transcription perform note extraction prior to structure analysis, but accurate note extraction has been a difficult task. By contrast, thanks to the combined generative model, the present method performs note extraction and structure estimation simultaneously and thus the optimal solution is obtained within a unified framework. We show some of the transcription results obtained with the present method. © 2012 International Society for Music Information Retrieval.","","Computers; Forestry; Mathematics; Music; Spectrometers; Trees; Bayesian networks; Forestry; Information retrieval; Spectrographs; Trees (mathematics); Automatic music transcription; Bayesian model; Bayesian modeling; Context-free; Conventional approach; Estimation problem; Fundamental frequency estimation; Generative model; Music transcription; Musical notes; Onset detection; Optimal solutions; Simultaneous estimation; Spectrograms; Structure analysis; Structure estimation; Sub-problems; Tree structures; Unified framework; Computer music","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Neubarth K.; Bergeron M.; Conklin D.","Neubarth, Kerstin (25925322600); Bergeron, Mathieu (23992413700); Conklin, Darrell (57220096325)","25925322600; 23992413700; 57220096325","Associations between musicology and music information retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599842&partnerID=40&md5=8dd899a4cfe612f41d4469cbd7ada2e9","Canterbury Christ Church University, Canterbury, United Kingdom; CIRMMT, McGill University, Montreal, Canada; Universidad del País Vasco, San Sebastían, Spain; IKERBASQUE, Basque Foundation for Science, Spain","Neubarth K., Canterbury Christ Church University, Canterbury, United Kingdom; Bergeron M., CIRMMT, McGill University, Montreal, Canada; Conklin D., Universidad del País Vasco, San Sebastían, Spain, IKERBASQUE, Basque Foundation for Science, Spain","A higher level of interdisciplinary collaboration between music information retrieval (MIR) and musicology has been proposed both in terms of MIR tools for musicology, and musicological motivation and interpretation of MIR research. Applying association mining and content citation analysis methods to musicology references in ISMIR papers, this paper explores which musicological subject areas are of interest to MIR, whether references to specific musicology areas are significantly over-represented in specific MIR areas, and precisely why musicology is cited in MIR. © 2011 International Society for Music Information Retrieval.","","Association mining; Citation analysis; Interdisciplinary collaborations; Music information retrieval; Information retrieval","K. Neubarth; Canterbury Christ Church University, Canterbury, United Kingdom; email: kerstin.neubarth@canterbury.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Otsuka T.; Nakadai K.; Ogata T.; Okuno H.G.","Otsuka, Takuma (35410110900); Nakadai, Kazuhiro (6603685669); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","35410110900; 6603685669; 7402000772; 7102397930","Incremental bayesian audio-to-score alignment with flexible harmonic structure models","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592130&partnerID=40&md5=0b380a2ab9d0d55c35f9b8592dc6f7e9","School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Honda Research Institute Japan, Co., Ltd., Wako, Saitama 351-0114, Japan","Otsuka T., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Nakadai K., Honda Research Institute Japan, Co., Ltd., Wako, Saitama 351-0114, Japan; Ogata T., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","Music information retrieval, especially the audio-to-score alignment problem, often involves a matching problem between the audio and symbolic representations. We must cope with uncertainty in the audio signal generated from the score in a symbolic representation such as the variation in the timbre or temporal fluctuations. Existing audio-to-score alignment methods are sometimes vulnerable to the uncertainty in which multiple notes are simultaneously played with a variety of timbres because these methods rely on static observation models. For example, a chroma vector or a fixed harmonic structure template is used under the assumption that musical notes in a chord are all in the same volume and timbre. This paper presents a particle filterbased audio-to-score alignment method with a flexible observation model based on latent harmonic allocation. Our method adapts to the harmonic structure for the audio-toscore matching based on the observation of the audio signal through Bayesian inference. Experimental results with 20 polyphonic songs reveal that our method is effective when more number of instruments are involved in the ensemble. © 2011 International Society for Music Information Retrieval.","","Alignment; Bayesian networks; Inference engines; Information retrieval; A-particles; Alignment methods; Alignment Problems; Audio signal; Bayesian; Bayesian inference; Filter-based; Harmonic structures; Matching problems; Music information retrieval; Musical notes; Observation model; Polyphonic songs; Symbolic representation; Temporal fluctuation; Harmonic analysis","T. Otsuka; School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: ohtsuka@kuis.kyoto-u.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Müller M.; Grosche P.; Jiang N.","Müller, Meinard (7404689873); Grosche, Peter (55413290700); Jiang, Nanzhu (55552898300)","7404689873; 55413290700; 55552898300","A segment-based fitness measure for capturing repetitive structures of music recordings","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602790&partnerID=40&md5=640871dd654420d16cd809efb0250ff5","MPI Informatik, Saarland University, Germany","Müller M., MPI Informatik, Saarland University, Germany; Grosche P., MPI Informatik, Saarland University, Germany; Jiang N., MPI Informatik, Saarland University, Germany","In this paper, we deal with the task of determining the audio segment that best represents a given music recording (similar to audio thumbnailing). Typically, such a segment has many (approximate) repetitions covering large parts of the music recording. As main contribution, we introduce a novel fitness measure that assigns to each segment a fitness value that expresses how much and how well the segment ""explains"" the repetitive structure of the recording. In combination with enhanced feature representations, we show that our fitness measure can cope even with strong variations in tempo, instrumentation, and modulations that may occur within and across related segments. We demonstrate the practicability of our approach by means of several challenging examples including field recordings of folk music and recordings of classical music. © 2011 International Society for Music Information Retrieval.","","Health; Information retrieval; Classical musics; Feature representation; Field recording; Fitness measures; Fitness values; Music recording; Repetitive structure; Segment-based; Audio recordings","M. Müller; MPI Informatik, Saarland University, Germany; email: meinard@mpi-inf.mpg.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Thomas V.; Wagner C.; Clausen M.","Thomas, Verena (56519204500); Wagner, Christian (56660013900); Clausen, Michael (56225233200)","56519204500; 56660013900; 56225233200","OCR-Based post-processing of OMR for the recovery of transposing instruments in complex orchestral scores","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606472&partnerID=40&md5=32a3ac5c6bd68e625bfe4e1051cfc72b","Computer Science III, University of Bonn, Germany","Thomas V., Computer Science III, University of Bonn, Germany; Wagner C., Computer Science III, University of Bonn, Germany; Clausen M., Computer Science III, University of Bonn, Germany","Given a scanned score page, Optical Music Recognition (OMR) attempts to reconstruct all contained music information. However, the available OMR systems lack the ability to recognize transposition information contained in complex orchestral scores. 1 An additional unsolved OMR problem is the handling of orchestral scores using compressed notation. 2 Here, the information of which instrument has to play which staff is crucial for a correct interpretation of the score. But this mapping is lost along the pages of the score during the OMR process. In this paper, we present a method for retrieving the instrumentation and transposition information of orchestral scores. In our approach, we combine the results of Optical Character Recognition (OCR) and OMR to regain the information available through text annotations of the score. In addition, a method to reconstruct the instrument and transposition information for staves where text annotations were omitted or not recognized is presented. In an evaluation we analyze the impact of transposition information on the quality of score-audio synchronizations of orchestral music. The results show that the knowledge of transposing instruments improves the synchronization accuracy and that our method helps in regaining this knowledge. © 2011 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Optical character recognition; Music information; Optical character recognition (OCR); Optical music recognition; Post processing; Text annotations; Instruments","V. Thomas; Computer Science III, University of Bonn, Germany; email: thomas@iai.uni-bonn.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bogdanov D.; Herrera P.","Bogdanov, Dmitry (35748642000); Herrera, Perfecto (24824250300)","35748642000; 24824250300","Howmuch metadata do we need in music recommendation? A subjective evaluation using preference sets","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595124&partnerID=40&md5=ab4efc746d140ee9610744c374a82b4f","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Bogdanov D., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this work we consider distance-based approaches to music recommendation, relying on an explicit set of music tracks provided by the user as evidence of his/her music preferences. Firstly, we propose a purely content-based approach, working on low-level (timbral, temporal, and tonal) and inferred high-level semantic descriptions of music. Secondly, we consider its simple refinement by adding a minimum amount of genre metadata. We compare the proposed approaches with one content-based and three metadata-based baselines. As such, we consider content-based approach working on inferred semantic descriptors, a tag-based recommender exploiting artist tags, a commercial black-box recommender partially employing collaborative filtering information, and a simple genre-based random recommender. We conduct a listening experiment with 19 participants. The obtained results reveal that although the low-level/semantic content-based approach does not achieve the performance of the baseline working exclusively on the inferred semantic descriptors, the proposed refinement provides significant improvement in the listeners' satisfaction comparable with metadata-based approaches, and surpasses these approaches by the number of novel relevant recommendations. We conclude that the proposed content-based approach refined by simple genre metadata is suited for music discovery not only in the long-tail but also within popular music items. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Semantics; Black boxes; Content-based; Content-based approach; Descriptors; Distance-based; High level semantics; Music preferences; Music recommendation; Popular music; Subjective evaluations; Tag-based; Metadata","D. Bogdanov; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: dmitry.bogdanov@upf.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Zapata J.R.; Holzapfel A.; Davies M.E.P.; Oliveira J.L.; Gouyon F.","Zapata, José R. (55349861800); Holzapfel, André (18041818000); Davies, Matthew E.P. (55349903900); Oliveira, João L. (49864144600); Gouyon, Fabien (8373002800)","55349861800; 18041818000; 55349903900; 49864144600; 8373002800","Assigning a confidence threshold on automatic beat annotation in large datasets","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431245&partnerID=40&md5=6e7a67c86acb16ea47e75885c85bf2da","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sound and Music Computing Group, INESC TEC, Porto, Portugal; Faculty of Engineering, University of Porto, Porto, Portugal","Zapata J.R., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Holzapfel A., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Davies M.E.P., Sound and Music Computing Group, INESC TEC, Porto, Portugal; Oliveira J.L., Sound and Music Computing Group, INESC TEC, Porto, Portugal, Faculty of Engineering, University of Porto, Porto, Portugal; Gouyon F., Sound and Music Computing Group, INESC TEC, Porto, Portugal, Faculty of Engineering, University of Porto, Porto, Portugal","In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the first step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73%, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a confidence value for beat tracking in non-annotated data and to choose between a number of beat tracker outputs. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Beat tracking; Committee members; Confidence threshold; Confidence values; Data sets; Large datasets; Listening tests; Statistical tests","J.R. Zapata; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: joser.zapata@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Battenberg E.; Wessel D.","Battenberg, Eric (55567028300); Wessel, David (7005264358)","55567028300; 7005264358","Analyzing drum patterns using conditional deep belief networks","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426072&partnerID=40&md5=9ed762da3067406ac90cd0c79b8077cf","University of California, Berkeley, Dept. of Electrical Engineering and Computer Sciences, United States; University of California, Berkeley, Center for New Music and Audio Technologies, United States","Battenberg E., University of California, Berkeley, Dept. of Electrical Engineering and Computer Sciences, United States; Wessel D., University of California, Berkeley, Center for New Music and Audio Technologies, United States","We present a system for the high-level analysis of beat-synchronous drum patterns to be used as part of a comprehensive rhythmic understanding system. We use a multilayer neural network, which is greedily pre-trained layer-by-layer using restriced Boltzmann machines (RBMs), in order to model the contextual time-sequence information of a drum pattern. For the input layer of the network, we use a conditional RBM, which has been shown to be an effective generative model of multi-dimensional sequences. Subsequent layers of the neural network can be pre-trained as conditional or standard RBMs in order to learn higherlevel rhythmic features. We show that this model can be fine-tuned in a discriminative manner to make accurate predictions about beat-measure alignment. The model generalizes well to multiple rhythmic styles due to the distributed state-space of the multi-layer neural network. In addition, the outputs of the discriminative network can serve as posterior probabilities over beat-alignment labels. These posterior probabilities can be used for Viterbi decoding in a hidden Markov model in order to maintain temporal continuity of the predicted information. © 2012 International Society for Music Information Retrieval.","","Alignment; Hidden Markov models; Information retrieval; Multilayer neural networks; Viterbi algorithm; Accurate prediction; Boltzmann machines; Deep belief networks; Discriminative networks; Generative model; High-level analysis; Input layers; Layer-by-layers; Posterior probability; Rhythmic features; State-space; Temporal continuity; Viterbi decoding; Network layers","E. Battenberg; University of California, Berkeley, Dept. of Electrical Engineering and Computer Sciences, United States; email: ericb@eecs.berkeley.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Müller M.; Jiang N.","Müller, Meinard (7404689873); Jiang, Nanzhu (55552898300)","7404689873; 55552898300","A scape plot representation for visualizing repetitive structures of music recordings","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873434119&partnerID=40&md5=d961bc26d6357f4a324479f5b2e240cd","Bonn University, MPI Informatik, Germany; Saarland University, MPI Informatik, Germany","Müller M., Bonn University, MPI Informatik, Germany; Jiang N., Saarland University, MPI Informatik, Germany","The development of automated methods for revealing the repetitive structure of a given music recording is of central importance in music information retrieval. In this paper, we present a novel scape plot representation that allows for visualizing repetitive structures of the entire music recording in a hierarchical, compact, and intuitive way. In a scape plot, each point corresponds to an audio segment identified by its center and length. As our main contribution, we assign to each point a color value so that two segment properties become apparent. Firstly, we use the lightness component of the color to indicate the repetitive-ness of the encoded segment, where we revert to a recently introduced fitness measure. Secondly, we use the hue component of the color to reveal the relations between different segments. To this end, we introduce a novel grouping procedure that automatically maps related segments to similar hue values. By discussing a number of popular and classical music examples, we illustrate the potential and visual appeal of our representation and also indicate limitations. © 2012 International Society for Music Information Retrieval.","","Color; Information retrieval; Automated methods; Color values; Fitness measures; Hue values; Music information retrieval; Music recording; Repetitive structure; Audio recordings","M. Müller; Bonn University, MPI Informatik, Germany; email: meinard@mpi-inf.mpg.de","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Chen R.; Shen W.; Srinivasamurthy A.; Chordia P.","Chen, Ruofeng (55583646800); Shen, Weibin (55582944200); Srinivasamurthy, Ajay (55583336200); Chordia, Parag (24723695700)","55583646800; 55582944200; 55583336200; 24723695700","Chord recognition using duration-explicit Hidden Markov Models","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435049&partnerID=40&md5=7cd4b96a16c80b68e93cd2af11cabdeb","Georgia Tech Center for Music Technology, United States; Smule Inc., United States","Chen R., Georgia Tech Center for Music Technology, United States; Shen W., Georgia Tech Center for Music Technology, United States; Srinivasamurthy A., Georgia Tech Center for Music Technology, United States; Chordia P., Smule Inc., United States","We present an audio chord recognition system based on a generalization of the Hidden Markov Model (HMM) in which the duration of chords is explicitly considered - a type of HMM referred to as a hidden semi-Markov model, or duration-explicit HMM (DHMM). We find that such a system recognizes chords at a level consistent with the state-of-the-art systems - 84.23% on Uspop dataset at the major/minor level. The duration distribution is estimated from chord duration histograms on the training data. It is found that the state-of-the-art recognition result can be improved upon by using several duration distributions, which are found automatically by clustering song-level duration histograms. The paper further describes experiments which shed light on the extent to which context information, in the sense of transition matrices, is useful for the audio chord recognition task. We present evidence that the context provides surprisingly little improvement in performance, compared to isolated frame-wise recognition with simple smoothing. We discuss possible reasons for this, such as the inherent entropy of chord sequences in our training database. © 2012 International Society for Music Information Retrieval.","","Graphic methods; Information retrieval; Chord sequence; Context information; Data sets; Hidden semi-Markov models; Recognition systems; State-of-the-art system; Training data; Training database; Transition matrices; Hidden Markov models","R. Chen; Georgia Tech Center for Music Technology, United States; email: ruofengchen@gatech.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Neubarth K.; Goienetxea I.; Johnson C.G.; Conklin D.","Neubarth, Kerstin (25925322600); Goienetxea, Izaro (55135997200); Johnson, Colin G. (15042662200); Conklin, Darrell (57220096325)","25925322600; 55135997200; 15042662200; 57220096325","Association mining of folk music genres and toponyms","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873438951&partnerID=40&md5=acf845aed4c4f24401d5982a4e8bb5ae","Canterbury Christ Church University, Canterbury, United Kingdom; School of Computing, University of Kent, Canterbury, United Kingdom; Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, San Sebastián, Spain; IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","Neubarth K., Canterbury Christ Church University, Canterbury, United Kingdom, School of Computing, University of Kent, Canterbury, United Kingdom; Goienetxea I., Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, San Sebastián, Spain; Johnson C.G., School of Computing, University of Kent, Canterbury, United Kingdom; Conklin D., Department of Computer Science and Artificial Intelligence, University of the Basque Country UPV/EHU, San Sebastián, Spain, IKERBASQUE, Basque Foundation for Science, Bilbao, Spain","This paper demonstrates how association rule mining can be applied to discover relations between two ontologies of folk music: a genre and a region ontology. Genreregion associations have been widely studied in folk music research but have been neglected in music information retrieval. We present a method of association rule mining with constraints consisting of rule templates and rule evaluation measures to identify different, musicologically motivated, categories of genre-region associations. The method is applied to a corpus of 1902 Basque folk tunes, and several interesting rules and rule sets are discovered. © 2012 International Society for Music Information Retrieval.","","Association mining; Interesting rules; Music genre; Music information retrieval; Rule evaluation; Rule set; Information retrieval","","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Bosch J.J.; Janer J.; Fuhrmann F.; Herrera P.","Bosch, Juan J. (36760347300); Janer, Jordi (35068089300); Fuhrmann, Ferdinand (36642587400); Herrera, Perfecto (24824250300)","36760347300; 35068089300; 36642587400; 24824250300","A comparison of sound segregation techniques for predominant instrument recognition in musical audio signals","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","79","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436256&partnerID=40&md5=f44c7ae816a805e4e9151380788d8ca3","Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain","Bosch J.J., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Janer J., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Fuhrmann F., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; Herrera P., Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain","The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32%. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Instrument errors; Instruments; Signal analysis; Audio stream; Data sets; Instrument recognition; Low-to-high; Musical audio signal; Original algorithms; Original signal; Recognition models; Separation algorithms; Sound segregation; Audio acoustics","J.J. Bosch; Universitat Pompeu Fabra, Music Technology Group, Barcelona, Roc Boronat 138, Spain; email: juanjo.bosch@gmail.com","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Barthet M.; Dixon S.","Barthet, Mathieu (24723525000); Dixon, Simon (7201479437)","24723525000; 7201479437","Ethnographic observations of musicologists at the british library: Implications for music information retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601686&partnerID=40&md5=5a82b561d0848ad966c113d64cd752c9","Centre for Digital Music, Queen Mary University of London, United Kingdom","Barthet M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Without a rich understanding of user behaviours and needs, music information retrieval (MIR) systems might not be ideally suited to their potential users. In this study, we followed an ethnographic methodology to elicit some of the strategies used by musicologists to explore and document musical performances, in order to investigate if and how technologies could enhance such a process. Observations of musicologists studying historical recordings of classical music were conducted at the British Library. The observations show that the musicologists alternate between a closed listening practice, relying exclusively on aural observations, and a multimodal listening practice, where they interact with various music representations and information sources using different media (e.g. metadata about the recordings and performers, sound visualisations, scores, lyrics and performance videos). The spoken parts of broadcast recordings brought historical/extra-musical clues helping to understand music performance practices. Sound visualisation and computational methods fostered the analysis of specific musical expression patterns. We suggest that software designed for musicologists should facilitate switching between closed and multimodal listening modes, interaction with scores and lyrics, and analysis and annotation of speech and music performance using content-based MIR techniques. © 2011 International Society for Music Information Retrieval.","","Behavioral research; Information retrieval; Metadata; Visualization; British Library; Classical musics; Content-based; Ethnographic observations; Information sources; Multi-modal; Music information retrieval; Music performance; Music representation; Musical expression; Musical performance; Potential users; User behaviour; Visualisations; Audio recordings","M. Barthet; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: mathieu.barthet@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mauch M.; Fujihara H.; Yoshii K.; Goto M.","Mauch, Matthias (36461512900); Fujihara, Hiromasa (16068753300); Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","36461512900; 16068753300; 7103400120; 7403505330","Timbre and melody features for the recognition of vocal activity and instrumental solos in polyphonic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","43","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599953&partnerID=40&md5=6354b13167e1bac04fc65bd5cd48b400","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Mauch M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We propose the task of detecting instrumental solos in polyphonic music recordings, and the usage of a set of four audio features for vocal and instrumental activity detection. Three of the features are based on the prior extraction of the predominant melody line, and have not been used in the context of vocal/instrumental activity detection. Using a support vector machine hidden Markov model we conduct 14 experiments to validate several combinations of our proposed features. Our results clearly demonstrate the benefit of combining the features: the best performance was always achieved by combining all four features. The top accuracy for vocal activity detection is 87.2%. The more difficult task of detecting instrumental solos equally benefits from the combination of all features and achieves an accuracy of 89.8% and a satisfactory precision of 61.1%. With this paper we also release to the public the 102 annotations we used for training and testing. The annotations offer not only vocal/nonvocal labels, but also distinguish between female and male singers, and different solo instruments. © 2011 International Society for Music Information Retrieval.","F0 segregation; Ground truth; Instrumental solo detection; Pitch fluctuation; SVM; Vocal activity detection","Hidden Markov models; Information retrieval; Support vector machines; Activity detection; Audio features; Ground truth; Pitch fluctuation; Polyphonic music; SVM; Training and testing; Instruments","M. Mauch; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: m.mauch@aist.go.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bertin-Mahieux T.; Ellis D.P.W.; Whitman B.; Lamere P.","Bertin-Mahieux, Thierry (49060926500); Ellis, Daniel P.W. (13609089200); Whitman, Brian (7006578587); Lamere, Paul (15765776900)","49060926500; 13609089200; 7006578587; 15765776900","The million song dataset","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","817","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597375&partnerID=40&md5=86d221873f2c64e30ffb6186a09ce10c","LabROSA, EE Dept., Columbia University, United States; Echo Nest, Somerville, MA, United States","Bertin-Mahieux T., LabROSA, EE Dept., Columbia University, United States; Ellis D.P.W., LabROSA, EE Dept., Columbia University, United States; Whitman B., Echo Nest, Somerville, MA, United States; Lamere P., Echo Nest, Somerville, MA, United States","We introduce the Million Song Dataset, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. We describe its creation process, its content, and its possible uses. Attractive features of the Million Song Database include the range of existing resources to which it is linked, and the fact that it is the largest current research dataset in our field. As an illustration, we present year prediction as an example application, a task that has, until now, been difficult to study owing to the absence of a large set of suitable data. We show positive results on year prediction, and discuss more generally the future development of the dataset. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Audio features; Creation process; Popular music; Data processing","T. Bertin-Mahieux; LabROSA, EE Dept., Columbia University, United States; email: thierry@ee.columbia.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Burlet G.; Porter A.; Hankinson A.; Fujinaga I.","Burlet, Gregory (55582430200); Porter, Alastair (55582507300); Hankinson, Andrew (54970482500); Fujinaga, Ichiro (9038140900)","55582430200; 55582507300; 54970482500; 9038140900","NEON.Js: Neume editor online","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415214&partnerID=40&md5=9b694d3addbebfc1b240344f987587a7","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Burlet G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Porter A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper introduces Neon.js, a browser-based music notation editor written in JavaScript. The editor can be used to manipulate digitally encoded musical scores in square-note notation. This type of notation presents certain challenges to a music notation editor, since many neumes (groups of pitches) are ligatures - continuous graphical symbols that represent multiple notes. Neon.js will serve as a component within an online optical music recognition framework. The primary purpose of the editor is to provide a readily accessible interface to easily correct errors made in the process of optical music recognition. In this context, we envision an environment that promotes crowdsourcing to further the creation of editable and searchable online symbolic music collections and for generating and editing ground-truth data to train optical music recognition algorithms. © 2012 International Society for Music Information Retrieval.","","Correct error; Crowdsourcing; Javascript; Music collection; Music notation; Music recognition; Musical score; Information retrieval","G. Burlet; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; email: gregory.burlet@mail.mcgill.ca","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Nieto O.; Humphrey E.J.; Bello J.P.","Nieto, Oriol (55583364500); Humphrey, Eric J. (55060792500); Bello, Juan Pablo (7102889110)","55583364500; 55060792500; 7102889110","Compressing music recordings into audio summaries","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423876&partnerID=40&md5=e884f45e7c15ac9fa5a272090e9b7fd7","New York University, United States","Nieto O., New York University, United States; Humphrey E.J., New York University, United States; Bello J.P., New York University, United States","We present a criterion to generate audible summaries of music recordings that optimally explain a given track with mutually disjoint segments of itself. We represent audio as sequences of beat-synchronous harmonic features and use an exhaustive search to identify the best summary. To demonstrate the merit of this approach, we evaluate the criterion and show consistency across a collection of multiple recordings of different works. Finally, we present a fast algorithm that approximates the exhaustive search and allows us to automatically learn the hyperparameters of the algorithm for a given track. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Disjoint segments; Exhaustive search; Fast algorithms; Hyperparameters; Music recording; Audio recordings","O. Nieto; New York University, United States; email: oriol@nyu.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Sprechmann P.; Bronstein A.; Sapiro G.","Sprechmann, Pablo (23010370200); Bronstein, Alex (7103163126); Sapiro, Guillermo (7005450011)","23010370200; 7103163126; 7005450011","Real-time online singing voice separation from monaural recordings using robust low-rank modeling","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","56","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423755&partnerID=40&md5=28847d3a7d519d59ba2bb71aa51ef967","University of Minnesota, United States; Tel Aviv University, Israel","Sprechmann P., University of Minnesota, United States; Bronstein A., Tel Aviv University, Israel; Sapiro G., University of Minnesota, United States","Separating the leading vocals from the musical accompaniment is a challenging task that appears naturally in several music processing applications. Robust principal component analysis (RPCA) has been recently employed to this problem producing very successful results. The method decomposes the signal into a low-rank component corresponding to the accompaniment with its repetitive structure, and a sparse component corresponding to the voice with its quasi-harmonic structure. In this paper we first introduce a non-negative variant of RPCA, termed as robust low-rank non-negative matrix factorization (RNMF). This new framework better suits audio applications. We then propose two efficient feed-forward architectures that approximate the RPCA and RNMF with low latency and a fraction of the complexity of the original optimization method. These approximants allow incorporating elements of unsupervised, semi- and fully-supervised learning into the RPCA and RNMF frameworks. Our basic implementation shows several orders of magnitude speedup compared to the exact solvers with no performance degradation, and allows online and faster-than-real-time processing. Evaluation on the MIR-1K dataset demonstrates state-of-the-art performance. © 2012 International Society for Music Information Retrieval.","","Principal component analysis; Approximants; Audio applications; Data sets; Feed-Forward; Low latency; Nonnegative matrix factorization; Optimization method; Orders of magnitude; Performance degradation; Processing applications; Repetitive structure; Robust principal component analysis; State-of-the-art performance; Voice separation; Information retrieval","P. Sprechmann; University of Minnesota, United States; email: sprec009@umn.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Schreiber H.; Grosche P.; Müller M.","Schreiber, Hendrik (55586286200); Grosche, Peter (55413290700); Müller, Meinard (7404689873)","55586286200; 55413290700; 7404689873","A re-ordering strategy for accelerating index-based audio fingerprinting","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873608121&partnerID=40&md5=7161738ebad52bc6c283105fdf073664","Tagtraum Industries Incorporated, United States; MPI Informatik, Saarland University, Germany","Schreiber H., Tagtraum Industries Incorporated, United States; Grosche P., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany","The Haitsma/Kalker audio fingerprinting system [4] has been in use for years, but its search algorithm's scalability has not been researched very well. In this paper we show that by simple re-ordering of the query fingerprint's subprints in the index-based retrieval step, the overall search performance can be increased significantly. Furthermore, we show that combining longer fingerprints with re-ordering can lead to even higher performance gains, up to a factor of 9.8. The proposed re-ordering scheme is based on the observation that sub-prints, which are elements of n-runs of identical consecutive sub-prints, have a higher survival rate in distorted copies of a signal (e.g. after mp3 compression) than other sub-prints. © 2011 International Society for Music Information Retrieval.","","Audio systems; Audio fingerprinting; MP3 compression; Performance Gain; Query fingerprint; Re orderings; Search Algorithms; Search performance; Survival rate; Information retrieval","H. Schreiber; Tagtraum Industries Incorporated, United States; email: hs@tagtraum.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hu X.; Kando N.","Hu, Xiao (55496358400); Kando, Noriko (6602320153)","55496358400; 6602320153","User-centered Measures Vs. System effectiveness in finding similar songs","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422267&partnerID=40&md5=2ffc6325846771416558dc7d4adbefd3","Faculty of Education, University of Hong Kong, Hong Kong; National Institute of Informatics, Japan","Hu X., Faculty of Education, University of Hong Kong, Hong Kong; Kando N., National Institute of Informatics, Japan","User evaluation in the domain of Music Information Retrieval (MIR) has been very scarce, while algorithms and systems in MIR have been improving rapidly. With the maturity of system-centered evaluation in MIR, time is ripe for MIR evaluation to involve users. In this study, we compare user-centered measures to a system effectiveness measure on the task of retrieving similar songs. To collect user-centered measures, we conducted a user experiment with 50 participants using a set of music retrieval systems that have been evaluated by a system-centered approach in the Music Information Retrieval Evaluation eXchange (MIREX). The results reveal weak correlation between user-centered measures and system effectiveness. It is also found that user-centered measures can disclose difference between systems when there was no difference on system-effectiveness. © 2012 International Society for Music Information Retrieval.","","Music information retrieval; Music retrieval systems; System effectiveness; User evaluations; User-centered measures; Weak correlation; Information retrieval","X. Hu; Faculty of Education, University of Hong Kong, Hong Kong; email: xiaoxhu@hku.hk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Schmidt E.M.; Scott J.; Kim Y.E.","Schmidt, Erik M. (36053813000); Scott, Jeffrey (35312262000); Kim, Youngmoo E. (24724623000)","36053813000; 35312262000; 24724623000","Feature learning in dynamic environments: Modeling the acoustic structure of musical emotion","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420121&partnerID=40&md5=17c484752499451352316d4166a625cb","Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States","Schmidt E.M., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States; Scott J., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States","While emotion-based music organization is a natural process for humans, quantifying it empirically proves to be a very difficult task, and as such no dominant feature representation for music emotion recognition has yet emerged. Much of the difficulty in developing emotion-based features is the ambiguity of the ground-truth. Even using the smallest time window, opinions about emotion are bound to vary and reflect some disagreement between listeners. In previous work, we have modeled human response labels to music in the arousal-valence (A-V) emotion space with time-varying stochastic distributions. Current methods for automatic detection of emotion in music seek performance increases by combining several feature domains (e.g. loudness, timbre, harmony, rhythm). Such work has focused largely in dimensionality reduction for minor classification performance gains, but has provided little insight into the relationship between audio and emotional associations. In this work, we seek to employ regression-based deep belief networks to learn features directly from magnitude spectra. Taking into account the dynamic nature of music, we investigate combining multiple timescales of aggregated magnitude spectra as a basis for feature learning. © 2012 International Society for Music Information Retrieval.","","Acoustic structures; Automatic detection of emotion; Classification performance; Deep belief networks; Dimensionality reduction; Dynamic environments; Dynamic nature; Emotion recognition; Feature domain; Feature learning; Feature representation; Human response; Magnitude spectrum; Multiple timescales; Musical emotion; Natural process; Stochastic distribution; Time varying; Time windows; Information retrieval","E.M. Schmidt; Music and Entertainment Technology Laboratory (MET-Lab.), Electrical and Computer Engineering, Drexel University, United States; email: eschmidt@drexel.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Mauch M.; Levy M.","Mauch, Matthias (36461512900); Levy, Mark (14021515000)","36461512900; 14021515000","Structural change on multiple time scales as a correlate of musical complexity","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596725&partnerID=40&md5=98a4c15c900c57e5a15d342306c116d1","Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom","Mauch M., Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom; Levy M., Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom","We propose the novel audio feature structural change for the analysis and visualisation of recorded music, and argue that it is related to a particular notion of musical complexity. Structural change is a meta feature that can be calculated from an arbitrary frame-wise basis feature, with each element in the structural change feature vector representing the change of the basis feature at a different time scale. We describe an efficient implementation of the feature and discuss its properties based on three basis features pertaining to harmony, rhythm and timbre. We present a novel flowerlike visualisation that allows us to illustrate the overall structural change characteristics of a piece of audio in a compact way. Several examples of real-world music and synthesised audio exemplify the characteristics of the structural change feature. We present the results of a web-based listening experiment with 197 participants to show the validity of the proposed feature. © 2011 International Society for Music Information Retrieval.","Audio; Musical complexity; Visualisation","Information retrieval; Visualization; Audio; Audio features; Different time scale; Efficient implementation; Feature vectors; Meta-features; Multiple time scale; Musical complexity; Real-world; Structural change; Synthesised; Audio acoustics","M. Mauch; Karen House, Last.fm, London, N1 6DL, 1-11 Bache's Street, United Kingdom; email: matthias@last.fm","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Gulluni S.; Buisson O.; Essid S.; Richard G.","Gulluni, Sébastien (55585918700); Buisson, Olivier (6701352313); Essid, Slim (16033218700); Richard, Gaël (57195915952)","55585918700; 6701352313; 16033218700; 57195915952","An interactive system for electro-acoustic music analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597139&partnerID=40&md5=b9d53692b2abce4b9babcf5700e3fc0e","Institut National de L'Audiovisuel, Bry-sur-marne, France; Institut Telecom, Telecom ParisTech, Paris, France","Gulluni S., Institut National de L'Audiovisuel, Bry-sur-marne, France; Buisson O., Institut National de L'Audiovisuel, Bry-sur-marne, France; Essid S., Institut Telecom, Telecom ParisTech, Paris, France; Richard G., Institut Telecom, Telecom ParisTech, Paris, France","This paper, presents an interactive approach for the analysis of electro-acoustic music. An original classification scheme is devised using relevance feedback and active-learning segment selection in an interactive loop. Validation and correction information given by the user is injected in the learning process at each iteration to achieve more accurate classification. An experimental study is conducted to evaluate and compare the different classification and relevance feedback approaches that are envisaged, using a database of polyphonic pieces (with a varying degree of polyphony). The results show that the different approaches are adapted to different applications and they achieve satisfying performance in a reasonable number of iterations. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Classification scheme; Electro-acoustic; Experimental studies; Interactive approach; Interactive system; Learning process; Music analysis; Number of iterations; Relevance feedback; Segment selection; Computer music","S. Gulluni; Institut National de L'Audiovisuel, Bry-sur-marne, France; email: sgulluni@ina.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Dieleman S.; Brakel P.; Schrauwen B.","Dieleman, Sander (55418865400); Brakel, Philémon (55418485600); Schrauwen, Benjamin (22941905500)","55418865400; 55418485600; 22941905500","Audio-based music classification with a pretrained convolutional network","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","93","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602768&partnerID=40&md5=d7898fbba31372e9d54480dc8e638c5d","Electronics and Information Systems Department, Ghent University, Belgium","Dieleman S., Electronics and Information Systems Department, Ghent University, Belgium; Brakel P., Electronics and Information Systems Department, Ghent University, Belgium; Schrauwen B., Electronics and Information Systems Department, Ghent University, Belgium","Recently the 'Million Song Dataset', containing audio features and metadata for one million songs, was made available. In this paper, we build a convolutional network that is then trained to perform artist recognition, genre recognition and key detection. The network is tailored to summarize the audio features over musically significant timescales. It is infeasible to train the network on all available data in a supervised fashion, so we use unsupervised pretraining to be able to harness the entire dataset: we train a convolutional deep belief network on all data, and then use the learnt parameters to initialize a convolutional multilayer perceptron with the same architecture. The MLP is then trained on a labeled subset of the data for each task. We also train the same MLP with randomly initialized weights. We find that our convolutional approach improves accuracy for the genre recognition and artist recognition tasks. Unsupervised pretraining improves convergence speed in all cases. For artist recognition it improves accuracy as well. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Audio features; Audio-based; Convergence speed; Convolutional networks; Deep belief networks; Multi layer perceptron; Music classification; Pre-training; Time-scales; Convolution","S. Dieleman; Electronics and Information Systems Department, Ghent University, Belgium; email: sander.dieleman@elis.ugent.be","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bohak C.; Marolt M.","Bohak, Ciril (55582372100); Marolt, Matija (6603601816)","55582372100; 6603601816","Finding repeating stanzas in folk songs","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431799&partnerID=40&md5=416b509198c5e3bb1c5dffaf7e182771","University of Ljubljana, Slovenia","Bohak C., University of Ljubljana, Slovenia; Marolt M., University of Ljubljana, Slovenia","Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Audio segmentation; Folk songs; Global thresholding; Music recording; Peak-picking method; Pitch changes; Scoring functions; Segmentation methods; Similarity measure; Audio recordings","C. Bohak; University of Ljubljana, Slovenia; email: ciril.bohak@fri.uni-lj.si","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Velasco M.J.; Large E.W.","Velasco, Marc J. (35410678700); Large, Edward W. (6604072204)","35410678700; 6604072204","Pulse detection in syncopated rhythms using neural oscillators","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604700&partnerID=40&md5=0f132aa18bd198d1509d365fd8ff83c3","Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States","Velasco M.J., Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States; Large E.W., Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States","Pulse and meter are remarkable in part because these perceived periodicities can arise from rhythmic stimuli that are not periodic. This phenomenon is most striking in syncopated rhythms, found in many genres of music, including music of non-Western cultures. In general, syncopated rhythms may have energy at frequencies that do not correspond to perceived pulse or meter, and perceived metrical frequencies that are weak or absent in the objective rhythmic stimulus. In this paper, we consider syncopated rhythms that contain little or no energy at the pulse frequency. We used 16 rhythms (3 simple, 13 syncopated) to test a model of pulse/meter perception based on nonlinear resonance, comparing the nonlinear resonance model with a linear analysis. Both models displayed the ability to differentiate between duple and triple meters, however, only the nonlinear model exhibited resonance at the pulse frequency for the most challenging syncopated rhythms. This result suggests that nonlinear resonance may provide a viable approach to pulse detection in syncopated rhythms. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Linear analysis; Neural oscillator; Non-linear model; Nonlinear resonance; Perception-based; Pulse detection; Pulse frequencies; Resonance","M.J. Velasco; Center for Complex Systems and Brain Sciences, Florida Atlantic University, United States; email: velasco@ccs.fau.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Yoshii K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","7103400120; 7403505330","A vocabulary-free infinity-gram model for nonparametric bayesian chord progression analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597170&partnerID=40&md5=9d51b1a072c192a21ba77e9c41452759","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents probabilistic n-gram models for symbolic chord sequences. To overcome the fundamental limitations in conventional models-that the model optimality is not guaranteed, that the value of n is fixed uniquely, and that a vocabulary of chord types (e.g., major, minor, · · · ) is defined in an arbitrary way-we propose a vocabulary-free infinity-gram model based on Bayesian nonparametrics. It accepts any combinations of notes as chord types and allows each chord appearing in a sequence to have an unbounded and variable-length context. All possibilities of n are taken into account when calculating the predictive probability of a next chord given a particular context, and when an unseen chord type emerges we can avoid out-of-vocabulary error by adaptively evaluating the 0-gram probability, i.e., the combinatorial probability of note components. Our experiments using Beatles songs showed that the predictive performance of the proposed model is better than that of the state-of-theart models and that we could find stochastically-coherent chord patterns by sorting variable-length n-grams in a line according to their generative probabilities. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Bayesian nonparametrics; Chord sequence; Fundamental limitations; Model-based OPC; N-gram models; N-grams; Non-parametric Bayesian; Optimality; Predictive performance; Progression analysis; Probability","K. Yoshii; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: k.yoshii@aist.go.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Corrêa D.C.; Costa L.D.F.; Levada A.L.M.","Corrêa, Débora C. (24823924200); Costa, Luciano Da F. (35582873300); Levada, Alexandre L.M. (24468560700)","24823924200; 35582873300; 24468560700","Finding community structure in music genres networks","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588177&partnerID=40&md5=95e4979068b6fa24f43a6026bc4c7dc3","Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; Departamento de Computação, Universidade Federal de São Carlos, Brazil","Corrêa D.C., Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; Costa L.D.F., Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; Levada A.L.M., Departamento de Computação, Universidade Federal de São Carlos, Brazil","Complex networks have shown to be promising mechanisms to represent several aspects of nature, since their topological and structural features help in the understanding of relations, properties and intrinsic characteristics of the data. In this context, we propose to build music networks in order to find community structures of music genres. Our main contributions are twofold: 1) Define a totally unsupervised approach for music genres discrimination; 2) Incorporate topological features in music data analysis. We compared different distance metrics and clustering algorithms. Each song is represented by a vector of conditional probabilities for the note values in its percussion track. Initial results indicate the effectiveness of the proposed methodology. © 2011 International Society for Music Information Retrieval.","","Clustering algorithms; Information retrieval; Topology; Community structures; Conditional probabilities; Distance metrics; Intrinsic characteristics; Music genre; Structural feature; Topological features; Unsupervised approaches; Social sciences","D.C. Corrêa; Instituto de Física de São Carlos, Universidade de São Paulo, Brazil; email: debcris.cor@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Niitsuma M.; Tomita Y.","Niitsuma, Masahiro (36638174100); Tomita, Yo (8535749100)","36638174100; 8535749100","Classifying bach's handwritten c-clefs","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873589025&partnerID=40&md5=ed44ac026578a3d5994947acce307474","School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom","Niitsuma M., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; Tomita Y., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom","The aim of this study is to explore how we could use computational technology to help determination of the chronology of music manuscripts. Applying a battery of techniques to Bach's manuscripts reveals the limitation in current image processing techniques, thereby clarifying future tasks. Analysis of C-clefs, the chosen musical symbol for this study, extracted from Bach's manuscripts dating from 1708-1748, is also carried out. Random forest using 15 features produces significant accuracy for chronological classification. © 2011 International Society for Music Information Retrieval.","","Decision trees; Image processing; Computational technology; Current image; Musical symbols; Random forests; Information retrieval","M. Niitsuma; School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; email: mniitsuma01@qub.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Xia G.; Liang D.; Dannenberg R.B.; Harvilla M.J.","Xia, Guangyu (55586682600); Liang, Dawen (55586031200); Dannenberg, Roger B. (7003266250); Harvilla, Mark J. (55350796800)","55586682600; 55586031200; 7003266250; 55350796800","Segmentation, clustering, and display in a personal audio database for musicians","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596073&partnerID=40&md5=2bfc516193318f0d02191f3a5643c345","Carnegie Mellon University, United States","Xia G., Carnegie Mellon University, United States; Liang D., Carnegie Mellon University, United States; Dannenberg R.B., Carnegie Mellon University, United States; Harvilla M.J., Carnegie Mellon University, United States","Managing music audio databases for practicing musicians presents new and interesting challenges. We describe a systematic investigation to provide useful capabilities to musicians both in rehearsal and when practicing alone. Our goal is to allow musicians to automatically record, organize, and retrieve rehearsal (and other) audio to facilitate review and practice (for example, playing along with difficult passages). We introduce a novel music classification system based on Eigenmusic and Adaboost to separate rehearsal recordings into segments, an unsupervised clustering and alignment process to organize segments, and a digital music display interface that provides both graphical input and output in terms of conventional music notation. © 2011 International Society for Music Information Retrieval.","","Adaptive boosting; Information retrieval; Audio database; Digital music; Display interfaces; Input and outputs; Music classification; Music notation; Systematic investigations; Unsupervised clustering; Audio acoustics","G. Xia; Carnegie Mellon University, United States; email: gxia@andrew.cmu.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Salamon J.; Peeters G.; Robel A.","Salamon, Justin (55184866100); Peeters, Geoffroy (22433836000); Robel, Axel (7801333053)","55184866100; 22433836000; 7801333053","Statistical characterisation of melodic pitch contours and its application for melody extraction","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420493&partnerID=40&md5=4f6e734462e094d45d03d7e12b50bfcf","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sound Analysis-Synthesis Team, IRCAM, CNRS STMS, 75004 Paris, France","Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Peeters G., Sound Analysis-Synthesis Team, IRCAM, CNRS STMS, 75004 Paris, France; Robel A., Sound Analysis-Synthesis Team, IRCAM, CNRS STMS, 75004 Paris, France","In this paper we present a method for the statistical characterisation of melodic pitch contours, and apply it to automatic melody extraction from polyphonic music signals. Within the context of melody extraction, pitch contours represent time and frequency continuous sequences of pitch candidates out of which the melody must be selected. In previous studies we presented a melody extraction algorithm in which contour features are used in a heuristic manner to filter out non-melodic contours. In our current work, we present a method for the statistical modelling of these features, and propose an algorithm for melody extraction based on the obtained model. The algorithm exploits the learned model to compute a ""melodiness"" index for each pitch contour, which is then used to select the melody out of all pitch contours generated for an excerpt of polyphonic music. The proposed approach has the advantage that new contour features can be easily incorporated into the model without the need to manually devise rules to address each feature individually. The method is evaluated in the context of melody extraction and obtains promising results, performing comparably to a state-of-the-art heuristic-based algorithm. © 2012 International Society for Music Information Retrieval.","","Algorithms; Heuristic methods; Information retrieval; Contour features; Extraction algorithms; Pitch contours; Polyphonic music; Statistical modelling; Mathematical models","J. Salamon; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: justin.salamon@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Yoshii K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","7103400120; 7403505330","Infinite composite autoregressive models for music signal analysis","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433845&partnerID=40&md5=02a8b4fc55e2d500ed31d48770810f4b","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents novel probabilistic models that can be used to estimate multiple fundamental frequencies (F0s) from polyphonic audio signals. These models are nonpara-metric Bayesian extensions of nonnegative matrix factorization (NMF) based on the source-filter paradigm, and in them an amplitude or power spectrogram is decomposed as the product of two kinds of spectral atoms (sources and filters) and time-varying gains of source-filter pairs. In this study we model musical instruments as autoregressive systems that combine two types of sources - periodic signals (comb-shaped densities) and white noise (flat density) - with all-pole filters representing resonance characteristics. One of the main problems with such composite autoregressive models (CARMs) is that the numbers of sources and filters should be given in advance. To solve this problem, we propose nonparametric Bayesian models based on gamma processes and efficient variational and multiplicative learning algorithms. These infinite CARMs (iCARMs) can discover appropriate numbers of sources and filters in a data-driven manner. We report the experimental results of multipitch analysis on the MAPS piano database. © 2012 International Society for Music Information Retrieval.","","Bayesian networks; Information retrieval; Learning algorithms; Musical instruments; Signal analysis; White noise; All-pole filters; Audio signal; Auto regressive models; Autoregressive systems; Fundamental frequencies; Gamma process; Music signal analysis; Non-parametric Bayesian; Nonnegative matrix factorization; Periodic signal; Probabilistic models; Resonance characteristic; Spectrograms; Time-varying gains; Audio signal processing","K. Yoshii; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: k.yoshii@aist.go.jp","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Lee J.H.; Cunningham S.J.","Lee, Jin Ha (57190797465); Cunningham, Sally Jo (7201937110)","57190797465; 7201937110","The impact (or non-impact) of user studies in Music Information Retrieval","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435748&partnerID=40&md5=da5f1c1029a68923afb76674c35c707f","Information School, University of Washington, United States; Department of Computer Science, University of Waikato, New Zealand","Lee J.H., Information School, University of Washington, United States; Cunningham S.J., Department of Computer Science, University of Waikato, New Zealand","Most Music Information Retrieval (MIR) researchers will agree that understanding users' needs and behaviors is critical for developing a good MIR system. The number of user studies in the MIR domain has been gradually increasing since the early 2000s reflecting the need for empirical studies of users. However, despite the growing number of user studies and the wide recognition of their importance, it is unclear how large their impact has been in the field; on how systems are developed, evaluation tasks are created, and how we understand critical concepts such as music similarity or music mood. In this paper, we present our analysis on the growth, publication and citation patterns, and design of 155 user studies. This is followed by a discussion of a number of issues/challenges in conducting MIR user studies and distributing the research results. We conclude by making recommendations to increase the visibility and impact of user studies in the field. © 2012 International Society for Music Information Retrieval.","","Empirical studies; Music information retrieval; Music similarity; Research results; User study; Information retrieval","J.H. Lee; Information School, University of Washington, United States; email: jinhalee@uw.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Sioros G.; Holzapfel A.; Guedes C.","Sioros, George (23091906900); Holzapfel, André (18041818000); Guedes, Carlos (23134475200)","23091906900; 18041818000; 23134475200","On measuring syncopation to drive an interactive music system","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426250&partnerID=40&md5=62c01ed7ecf234aba7af39182319768e","Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal; Music Technology Group, Universitat Pompeu Fabra, Spain","Sioros G., Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal; Holzapfel A., Music Technology Group, Universitat Pompeu Fabra, Spain; Guedes C., Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal","In this paper we address the problem of measuring syncopation in order to mediate a musically meaningful interaction between a live music performance and an automatically generated rhythm. To this end we present a simple, yet effective interactive music system we developed. We shed some light on the complex nature of syncopation by looking into MIDI data from drum loops and whole songs. We conclude that segregation into individual rhythmic layers is necessary in order to measure the syncopation of a music ensemble. This implies that measuring syncopation on polyphonic audio signals is not yet tractable using the current state-of-the-art in audio analysis. © 2012 International Society for Music Information Retrieval.","","Information retrieval; Audio analysis; Audio signal; Automatically generated; Complex nature; Interactive music; Music performance; Digital storage","G. Sioros; Faculdade de Engenharia, Universidade do Porto, INESC Porto, Portugal; email: gsioros@gmail.com","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Salamon J.; Urbano J.","Salamon, Justin (55184866100); Urbano, Julian (36118414700)","55184866100; 36118414700","Current challenges in the evaluation of predominant Melody Extraction algorithms","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432474&partnerID=40&md5=f3c6465db186d1528e4c7801915e025d","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Department of Computer Science, University Carlos III of Madrid, Leganés, Spain","Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Urbano J., Department of Computer Science, University Carlos III of Madrid, Leganés, Spain","In this paper we analyze the reliability of the evaluation of Audio Melody Extraction algorithms. We focus on the procedures and collections currently used as part of the annual Music Information Retrieval Evaluation eXchange (MIREX), which has become the de-facto benchmark for evaluating and comparing melody extraction algorithms. We study several factors: the duration of the audio clips, time offsets in the ground truth annotations, and the size and musical content of the collection. The results show that the clips currently used are too short to predict performance on full songs, highlighting the paramount need to use complete musical pieces. Concerning the ground truth, we show how a minor error, specifically a time offset between the annotation and the audio, can have a dramatic effect on the results, emphasizing the importance of establishing a common protocol for ground truth annotation and system output. We also show that results based on the small ADC04, MIREX05 and INDIAN08 collections are unreliable, while the MIREX09 collections are larger than necessary. This evidences the need for new and larger collections containing realistic music material, for reliable and meaningful evaluation of Audio Melody Extraction. © 2012 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Audio clips; Extraction algorithms; Ground truth; Music information retrieval; Music materials; Musical pieces; System output; Time offsets; Audio acoustics","J. Salamon; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: justin.salamon@upf.edu","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Şentürk S.; Chordia P.","Şentürk, Sertan (43461595100); Chordia, Parag (24723695700)","43461595100; 24723695700","Modeling melodic improvisation in turkish folk music using variable-length markov models","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598006&partnerID=40&md5=feefe30eed296b3a95fa93b65cfd9a1a","Georgia Tech Center for Music Technology, Atlanta, GA, United States","Şentürk S., Georgia Tech Center for Music Technology, Atlanta, GA, United States; Chordia P., Georgia Tech Center for Music Technology, Atlanta, GA, United States","The paper describes a new database, which currently consists of 64 songs encompassing approximately 6600 notes, and a system, which uses Variable-Length Markov Models (VLMM) to predict the melodies in the uzun hava (long tune) form, a melodic structure in Turkish folk music. The work shows VLMMs are highly predictive. This suggests that variable-length Markov models (VLMMs) may be applied to makam-based and non-metered musical forms, in addition to Western musical traditions. To the best of our knowledge, the work presents the first symbolic, machine readable database of uzun havas and the first application of predictive modeling in Turkish folk music. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Markov processes; Melodic structure; Predictive modeling; Turkishs; Variable-length markov models; Computer music","S. Şentürk; Georgia Tech Center for Music Technology, Atlanta, GA, United States; email: sertansenturk@gatech.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Wolff D.; Weyde T.; Stober S.; Nürnberger A.","Wolff, Daniel (35091498700); Weyde, Tillman (24476899500); Stober, Sebastian (14027561800); Nürnberger, Andreas (14027288100)","35091498700; 24476899500; 14027561800; 14027288100","A systematic comparison of music similarity adaptation approaches","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436680&partnerID=40&md5=61b26b28bc09e2315e8e372a30a72776","MIRG, School of Informatics, City University London, United Kingdom; Data and Knowledge Engineering Group, Otto-von-Guericke-Universität, Magdeburg, Germany","Wolff D., MIRG, School of Informatics, City University London, United Kingdom; Weyde T., MIRG, School of Informatics, City University London, United Kingdom; Stober S., Data and Knowledge Engineering Group, Otto-von-Guericke-Universität, Magdeburg, Germany; Nürnberger A., Data and Knowledge Engineering Group, Otto-von-Guericke-Universität, Magdeburg, Germany","In order to support individual user perspectives and different retrieval tasks, music similarity can no longer be considered as a static element of Music Information Retrieval (MIR) systems. Various approaches have been proposed recently that allow dynamic adaptation of music similarity measures. This paper provides a systematic comparison of algorithms for metric learning and higher-level facet distance weighting on the MagnaTagATune dataset. A cross-validation variant taking into account clip availability is presented. Applied on user generated similarity data, its effect on adaptation performance is analyzed. Special attention is paid to the amount of training data necessary for making similarity predictions on unknown data, the number of model parameters and the amount of information available about the music itself. © 2012 International Society for Music Information Retrieval.","","Amount of information; Cross validation; Data sets; Dynamic adaptations; Metric learning; Model parameters; Music information retrieval; Music similarity; Training data; Information retrieval","D. Wolff; MIRG, School of Informatics, City University London, United Kingdom; email: daniel.wolff.1@city.ac.uk","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Anan Y.; Hatano K.; Bannai H.; Takeda M.; Satoh K.","Anan, Yoko (55582495800); Hatano, Kohei (14021639200); Bannai, Hideo (7004530566); Takeda, Masayuki (7403299627); Satoh, Ken (7403151118)","55582495800; 14021639200; 7004530566; 7403299627; 7403151118","Polyphonic music classification on symbolic data using dissimilarity functions","2012","Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415756&partnerID=40&md5=692617e8e75f6f2c0c6f842cdac99d5b","Department of Informatics, Kyushu University, Japan; National Institute of Informatics, Japan","Anan Y., Department of Informatics, Kyushu University, Japan; Hatano K., Department of Informatics, Kyushu University, Japan; Bannai H., Department of Informatics, Kyushu University, Japan; Takeda M., Department of Informatics, Kyushu University, Japan; Satoh K., National Institute of Informatics, Japan","This paper addresses the polyphonic music classification problem on symbolic data. A new method is proposed which converts music pieces into binary chroma vector sequences and then classifies them by applying the dissimilarity-based classification method TWIST proposed in our previous work. One advantage of using TWIST is that it works with any dissimilarity measure. Computational experiments show that the proposed method drastically outperforms SVM and k-NN, the state-of-the-art classification methods. © 2012 International Society for Music Information Retrieval.","","Binary sequences; Information retrieval; Classification methods; Computational experiment; Dissimilarity function; Dissimilarity measures; Polyphonic music; Symbolic data; Classification (of information)","Y. Anan; Department of Informatics, Kyushu University, Japan; email: yoko.anan@inf.kyushu-u.ac.jp","","13th International Society for Music Information Retrieval Conference, ISMIR 2012","8 October 2012 through 12 October 2012","Porto","95398"
"Foster P.; Klapuri A.; Plumbley M.D.","Foster, Peter (55328977600); Klapuri, Anssi (6602945099); Plumbley, Mark D. (6603774794)","55328977600; 6602945099; 6603774794","Causal prediction of continuous-valued music features","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606282&partnerID=40&md5=a9ba8481e2d9b1365691bf260e5ea985","Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","Foster P., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Klapuri A., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Plumbley M.D., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","This paper investigates techniques for predicting sequences of continuous-valued feature vectors extracted from musical audio. In particular, we consider prediction of beatsynchronous Mel-frequency cepstral coefficients and chroma features in a causal setting, where features are predicted as they unfold in time. The methods studied comprise autoregressive models, N-gram models incorporating a smoothing scheme, and a novel technique based on repetition detection using a self-distance matrix. Furthermore, we propose a method for combining predictors, which relies on a running estimate of the error variance of the predictors to inform a linear weighting of the predictor outputs. Results indicate that incorporating information on long-term structure improves the prediction performance for continuous-valued, sequential musical data. For the Beatles data set, combining the proposed self-distance based predictor with both N-gram and autoregressive methods results in an average of 13% improvement compared to a linear predictive baseline. © 2011 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Auto regressive models; Autoregressive methods; Chroma features; Data set; Error variance; Feature vectors; Long-term structures; Mel-frequency cepstral coefficients; Musical audio; N-gram models; Novel techniques; Prediction performance; Repetition detections; Self-distance; Forecasting","P. Foster; Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; email: peter.foster@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Gunaratna C.; Stoner E.; Menezes R.","Gunaratna, Charith (53879749900); Stoner, Evan (55389093900); Menezes, Ronaldo (7006750290)","53879749900; 55389093900; 7006750290","Using network sciences to rank musicians and composers in brazilian popular music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592265&partnerID=40&md5=1d6fa35c7d0e26ee940c9090ebb054bf","Computer Sciences, Florida Tech, United States","Gunaratna C., Computer Sciences, Florida Tech, United States; Stoner E., Computer Sciences, Florida Tech, United States; Menezes R., Computer Sciences, Florida Tech, United States","Music fascinates and touches most people. This fascination leads to opinions about the music pieces that reflects people's exposure and personal experience. This inherent bias of people towards music indicates that personal opinion is inappropriate for defining the quality of music and musicians. This paper takes a holistic view of the problem and delves into the understanding of the structure of Brazilian music rooted in Network Sciences. In this paper we work with a large database of albums of Brazilian music and study the structure of collaborations between all the musicians and composers. The collaboration is modelled as a social network of musicians and then analyzed from different perspectives with the goal of describing what we call the structure of that musical genre as well as provide a ranking of musicians and composers. © 2011 International Society for Music Information Retrieval.","","Brazilian music; Holistic view; In networks; Large database; Musical genre; Network science; Personal experience; Popular music; Social Networks; Information retrieval","C. Gunaratna; Computer Sciences, Florida Tech, United States; email: agunaratna2009@my.fit.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Stowell D.; Dixon S.","Stowell, Dan (26649050000); Dixon, Simon (7201479437)","26649050000; 7201479437","Mir in school? Lessons from ethnographic observation of secondary school music classes","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585522&partnerID=40&md5=e3a7f71b6dbef4a160c582cc753eea1c","Centre for Digital Music, Queen Mary University of London, United Kingdom","Stowell D., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","To help maximise the usefulness of MIR technologies in the wider community, we conducted an ethnographic study of music lessons in secondary schools in London, UK. The purpose is to understand better how musical concepts are negotiated with and without technology, so we can understand when and how MIR tools might be useful. We report on some of the themes uncovered, both about the range of technologies deployed in schools and about the ways different musical concepts are discussed. Importantly, this rich observation elicits some of the nuances between various highand low-technologies. In particular, we discuss issues of multimodality and the role of technologies such as Youtube, as well as specific issues around musical concepts such as genre and rhythm. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Ethnographic observations; Ethnographic study; London , UK; Multi-modality; Musical concepts; Role of technologies; Secondary schools; YouTube; Technology","D. Stowell; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: dan.stowell@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Weigl D.M.; Guastavino C.","Weigl, David M. (55359633400); Guastavino, Catherine (8239911100)","55359633400; 8239911100","User studies in the music information retrieval literature","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433514&partnerID=40&md5=7db322a247f520c389d3d5ede988ff62","School of Information Studies, McGill University, Montreal QC, Canada","Weigl D.M., School of Information Studies, McGill University, Montreal QC, Canada; Guastavino C., School of Information Studies, McGill University, Montreal QC, Canada","This paper presents an overview of user studies in the Music Information Retrieval (MIR) literature. A focus on the user has repeatedly been identified as a key requirement for future MIR research; yet empirical user studies have been relatively sparse in the literature, the overwhelming research attention in MIR remaining systems-focused. We present research topics, methodologies, and design implications covered in the user studies conducted thus far. © 2011 International Society for Music Information Retrieval.","","Research; Design implications; Music information retrieval; Research topics; User study; Information retrieval","D.M. Weigl; School of Information Studies, McGill University, Montreal QC, Canada; email: david.weigl@mail.mcgill.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Burgoyne J.A.; Wild J.; Fujinaga I.","Burgoyne, John Ashley (23007865600); Wild, Jonathan (57204258099); Fujinaga, Ichiro (9038140900)","23007865600; 57204258099; 9038140900","An expert ground-truth set for audio chord recognition and music analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","89","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572845&partnerID=40&md5=f33417e99e623642b55ece9762284ed3","Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Wild J., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada","Audio chord recognition has attracted much interest in recent years, but a severe lack of reliable training data-both in terms of quantity and range of sampling-has hindered progress. Working with a team of trained jazz musicians, we have collected time-aligned transcriptions of the harmony in more than a thousand songs selected randomly from the Billboard ""Hot 100"" chart in the United States between 1958 and 1991. These transcriptions contain complete information about upper extensions and alterations as well as information about meter, phrase, and larger musical structure. We expect that these transcriptions will enable significant advances in the quality of training for audio-chord-recognition algorithms, and furthermore, because of an innovative sampling methodology, the data are usable as they stand for computational musicology. The paper includes some summary figures and statistics to help readers understand the scope of the data as well as information for obtaining the transcriptions for their own research. © 2011 International Society for Music Information Retrieval.","","Hot working; Information retrieval; Chord recognition; Complete information; Music analysis; Musical structures; Transcription","J.A. Burgoyne; Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montréal, QC, Canada; email: ashley@music.mcgill.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Six J.; Cornelis O.","Six, Joren (55585855700); Cornelis, Olmo (27267474100)","55585855700; 27267474100","Tarsos - A platform to explore pitch scales in non-western and western music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871334099&partnerID=40&md5=d3d11dbbff6589476bd5d05ac897589e","Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium","Six J., Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium; Cornelis O., Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium","This paper presents Tarsos 1 , a modular software platform to extract and analyze pitch and scale organization in music, especially geared towards the analysis of non-Western music. Tarsos aims to be a user-friendly, graphical tool to explore tone scales and pitch organization in music of the world. With Tarsos pitch annotations are extracted from an audio signal that are then processed to form musicologically meaningful representations. These representations cover more than the typicalWestern 12 pitch classes, since a fine-grained resolution of 1200 cents is used. Both scales with and without octave equivalence can be displayed graphically. The Tarsos API 2 creates opportunities to analyse large sets of - ethnic - music automatically. The graphical user interface can be used for detailed, manually adjusted analysis of specific songs. Several output modalities make Tarsos an interesting tool for musicological analysis, educational purposes and even for artistic productions. © 2011 International Society for Music Information Retrieval.","","Graphical user interfaces; Audio signal; Graphical tools; Modular softwares; Output modality; Tone scale; Information retrieval","J. Six; Royal Academy of Fine Arts and Royal Conservatory, University College Ghent, Belgium; email: joren.six@hogent.be","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Duggan B.; O'Shea B.","Duggan, Bryan (24823868500); O'Shea, Brendan (23390273100)","24823868500; 23390273100","Tunepal - Disseminating a music information retrieval system to the traditional irish music community","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601771&partnerID=40&md5=89bba0c20694990a62e33a80518310aa","Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland","Duggan B., Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland; O'Shea B., Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland","In this paper we present two new query-by-playing (QBP) music information retrieval (MIR) systems aimed at musicians playing traditional Irish dance music. Firstly, a browser hosted system - tunepal.org is presented. Secondly, we present Tunepal for iPhone/iPod touch devices - a QBP system that can be used in situ in traditional music sessions. Both of these systems use a backend corpus of 13,290 tunes drawn from community sources and ""standard"" references. These systems have evolved from academic research to become popular tools used by musicians around the world. 16,064 queries have been logged since the systems were launched on 31 July, 2009 and 11 February, 2010 respectively to 18 May 2010. As we log data on every query made, including geocoding queries made on the iPhone, we propose that these tools may be used to follow trends in the playing of traditional music. We also present an analysis of the data we have collected on the usage of these systems. © 2010 International Society for Music Information Retrieval.","","Academic research; Community sources; Geo coding; Log data; Music information retrieval","B. Duggan; Dublin Institute of Technology, School of Computing, Dublin 8, Kevin St, Ireland; email: bryan.duggan@dit.ie","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Tjoa S.K.; Ray Liu K.J.","Tjoa, Steven K. (18042682600); Ray Liu, K.J. (7404200118)","18042682600; 7404200118","Factorization of overlapping harmonic sounds using approximate matching pursuit","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583863&partnerID=40&md5=713dced090628387d39512aaa6a4b42b","Imagine Research, San Francisco, CA 94114, United States; University of Maryland, College Park, MD 20742, United States","Tjoa S.K., Imagine Research, San Francisco, CA 94114, United States; Ray Liu K.J., University of Maryland, College Park, MD 20742, United States","Factorization of polyphonic musical signals remains a difficult problem due to the presence of overlapping harmonics. Existing dictionary learning methods cannot guarantee that the learned dictionary atoms are semantically meaningful. In this paper, we explore the factorization of harmonic musical signals when a fixed dictionary of harmonic sounds is already present. We propose a method called approximate matching pursuit (AMP) that can efficiently decompose harmonic sounds by using a known predetermined dictionary. We illustrate the effectiveness of AMP by decomposing polyphonic musical spectra with respect to a large dictionary of instrumental sounds. AMP executes faster than orthogonal matching pursuit yet performs comparably based upon recall and precision. © 2011 International Society for Music Information Retrieval.","","Factorization; Information retrieval; Approximate matching; Dictionary learning; Learned dictionaries; Musical signals; Orthogonal matching pursuit; Polyphonic musical signals; Recall and precision; Harmonic analysis","S.K. Tjoa; Imagine Research, San Francisco, CA 94114, United States; email: steve@imagine-research.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Orio N.; Rizo D.; Miotto R.; Montecchio N.; Schedl M.; Lartillot O.","Orio, Nicola (6507928255); Rizo, David (14042169500); Miotto, Riccardo (57202034402); Montecchio, Nicola (25929429200); Schedl, Markus (8684865900); Lartillot, Olivier (6507137446)","6507928255; 14042169500; 57202034402; 25929429200; 8684865900; 6507137446","Musiclef: A benchmark activity in multimodal music information retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861082718&partnerID=40&md5=f5553ab5ec3dead921042387c0f4ec7b","University of Padova, Italy; University of Alicante, Spain; Johannes Kepler University, Austria; Academy of Finland, Finland","Orio N., University of Padova, Italy; Rizo D., University of Alicante, Spain; Miotto R., University of Padova, Italy; Montecchio N., University of Padova, Italy; Schedl M., Johannes Kepler University, Austria; Lartillot O., Academy of Finland, Finland","This work presents the rationale, tasks and procedures of MusiCLEF, a novel benchmarking activity that has been developed along with the Cross-Language Evaluation Forum (CLEF). The main goal of MusiCLEF is to promote the development of new methodologies for music access and retrieval on real public music collections, which can combine content-based information, automatically extracted from music files, with contextual information, provided by users via tags, comments, or reviews. Moreover, MusiCLEF aims at maintaining a tight connection with real application scenarios, focusing on issues on music access and retrieval that are faced by professional users. To this end, this year's evaluation campaign focused on two main tasks: automatic categorization of music to be used as soundtrack of TV shows and automatic identification of the digitized material of a music digital library. © 2011 International Society for Music Information Retrieval.","","Automation; Digital libraries; Automatic categorization; Automatic identification; Benchmarking activity; Content-based information; Contextual information; Cross-language evaluation forums; Music digital libraries; Music information retrieval; Information retrieval","N. Orio; University of Padova, Italy; email: orio@dei.unipd.it","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Coviello E.; Miotto R.; Lanckriet G.R.G.","Coviello, Emanuele (35147534800); Miotto, Riccardo (57202034402); Lanckriet, Gert R. G. (7801431767)","35147534800; 57202034402; 7801431767","Combining content-based auto-taggers with decision-fusion","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873571639&partnerID=40&md5=388a564e5ea0c188f18aa0b9af3830af","University of California, San Diego, United States; University of Padova, Italy","Coviello E., University of California, San Diego, United States; Miotto R., University of Padova, Italy; Lanckriet G.R.G., University of California, San Diego, United States","To automatically annotate songs with descriptive keywords, a variety of content-based auto-tagging strategies have been proposed in recent years. Different approaches may capture different aspects of a song's musical content, such as timbre, temporal dynamics, rhythmic qualities, etc. As a result, some auto-taggers may be better suited to model the acoustic characteristics commonly associated with one set of tags, while being less predictive for other tags. This paper proposes decision-fusion, a principled approach to combining the predictions of a diverse collection of content-based autotaggers that focus on various aspects of the musical signal. By modeling the correlations between tag predictions of different auto-taggers, decision-fusion leverages the benefits of each of the original auto-taggers, and achieves superior annotation and retrieval performance. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Acoustic characteristic; Content-based; Musical signals; Retrieval performance; Temporal dynamics; Forecasting","E. Coviello; University of California, San Diego, United States; email: ecoviell@ucsd.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Humphrey E.","Humphrey, Eric (55060792500)","55060792500","Automatic characterization of digital music for rhythmic auditory stimulation","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607237&partnerID=40&md5=d794aeb094c8d6bec2c1d0be78e27215","Music Engineering Technology Group, University of Miami, Coral Gables, FL 33124, United States","Humphrey E., Music Engineering Technology Group, University of Miami, Coral Gables, FL 33124, United States","A computational rhythm analysis system is proposed to characterize the suitability of musical recordings for rhythmic auditory stimulation, a neurologic music therapy technique that uses rhythm to entrain periodic physical motion. Current applications of RAS are limited by the general inability to take advantage of the enormous amount of digital music that exists today. The system aims to identify motor-rhythmic music for the entrainment of neuromuscular activity for rehabilitation and exercise, motivating the concept of musical ""use-genres."" This work builds upon prior research in meter and tempo analysis to establish a representation of rhythm chroma and alternatively describe beat spectra. © 2010 International Society for Music Information Retrieval.","","Analysis system; Auditory stimulation; Digital music; Music therapy; Neuromuscular activity; Physical motion; Tempo analysis; Information retrieval","E. Humphrey; Music Engineering Technology Group, University of Miami, Coral Gables, FL 33124, United States; email: humphrey.eric@gmail.com","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Bertin-Mahieux T.; J.weiss R.; Ellis D.P.W.","Bertin-Mahieux, Thierry (49060926500); J.weiss, Ron (57226353962); Ellis, Daniel P.W. (13609089200)","49060926500; 57226353962; 13609089200","Clustering beat-chroma patterns in a large music database","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602984&partnerID=40&md5=e82252b92a519dc1fd3c20b6f375b5c4","Columbia University, United States; New York University, United States","Bertin-Mahieux T., Columbia University, United States; J.weiss R., New York University, United States; Ellis D.P.W., Columbia University, United States","A musical style or genre implies a set of common conventions and patterns combined and deployed in different ways to make individual musical pieces; for instance, most would agree that contemporary pop music is assembled from a relatively small palette of harmonic and melodic patterns. The purpose of this paper is to use a database of tens of thousands of songs in combination with a compact representation of melodic-harmonic content (the beatsynchronous chromagram) and data-mining tools (clustering) to attempt to explicitly catalog this palette - at least within the limitations of the beat-chroma representation. We use online k-means clustering to summarize 3.7 million 4-beat bars in a codebook of a few hundred prototypes. By measuring how accurately such a quantized codebook can reconstruct the original data, we can quantify the degree of diversity (distortion as a function of codebook size) and temporal structure (i.e. the advantage gained by joint quantizing multiple frames) in this music. The most popular codewords themselves reveal the common chords used in the music. Finally, the quantized representation of music can be used for music retrieval tasks such as artist and genre classification, and identifying songs that are similar in terms of their melodic-harmonic content. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Code-words; Codebooks; Compact representation; Data-mining tools; Degree of diversity; Genre classification; K-means clustering; Multiple-frame; Music database; Music retrieval; Musical pieces; Temporal structures; Harmonic analysis","T. Bertin-Mahieux; Columbia University, United States; email: tb2332@columbia.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Marques C.; Guilherme I.R.; Nakamura R.Y.M.; Papa J.P.","Marques, C. (54393700000); Guilherme, I.R. (6508331514); Nakamura, R.Y.M. (54783080800); Papa, J.P. (23397842300)","54393700000; 6508331514; 54783080800; 23397842300","New trends in musical genre classification using optimum-path forest","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575554&partnerID=40&md5=b793293111d753a0ee5373beeee78bda","Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; Department of Computing, UNESP - Univ Estadual Paulista, Bauru, SP, Brazil","Marques C., Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; Guilherme I.R., Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; Nakamura R.Y.M., Department of Computing, UNESP - Univ Estadual Paulista, Bauru, SP, Brazil; Papa J.P., Department of Computing, UNESP - Univ Estadual Paulista, Bauru, SP, Brazil","Musical genre classification has been paramount in the last years, mainly in large multimedia datasets, in which new songs and genres can be added at every moment by anyone. In this context, we have seen the growing of musical recommendation systems, which can improve the benefits for several applications, such as social networks and collective musical libraries. In this work, we have introduced a recent machine learning technique named Optimum-Path Forest (OPF) for musical genre classification, which has been demonstrated to be similar to the state-of-the-art pattern recognition techniques, but much faster for some applications. Experiments in two public datasets were conducted against Support Vector Machines and a Bayesian classifier to show the validity of our work. In addition, we have executed an experiment using very recent hybrid feature selection techniques based on OPF to speed up feature extraction process. © 2011 International Society for Music Information Retrieval.","","Classification; Experimentation; Information Retrieval; Experiments; Feature extraction; Forestry; Information retrieval; Learning systems; Bayesian classifier; Hybrid feature selections; Machine learning techniques; Musical genre classification; Optimum-path forests; Pattern recognition techniques; Social Networks; Speed up; Classification (of information)","C. Marques; Dep. of Statistics, Applied Math. and Computation, UNESP - Univ Estadual Paulista, Rio Claro, SP, Brazil; email: marques@caiena.net","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Haas W.B.D.; Magalhães J.P.; Veltkamp R.C.; Wiering F.","Haas, W.Bas De (57197447597); Magalhães, José Pedro (35793830900); Veltkamp, Remco C. (7003421646); Wiering, Frans (8976178100)","57197447597; 35793830900; 7003421646; 8976178100","Harmtrace: Improving harmonic similarity estimation using functional harmony analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578029&partnerID=40&md5=07ff5e20aa783580e94127c89a10d7d2","Utrecht University, Netherlands","Haas W.B.D., Utrecht University, Netherlands; Magalhães J.P., Utrecht University, Netherlands; Veltkamp R.C., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands","Harmony theory has been essential in composing, analysing, and performing music for centuries. Since Western tonal harmony exhibits a considerable amount of structure and regularity, it lends itself to formalisation. In this paper we present HARMTRACE, a system that, given a sequence of symbolic chord labels, automatically derives the harmonic function of a chord in its tonal context. Among other applications, these functional annotations can be used to improve the estimation of harmonic similarity in a local alignment of two annotated chord sequences. We evaluate HARMTRACE and three other harmonic similarity measures on a corpus of 5,028 chord sequences that contains harmonically related pieces. The results show that HARMTRACE outperforms all three other similarity measures, and that information about the harmonic function of a chord improves the estimation of harmonic similarity between two chord sequences. © 2011 International Society for Music Information Retrieval.","","Estimation; Harmonic functions; Information retrieval; Chord sequence; Formalisation; Functional annotation; Harmony analysis; Harmony theory; Local alignment; Similarity estimation; Similarity measure; Harmonic analysis","W.B.D. Haas; Utrecht University, Netherlands; email: W.B.deHaas@uu.nl","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Vigliensoni G.; Burgoyne J.A.; Hankinson A.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); Burgoyne, John Ashley (23007865600); Hankinson, Andrew (54970482500); Fujinaga, Ichiro (9038140900)","55217696900; 23007865600; 54970482500; 9038140900","Automatic pitch recognition in printed square-note notation","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861085383&partnerID=40&md5=1ae6f2b3c37a2a91a122cc7f331c8ee8","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","In this paper we present our research in the development of a pitch-finding system to extract the pitches of neumes-some of the oldest representations of pitch in Western music- from the Liber Usualis, a well-known compendium of plainchant as used in the Roman Catholic church. Considerations regarding the staff position, staff removal, space- and linezones, as well as how we treat specific neume classes and modifiers are covered. This type of notation presents a challenge for traditional optical music recognition (OMR) systems because individual note pitches are indivisible from the larger ligature group that forms the neume. We have created a dataset of correctly-notated transcribed chant for comparing the performance of different variants of our pitch-finding system. The best result showed a recognition rate of 97% tested with more than 2000 neumes. © 2011 International Society for Music Information Retrieval.","","Catholic church; Optical music recognition; Pitch recognition; Recognition rates; Information retrieval","G. Vigliensoni; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; email: gabriel@music.mcgill.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Nam J.; Ngiam J.; Lee H.; Slaney M.","Nam, Juhan (35812266500); Ngiam, Jiquan (52264237700); Lee, Honglak (15056237200); Slaney, Malcolm (6701855101)","35812266500; 52264237700; 15056237200; 6701855101","A classification-based polyphonic piano transcription approach using learned feature representations","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578548&partnerID=40&md5=9fa52e630c645bf95285b818120ebd1d","Stanford University, United States; Univ. of Michigan, Ann Arbor, United States; Yahoo Research, United States","Nam J., Stanford University, United States; Ngiam J., Stanford University, United States; Lee H., Univ. of Michigan, Ann Arbor, United States; Slaney M., Yahoo Research, United States","Recently unsupervised feature learning methods have shown great promise as a way of extracting features from high dimensional data, such as image or audio. In this paper, we apply deep belief networks to musical data and evaluate the learned feature representations on classification-based polyphonic piano transcription. We also suggest a way of training classifiers jointly for multiple notes to improve training speed and classification performance. Our method is evaluated on three public piano datasets. The results show that the learned features outperform the baseline features, and also our method gives significantly better frame-level accuracy than other state-of-the-art music transcription methods. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Transcription; Classification performance; Deep belief networks; Extracting features; Feature representation; High dimensional data; Music transcription; Training speed; Unsupervised feature learning; Musical instruments","J. Nam; Stanford University, United States; email: juhan@ccrma.stanford.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schmidt E.M.; Kim Y.E.","Schmidt, Erik M. (36053813000); Kim, Youngmoo E. (24724623000)","36053813000; 24724623000","Modeling musical emotion dynamics with conditional random fields","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","54","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872700353&partnerID=40&md5=2206507b9b94d9731f6b7a3e74103d18","Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","Schmidt E.M., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","Human emotion responses to music are dynamic processes that evolve naturally over time in synchrony with the music. It is because of this dynamic nature that systems which seek to predict emotion in music must necessarily analyze such processes on short-time intervals, modeling not just the relationships between acoustic data and emotion parameters, but how those relationships evolve over time. In this work we seek to model such relationships using a conditional random field (CRF), a powerful graphical model which is trained to predict the conditional probability p(y|x) for a sequence of labels y given a sequence of features x. Treating our features as deterministic, we retain the rich local subtleties present in the data, which is especially applicable to contentbased audio analysis, given the abundance of data in these problems. We train our graphical model on the emotional responses of individual annotators in an 11×11 quantized representation of the arousal-valence (A-V) space. Our model is fully connected, and can produce estimates of the conditional probability for each A-V bin, allowing us to easily model complex emotion-space distributions (e.g. multimodal) as an A-V heatmap. © 2011 International Society for Music Information Retrieval.","","Graphic methods; Image segmentation; Information retrieval; Random processes; Acoustic data; Audio analysis; Conditional probabilities; Conditional random field; Content-based; Dynamic nature; Dynamic process; Emotional response; GraphicaL model; Human emotion; Model complexes; Multi-modal; Musical emotion; Computer music","E.M. Schmidt; Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; email: eschmidt@drexel.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schedl M.; Pohle T.; Koenigstein N.; Knees P.","Schedl, Markus (8684865900); Pohle, Tim (14036302300); Koenigstein, Noam (26534475900); Knees, Peter (8219023200)","8684865900; 14036302300; 26534475900; 8219023200","What's hot ? Estimating country-specific artist popularity","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604093&partnerID=40&md5=50490c3e076a16d96a07d8ef836c4b2a","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Faculty of Engineering, Tel Aviv University, Tel Aviv, Israel","Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Koenigstein N., Faculty of Engineering, Tel Aviv University, Tel Aviv, Israel; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Predicting artists that are popular in certain regions of the world is a well desired task, especially for the music industry. Also the cosmopolitan and cultural-aware music aficionado is likely be interested in which music is currently ""hot"" in other parts of the world. We therefore propose four approaches to determine artist popularity rankings on the country-level. To this end, we mine the following data sources: page counts from Web search engines, user posts on Twitter, shared folders on the Gnutella file sharing network, and playcount data from last.fm. We propose methods to derive artist rankings based on these four sources and perform cross-comparison of the resulting rankings via overlap scores. We further elaborate on the advantages and disadvantages of all approaches as they yield interestingly diverse results. © 2010 International Society for Music Information Retrieval.","","Search engines; Data-sources; File sharing networks; Gnutella; Last.fm; Music industry; Popularity ranking; Information retrieval","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Draman N.A.; Wilson C.; Ling S.","Draman, Noor Azilah (43061369400); Wilson, Campbell (24463026300); Ling, Sea (7102701266)","43061369400; 24463026300; 7102701266","Modified AIS-based classifier for music genre classification","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599760&partnerID=40&md5=c489a9800ca45fcb9478997805eb8b63","Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia","Draman N.A., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia; Wilson C., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia; Ling S., Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia","Automating human capabilities for classifying different genre of songs is a difficult task. This has led to various studies that focused on finding solutions to solve this problem. Analyzing music contents (often referred as content- based analysis) is one of many ways to identify and group similar songs together. Various music contents, for example beat, pitch, timbral and many others were used and analyzed to represent the music. To be able to manipulate these content representations for recognition: feature extraction and classification are two major focuses of investigation in this area. Though various classification techniques proposed so far, we are introducing yet another one. The objective of this paper is to introduce a possible new technique in the Artificial Immune System (AIS) domain called a modified immune classifier (MIC) for music genre classification. MIC is the newest version of Negative Selection Algorithm (NSA) where it stresses the self and non-self cells recognition and a complementary process for generating detectors. The discussion will detail out the MIC procedures applied and the modified part in solving the classification problem. At the end, the results of proposed framework will be presented, discussed and directions for future work are given. © 2010 International Society for Music Information Retrieval.","","Feature extraction; Artificial Immune System; Classification technique; Content representation; Feature extraction and classification; Finding solutions; Human capability; Music contents; Music genre classification; Negative selection algorithm; Information retrieval","N.A. Draman; Faculty of Information Technology, Caulfield School of IT, Monash University, Melbourne, VIC, 3145, Caulfield East, Australia; email: nadra1@student.monash.edu.au","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Anan Y.; Hatano K.; Bannai H.; Takeda M.","Anan, Yoko (55582495800); Hatano, Kohei (14021639200); Bannai, Hideo (7004530566); Takeda, Masayuki (7403299627)","55582495800; 14021639200; 7004530566; 7403299627","Music genre classification using similarity functions","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873330630&partnerID=40&md5=f210348e9ee0ea33c937ac65fe4ee8cb","Department of Informatics, Kyushu University, Japan","Anan Y., Department of Informatics, Kyushu University, Japan; Hatano K., Department of Informatics, Kyushu University, Japan; Bannai H., Department of Informatics, Kyushu University, Japan; Takeda M., Department of Informatics, Kyushu University, Japan","We consider music classification problems. A typical machine learning approach is to use support vector machines with some kernels. This approach, however, does not seem to be successful enough for classifying music data in our experiments. In this paper, we follow an alternative approach. We employ a (dis)similarity-based learning framework proposed byWang et al. This (dis)similarity-based approach has a theoretical guarantee that one can obtain accurate classifiers using (dis)similarity measures under a natural assumption. We demonstrate the effectiveness of our approach in computational experiments using Japanese MIDI data. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Alternative approach; Computational experiment; Learning frameworks; Machine learning approaches; Music classification; Music data; Music genre classification; Similarity functions; Similarity measure; Theoretical guarantees; Experiments","Y. Anan; Department of Informatics, Kyushu University, Japan; email: yoko.anan@inf.kyushu-u.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Chandrasekhar V.; Sharifi M.; Ross D.A.","Chandrasekhar, Vijay (15064023000); Sharifi, Matt (55586493900); Ross, David A. (56862626300)","15064023000; 55586493900; 56862626300","Survey and evaluation of audio fingerprinting schemes for mobile query-by-example applications","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586601&partnerID=40&md5=4160a6508c991a66293b808c96d05034","","","We survey and evaluate popular audio fingerprinting schemes in a common framework with short query probes captured from cell phones. We report and discuss results important for mobile applications: Receiver Operating Characteristic (ROC) performance, size of fingerprints generated compared to size of audio probe, and transmission delay if the fingerprint data were to be transmitted over a wireless link. We hope that the evaluation in this work will guide work towards reducing latency in practical mobile audio retrieval applications. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Probes; Audio fingerprinting; Audio retrieval; Cell phone; Mobile applications; Query-by-example; Receiver operating characteristics; Transmission delays; Wireless link; Surveys","V. Chandrasekhar; email: vijayc@stanford.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schedl M.; Knees P.; Böck S.","Schedl, Markus (8684865900); Knees, Peter (8219023200); Böck, Sebastian (55413719000)","8684865900; 8219023200; 55413719000","Investigating the similarity space of music artists on the micro-blogosphere","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591875&partnerID=40&md5=8fa5f656fb0e2147a14ae2a4c7d7197d","Department of Computational Perception, Johannes Kepler University Linz, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University Linz, Austria; Böck S., Department of Computational Perception, Johannes Kepler University Linz, Austria","Microblogging services such as Twitter have become an important means to share information. In this paper, we thoroughly analyze their potential for a key challenge in the field of MIR, namely the elaboration of perceptually meaningful similarity measures. To this end, comprehensive evaluation experiments were conducted using Twitter posts gathered during a period of several months. We investigated 23,100 combinations of different term weighting strategies, normalization methods, index term sets, Twitter query schemes, and similarity measurement techniques, aiming at determining in which way they influence the similarity estimates' quality. Evaluation was performed on the task of similar artist retrieval. Two data sets were used: one of 224 well-known artists with a uniform genre distribution, the other constituting a collection of 3,000 artists extracted from last.fm and allmusic.com. © 2011 International Society for Music Information Retrieval.","","Blogs; Information retrieval; Comprehensive evaluation; Micro-blogging services; Normalization methods; Similarity estimate; Similarity measure; Similarity measurements; Similarity spaces; Term weighting; Social networking (online)","M. Schedl; Department of Computational Perception, Johannes Kepler University Linz, Austria; email: markus.schedl@jku.at","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Sioros G.; Guedes C.","Sioros, George (23091906900); Guedes, Carlos (23134475200)","23091906900; 23134475200","Complexity driven recombination of midi loops","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453416&partnerID=40&md5=20052222236653961615f8b13a1ae09e","University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal","Sioros G., University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal; Guedes C., University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal","An algorithm and a software application for recombining in real time MIDI drum loops that makes use of a novel analysis of rhythmic patterns that sorts them in order of their complexity is presented. We measure rhythmic complexity by comparing each rhythmic pattern found in the loops to a metrical template characteristic of its time signature. The complexity measure is used to sort the MIDI loops prior to utilizing them in the recombination algorithm. This way, the user can effectively control the complexity and variation in the generated rhythm during performance. © 2011 International Society for Music Information Retrieval.","","Algorithms; Application programs; Complexity measures; Real time; Recombination algorithms; Rhythmic patterns; Software applications; Information retrieval","G. Sioros; University of Porto, FEUP, INESC, 4200-465 Porto, Rua Dr. Roberto Frias, s/n, Portugal; email: gsioros@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Weninger F.; Wöllmer M.; Schuller B.","Weninger, Felix (13610240900); Wöllmer, Martin (25621966400); Schuller, Björn (6603767415)","13610240900; 25621966400; 6603767415","Automatic assessment of singer traits in popular music: Gender, age, height and race","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870515394&partnerID=40&md5=8eea548623ada988e28fbcd8f660f833","Institute for Human-Machine Communication, Technische Universität München, Germany","Weninger F., Institute for Human-Machine Communication, Technische Universität München, Germany; Wöllmer M., Institute for Human-Machine Communication, Technische Universität München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität München, Germany","We investigate fully automatic recognition of singer traits, i. e., gender, age, height and 'race' of the main performing artist(s) in recorded popular music. Monaural source separation techniques are combined to simultaneously enhance harmonic parts and extract the leading voice. For evaluation the UltraStar database of 581 pop music songs with 516 distinct singers is chosen. Extensive test runs with Long Short-Term Memory sequence classification reveal that binary classification of gender, height, race and age reaches up to 89.6, 72.1, 63.3 and 57.6% unweighted accuracy on beat level in unseen test data. © 2011 International Society for Music Information Retrieval.","","Automatic assessment; Automatic recognition; Binary classification; Long short-term memories; Monaural source; Popular music; Separation techniques; Test data; Test runs; Information retrieval","F. Weninger; Institute for Human-Machine Communication, Technische Universität München, Germany; email: weninger@tum.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Scott J.; Migneco R.; Morton B.; Hahn C.M.; Diefenbach P.; Kim Y.E.","Scott, Jeffrey (35312262000); Migneco, Raymond (35311311700); Morton, Brandon (36457242900); Hahn, Christian M. (35310599300); Diefenbach, Paul (6506343338); Kim, Youngmoo E. (24724623000)","35312262000; 35311311700; 36457242900; 35310599300; 6506343338; 24724623000","An audio processing library for MIR application development in flash","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602337&partnerID=40&md5=45ecf7ed75aef6f43f89f26154585711","Department of Electrical and Computer Engineering, Drexel University, United States; Department of Media Arts and Design, Drexel University, United States","Scott J., Department of Electrical and Computer Engineering, Drexel University, United States; Migneco R., Department of Electrical and Computer Engineering, Drexel University, United States; Morton B., Department of Electrical and Computer Engineering, Drexel University, United States; Hahn C.M., Department of Electrical and Computer Engineering, Drexel University, United States; Diefenbach P., Department of Media Arts and Design, Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Drexel University, United States","In recent years, the Adobe Flash platform has risen as a credible and universal platform for rapid development and deployment of interactive web-based applications. It is also the accepted standard for delivery of streaming media, and many web applications related to music information retrieval, such as Pandora, Last.fm and Musicovery, are built using Flash. The limitations of Flash, however, have made it difficult for music-IR researchers and developers to utilize complex sound and music signal processing within their web applications. Furthermore, the real-time audio processing and synchronization required for some music-IR-related activities demands significant computational power and specialized audio algorithms, far beyond what is possible to implement using Flash scripting. By taking advantage of features recently added to the platform, including dynamic audio control and C crosscompilation for near-native performance, we have developed the Audio-processing Library for Flash (ALF), providing developers with a library of common audio processing routines and affording Flash developers a degree of sound interaction previously unavailable through webbased platforms. We present several music-IR-driven applications that incorporate ALF to demonstrate its utility. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Signal processing; World Wide Web; Application development; Audio processing; Complex sounds; Computational power; Last.fm; Music information retrieval; Music signal processing; Real-time audio; Sound interactions; Streaming media; Universal platform; WEB application; Web based platform; Web-based applications; Audio acoustics","J. Scott; Department of Electrical and Computer Engineering, Drexel University, United States; email: jjscott@drexel.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Zacharakis A.; Pastiadis K.; Papadelis G.; Reiss J.D.","Zacharakis, Asteris (35182350900); Pastiadis, Kostantinos (39861784200); Papadelis, Georgios (36996069100); Reiss, Joshua D. (10140139100)","35182350900; 39861784200; 36996069100; 10140139100","An investigation of musical timbre: Uncovering salient semantic descriptors and perceptual dimensions","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572972&partnerID=40&md5=f92350742c17bc1bd4222153c839aee3","Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom; School of Music Studies, Aristotle University of Thessaloniki, Thessaloniki, Greece","Zacharakis A., Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom; Pastiadis K., School of Music Studies, Aristotle University of Thessaloniki, Thessaloniki, Greece; Papadelis G., School of Music Studies, Aristotle University of Thessaloniki, Thessaloniki, Greece; Reiss J.D., Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom","A study on the verbal attributes of musical timbre was conducted in an effort to identify the most significant semantic descriptors and to quantify the association between prominent timbral aspects and several categorical properties of environmental entities. A verbal attribute magnitude estimation (VAME) type of listening test in which participants were asked to describe 23 musical sounds using 30 Greek adjectives together with verbal terms of their own choice was designed and conducted for this purpose. Factor and Cluster Analysis were performed on the subjective evaluation data in order to shed some light on the relationships between the adjectives that were proposed and to conclude to the number and quality of the salient perceptual dimensions required for the description of this set of sounds. © 2011 International Society for Music Information Retrieval.","","Cluster analysis; Information retrieval; Descriptors; Factor and cluster analysis; Listening tests; Magnitude estimation; Musical sounds; Musical timbres; Perceptual dimensions; Subjective evaluations; Semantics","A. Zacharakis; Centre for Digital Music, Queen Mary University of London, London, UK, United Kingdom; email: asteriosz@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Henaff M.; Jarrett K.; Kavukcuoglu K.; Lecun Y.","Henaff, Mikael (55760206000); Jarrett, Kevin (36137154900); Kavukcuoglu, Koray (25646533000); Lecun, Yann (55666793600)","55760206000; 36137154900; 25646533000; 55666793600","Unsupervised learning of sparse features for scalable audio classification","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","89","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864122549&partnerID=40&md5=ba42d8fc371b714c28ee450421cb8eed","Courant Institute of Mathematical Sciences, New York University, United States","Henaff M., Courant Institute of Mathematical Sciences, New York University, United States; Jarrett K., Courant Institute of Mathematical Sciences, New York University, United States; Kavukcuoglu K., Courant Institute of Mathematical Sciences, New York University, United States; Lecun Y., Courant Institute of Mathematical Sciences, New York University, United States","In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4% accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets. © 2011 International Society for Music Information Retrieval.","","Audio acoustics; Feature extraction; Information retrieval; Audio classification; Iterative procedures; Large datasets; Learned dictionaries; Linear classifiers; Linear Support Vector Machines; Over-complete dictionaries; Sparse codes; Sparse features; Sparse representation; Spectrograms; State-of-the-art approach; Support vector machines","M. Henaff; Courant Institute of Mathematical Sciences, New York University, United States; email: mbh305@nyu.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schnitzer D.; Flexer A.; Schedl M.; Widmer G.","Schnitzer, Dominik (23996271700); Flexer, Arthur (7004555682); Schedl, Markus (8684865900); Widmer, Gerhard (7004342843)","23996271700; 7004555682; 8684865900; 7004342843","Using mutual proximity to improve content-based audio similarity","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421776&partnerID=40&md5=2f3d52564e946f714889d0ae3979f13f","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","This work introduces Mutual Proximity, an unsupervised method which transforms arbitrary distances to similarities computed from the shared neighborhood of two data points. This reinterpretation aims to correct inconsistencies in the original distance space, like the hub phenomenon. Hubs are objects which appear unwontedly often as nearest neighbors in predominantly high-dimensional spaces. We apply Mutual Proximity to a widely used and standard content-based audio similarity algorithm. The algorithm is known to be negatively affected by the high number of hubs it produces. We show that without a modification of the audio similarity features or inclusion of additional knowledge about the datasets, applying Mutual Proximity leads to a significant increase of retrieval quality: (1) hubs decrease and (2) the k-nearest-neighbor classification rates increase significantly. The results of this paper show that taking the mutual neighborhood of objects into account is an important aspect which should be considered for this class of content-based audio similarity algorithms. © 2011 International Society for Music Information Retrieval.","","Classification (of information); Information retrieval; Additional knowledge; Audio similarities; Classification rates; Content-based; Data points; Distance space; High dimensional spaces; K-nearest neighbors; Nearest neighbors; Retrieval quality; Unsupervised method; Algorithms","D. Schnitzer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: dominik.schnitzer@ofai.at","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mcvicar M.; Ni Y.; De Bie T.; Santos-Rodriguez R.","Mcvicar, Matt (36721968600); Ni, Yizhao (25823257400); De Bie, Tijl (57203775071); Santos-Rodriguez, Raul (27968154800)","36721968600; 25823257400; 57203775071; 27968154800","Leveraging noisy online databases for use in chord recognition","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873577341&partnerID=40&md5=18c6426f97240ba0bbc6426f9402c899","B University of Bristol, Intelligent Systems La, United Kingdom; III of Madrid, University Carlos, Spain","Mcvicar M., B University of Bristol, Intelligent Systems La, United Kingdom; Ni Y., B University of Bristol, Intelligent Systems La, United Kingdom; De Bie T., B University of Bristol, Intelligent Systems La, United Kingdom; Santos-Rodriguez R., III of Madrid, University Carlos, Spain","The most significant problem faced by Machine Learningbased chord recognition systems is arguably the lack of highquality training examples. In this paper, we address this problem by leveraging the availability of chord annotations from guitarist websites. We show that such annotations can be used as partial supervision of a semi-supervised chord recognition method-partial since accurate timing information is lacking. A particular challenge in the exploitation of these data is their low quality, potentially even leading to a performance degradation if used directly. We demonstrate however that a curriculum learning strategy can be used to automatically rank annotations according to their potential for improving the performance. Using this strategy, our experiments show a modest improvement for a simple major/ minor chord alphabet, but a highly significant improvement for a much larger chord alphabet. © 2011 International Society for Music Information Retrieval.","","Accurate timing; Chord recognition; Learning strategy; Low qualities; Online database; Performance degradation; Semi-supervised; Training example; Information retrieval","M. Mcvicar; B University of Bristol, Intelligent Systems La, United Kingdom; email: matt.mcvicar@bris.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Baur D.; Steinmayr B.; Butz A.","Baur, Dominikus (24477437400); Steinmayr, Bartholomäus (52664238300); Butz, Andreas (55150450600)","24477437400; 52664238300; 55150450600","Songwords: Exploring music collections through lyrics","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593138&partnerID=40&md5=f418e5f9742a437410df498aa3d926c3","Media Informatics Group, University of Munich (LMU), Munich, Germany","Baur D., Media Informatics Group, University of Munich (LMU), Munich, Germany; Steinmayr B., Media Informatics Group, University of Munich (LMU), Munich, Germany; Butz A., Media Informatics Group, University of Munich (LMU), Munich, Germany","The lyrics of a song are an interesting, yet underused type of symbolic music data. We present SongWords, an application for tabletop computers that allows browsing and exploring a music collection based on its lyrics. Song- Words can present the collection in a self-organizing map or sorted along different dimensions. Songs can be ordered by lyrics, user-generated tags or alphabetically by name, which allows exploring simple correlations, e.g., between genres (such as gospel) and words (such as lord). In this paper, we discuss the design rationale and implementation of SongWords as well as a user study with personal music collections. We found that lyrics indeed enable a different access to music collections and identified some challenges for future lyrics-based interfaces. © 2010 International Society for Music Information Retrieval.","","Computer applications; Conformal mapping; Information retrieval; Design rationale; Music collection; Music data; Personal music collection; User study; User-generated; Audio recordings","D. Baur; Media Informatics Group, University of Munich (LMU), Munich, Germany; email: dominikus.baur@ifi.lmu.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Scott J.; Kim Y.E.","Scott, Jeffrey (35312262000); Kim, Youngmoo E. (24724623000)","35312262000; 24724623000","Analysis of acoustic features for automated multi-track mixing","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575046&partnerID=40&md5=6fe3491c47a6b1d74249b18d01c8a116","Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","Scott J., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States","The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a finished product, namely determining the relative gain levels of each track to produce a final, mixed song. By modeling the time-varying mixing coefficients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Linear control systems; Acoustic features; Audio content; Digital music; Finished products; Linear dynamical systems; Mixing coefficient; Multi-track recording; Time varying; Train model; Weight vector; Mixing","J. Scott; Music and Entertainment Technology Laboratory (MET-lab), Drexel University, Electrical and Computer Engineering, United States; email: jjscott@drexel.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Urbano J.","Urbano, Julián (36118414700)","36118414700","Information retrieval meta-evaluation: Challenges and opportunities in the music domain","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861015604&partnerID=40&md5=5812f0d6e5e3054aeca1f917071def04","Department of Computer Science, University Carlos III of Madrid, Spain","Urbano J., Department of Computer Science, University Carlos III of Madrid, Spain","The Music Information Retrieval field has acknowledged the need for rigorous scientific evaluations for some time now. Several efforts were set out to develop and provide the necessary infrastructure, technology and methodologies to carry out these evaluations, out of which the annual Music Information Retrieval Evaluation eXchange emerged. The community as a whole has enormously gained from this evaluation forum, but very little attention has been paid to reliability and correctness issues. From the standpoint of the analysis of experimental validity, this paper presents a survey of past meta-evaluation work in the context of Text Information Retrieval, arguing that the music community still needs to address various issues concerning the evaluation of music systems and the IR cycle, pointing out directions for further research and proposals in this line. © 2011 International Society for Music Information Retrieval.","","Music information retrieval; Scientific evaluations; Text information retrievals; Information retrieval","J. Urbano; Department of Computer Science, University Carlos III of Madrid, Spain; email: jurbano@inf.uc3m.es","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Van Kranenburg P.; Biró D.P.; Ness S.; Tzanetakis G.","Van Kranenburg, Peter (35108158000); Biró, Dániel Ṕeter (57189910706); Ness, Steven (35076349900); Tzanetakis, George (6602262192)","35108158000; 57189910706; 35076349900; 6602262192","A computational investigation of melodic contour stability in jewish torah trope performance traditions","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871324695&partnerID=40&md5=796acfd76a0f4bc5d50b404586275784","Meertens Institute, Canada; University of Victoria, Canada","Van Kranenburg P., Meertens Institute, Canada; Biró D.P., University of Victoria, Canada; Ness S., University of Victoria, Canada; Tzanetakis G., University of Victoria, Canada","The cantillation signs of the Jewish Torah trope are of particular interest to chant scholars interested in the gradual transformation of oral music performance into notation. Each sign, placed above or below the text, acts as a ""melodic idea"" which either connects or divides words in order to clarify the syntax, punctuation and, in some cases, meaning of the text. Unlike standard music notation, the interpretations of each sign are flexible and influenced by regional traditions, practices of given Jewish communities, larger musical influences beyond Jewish communities, and improvisatory elements incorporated by a given reader. In this paper we describe our collaborative work in developing and using computational tools to assess the stability of melodic formulas of cantillation signs based on two different performance traditions. We also show that a musically motivated alignment algorithm obtains better results than the more commonly used dynamic time warping method for calculating similarity between pitch contours. Using a participatory design process our team, which includes a domain expert, has developed an interactive web-based interface that enables researches to explore aurally and visually chant recordings and explore the relations between signs, gestures and musical representations. © 2011 International Society for Music Information Retrieval.","","Multimedia systems; Alignment algorithms; Calculating similarities; Collaborative Work; Computational investigation; Computational tools; Domain experts; Dynamic time warping; Gradual transformations; Music notation; Music performance; Musical representations; Participatory design; Pitch contours; Web-based interface; Information retrieval","P. Van Kranenburg; Meertens Institute, Canada; email: peter.van.kranenburg@meertens.knaw.nl","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Foucard R.; Essid S.; Lagrange M.; Richard G.","Foucard, Rémi (36607992100); Essid, Slim (16033218700); Lagrange, Mathieu (18042165200); Richard, Gaël (57195915952)","36607992100; 16033218700; 18042165200; 57195915952","Multi-scale temporal fusion by boosting for music classification","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587720&partnerID=40&md5=61b18357a0ac869b5afa7f953bfd2f87","CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; CNRS-STMS, Ircam, 75004 Paris, 1, place Igor Stravinsky, France","Foucard R., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; Essid S., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; Lagrange M., CNRS-STMS, Ircam, 75004 Paris, 1, place Igor Stravinsky, France; Richard G., CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France","Short-term and long-term descriptors constitute complementary pieces of information in the analysis of audio signals. However, because they are extracted over different time horizons, it is difficult to exploit them concurrently in a fully effective manner. In this paper we propose a novel temporal fusion method that leverages the effectiveness of a given set of features by efficiently combining multi-scale versions of them. This fusion is achieved using a boosting technique exploiting trees as weak classifiers, which has the advantage of performing an embedded feature selection. We apply our algorithm to two standard classification tasks, namely musical instrument recognition and multi-tag classification. Our experiments indicate that the multi-scale approach is able to select different features at different scales and significantly outperforms the mono-scale systems in terms of classification performance. © 2011 International Society for Music Information Retrieval.","","Audio signal; Classification performance; Classification tasks; Descriptors; Different scale; Embedded feature selections; Fusion methods; Multi-scale approaches; Multiscales; Music classification; Musical instrument recognition; Time horizons; Weak classifiers; Information retrieval","R. Foucard; CNRS-LTCI, TELECOM ParisTech, 75014 Paris, 37, rue Dareau, France; email: remi.foucard@telecom-paristech.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Fazekas G.; Sandler M.B.","Fazekas, György (37107520200); Sandler, Mark B. (7202740804)","37107520200; 7202740804","The studio ontology framework","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583402&partnerID=40&md5=aa7169f0bf40491165b627685b76c5a0","Centre for Digital Music, Queen Mary University, London, United Kingdom","Fazekas G., Centre for Digital Music, Queen Mary University, London, United Kingdom; Sandler M.B., Centre for Digital Music, Queen Mary University, London, United Kingdom","This paper introduces the Studio Ontology Framework for describing and sharing detailed information about music production. The primary aim of this ontology is to capture the nuances of record production by providing an explicit, application and situation independent conceptualisation of the studio environment. We may use the ontology to describe real-world recording scenarios involving physical hardware, or (post) production on a personal computer. It builds on Semantic Web technologies and previously published ontologies for knowledge representation and knowledge sharing. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Knowledge representation; Personal computers; Knowledge-sharing; Music production; Real-world; Semantic Web technology; Studios","G. Fazekas; Centre for Digital Music, Queen Mary University, London, United Kingdom; email: gyorgy.fazekas@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Dessein A.; Cont A.; Lemaitre G.","Dessein, Arnaud (56684772200); Cont, Arshia (12344985300); Lemaitre, Guillaume (23019212700)","56684772200; 12344985300; 23019212700","Real-time polyphonic music transcription with non-negative matrix factorization and beta-divergence","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","66","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602321&partnerID=40&md5=c3f46f7bf3d4410d91380632844ba88d","IRCAM - CNRS UMR 9912, Paris, France","Dessein A., IRCAM - CNRS UMR 9912, Paris, France; Cont A., IRCAM - CNRS UMR 9912, Paris, France; Lemaitre G., IRCAM - CNRS UMR 9912, Paris, France","In this paper, we investigate the problem of real-time polyphonic music transcription by employing non-negative matrix factorization techniques and the β-divergence as a cost function. We consider real-world setups where the music signal arrives incrementally to the system and is transcribed as it unfolds in time. The proposed transcription system is addressed with a modified non-negative matrix factorization scheme, called non-negative decomposition, where the incoming signal is projected onto a fixed basis of templates learned off-line prior to the decomposition. We discuss the use of non-negative matrix factorization with the β-divergence to achieve the real-time decomposition. The proposed system is evaluated on the specific task of piano music transcription and the results show that it can outperform several state-of-the-art off-line approaches. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Music signals; Non negatives; Nonnegative matrix factorization; Off-line approaches; Piano music; Polyphonic music; Real-world; Specific tasks; Audio signal processing","A. Dessein; IRCAM - CNRS UMR 9912, Paris, France; email: dessein@ircam.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Mann M.; Cox T.J.; Li F.F.","Mann, M. (55586106800); Cox, T.J. (7203000240); Li, F.F. (7406053605)","55586106800; 7203000240; 7406053605","Music mood classification of television theme tunes","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873456455&partnerID=40&md5=f70fc79abd877cc8c54fafded54a0def","BBC R and D, London, United Kingdom; University of Salford, United Kingdom","Mann M., BBC R and D, London, United Kingdom; Cox T.J., University of Salford, United Kingdom; Li F.F., University of Salford, United Kingdom","This paper introduces methods used for Music Mood Classification to assist in the automated tagging of television programme theme tunes for the first time. The methods employed use a knowledge driven approach with tailored parameters extractable from the Matlab MIR Toolbox [1]. Four new features were developed, three based on tonality and one on tempo, to enable a degree of quantified tagging, using support vector machines, employing various kernels, optimised along six mood axes. Using a ""nearest neighbour"" method of optimisation, a success rate in the range of 80-94% was achieved in being able to classify musical audio on a five point mood scale. © 2011 International Society for Music Information Retrieval.","","Information retrieval; MATLAB; Television broadcasting; Tools; Musical audio; Nearest neighbour; Optimisations; Audio acoustics","M. Mann; BBC R and D, London, United Kingdom; email: mark.mann@bbc.co.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Funasawa S.; Ishizaki H.; Hoashi K.; Takishima Y.; Katto J.","Funasawa, Shintaro (26025114600); Ishizaki, Hiromi (16068647300); Hoashi, Keiichiro (6603899930); Takishima, Yasuhiro (6602909921); Katto, Jiro (55903062000)","26025114600; 16068647300; 6603899930; 6602909921; 55903062000","Automated music slideshow generation using web images based on lyrics","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598613&partnerID=40&md5=38062ecdecdc980bf1c74593a418ae89","Waseda University, Japan; KDDI R and D Laboratories Inc., Japan","Funasawa S., Waseda University, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Japan; Hoashi K., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan; Katto J., Waseda University, Japan","In this paper, we propose a system which automatically generates slideshows for music, by utilizing images retrieved from photo sharing web sites, based on query words extracted from song lyrics. The proposed system consists of two major steps: (1) query extraction from song lyrics, (2) image selection from web image search results. Moreover, in order to improve the display duration of each image in the slideshow, we adjust image transition timing by analyzing the duration of each lyric line in the input song. We have conducted subjective evaluation experiments, which prove that the proposal can generate impressive music slideshows for any input song. © 2010 International Society for Music Information Retrieval.","","Computer music; Image processing; Information retrieval; Image selection; Photo sharing; Query extractions; Query words; Subjective evaluations; Web image search; Web images; Search engines","S. Funasawa; Waseda University, Japan; email: shint@katto.comm.waseda.ac.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Duan Z.; Pardo B.","Duan, Zhiyao (24450312900); Pardo, Bryan (10242155400)","24450312900; 10242155400","Aligning semi-improvised music audio with its lead sheet","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574923&partnerID=40&md5=a4d0b2e4a4dfb1919b48b3b6b8a1642c","Department of Electrical Engineering and Computer Science, Northwestern University, United States","Duan Z., Department of Electrical Engineering and Computer Science, Northwestern University, United States; Pardo B., Department of Electrical Engineering and Computer Science, Northwestern University, United States","Existing audio-score alignment methods assume that the audio performance is faithful to a fully-notated MIDI score. For semi-improvised music (e.g. jazz), this assumption is strongly violated. In this paper, we address the problem of aligning semi-improvised music audio with a lead sheet. Our approach does not require prior training on performances of the lead sheet to be aligned. We start by analyzing the problem and propose to represent the lead sheet as a MIDI file together with a structural information file. Then we propose a dynamic-programming-based system to align the chromagram representations of the audio performance and the MIDI score. Techniques are proposed to address the chromagram scaling, key transposition and structural change (e.g. a performer unexpectedly repeats a section) problems. We test our system on 3 jazz lead sheets. For each sheet we align a set of solo piano performances and a set of fullband commercial recordings with different instrumentation and styles. Results show that our system achieves promising results on some highly improvised music. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Alignment methods; Full band; MIDI files; Structural change; Structural information; Audio acoustics","Z. Duan; Department of Electrical Engineering and Computer Science, Northwestern University, United States; email: zhiyaoduan00@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Laitinen M.; Lemström K.","Laitinen, Mika (7004962831); Lemström, Kjell (7006564183)","7004962831; 7006564183","Dynamic programming in transposition and time-warp invariant polyphonic content-based music retrieval","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573437&partnerID=40&md5=84175f909aef369607a0f930a52dd378","Department of Computer Science, University of Helsinki, Finland","Laitinen M., Department of Computer Science, University of Helsinki, Finland; Lemström K., Department of Computer Science, University of Helsinki, Finland","We consider the problem of transposition and time-warp invariant (TTWI) polyphonic content-based music retrieval (CBMR) in symbolically encoded music. For this setting, we introduce two new algorithms based on dynamic programming. Given a query point set, of sizem, to be searched for in a database point set, of size n, and applying a search window of width w, our algorithms run in time O(mnw) for finding exact TTWI occurrences, and O(mnw2) for partial occurrences. Our new algorithms are computationally more efficient as their counterparts in the worst case scenario. More importantly, the elegance of our algorithms lies in their simplicity: they are much easier to implement and to understand than the rivalling sweepline-based algorithms. Our solution bears also theoretical interest. Dynamic programming has been used in very basic content-based retrieval problems, but generalizing them to more complex cases has proven to be challenging. In this special, seemingly more complex case, however, dynamic programming seems to be a viable option. © 2011 International Society for Music Information Retrieval.","","Content based retrieval; Dynamic programming; Geometry; Information retrieval; Query processing; Content-based music retrieval; Point set; Query points; Search windows; Worst case scenario; Algorithms","M. Laitinen; Department of Computer Science, University of Helsinki, Finland; email: mikalait@cs.helsinki.fi","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Müller M.; Ewert S.","Müller, Meinard (7404689873); Ewert, Sebastian (32667575400)","7404689873; 32667575400","Chroma toolbox: Matlab implementations for extracting variants of chroma-based audio features","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","148","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870562580&partnerID=40&md5=bfc257295369eb972f9755a950a6b772","MPI Informatik, Saarland University, Germany; Computer Science III, University of Bonn, Germany","Müller M., MPI Informatik, Saarland University, Germany; Ewert S., Computer Science III, University of Bonn, Germany","Chroma-based audio features, which closely correlate to the aspect of harmony, are a well-established tool in processing and analyzing music data. There are many ways of computing and enhancing chroma features, which results in a large number of chroma variants with different properties. In this paper, we present a chroma toolbox [13], which contains MATLAB implementations for extracting various types of recently proposed pitch-based and chroma-based audio features. Providing the MATLAB implementations on a welldocumented website under a GNU-GPL license, our aim is to foster research in music information retrieval. As another goal, we want to raise awareness that there is no single chroma variant that works best in all applications. To this end, we discuss two example applications showing that the final music analysis result may crucially depend on the initial feature design step. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Chroma features; Design steps; Music analysis; Music data; Music information retrieval; MATLAB","M. Müller; MPI Informatik, Saarland University, Germany; email: meinard@mpi-inf.mpg.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Chen R.; Li M.","Chen, Ruofeng (55583646800); Li, Ming (56994217300)","55583646800; 56994217300","Music structural segmentation by combining harmonic and timbral information","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869803250&partnerID=40&md5=a02f04b54f66891a08308def71f0d9c5","Georgia Tech Center for Music Technology, Georgia Institute of Technology, United States; Chinese Academy of Sciences, Institute of Acoustics, China","Chen R., Georgia Tech Center for Music Technology, Georgia Institute of Technology, United States; Li M., Chinese Academy of Sciences, Institute of Acoustics, China","We propose a novel model for music structural segmentation aiming at combining harmonic and timbral information. We use two-level clustering with splitting initialization and random turbulence to produce segment labels using chroma and MFCC separately as feature. We construct a score matrix to combine segment labels from both aspects. Finally Nonnegative Matrix Factorization and Maximum Likelihood are applied to extract the final segment labels. By comparing sparseness, our method is capable of automatically determining the number of segment types in a given song. The pairwise F-measure of our algorithm can reach 0.63 without rules of music knowledge, running on 180 Beatles songs. We show our model can be easily associated with more sophisticated structural segmentation algorithms and extended to probabilistic models. © 2011 International Society for Music Information Retrieval.","","Information retrieval; F-measure; Music knowledge; Nonnegative matrix factorization; Probabilistic models; Random turbulence; Score matrixes; Segmentation algorithms; Matrix algebra","R. Chen; Georgia Tech Center for Music Technology, Georgia Institute of Technology, United States; email: ruofengchen@gatech.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Wu F.-H.F.; Lee T.-C.; Jang J.-S.R.; Chang K.K.; Lu C.H.; Wang W.N.","Wu, Fu-Hai Frank (55586742700); Lee, Tsung-Chi (55586608900); Jang, Jyh-Shing Roger (7402965041); Chang, Kaichun K. (55516277600); Lu, Chun Hung (56174622600); Wang, Wen Nan (35410685100)","55586742700; 55586608900; 7402965041; 55516277600; 56174622600; 35410685100","A two-fold dynamic programming approach to beat tracking for audio music with time-varying tempo","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574002&partnerID=40&md5=ccad15499d8d2988808a611ac6d01011","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, King's College London, London, United Kingdom; Institute for Information Industry, IDEAS, Taipei, Taiwan","Wu F.-H.F., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Lee T.-C., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Chang K.K., Department of Computer Science, King's College London, London, United Kingdom; Lu C.H., Institute for Information Industry, IDEAS, Taipei, Taiwan; Wang W.N., Institute for Information Industry, IDEAS, Taipei, Taiwan","Automatic beat tracking and tempo estimation are challenging tasks, especially for audio music with timevarying tempo. This paper proposes a two-fold dynamic programming (DP) approach to deal with beat tracking with time-varying tempo. In particular, the first DP computes the tempo curve from the tempogram. The second DP identifies the optimum beat positions from the novelty and tempo curves. Experimental results demonstrate satisfactory performance for music with significant tempo variations. The proposed approach was submitted to the task of audio beat tracking in MIREX 2010 and was ranked no. 1 for 6 performance indices out of 10, for the dataset with variable tempo. © 2011 International Society for Music Information Retrieval.","Beat tracking; Dynamic programming; Tempogram; Time-varying tempo; Viterbi search","Dynamic positioning; Information retrieval; Viterbi algorithm; Audio beat tracking; Audio music; Beat tracking; Performance indices; Tempo estimations; Tempogram; Time varying; Viterbi search; Dynamic programming","F.-H.F. Wu; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; email: frankwu@mirlab.org","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Lee J.H.","Lee, Jin Ha (57190797465)","57190797465","How similar is too similar?: Exploring users perceptions of similarity in playlist evaluation","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867180950&partnerID=40&md5=91277f3b081402deb9800505714728df","University of Washington, United States","Lee J.H., University of Washington, United States","The Audio Music Similarity and Retrieval (AMS) task in the annual Music Information Retrieval eXchange relies on human-evaluation. One limitation of the current design of AMS is that evaluators are provided with scarce contextual information as to why they are evaluating the similarity of the songs and how this information will be used. This study explores the potential use of AMS results for generating playlists based on similarity. We asked participants to listen to a subset of results from the 2010 AMS task and evaluate the set of candidates generated by the algorithms as a playlist generated from a seed song (the query). We found that while similarity does affect how people feel about the candidate set as a playlist, other factors such as variety, metadata, personal preference, familiarity, mix of familiar and new music, etc. also strongly affect users' perceptions of playlist quality as well. We discuss six user behaviors in detail and the implications for the AMS evaluation task. © 2011 International Society for Music Information Retrieval.","","Behavioral research; Metadata; Audio music; Candidate sets; Contextual information; Music information retrieval; Personal preferences; User behaviors; Users perceptions; Users' perception; Information retrieval","J.H. Lee; University of Washington, United States; email: jinhalee@uw.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Urbano J.; Martín D.; Marrero M.; Morato J.","Urbano, Julián (36118414700); Martín, Diego (13905154700); Marrero, Mónica (36117926400); Morato, Jorge (57219289030)","36118414700; 13905154700; 36117926400; 57219289030","Audio music similarity and retrieval: Evaluation power and stability","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861089565&partnerID=40&md5=5a1690f2488c105cd6840688c971cacd","Department of Computer Science, University Carlos III of Madrid, Spain","Urbano J., Department of Computer Science, University Carlos III of Madrid, Spain; Martín D., Department of Computer Science, University Carlos III of Madrid, Spain; Marrero M., Department of Computer Science, University Carlos III of Madrid, Spain; Morato J., Department of Computer Science, University Carlos III of Madrid, Spain","In this paper we analyze the reliability of the results in the evaluation of Audio Music Similarity and Retrieval systems. We focus on the power and stability of the evaluation, that is, how often a significant difference is found between systems and how often these significant differences are incorrect. We study the effect of using different effectiveness measures with different sets of relevance judgments, for varying number of queries and alternative statistical procedures. Different measures are shown to behave similarly overall, though some are much more sensitive and stable than others. The use of different statistical procedures does improve the reliability of the results, and it allows using as little as half the number of queries currently used in MIREX evaluations while still offering very similar reliability levels. We also conclude that experimenters can be very confident that if a significant difference is found between two systems, the difference is indeed real. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Statistical methods; Audio music; Effectiveness measure; Relevance judgment; Reliability level; Retrieval systems; Significant differences; Reliability","J. Urbano; Department of Computer Science, University Carlos III of Madrid, Spain; email: jurbano@inf.uc3m.es","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Li J.; Li T.; Ogihara M.","Li, Jingxuan (54951373400); Li, Tao (55553727585); Ogihara, Mitsunori (54420747900)","54951373400; 55553727585; 54420747900","Hierarchical co-clustering of music artists and tags","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597367&partnerID=40&md5=8226bf67640f247ef3e59a84c63dfc3b","School of Computer Science, Florida International University, Miami, FL, United States; Department of Computer Science, University of Miami, Coral Gables, FL, United States","Li J., School of Computer Science, Florida International University, Miami, FL, United States; Li T., School of Computer Science, Florida International University, Miami, FL, United States; Ogihara M., Department of Computer Science, University of Miami, Coral Gables, FL, United States","The user-assigned tag is a growingly important research topic in MIR. Noticing that some tags are more specific versions of others, this paper studies the problem of organizing tags into a hierarchical structure by taking into ac- count the fact that the corresponding artists are organized into a hierarchy based on genre and style. A novel clustering algorithm, Hierarchical Co-clustering Algorithm (HCC), is proposed as a solution. Unlike traditional hierarchical clustering algorithms that deal with homogeneous data only, the proposed algorithm simultaneously organizes two distinct data types into hierarchies. HCC is additionally able to receive constraints that state certain objects ""must-be-together"" or ""should-be-together"" and build clusters so as to satisfying the constraints. HCC may lead to better and deeper understandings of relationship between artists and tags assigned to them. An experiment finds that by trying to hierarchically cluster the two types of data better clusters are obtained for both. It is also shown that HCC is able to incorporate instance-level constraints on artists and/or tags to improve the clustering process. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Clustering process; Co-clustering; Data type; Hierarchical clustering algorithms; Hierarchical structures; Research topics; Clustering algorithms","J. Li; School of Computer Science, Florida International University, Miami, FL, United States; email: jli003@cs.fiu.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Xiao Q.; Suzuki M.; Kita K.","Xiao, Qingmei (53664628200); Suzuki, Motoyuki (56037004700); Kita, Kenji (7201529301)","53664628200; 56037004700; 7201529301","Fast hamming space search for audio fingerprinting systems","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578580&partnerID=40&md5=7c75d42a1b862f8ecaa9ebaf3619de30","Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan","Xiao Q., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan; Suzuki M., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan; Kita K., Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan","In music information retrieval, a huge search space has to be explored because a query audio clip can start at any position of any music in the database, and also a query is often corrupted by significant noise and distortion. Audio fingerprints have recently attracted much attention in music information retrieval, for they provide a compact representation of the perceptually relevant parts of audio signals. In this paper, we propose an extremely fast method of exploring a huge Hamming space for audio fingerprinting systems. The effectiveness of the proposed method has been evaluated by experiments using a database of 8,740 songs. © 2011 International Society for Music Information Retrieval.","","Audio systems; Query processing; Audio clips; Audio fingerprint; Audio fingerprinting; Audio signal; Compact representation; Fast methods; Hamming space; Music information retrieval; Search spaces; Information retrieval","Q. Xiao; Faculty of Engineering, University of Tokushima, Tokushima 770-8506, Japan; email: xiaoqingmei@iss.tokushima-u.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Sanden C.; Zhang J.Z.","Sanden, Chris (25723672800); Zhang, John Z. (16551442100)","25723672800; 16551442100","An empirical study of multi-label classifiers for music tag annotation","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584325&partnerID=40&md5=822e0d8da41951b08c742205a916e125","Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada","Sanden C., Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada; Zhang J.Z., Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada","In this paper we study the problem of automatic music tag annotation. Treating tag annotation as a computational classification process, we attempt to explore the relationship between acoustic features and music tags. Toward this end, we conduct a series of empirical experiments to evaluate a set of multi-label classifiers and demonstrate which ones are more suitable for music tag annotation. Furthermore, we discuss various factors in the classification process, such as feature sets, frame sizes, etc. Experiments on two publicly available datasets show that the Calibrated Label Ranking (CLR) algorithm outperforms the other classifiers for a selection of evaluation measures. © 2011 International Society for Music Information Retrieval.","","Experiments; Information retrieval; Acoustic features; Classification process; Empirical experiments; Empirical studies; Evaluation measures; Feature sets; Frame size; Label rankings; Multi-label; Classification (of information)","C. Sanden; Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada; email: sanden@cs.uleth.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Ahonen T.E.; Lemstr̈om K.; Linkola S.","Ahonen, Teppo E. (36719987900); Lemstr̈om, Kjell (7006564183); Linkola, Simo (55586522400)","36719987900; 7006564183; 55586522400","Compression-based similarity measures in symbolic, polyphonic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873441308&partnerID=40&md5=fdec2b19718cab65f153c224b72bb9e4","Department of Computer, Science University of Helsinki, Finland","Ahonen T.E., Department of Computer, Science University of Helsinki, Finland; Lemstr̈om K., Department of Computer, Science University of Helsinki, Finland; Linkola S., Department of Computer, Science University of Helsinki, Finland","We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Classical musics; Measuring similarities; MIDI files; Normalized compression distance; Polyphonic music; Precision and recall; Similarity measure; Similarity measuring; Test sets; Binary sequences","T.E. Ahonen; Department of Computer, Science University of Helsinki, Finland; email: teahonen@cs.helsinki.fi","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"McFee B.; Lanckriet G.","McFee, Brian (34875379700); Lanckriet, Gert (7801431767)","34875379700; 7801431767","The natural language of playlists","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","85","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581058&partnerID=40&md5=47525511cb6c5c9da4ad3c14b72f66d5","Department of Computer Science and Engineering, University of California, San Diego, United States; Department of Electrical and Computer Engineering, University of California, San Diego, United States","McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States","We propose a simple, scalable, and objective evaluation procedure for playlist generation algorithms. Drawing on standard techniques for statistical natural language processing, we characterize playlist algorithms as generative models of strings of songs belonging to some unknown language. To demonstrate the procedure, we compare several playlist algorithms derived from content, semantics, and meta-data. We then develop an efficient algorithm to learn an optimal combination of simple playlist algorithms. Experiments on a large collection of naturally occurring playlists demonstrate the efficacy of the evaluation procedure and learning algorithm. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Learning algorithms; Semantics; Generation algorithm; Generative model; Natural languages; Naturally occurring; Objective evaluation; Optimal combination; Statistical natural language processing; Natural language processing systems","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Nakashika T.; Takiguchi T.; Ariki Y.","Nakashika, Toru (35226650700); Takiguchi, Tetsuya (7005058510); Ariki, Yasuo (7003402234)","35226650700; 7005058510; 7003402234","Constrained spectrum generation using a probabilistic spectrum envelope for mixed music analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585493&partnerID=40&md5=eb02ef3bfcd04673e1d9d06d39795520","Department of Computer Science and Systems Engineering, Kobe University, Japan","Nakashika T., Department of Computer Science and Systems Engineering, Kobe University, Japan; Takiguchi T., Department of Computer Science and Systems Engineering, Kobe University, Japan; Ariki Y., Department of Computer Science and Systems Engineering, Kobe University, Japan","NMF (Non-negative Matrix Factorization) has been one of the most widely-used techniques for musical signal analysis in recent years. In particular, the supervised type of NMF is garnering much attention in source separation with respect to the analysis accuracy and speed. In this approach, a large number of spectral samples is used for analyzing a signal. If the system has a minimal number of samples, the accuracy deteriorates. Because such methods require all the possible samples for the analysis, it is hard to build a practical analysis system. To analyze signals properly even when short of samples, we propose a novel method that combines a supervised NMF and probabilistic search algorithms. In this approach, it is assumed that each instrumental category has a model-invariant feature called a probabilistic spectrum envelope (PSE). The algorithm starts with learning the PSEs of each category using a technique based on Gaussian Process Regression. Using the PSEs for spectrum generation, an observed spectrum is analyzed under the framework of a supervised NMF. The optimum spectrum can be searched by Genetic Algorithm using sparseness and density constraints. © 2011 International Society for Music Information Retrieval.","","Algorithms; Factorization; Information retrieval; Signal analysis; Analysis accuracy; Analysis system; Density constraints; Gaussian process regression; Music analysis; Nonnegative matrix factorization; Number of samples; Probabilistic search algorithms; Spectrum analysis","T. Nakashika; Department of Computer Science and Systems Engineering, Kobe University, Japan; email: nakashika@me.cs.scitec.kobe-u.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Gang R.; Bocko G.; Lundberg J.; Roessner S.; Headla D.; Bocko M.F.","Gang, Ren (23392600200); Bocko, Gregory (37074115100); Lundberg, Justin (35766750100); Roessner, Stephen (55134460500); Headla, Dave (55586621200); Bocko, Mark F. (7004544285)","23392600200; 37074115100; 35766750100; 55134460500; 55586621200; 7004544285","A real-time signal processing framework of musical expressive feature extraction using matlab","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475009&partnerID=40&md5=d999bf0253c8046467868445f25036f9","Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Dept. of Music Theory, Eastman School of Music, University of Rochester, United States","Gang R., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Bocko G., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Lundberg J., Dept. of Music Theory, Eastman School of Music, University of Rochester, United States; Roessner S., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; Headla D., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States, Dept. of Music Theory, Eastman School of Music, University of Rochester, United States; Bocko M.F., Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States, Dept. of Music Theory, Eastman School of Music, University of Rochester, United States","In this paper we propose a real-time signal processing framework for musical audio that 1) aligns the audio with an existing music score or creates a musical score by automated music transcription algorithms; and 2) obtains the expressive feature descriptors of music performance by comparing the score with the audio. Real-time audio segmentation algorithms are implemented to identify the onset points of music notes in the incoming audio stream. The score related features and musical expressive features are extracted based on these segmentation results. In a realtime setting, these audio segmentation and feature extraction operations have to be accomplished at (or shortly after) the note onset points, when an incomplete length of audio signal is captured. To satisfy real-time processing requirements while maintaining feature accuracy, our proposed framework combines the processing stages of prediction, estimation, and updating in both audio segmentation and feature extraction algorithms in an integrated refinement process. The proposed framework is implemented in a MATLAB real-time signal processing framework. © 2011 International Society for Music Information Retrieval.","","Algorithms; Audio signal processing; Audio systems; Computer music; Feature extraction; Image segmentation; Information retrieval; Signal analysis; Audio segmentation; Audio signal; Audio stream; Extraction operation; Feature descriptors; Feature extraction algorithms; Music notes; Music performance; Music scores; Music transcription; Musical audio; Musical score; Processing stage; Real-time audio; Real-time settings; Real-time signal processing; Realtime processing; Refinement process; Segmentation results; Audio acoustics","R. Gang; Dept. of Electrical and Computer Engineering, Edmund A. Hajim School of Engineering and Applied Sciences, University of Rochester, United States; email: g.ren@rochester.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Koenigstein N.; Shavitt Y.; Weinsberg E.; Weinsberg U.","Koenigstein, Noam (26534475900); Shavitt, Yuval (7004130839); Weinsberg, Ela (36626525800); Weinsberg, Udi (23468130500)","26534475900; 7004130839; 36626525800; 23468130500","On the applicability of peer-to-peer data in music information retrieval research","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606759&partnerID=40&md5=b9239696e5583b9b759c2eee5bd79af8","School of Electrical Engineering, Tel-Aviv University, Israel; Dept. of Industrial Engineering, Tel-Aviv University, Israel","Koenigstein N., School of Electrical Engineering, Tel-Aviv University, Israel; Shavitt Y., School of Electrical Engineering, Tel-Aviv University, Israel; Weinsberg E., Dept. of Industrial Engineering, Tel-Aviv University, Israel; Weinsberg U., School of Electrical Engineering, Tel-Aviv University, Israel","Peer-to-Peer (P2P) networks are being increasingly adopted as an invaluable resource for variousmusic information retrieval (MIR) tasks, including music similarity, recommendation and trend prediction. However, these networks are usually extremely large and noisy, which raises doubts regarding the ability to actually extract sufficiently accurate information. This paper evaluates the applicability of using data originating from p2p networks for MIR research, focusing on partial crawling, inherent noise and localization of songs and search queries. These aspects are quantified using songs collected from the Gnutella p2p network. We show that the power-law nature of the network makes it relatively easy to capture an accurate view of the main-streams using relatively little effort. However, some applications, like trend prediction, mandate collection of the data from the ""long tail"", hence a much more exhaustive crawl is needed. Furthermore, we present techniques for overcoming noise originating from user generated content and for filtering non informative data, while minimizing information loss. © 2010 International Society for Music Information Retrieval.","","Distributed computer systems; Information filtering; Information retrieval; Gnutella; Information loss; Inherent noise; Long tail; Music information retrieval; Music similarity; P2P network; Peer to peer; Peer to Peer (P2P) network; Power-law; Search queries; Trend prediction; User-generated content; Peer to peer networks","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Wolff D.; Weyde T.","Wolff, Daniel (35091498700); Weyde, Tillman (24476899500)","35091498700; 24476899500","Adapting metrics for music similarity using comparative ratings","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578459&partnerID=40&md5=f98ac4f3744bd035945e3f94f49639dc","Department of Computing, City University London, United Kingdom","Wolff D., Department of Computing, City University London, United Kingdom; Weyde T., Department of Computing, City University London, United Kingdom","Understanding how we relate and compare pieces of music has been a topic of great interest in musicology as well as for business applications, such as music recommender systems. The way music is compared seems to vary among both individuals and cultures. Adapting a generic model to user ratings is useful for personalisation and can help to better understand such differences. This paper presents an approach to use machine learning techniques for analysing user data that specifies song similarity. We explore the potential for learning generalisable similarity measures with two stateof- the-art algorithms for learning metrics. We use the audio clips and user ratings in the MagnaTagATune dataset, enriched with genre annotations from the Magnatune label. © 2011 International Society for Music Information Retrieval.","","Learning systems; Audio clips; Business applications; Generic models; Learning metrics; Machine learning techniques; Music recommender systems; Music similarity; Personalisation; Similarity measure; User data; User rating; Information retrieval","D. Wolff; Department of Computing, City University London, United Kingdom; email: daniel.wolff.1@soi.city.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Panagakis Y.; Kotropoulos C.; Arce G.R.","Panagakis, Yannis (35503932300); Kotropoulos, Constantine (35563688200); Arce, Gonzalo R. (7006653894)","35503932300; 35563688200; 7006653894","ℓ1-Graph based music structure analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869827694&partnerID=40&md5=de17a721d0e15a011c4e49f0c812bf2a","Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE 19716-3130, United States","Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE 19716-3130, United States","An unsupervised approach for automatic music structure analysis is proposed resorting to the following assumption: If the feature vectors extracted from a specific music segment are drawn from a single subspace, then the sequence of feature vectors extracted from a music recording will lie in a union of as many subspaces as the music segments in this recording are. It is well known that each feature vector stemming from a union of independent linear subspaces admits a sparse representation with respect to a dictionary formed by all other feature vectors with nonzero coefficients associated only to feature vectors that stem from its own subspace. Such sparse representation reveals the relationships among the feature vectors and it is used to construct a similarity graph, the so-called ℓ1-graph. Accordingly, the segmentation of audio features is obtained by applying spectral clustering to the ℓ1-graph. The performance of the just described approach is assessed by conducting experiments on the Pop- Music and the UPF Beatles benchmark datasets. Promising results are reported. © 2011 International Society for Music Information Retrieval.","","Audio recordings; Benchmarking; Information retrieval; Audio features; Benchmark datasets; Feature vectors; Linear subspace; Music recording; Music segments; Music structure analysis; Non-zero coefficients; Sparse representation; Spectral clustering; Unsupervised approaches; Vectors","Y. Panagakis; Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; email: panagakis@aiia.csd.auth.gr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Constantin C.; Faget Z.; Mouza C.D.; Rigaux P.","Constantin, Camelia (14321257900); Faget, Zoé (6505812277); Mouza, Cédric Du (9432607600); Rigaux, Philippe (57204372163)","14321257900; 6505812277; 9432607600; 57204372163","The melodic signature index for fast content-based retrieval of symbolic scores","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865823620&partnerID=40&md5=af0543c3259118c06e5deb2fb38e90ab","LIP6, Univ. Paris 6, Paris, France; Armadillo and Univ. Paris-Dauphine, France; CEDRIC, CNAM, France","Constantin C., LIP6, Univ. Paris 6, Paris, France; Faget Z., Armadillo and Univ. Paris-Dauphine, France; Mouza C.D., CEDRIC, CNAM, France; Rigaux P., Armadillo and Univ. Paris-Dauphine, France","NEUMA is an on-line library that stores collections of symbolic scores and proposes a public interface to search for melodic pieces based on several kinds of patterns: pitchesbased, with or without rhythms, transposed or not. In addition, searches can be either exact or approximate. We describe an index structure apt at supporting all these searches in a consistent setting. Its distinctive feature is an encoding of the various information that might be involved in the pattern-matching process with algebraic signatures. The properties of these signatures are suitable to represent in a compact and expressive way the sequences of complex features that constitute a melodic description. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Algebraic signatures; Index structure; Content based retrieval","C. Constantin; LIP6, Univ. Paris 6, Paris, France; email: camelia.constantin@lip6.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Abdoli S.","Abdoli, Sajjad (55586242200)","55586242200","Iranian traditional music dastgah classification","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869173020&partnerID=40&md5=35eff515d5430453c8ab68d698f7e6c0","Computer Department, Central Tehran Branch, Islamic Azad University, Tehran, Iran","Abdoli S., Computer Department, Central Tehran Branch, Islamic Azad University, Tehran, Iran","In this study, a system for Iranian traditional music Dastgah classification is presented. Persian music is based upon a set of seven major Dastgahs. The Dastgah in Persian music is similar to western musical scales and also Maqams in Turkish and Arabic music. Fuzzy logic type 2 as the basic part of our system has been used for modeling the uncertainty of tuning the scale steps of each Dastgah. The method assumes each performed note as a Fuzzy Set (FS), so each musical piece is a set of FSs. The maximum similarity between this set and theoretical data indicates the desirable Dastgah. In this study, a collection of small-sized dataset for Persian music is also given. The results indicate that the system works accurately on the dataset. © 2011 International Society for Music Information Retrieval.","","Fuzzy logic; Fuzzy sets; Uncertainty analysis; Musical pieces; Musical scale; Persian musics; Turkishs; Information retrieval","S. Abdoli; Computer Department, Central Tehran Branch, Islamic Azad University, Tehran, Iran; email: Saj.abdoli@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Kirlin P.B.; Jensen D.D.","Kirlin, Phillip B. (55582044600); Jensen, David D. (7402549436)","55582044600; 7402549436","Probabilistic modeling of hierarchical music analysis","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867605584&partnerID=40&md5=1e9500f030275c8944ac3e4f6d3cacac","Department of Computer Science, University of Massachusetts, Amherst, United States","Kirlin P.B., Department of Computer Science, University of Massachusetts, Amherst, United States; Jensen D.D., Department of Computer Science, University of Massachusetts, Amherst, United States","Hierarchical music analysis, as exemplified by Schenkerian analysis, describes the structure of a musical composition by a hierarchy among its notes. Each analysis defines a set of prolongations, where musical objects persist in time even though others are present. We present a formal model for representing hierarchical music analysis, probabilistic interpretations of that model, and an efficient algorithm for computing the most probable analysis under these interpretations. We represent Schenkerian analyses as maximal outerplanar graphs (MOPs). We use this representation to encode the largest known data set of computer-processable Schenkerian analyses, and we use these data to identify statistical regularities in the human-generated analyses. We show that a dynamic programming algorithm can be applied to these regularities to identify the maximum likelihood analysis for a given piece of music. © 2011 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Data set; Dynamic programming algorithm; Formal model; Maximum likelihood analysis; Music analysis; Musical composition; Outerplanar graph; Probabilistic interpretation; Probabilistic modeling; Statistical regularity; Maximum likelihood estimation","P.B. Kirlin; Department of Computer Science, University of Massachusetts, Amherst, United States; email: pkirlin@cs.umass.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Grosche P.; Müller M.; Sapp C.S.","Grosche, Peter (55413290700); Müller, Meinard (7404689873); Sapp, Craig Stuart (14032002500)","55413290700; 7404689873; 14032002500","What makes beat tracking difficult? A case study on chopin mazurkas","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595480&partnerID=40&md5=2675eb793e97702ba70a208b7ba407cd","MPI Informatik, Saarland University, Germany; CCRMA / CCARH, Standford University, United States","Grosche P., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany; Sapp C.S., CCRMA / CCARH, Standford University, United States","The automated extraction of tempo and beat information from music recordings is a challenging task. Especially in the case of expressive performances, current beat tracking approaches still have significant problems to accurately capture local tempo deviations and beat positions. In this paper, we introduce a novel evaluation framework for detecting critical passages in a piece of music that are prone to tracking errors. Our idea is to look for consistencies in the beat tracking results over multiple performances of the same underlying piece. As another contribution, we further classify the critical passages by specifying musical properties of certain beats that frequently evoke tracking errors. Finally, considering three conceptually different beat tracking procedures, we conduct a case study on the basis of a challenging test set that consists of a variety of piano performances of Chopin Mazurkas. Our experimental results not only make the limitations of state-of-the-art beat trackers explicit but also deepens the understanding of the underlying music material. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Automated extraction; Beat tracking; Evaluation framework; Expressive performance; Music materials; Music recording; Test sets; Tracking errors; Errors","P. Grosche; MPI Informatik, Saarland University, Germany; email: pgrosche@mpi-inf.mpg.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Knees P.; Schedl M.; Pohle T.; Seyerlehner K.; Widmer G.","Knees, Peter (8219023200); Schedl, Markus (8684865900); Pohle, Tim (14036302300); Seyerlehner, Klaus (23996001400); Widmer, Gerhard (7004342843)","8219023200; 8684865900; 14036302300; 23996001400; 7004342843","Supervised and unsupervised web document filtering techniques to improve text-based music retrieval","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594554&partnerID=40&md5=906d104a99f8318679d8cf48c16c23ac","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We aim at improving a text-based music search engine by applying different techniques to exclude misleading information from the indexing process. The idea of the original approach is to index music pieces by ""contextual"" information, more precisely, by all texts to be found on Web pages retrieved via a commonWeb search engine. This representation allows for issuing arbitrary textual queries to retrieve relevant music pieces. The goal of this work is to improve precision of the retrieved set of music pieces by filtering outWeb pages that lead to irrelevant tracks. To this end we present two unsupervised and two supervised filtering approaches. Evaluation is carried out on two collections previously used in the literature. The obtained results suggest that the proposed filtering techniques can improve results significantly but are only effective when applied to large and diverse music collections with millions of Web pages associated. © 2010 International Society for Music Information Retrieval.","","Search engines; Websites; Filtering technique; Indexing process; Misleading informations; Music collection; Music retrieval; Textual query; Web document; Information retrieval","P. Knees; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: peter.knees@jku.at","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Lee J.H.","Lee, Jin Ha (57190797465)","57190797465","Crowdsourcing music similarity judgments using mechanical turk","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598993&partnerID=40&md5=bc2baf2215eda3547385c2e9fbd5cc9b","University of Washington, United States","Lee J.H., University of Washington, United States","Collecting human judgments for music similarity evaluation has always been a difficult and time consuming task. This paper explores the viability of Amazon Mechanical Turk (MTurk) for collecting human judgments for audio music similarity evaluation tasks. We compared the similarity judgments collected from Evalutron6000 (E6K) and MTurk using the Music Information Retrieval Evaluation eXchange 2009 Audio Music Similarity and Retrieval task dataset. Our data show that the results are highly comparable, and MTurk may be a useful method for collecting subjective ground truth data. Furthermore, there are several benefits to using MTurk over the traditional E6K infrastructure. We conclude that using MTurk is a practical alternative of music similarity when it is used with some precautions. © 2010 International Society for Music Information Retrieval.","","Amazon mechanical turks; Audio music; Crowdsourcing; Ground truth data; Human judgments; Mechanical turks; Music information retrieval; Music similarity; Time-consuming tasks; Information retrieval","J.H. Lee; University of Washington, United States; email: jinhalee@uw.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Serrá J.; Koduri G.K.; Miron M.; Serra X.","Serrá, Joan (35749172500); Koduri, Gopala K. (36721381900); Miron, Marius (55585881400); Serra, Xavier (55892979900)","35749172500; 36721381900; 55585881400; 55892979900","Assessing the tuning of sung indian classical music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449697&partnerID=40&md5=9e0803372c5679e34a1c9cfab73634b4","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Serrá J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Koduri G.K., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Miron M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The issue of tuning in Indian classical music has been, historically, a matter of theoretical debate. In this paper, we study its contemporary practice in sung performances of Carnatic and Hindustani music following an empiric and quantitative approach. To do so, we select stable fundamental frequencies, estimated via a standard algorithm, and construct interval histograms from a pool of recordings. We then compare such histograms against the ones obtained for different music sources and against the theoretical values derived from 12-note just intonation and equal temperament. Our results evidence that the tunings in Carnatic and Hindustani music differ, the former tending to a just intonation system and the latter having much equal-tempered influences. Carnatic music also presents signs of a more continuous distribution of pitches. Further subdivisions of the octave are partially investigated, finding no strong evidence of them. © 2011 International Society for Music Information Retrieval.","","Graphic methods; Contemporary practices; Continuous distribution; Fundamental frequencies; Indian classical music; Quantitative approach; Standard algorithms; Theoretical values; Information retrieval","J. Serrá; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: joan.serraj@upf.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mcvicar M.; Freeman T.; Bie T.D.","Mcvicar, Matt (36721968600); Freeman, Tim (55586099300); Bie, Tijl De (57203775071)","36721968600; 55586099300; 57203775071","Mining the correlation between lyrical and audio features and the emergence of mood","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872703021&partnerID=40&md5=0a881fdf7e5d7d21b068a102272044e0","Intelligent Systems Lab, University of Bristol, United Kingdom; University of Bristol, Engineering Mathematics, United Kingdom","Mcvicar M., Intelligent Systems Lab, University of Bristol, United Kingdom; Freeman T., University of Bristol, Engineering Mathematics, United Kingdom; Bie T.D., Intelligent Systems Lab, University of Bristol, United Kingdom","Understanding the mood of music holds great potential for recommendation and genre identification problems. Unfortunately, hand-annotating music with mood tags is usually an expensive, time-consuming and subjective process, to such an extent that automatic mood recognition methods are required. In this paper we present a new unsupervised learning approach for mood recognition, based on the lyrics and the audio of a song. Our system thus eliminates the need for ground truth mood annotations, even for training the system. We hypothesize that lyrics and audio are both partially determined by the mood, and that there are no other strong common effects affecting these aspects of music. Based on this assumption, mood can be detected by performing a multi-modal analysis, identifying what lyrics and audio have in common. We demonstrate the effectiveness of this using Canonical Correlation Analysis, and confirm our hypothesis in a subsequent analysis of the results. © 2011 International Society for Music Information Retrieval.","","Modal analysis; Audio features; Canonical correlation analysis; Genre identification; Ground truth; Multi-modal; Recognition methods; Information retrieval","M. Mcvicar; Intelligent Systems Lab, University of Bristol, United Kingdom; email: matt.mcvicar@bris.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Kolozali S.; Barthet M.; Fazekas G.; Sandler M.","Kolozali, Sefki (49961785700); Barthet, Mathieu (24723525000); Fazekas, György (37107520200); Sandler, Mark (7202740804)","49961785700; 24723525000; 37107520200; 7202740804","Knowledge representation issues in musical instrument ontology design","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587735&partnerID=40&md5=f1665e6d5f4664435305f43c916feb85","Centre for Digital Music, Queen Mary University of London, London, United Kingdom","Kolozali S., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Barthet M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, United Kingdom","This paper presents preliminary work on musical instruments ontology design, and investigates heterogeneity and limitations in existing instrument classification schemes. Numerous research to date aims at representing information about musical instruments. The works we examined are based on the well known Hornbostel and Sach's classification scheme. We developed representations using the Ontology Web Language (OWL), and compared terminological and conceptual heterogeneity using SPARQL queries. We found evidence to support that traditional designs based on taxonomy trees lead to ill-defined knowledge representation, especially in the context of an ontology for the Semantic Web. In order to overcome this issue, it is desirable to have an instrument ontology that exhibits a semantically rich structure. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Knowledge representation; Classification scheme; Ontology design; Ontology web language; Rich structure; Sparql queries; Musical instruments","S. Kolozali; Centre for Digital Music, Queen Mary University of London, London, United Kingdom; email: sefki.kolozali@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hankinson A.; Roland P.; Fujinaga I.","Hankinson, Andrew (54970482500); Roland, Perry (55583223600); Fujinaga, Ichiro (9038140900)","54970482500; 55583223600; 9038140900","The music encoding initiative as a document-encoding framework","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581512&partnerID=40&md5=f3c5f7a4eab15ce467dca2b0640e20eb","CIRMMT, Schulich School of Music, McGill University, Canada; University of Virginia, United States","Hankinson A., CIRMMT, Schulich School of Music, McGill University, Canada; Roland P., University of Virginia, United States; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","Recent changes in the Music Encoding Initiative (MEI) have transformed it into an extensible platform from which new notation encoding schemes can be produced. This paper introduces MEI as a document-encoding framework, and illustrates how it can be extended to encode new types of notation, eliminating the need for creating specialized and potentially incompatible notation encoding standards. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Encoding schemes; Encoding (symbols)","A. Hankinson; CIRMMT, Schulich School of Music, McGill University, Canada; email: andrew.hankinson@mail.mcgill.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Viro V.","Viro, Vladimir (55586304300)","55586304300","Peachnote: Music score search and analysis platform","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572779&partnerID=40&md5=218b4cf371b48255fe35838cc56644e2","Ludwig-Maximilians-University, Munich, Germany","Viro V., Ludwig-Maximilians-University, Munich, Germany","Hundreds of thousands of music scores are being digitized by libraries all over the world. In contrast to books, they generally remain inaccessible for content-based retrieval and algorithmic analysis. There is no analogue to Google Books for music scores, and there exist no large corpora of symbolic music data that would empower musicology in the way large text corpora are empowering computational linguistics, sociology, history, and other humanities that have printed word as their major source of evidence about their research subjects. We want to help change that. In this paper we present the first result of our work in this direction - the Music Ngram Viewer and search engine, an analog of Google Books Ngram Viewer and Google Books search for music scores. © 2011 International Society for Music Information Retrieval.","","Computational linguistics; Content based retrieval; Search engines; Algorithmic analysis; Large corpora; Music data; Music scores; Research subjects; Text corpora; Information retrieval","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Levé F.; Groult R.; Arnaud G.; Séguin C.; Gaymay R.; Giraud M.","Levé, Florence (55893852300); Groult, Richard (6507884031); Arnaud, Guillaume (55586482500); Séguin, Cyril (55586265000); Gaymay, Rémi (55585843400); Giraud, Mathieu (8700367400)","55893852300; 6507884031; 55586482500; 55586265000; 55585843400; 8700367400","Rhythm extraction from polyphonic symbolic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572935&partnerID=40&md5=4e4089d1e01a7a508e2481f184a733eb","MIS, Université de Picardie Jules Verne, Amiens, France; LIFL, Université Lille 1, CNRS, France","Levé F., MIS, Université de Picardie Jules Verne, Amiens, France; Groult R., MIS, Université de Picardie Jules Verne, Amiens, France; Arnaud G., MIS, Université de Picardie Jules Verne, Amiens, France; Séguin C., MIS, Université de Picardie Jules Verne, Amiens, France; Gaymay R., LIFL, Université Lille 1, CNRS, France; Giraud M., LIFL, Université Lille 1, CNRS, France","In this paper, we focus on the rhythmic component of symbolic music similarity, proposing several ways to extract a monophonic rhythmic signature from a symbolic polyphonic score. To go beyond the simple extraction of all time intervals between onsets (noteson extraction), we select notes according to their length (short and long extractions) or their intensities (intensity+/- extractions). Once the rhythm is extracted, we use dynamic programming to compare several sequences. We report results of analysis on the size of rhythm patterns that are specific to a unique piece, as well as experiments on similarity queries (ragtime music and Bach chorale variations). These results show that long and intensity+ extractions are often good choices for rhythm extraction. Our conclusions are that, even from polyphonic symbolic music, rhythm alone can be enough to identify a piece or to perform pertinent music similarity queries, especially when using wise rhythm extractions. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Music similarity; Polyphonic scores; Similarity query; Time interval; Extraction","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Smith J.B.L.; Burgoyne J.A.; Fujinaga I.; De Roure D.; Downie J.S.","Smith, Jordan B. L. (55582613300); Burgoyne, J. Ashley (23007865600); Fujinaga, Ichiro (9038140900); De Roure, David (6701509117); Downie, J. Stephen (7102932568)","55582613300; 23007865600; 9038140900; 6701509117; 7102932568","Design and creation of a large-scale database of structural annotations","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","93","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863546487&partnerID=40&md5=3bf7cd5c9c7b6b412cea664e9d93fb0b","University of Southern California, United States; McGill University, Canada; University of Oxford, United Kingdom; University of Illinois, Urbana-Champaign, United States","Smith J.B.L., University of Southern California, United States; Burgoyne J.A., McGill University, Canada; Fujinaga I., McGill University, Canada; De Roure D., University of Oxford, United Kingdom; Downie J.S., University of Illinois, Urbana-Champaign, United States","This paper describes the design and creation of an unprecedentedly large database of over 2400 structural annotations of nearly 1400 musical recordings. The database is intended to be a test set for algorithms that will be used to analyze a much larger corpus of hundreds of thousands of recordings, as part of the Structural Analysis of Large Amounts of Musical Information (SALAMI) project. This paper describes the design goals of the database and the practical issues that were encountered during its creation. In particular, we discuss the selection of the recordings, the development of an annotation format and procedure that adapts work by Peeters and Deruty [10], and the management and execution of the project. We also summarize some of the properties of the resulting corpus of annotations, including average inter-annotator agreement. © 2011 International Society for Music Information Retrieval.","","Database systems; Information retrieval; Design goal; Large amounts; Large database; Large-scale database; Musical information; Practical issues; Test sets; Audio recordings","J.B.L. Smith; University of Southern California, United States; email: jordans@usc.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Sargent G.; Bimbot F.; Vincent E.","Sargent, Gabriel (55583665800); Bimbot, Frédéric (6701567957); Vincent, Emmanuel (14010158800)","55583665800; 6701567957; 14010158800","A regularity-constrained viterbi algorithm and its application to the structural segmentation of songs","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868284061&partnerID=40&md5=7aedeadb2c1f552dba3c76fe85358010","IRISA (UMR 6074), Universit́e de Rennes 1, France; CNRS, IRISA (UMR 6074), France; INRIA Rennes, Bretagne Atlantique, France","Sargent G., IRISA (UMR 6074), Universit́e de Rennes 1, France; Bimbot F., CNRS, IRISA (UMR 6074), France; Vincent E., INRIA Rennes, Bretagne Atlantique, France","This paper presents a general approach for the structural segmentation of songs. It is formalized as a cost optimization problem that combines properties of the musical content and prior regularity assumption on the segment length. A versatile implementation of this approach is proposed by means of a Viterbi algorithm, and the design of the costs are discussed. We then present two systems derived from this approach, based on acoustic and symbolic features respectively. The advantages of the regularity constraint are evaluated on a database of 100 popular songs by showing a significant improvement of the segmentation performance in terms of F-measure. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Cost optimization; F-measure; General approach; ITS applications; Popular song; Regularity assumption; Segment lengths; Segmentation performance; Symbolic features; Viterbi algorithm","G. Sargent; IRISA (UMR 6074), Universit́e de Rennes 1, France; email: gabriel.sargent@irisa.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Miller S.; Reimer P.; Ness S.; Tzanetakis G.","Miller, Scott (55613234782); Reimer, Paul (51864639600); Ness, Steven (35076349900); Tzanetakis, George (6602262192)","55613234782; 51864639600; 35076349900; 6602262192","Geoshuffle: Location-aware, content-based music browsing using self-organizing tag clouds","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602385&partnerID=40&md5=6f77dc4b0813111d97894bbbf8e6ce57","Department of Electrical and Computer Engineering, University of Victoria, Canada; Department of Computer Science, University of Victoria, Canada","Miller S., Department of Electrical and Computer Engineering, University of Victoria, Canada; Reimer P., Department of Electrical and Computer Engineering, University of Victoria, Canada; Ness S., Department of Computer Science, University of Victoria, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, Canada","In the past few years the computational capabilities of mobile phones have been constantly increasing. Frequently these smartphones are also used as portable music players. In this paper we describe GeoShuffle - a prototype system for content-based music browsing and exploration that targets such devices. One of the most interesting aspects of these portable devices is the inclusion of positioning capabilities based on GPS. GeoShuffle adds location-based and time-based context to a user's listening preferences. Playlists are dynamically generated based on the location of the user, path and historical preferences. Browsing large music collections having thousands of tracks is challenging. The most common method of interaction is using long lists of textual metadata such as artist name or genre. Current smartphones are characterized by small screen real-estate which limits the amount of textual information that can be displayed. We propose selforganizing tag clouds, a 2D tag cloud representation that is based on an underlying self-organizing map calculated using automatically extracted audio features. To evalute the system the Magnatagatune database is utilized. The evaluation indicates that location and time context can improve the quality of music recommendation and that selforganizing tag clouds provide faster browsing and are more engaging than text-based tag clouds. © 2010 International Society for Music Information Retrieval.","","Conformal mapping; Information retrieval; Metadata; Smartphones; Audio features; Computational capability; Content-based; Large music collections; Location based; Location-aware; Music recommendation; Portable device; Portable music player; Prototype system; Real-estates; Self organizing; Small screens; Tag clouds; Textual information; Textual metadata; Time contexts; Data mining","S. Miller; Department of Electrical and Computer Engineering, University of Victoria, Canada; email: smiller@ece.uvic.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Cuthbert M.S.; Ariza C.; Friedland L.","Cuthbert, Michael Scott (15724599200); Ariza, Christopher (57204340082); Friedland, Lisa (23034455500)","15724599200; 57204340082; 23034455500","Feature extraction and machine learning on symbolic music using the music21 toolkit","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582967&partnerID=40&md5=7ab85de81198c56f1250611a0be38698","Music and Theater Arts, M.I.T., United States; Department of Computer Science, University of Massachusetts, Amherst, United States","Cuthbert M.S., Music and Theater Arts, M.I.T., United States; Ariza C., Music and Theater Arts, M.I.T., United States; Friedland L., Department of Computer Science, University of Massachusetts, Amherst, United States","Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the ""feature"" capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system's built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper's demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music. © 2011 International Society for Music Information Retrieval.","","Artificial intelligence; Data integration; Feature extraction; Information retrieval; Learning systems; Metadata; Audio data; Extraction method; Music data; Musical score; Open sources; Data mining","M.S. Cuthbert; Music and Theater Arts, M.I.T., United States; email: cuthbert@mit.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Cho T.; Bello J.P.","Cho, Taemin (55414501700); Bello, Juan P. (7102889110)","55414501700; 7102889110","A feature smoothing method for chord recognition using recurrence plots","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873375453&partnerID=40&md5=e21be71c47db97444688a0a728b55455","Music and Audio Research Laboratory (MARL), New York University, New York, United States","Cho T., Music and Audio Research Laboratory (MARL), New York University, New York, United States; Bello J.P., Music and Audio Research Laboratory (MARL), New York University, New York, United States","In this paper, we propose a feature smoothing technique for chord recognition tasks based on repeated patterns within a song. By only considering repeated segments of a song, our method can smooth the features without losing chord boundary information and fine details of the original feature. While a similar existing technique requires several hard decisions such as beat quantization and segmentation, our method uses a simple pragmatic approach based on recurrence plot to decide which repeated parts to include in the smoothing process. This approach uses a more formal definition of the repetition search and allows shorter (""chordsize"") repeated segments to contribute to the feature improvement process. In our experiments, our method outperforms conventional and popular smoothing techniques (a moving average filter and a median filter). In particular, it shows a synergistic effect when used with the Viterbi decoder. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Viterbi algorithm; Boundary information; Chord recognition; Formal definition; Hard decisions; Moving average filter; Recurrence plot; Repeated patterns; Repetition search; Smoothing methods; Smoothing process; Smoothing techniques; Synergistic effect; Viterbi decoder; Speech processing","T. Cho; Music and Audio Research Laboratory (MARL), New York University, New York, United States; email: tmc323@nyu.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Bryan N.J.; Wang G.","Bryan, Nicholas J. (57196969500); Wang, Ge (56144195500)","57196969500; 56144195500","Musical influence network analysis and rank of sample-based music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586928&partnerID=40&md5=b554f672f0271a92af1bc094f2db32fd","Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States","Bryan N.J., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; Wang G., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States","Computational analysis of musical influence networks and rank of sample-based music is presented with a unique outside examination of the WhoSampled.com dataset. The exemplary dataset maintains a large collection of artist-to-artist relationships of sample-based music, specifying the origins of borrowed or sampled material on a song-by-song basis. Directed song, artist, and musical genre networks are created from the data, allowing the application of social network metrics to quantify various trends and characteristics. In addition, a method of influence rank is proposed, unifying song-level networks to higher-level artist and genre networks via a collapse-and-sum approach. Such metrics are used to help interpret and describe interesting patterns of musical influence in sample-based music suitable for musicological analysis. Empirical results and visualizations are also presented, suggesting that sampled-based influence networks follow a power-law degree distribution; heavy influence of funk, soul, and disco music on modern hip-hop, R&B, and electronic music; and other musicological results. © 2011 International Society for Music Information Retrieval.","","Computer music; Computational analysis; Electronic music; Influence Network Analysis; Influence networks; Musical genre; Power-law degree distribution; Social Networks; Information retrieval","N.J. Bryan; Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; email: njb@ccrma.stanford.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Goto M.; Yoshii K.; Fujihara H.; Mauch M.; Nakano T.","Goto, Masataka (7403505330); Yoshii, Kazuyoshi (7103400120); Fujihara, Hiromasa (16068753300); Mauch, Matthias (36461512900); Nakano, Tomoyasu (24344775400)","7403505330; 7103400120; 16068753300; 36461512900; 24344775400","Songle: A web service for active music listening improved by user contributions","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","58","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860458729&partnerID=40&md5=5568ad3c03035e2cad1ef200d41a9f31","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Mauch M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Nakano T., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper describes a public web service for active music listening, Songle, that enriches music listening experiences by using music-understanding technologies based on signal processing. Although various research-level interfaces and technologies have been developed, it has not been easy to get people to use them in everyday life. Songle serves as a showcase to demonstrate how people can benefit from music-understanding technologies by enabling people to experience active music listening interfaces on the web. Songle facilitates deeper understanding of music by visualizing music scene descriptions estimated automatically, such as music structure, hierarchical beat structure, melody line, and chords. When using music-understanding technologies, however, estimation errors are inevitable. Songle therefore features an efficient error correction interface that encourages people to contribute by correcting those errors to improve the web service. We also propose a mechanism of collaborative training for music-understanding technologies, in which corrected errors will be used to improve the music-understanding performance through machine learning techniques. We hope Songle will serve as a research platform where other researchers can exhibit results of their music-understanding technologies to jointly promote the popularization of the field of music information research. © 2011 International Society for Music Information Retrieval.","","Error correction; Information retrieval; Learning systems; Research; Signal processing; Web services; Websites; Active music listening; Collaborative training; Estimation errors; Machine learning techniques; Music information; Music structures; Research platforms; Scene description; Audio systems","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Dixon S.; Tidhar D.; Benetos E.","Dixon, Simon (7201479437); Tidhar, Dan (36629391500); Benetos, Emmanouil (16067946900)","7201479437; 36629391500; 16067946900","The temperament police: The truth, the ground truth, and nothing but the truth","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864241911&partnerID=40&md5=468e9e8c598cd9aed876657ecf2ce3fa","Centre for Digital Music, Queen Mary University of London, United Kingdom; AHRC Research Centre for Musical Performance as Creative Practice, King's College London, United Kingdom","Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom; Tidhar D., AHRC Research Centre for Musical Performance as Creative Practice, King's College London, United Kingdom; Benetos E., Centre for Digital Music, Queen Mary University of London, United Kingdom","The tuning system of a keyboard instrument is chosen so that frequently used musical intervals sound as consonant as possible. Temperament refers to the compromise arising from the fact that not all intervals can be maximally consonant simultaneously. Recent work showed that it is possible to estimate temperament from audio recordings with no prior knowledge of the musical score, using a conservative (high precision, low recall) automatic transcription algorithm followed by frequency estimation using quadratic interpolation and bias correction from the log magnitude spectrum. In this paper we develop a harpsichord-specific transcription system to analyse over 500 recordings of solo harpsichord music for which the temperament is specified on the CD sleeve notes. We compare the measured temperaments with the annotations and discuss the differences between temperament as a theoretical construct and as a practical issue for professional performers and tuners. The implications are that ground truth is not always scientific truth, and that content-based analysis has an important role in the study of historical performance practice. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Linguistics; Automatic transcription; Bias correction; Content-based analysis; Ground truth; High precision; Historical performance; Magnitude spectrum; Musical score; Practical issues; Prior knowledge; Quadratic interpolation; Scientific truth; Tuning system; Audio recordings","S. Dixon; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: simond@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Hamel P.; Lemieux S.; Bengio Y.; Eck D.","Hamel, Philippe (8402361900); Lemieux, Simon (57207566904); Bengio, Yoshua (7003958245); Eck, Douglas (12141444300)","8402361900; 57207566904; 7003958245; 12141444300","Temporal pooling and multiscale learning for automatic annotation and ranking of music audio","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","77","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864146684&partnerID=40&md5=f7033f92ad7b42b55c540ec948974d4c","DIRO, Université de Montréal, Montréal, QC, Canada; Google Inc., Mountain View, CA, United States","Hamel P., DIRO, Université de Montréal, Montréal, QC, Canada; Lemieux S., DIRO, Université de Montréal, Montréal, QC, Canada; Bengio Y., DIRO, Université de Montréal, Montréal, QC, Canada; Eck D., Google Inc., Mountain View, CA, United States","This paper analyzes some of the challenges in performing automatic annotation and ranking of music audio, and proposes a few improvements. First, we motivate the use of principal component analysis on the mel-scaled spectrum. Secondly, we present an analysis of the impact of the selection of pooling functions for summarization of the features over time. We show that combining several pooling functions improves the performance of the system. Finally, we introduce the idea of multiscale learning. By incorporating these ideas in our model, we obtained state-of-the-art performance on the Magnatagatune dataset. © 2011 International Society for Music Information Retrieval.","","Principal component analysis; Automatic annotation; Multiscales; State-of-the-art performance; Temporal pooling; Information retrieval","P. Hamel; DIRO, Université de Montréal, Montréal, QC, Canada; email: hamelphi@iro.umontreal.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mattek A.; Casey M.","Mattek, Alison (55552871200); Casey, Michael (15080769900)","55552871200; 15080769900","Crossmodal aesthetics from a feature extraction perspective: A pilot study","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572708&partnerID=40&md5=b9e84ad3d6825a1b3ae29ef6c355a496","Dartmouth College, United States","Mattek A., Dartmouth College, United States; Casey M., Dartmouth College, United States","This paper investigates perceptual relationships between art in the auditory and visual domains. First, we conducted a behavioral experiment asking subjects to assess similarity between 10 musical recordings and 10 works of abstract art. We found a significant degree of agreement across subjects as to which images correspond to which audio, even though neither the audio nor the images possessed semantic content. Secondly, we sought to find the relationship between audio and images within a defined feature space that correlated with the subjective similarity judgments. We trained two regression models using leave- one-subject-out and leave-one-audio-out crossvalidation respectively, and exhaustively evaluated each model's ability to predict features of subject-ranked similar images using only a given audio clip's features. A retrieval task used the predicted image features to retrieve likely related images from the data set. The task was evaluated using the ground truth of subjects' actual similarity judgments. Our results show a mean cross-validated prediction accuracy of 0.61 with p<0.0001 for the first model, and a mean prediction accuracy of 0.51 with p<0.03 for the second model. © 2011 International Society for Music Information Retrieval.","","Feature extraction; Forecasting; Information retrieval; Regression analysis; Semantics; Abstract arts; Audio clips; Behavioral experiment; Cross validation; Cross-modal; Data set; Feature space; Ground truth; Image features; Pilot studies; Prediction accuracy; Regression model; Semantic content; Similar image; Audio recordings","A. Mattek; Dartmouth College, United States; email: Alison.M.Mattek@Dartmouth.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Chang K.K.; Jang J.-S.R.; Iliopoulos C.S.","Chang, Kaichun K. (55516277600); Jang, Jyh-Shing Roger (7402965041); Iliopoulos, Costas S. (7004240640)","55516277600; 7402965041; 7004240640","Music genre classification via compressive sampling","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607146&partnerID=40&md5=fab3176c47a2b71eaeb6957f1fd374c1","Department of Computer Science, King's College London, London, United Kingdom; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","Chang K.K., Department of Computer Science, King's College London, London, United Kingdom; Jang J.-S.R., Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Iliopoulos C.S., Department of Computer Science, King's College London, London, United Kingdom","Compressive sampling (CS) is a new research topic in signal processing that has piqued the interest of a wide range of researchers in different fields recently. In this paper, we present a CS-based classifier for music genre classification, with two sets of features, including short-time and long-time features of audio music. The proposed classifier generates a compact signature to achieve a significant reduction in the dimensionality of the audio music signals. The experimental results demonstrate that the computation time of the CS-based classifier is only about 20% of SVM on GTZAN dataset, with an accuracy of 92.7%. Several experiments were conducted in this study to illustrate the feasibility and robustness of the proposed methods as compared to other approaches. © 2010 International Society for Music Information Retrieval.","","Signal processing; Audio music; Compressive sampling; Computation time; Music genre classification; Research topics; Sets of features; Information retrieval","K.K. Chang; Department of Computer Science, King's College London, London, United Kingdom; email: ken.chang@kcl.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Thoshkahna B.; Nsabimana F.X.; Ramakrishnan K.R.","Thoshkahna, Balaji (15051329900); Nsabimana, Francois Xavier (24822832700); Ramakrishnan, K.R. (7101600367)","15051329900; 24822832700; 7101600367","A transient detection algorithm for audio using iterative analysis of STFT","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575018&partnerID=40&md5=6ec10853d53ce3a561f3906f91ae3728","Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; Project Group Hearing, Speech and Audio Technology, Fraunhofer Institute of Digital Media Technology, Oldenberg, Germany","Thoshkahna B., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; Nsabimana F.X., Project Group Hearing, Speech and Audio Technology, Fraunhofer Institute of Digital Media Technology, Oldenberg, Germany; Ramakrishnan K.R., Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India","We propose an iterative algorithm to detect transient segments in audio signals. Short time Fourier transform(STFT) is used to detect rapid local changes in the audio signal. The algorithm has two steps that iteratively - (a) calculate a function of the STFT and (b) build a transient signal. A dynamic thresholding scheme is used to locate the potential positions of transients in the signal. The iterative procedure ensures that genuine transients are built up while the localised spectral noise are suppressed by using an energy criterion. The extracted transient signal is later compared to a ground truth dataset. The algorithm performed well on two databases. On the EBU-SQAM database of monophonic sounds, the algorithmachieved an F-measure of 90% while on our database of polyphonic audio an F-measure of 91% was achieved. This technique is being used as a preprocessing step for a tempo analysis algorithm and a TSR (Transients + Sines + Residue) decomposition scheme. © 2011 International Society for Music Information Retrieval.","","Algorithms; Database systems; Information retrieval; Signal detection; Transients; Audio signal; Decomposition scheme; Dynamic thresholding; Energy criterion; F-measure; Ground-truth dataset; Iterative algorithm; Iterative analysis; Iterative procedures; Localised; Pre-processing step; Short time Fourier transforms; Spectral noise; Tempo analysis; Transient detection; Transient signal; Iterative methods","B. Thoshkahna; Dept. of Electrical Engineering, Indian Institute of Science, Bangalore, India; email: balajitn@ee.iisc.ernet.in","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Unal E.; Chew E.; Georgiou P.; Narayanan S.S.","Unal, Erdem (8300824800); Chew, Elaine (8706714000); Georgiou, Panayiotis (7003719827); Narayanan, Shrikanth S. (11539478500)","8300824800; 8706714000; 7003719827; 11539478500","A perplexity based cover song matching system for short length queries","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576743&partnerID=40&md5=cc96b20b15a61b33635addeaa6ec7d99","TÜBITAK BILGEM, Turkey; Queen Mary University of London, United Kingdom; University of Southern, CA, United States","Unal E., TÜBITAK BILGEM, Turkey; Chew E., Queen Mary University of London, United Kingdom; Georgiou P., University of Southern, CA, United States; Narayanan S.S., University of Southern, CA, United States","A music retrieval system that matches a short length music query with its variations in a database is proposed. In order to avoid the negative effects of different orchestration and performance style a nd t empo o n transcription and matching, a mid-level representation schema and a tonal modeling approach is used. The mid-level representation approach transcribes the music pieces into a sequence of music tags corresponding to major and minor triad labels. From the transcribed sequence, n-gram models are built to statistically represent the harmonic progression. For retrieval, a perplexity based similarity score is calculated between each n-gram in the database and that for the query. The retrieval performance of the system is presented for a dataset of 2000 classical music pieces modeled using ngrams of sizes 2 through 6. We observe improvements in retrieval performance with increasing query length and ngram order. The improvement converges to a little over one for all query lengths tested when n reaches 6. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Transcription; Classical musics; Cover songs; Matching system; Mid-level representation; Modeling approach; Music retrieval systems; N-gram models; N-grams; Query lengths; Retrieval performance; Similarity scores; Query processing","E. Unal; TÜBITAK BILGEM, Turkey; email: unal@uekae.tubitak.gov.tr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Dressler K.","Dressler, Karin (54390973100)","54390973100","An auditory streaming approach for melody extraction from polyphonic music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419027&partnerID=40&md5=f5718286b82d079ef71e42fc5a47b4ff","Fraunhofer Institute for Digital Media Technology IDMT, Ilmenau, Germany","Dressler K., Fraunhofer Institute for Digital Media Technology IDMT, Ilmenau, Germany","This paper proposes an efficient approach for the identification of the predominant voice from polyphonic musical audio. The algorithm implements an auditory streaming model which builds upon tone objects and salient pitches. The formation of voices is based on the regular update of the frequency and the magnitude of so called streaming agents, which aim at salient tones or pitches close to their preferred frequency range. Streaming agents which succeed to assemble a big magnitude start new voice objects, which in turn add adequate tones. The algorithm was evaluated as part of a melody extraction system during the MIREX audio melody extraction evaluation, where it gained very good results in the voicing detection and overall accuracy. © 2011 International Society for Music Information Retrieval.","","Acoustic streaming; Algorithms; Extraction; Information retrieval; Audio melody extractions; Auditory streaming; Frequency ranges; Melody extractions; Musical audio; Overall accuracies; Polyphonic music; Tone objects; Audio acoustics","K. Dressler; Fraunhofer Institute for Digital Media Technology IDMT, Ilmenau, Germany; email: kadressler@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Wang C.-C.; Jang J.-S.R.; Wang W.","Wang, Chung-Che (55413985200); Jang, Jyh-Shing Roger (7402965041); Wang, Wennen (35410685100)","55413985200; 7402965041; 35410685100","An improved query by singing/humming system using melody and lyrics information","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594172&partnerID=40&md5=3314de9636bdc44d725dfc7ace05f5d0","Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; Institute for Information Industry, Taiwan","Wang C.-C., Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; Jang J.-S.R., Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; Wang W., Institute for Information Industry, Taiwan","This paper proposes an improved query by singing/ humming (QBSH) system using both melody and lyrics information for achieving better performance. Singing/ humming discrimination (SHD) is first performed to distinguish singing from humming queries. For a humming query, we apply a pitch-only melody recognition method that has been used for QBSH task at MIREX with rank-1 performance. For a singing query, we combine the scores from melody recognition and lyrics recognition to take advantage of the extra lyrics information. Lyrics recognition is based on a modified tree lexicon that is commonly used in speech recognition. The performance of the overall QBSH system achieves 39.01% and 23.53% error reduction rates, respectively, for top-20 recognition under two experimental settings, indicating the feasibility of the proposed method. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Information use; Speech recognition; Better performance; Error reduction; Melody recognition; Query-by-singing; Search engines","C.-C. Wang; Dept. of CS, MIR Lab, Tsing Hua Univ., Taiwan; email: geniusturtle@mirlab.org","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Okumura K.; Sako S.; Kitamura T.","Okumura, Kenta (55586410400); Sako, Shinji (8967665400); Kitamura, Tadashi (35766620700)","55586410400; 8967665400; 35766620700","Stochastic modeling of a musical performance with expressive representations from the musical score","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586837&partnerID=40&md5=f1198f674902f4c277b5a6619aefd614","Nagoya Institute of Technology, Japan","Okumura K., Nagoya Institute of Technology, Japan; Sako S., Nagoya Institute of Technology, Japan; Kitamura T., Nagoya Institute of Technology, Japan","This paper presents a method for describing the characteristics of human musical performance. We consider the problem of building models that express the ways in which deviations from a strict interpretations of the score occurs in the performance, and that cluster these deviations automatically. The clustering process is performed using expressive representations unambiguously notated on the musical score, without any arbitrariness by the human observer. The result of clustering is obtained as hierarchical tree structures for each deviational factor that occurred during the operation of the instrument. This structure represents an approximation of the performer's interpretation with information notated on the score they used during the performance. This model represents the conditions that generate the difference in the fluctuation of performance expression and the amounts of deviational factors directly from the data of real performance. Through validations of applying the method to the data measured from real performances, we show that the use of information regarding expressive representation on the musical score enables the efficient estimation of generative-model for the musical performance. © 2011 International Society for Music Information Retrieval.","","Clustering algorithms; Trees (mathematics); Building model; Clustering process; Efficient estimation; Hierarchical tree; Human observers; Musical performance; Musical score; Information retrieval","K. Okumura; Nagoya Institute of Technology, Japan; email: k09@mmsp.nitech.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mauch M.; Dixon S.","Mauch, Matthias (36461512900); Dixon, Simon (7201479437)","36461512900; 7201479437","Approximate note transcription for the improved identification of difficult chords","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","134","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595823&partnerID=40&md5=933a0157821aba12bb3c81b0d467469a","Centre for Digital Music, Queen Mary University, London, United Kingdom","Mauch M., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord profiles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70% in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on finding good chord templates to fit the chroma features. In this paper we reverse this approach and seek to find chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80% accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a significant increase over the top result (74%) in MIREX 2009. The nature of some chords makes their identification particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these diffcult chords in particular is substantially improved by the prior approximate transcription using NNLS. © 2010 International Society for Music Information Retrieval.","Chord detection; Chord extraction; Chromagram; Non-negative least squares (NNLS); Transcription","Audio acoustics; Information retrieval; Insecticides; Automatic Detection; Chroma features; Chromagram; Computing-task; Detection tasks; Front end; Fundamental frequencies; High-level models; Least Square; Least squares problems; Non negatives; Time-series modelling; Transcription","M. Mauch; Centre for Digital Music, Queen Mary University, London, United Kingdom; email: matthias.mauch@elec.qmul.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"McFee B.; Barrington L.; Lanckriet G.","McFee, Brian (34875379700); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","34875379700; 14041197300; 7801431767","Learning similarity from collaborative filters","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596877&partnerID=40&md5=d773286b2bebab8b5206d0bfc60489fc","Department of Computer Science and Engineering, University of California, San Diego, United States; Department of Electrical and Computer Engineering, University of California, San Diego, United States","McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Department of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States","Collaborative filtering methods (CF) exploit the wisdom of crowds to capture deeply structured similarities in musical objects, such as songs, artists or albums. When CF is available, it frequently outperforms content-based methods in recommendation tasks. However, songs in the so-called ""long tail"" cannot reap the benefits of collaborative filtering, and practitioners must rely on content-based methods. We propose a method for improving contentbased recommendation in the long tail by learning an optimized similarity function from a sample of collaborative filtering data. Our experimental results demonstrate substantial improvements in accuracy by learning optimal similarity functions. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Optimization; Collaborative filtering methods; Content-based methods; Content-based recommendation; Learning similarity; Long tail; Similarity functions; Wisdom of crowds; Collaborative filtering","B. McFee; Department of Computer Science and Engineering, University of California, San Diego, United States; email: bmcfee@cs.ucsd.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Joo S.; Park S.; Jo S.; Yoo C.D.","Joo, Sihyun (42061629100); Park, Sanghun (55586200900); Jo, Seokhwan (18042330400); Yoo, Chang D. (7201746384)","42061629100; 55586200900; 18042330400; 7201746384","Melody extraction based on harmonic coded structure","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579451&partnerID=40&md5=78f659b1159c7f6b875f37d2940a3516","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","Joo S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Park S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Jo S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Yoo C.D., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","This paper considers a melody extraction algorithm that estimates the melody in polyphonic audio using the harmonic coded structure (HCS) to model melody in the minimum mean-square-error (MMSE) sense. The HCS is harmonically modulated sinusoids with the amplitudes defined by a set of codewords. The considered algorithm performs melody extraction in two steps: i) pitch-candidate estimation and ii) pitch-sequence identification. In the estimation step, pitch candidates are estimated such that the HCS best represents the polyphonic audio in the MMSE sense. In the identification step, a melody line is selected from many possible pitch sequences based on the properties of melody line. Posterior to the melody line selection, a smoothing process is applied to refine spurious pitches and octave errors. The performance of the algorithm is evaluated and compared using the ADC04 and the MIREX05 dataset. The results show that the performance of the proposed algorithm is better than or comparable to other algorithms submitted to MIREX2009. © 2011 International Society for Music Information Retrieval.","","Estimation; Extraction; Harmonic analysis; Information retrieval; Code-words; Line selections; Melody extractions; Minimum mean square errors (MMSE); Smoothing process; Algorithms","S. Joo; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; email: s.joo@kaist.ac.kr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Knight T.; Upham F.; Fujinaga I.","Knight, Trevor (54984876700); Upham, Finn (55184758200); Fujinaga, Ichiro (9038140900)","54984876700; 55184758200; 9038140900","The potential for automatic assessment of trumpet tone quality","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572020&partnerID=40&md5=0beafc0d0123f241d895ae288cd02e30","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","Knight T., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Upham F., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada","The goal of this study was to examine the possibility of training machine learning algorithms to differentiate between the performance of good notes and bad notes. Four trumpet players recorded a total of 239 notes from which audio features were extracted. The notes were subjectively graded by five brass players. The resulting dataset was used to train support vector machines with different groupings of ratings. Splitting the data set into two classes (good and bad) at the median rating, the classifier showed an average success rate of 72% when training and testing using cross-validation. Splitting the data into three roughly-equal classes (good, medium, and bad), the classifier correctly identified the class an average of 54% of the time. Even using seven classes, the classifier identified the correct class 46% of the time, which is better than the result expected from chance or from the strategy of picking the most populous class (36%). © 2011 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Automatic assessment; Cross validation; Data set; Training and testing; Training machines; Trumpet tones; Learning algorithms","T. Knight; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Canada; email: TrevorKnight@gmail.com","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Levy M.","Levy, Mark (14021515000)","14021515000","Improving perceptual tempo estimation with crowd-sourced annotations","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873422152&partnerID=40&md5=449b1a1b43851cbf030b686cec8380c5","Last.fm Ltd., Karen House, London N1 6DL, 1-11 Baches Street, United Kingdom","Levy M., Last.fm Ltd., Karen House, London N1 6DL, 1-11 Baches Street, United Kingdom","We report the design and results of a web-based experiment intended to support the development and evaluation of tempo estimation algorithms, in which users tap to music and select descriptive labels. Analysis of the tapping data and labels chosen shows that, while different listeners frequently entrain to different metrical levels for some pieces, they rarely disagree about which pieces are fast and which are slow. We show how this result can be used to improve both the evaluation metrics used for automatic tempo estimation and the estimation algorithms themselves. We also report the relative performance of two recent tempo estimation methods according to a further controlled experiment that does not depend on groundtruth values of any kind. © 2011 International Society for Music Information Retrieval.","","Algorithms; Experiments; Information retrieval; Labels; Controlled experiment; Estimation algorithm; Evaluation metrics; Perceptual tempo; Relative performance; Tempo estimations; Estimation","M. Levy; Last.fm Ltd., Karen House, London N1 6DL, 1-11 Baches Street, United Kingdom; email: mark@last.fm","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mak C.-M.; Lee T.; Senapati S.; Yeung Y.-T.; Lam W.-K.","Mak, Chun-Man (8323027600); Lee, Tan (7501439194); Senapati, Suman (57197887155); Yeung, Yu-Ting (36635241200); Lam, Wang-Kong (55954212400)","8323027600; 7501439194; 57197887155; 36635241200; 55954212400","Similarity measures for chinese pop music based on low-level audio signal attributes","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593366&partnerID=40&md5=8323d1c8b19f16c5300042769784e18e","Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong","Mak C.-M., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Lee T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Senapati S., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Yeung Y.-T., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; Lam W.-K., Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong","In this article a method of computing similarity of two Chinese pop songs is presented. It is based on five attributes extracted from the audio signal. They include music instrument, singing voice style, singer gender, tempo, and degree of noisiness. We compare the computed similarity measures with similarity scores obtained with subjective listening by over 200 human subjects. The results show that rhythm and mood related attributes like tempo and degree of noisiness are most correlated to human perception of Chinese pop songs. Instrument and singing style are relatively less relevant. The results of subjective evaluation also indicate that the proposed method of similarity computation is fairly correlated with human perception. © 2010 International Society for Music Information Retrieval.","","Audio signal; Human perception; Human subjects; Similarity computation; Similarity measure; Similarity scores; Singing styles; Singing voices; Subjective evaluations; Information retrieval","C.-M. Mak; Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong; email: cmmak@ee.cuhk.edu.hk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Bergstra J.; Mandel M.; Eck D.","Bergstra, James (15080566700); Mandel, Michael (14060460000); Eck, Douglas (12141444300)","15080566700; 14060460000; 12141444300","Scalable genre and tag prediction with spectral covariance","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599467&partnerID=40&md5=e5d2f1ec0f45525ac46293869ad58bb5","University of Montreal, Canada","Bergstra J., University of Montreal, Canada; Mandel M., University of Montreal, Canada; Eck D., University of Montreal, Canada","Cepstral analysis is effective in separating source from filter in vocal and monophonic [pitched] recordings, but is it a good general-purpose framework for working with music audio? We evaluate covariance in spectral features as an alternative to means and variances in cepstral features (particularly MFCCs) as summaries of frame-level features. We find that spectral covariance is more effective than mean, variance, and covariance statistics of MFCCs for genre and social tag prediction. Support for our model comes from strong and state-of-the-art performance on the GTZAN genre dataset, MajorMiner, and MagnaTagatune. Our classification strategy based on linear classifiers is easy to implement, exhibits very little sensitivity to hyper-parameters, trains quickly (even for web-scale datasets), is fast to apply, and offers competitive performance in genre and tag prediction. © 2010 International Society for Music Information Retrieval.","","Audio acoustics; Classification (of information); Information retrieval; Source separation; Cepstral analysis; Cepstral features; Competitive performance; Linear classifiers; Social Tags; Spectral feature; State-of-the-art performance; Forecasting","J. Bergstra; University of Montreal, Canada; email: bergstrj@iro.umontreal.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Cabredo R.; Legaspi R.; Numao M.","Cabredo, Rafael (35274548200); Legaspi, Roberto (35610218300); Numao, Masayuki (7004090356)","35274548200; 35610218300; 7004090356","Identifying emotion segments in music by discovering motifs in physiological data","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587633&partnerID=40&md5=d0103e447fb7905df7c440294efb0036","Institute of Scientific and Industrial Research, Osaka University, Japan","Cabredo R., Institute of Scientific and Industrial Research, Osaka University, Japan; Legaspi R., Institute of Scientific and Industrial Research, Osaka University, Japan; Numao M., Institute of Scientific and Industrial Research, Osaka University, Japan","Music can induce different emotions in people. We propose a system that can identifymusic segmentswhich induce specific emotions from the listener. The work involves building a knowledge base with mappings between affective states (happiness, sadness, etc.) and music features (rhythm, chord progression, etc.). Building this knowledge base requires background knowledge from music and emotions psychology. Psychophysiological responses of a user, particularly, the blood volume pulse, are taken while he listens to music. These signals are analyzed and mapped to various musical features of the songs he listened to. A motif discovery algorithm used in data mining is adapted to analyze signals of physiological data. Motif discovery finds patterns in the data that indicate points of interest in the music. The differentmotifs are stored in a library of patterns and used to identify other songs that have similar musical content. Results show that motifs selected have similar chord progressions. Some of which include frequently used chords in western pop music. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Knowledge based systems; Affective state; Back-ground knowledge; Blood volume pulse; Knowledge base; Motif discovery; Music and emotions; Musical features; Physiological data; Points of interest; Physiology","R. Cabredo; Institute of Scientific and Industrial Research, Osaka University, Japan; email: cabredo@ai.sanken.osaka-u.ac.jp","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Serra X.","Serra, Xavier (55892979900)","55892979900","A multicultural approach in music information research","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","62","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436679&partnerID=40&md5=7b4219f4fe82686f8c342791c301bcc0","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Serra X., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Our information technologies do not respond to the world's multicultural reality; in fact, we are imposing the paradigms of our market-driven western culture also on IT, thus facilitating the access of a small part of the world's information to a small part of the world's population. The current IT research efforts may even make it worse, and future IT will accentuate this information bias. Most IT research is being carried out with a western centered approach and as a result, most of our data models, cognition models, user models, interaction models, ontologies, etc., are culturally biased. This fact is quite evident in music information research, since, despite the world's richness in terms of musical culture, most research is centered on CDs and metadata of western commercial music. This is the motivation behind a large and ambitious project funded by the European Research Council entitled ""CompMusic: Computational Models for the discovery of the world's music."" In this paper we present the ideas supporting this project, the challenges that we want to work on, and the proposed approaches to tackle these challenges. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Research; CdS; Computational model; European Research Council; Information bias; Interaction model; IT research; Music information; User models; Information technology","X. Serra; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: xavier.serra@upf.edu","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mcfee B.; Lanckriet G.","Mcfee, Brian (34875379700); Lanckriet, Gert (7801431767)","34875379700; 7801431767","Large-scale music similarity search with spatial trees","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572318&partnerID=40&md5=682de5945f371af2c6235be17d8e5d5d","Computer Science and Engineering, University of California, San Diego, United States; Electrical and Computer Engineering, University of California, San Diego, United States","Mcfee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","T Many music information retrieval tasks require finding the nearest neighbors of a query item in a high-dimensional space. However, the complexity of computing nearest neighbors grows linearly with size of the database, making exact retrieval impractical for large databases. We investigate modern variants of the classical KD-tree algorithm, which efficiently index high-dimensional data by recursive spatial partitioning. Experiments on the Million Song Dataset demonstrate that content-based similarity search can be significantly accelerated by the use of spatial partitioning structures. © 2011 International Society for Music Information Retrieval.","","Data; Experimentation; Information Retrieval; Set; Structures; Forestry; Query processing; Content-based; High dimensional data; High dimensional spaces; Kd-tree; Large database; Music information retrieval; Music similarity; Nearest neighbors; Similarity search; Spatial partitioning; Information retrieval","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mayer R.; Rauber A.","Mayer, Rudolf (23397787500); Rauber, Andreas (57074846700)","23397787500; 57074846700","Musical genre classification by ensembles of audio and lyrics features","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862061913&partnerID=40&md5=c6d72228cf990562fd0158b90f71a509","Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Mayer R., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria","Algorithms that can understand and interpret characteristics of music, and organise them for and recommend them to their users can be of great assistance in handling the ever growing size of both private and commercial collections. Music is an inherently multi-modal type of data, and the lyrics associated with the music are as essential to the reception and the message of a song as is the audio. In this paper, we present advanced methods on how the lyrics domain of music can be combined with the acoustic domain. We evaluate our approach by means of a common task in music information retrieval, musical genre classification. Advancing over previous work that showed improvements with simple feature fusion, we apply the more sophisticated approach of result (or late) fusion. We achieve results superior to the best choice of a single algorithm on a single feature set. © 2011 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Acoustic domains; Best choice; Feature fusion; Feature sets; Multi-modal; Music information retrieval; Musical genre classification; Audio acoustics","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Laplante A.","Laplante, Audrey (37110930300)","37110930300","Social capital and music discovery: An examination of the ties through which late adolescents discover new music","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870573555&partnerID=40&md5=e370b5cb2f87bc38636074d2637dd0e5","ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Canada","Laplante A., ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Canada","Research on everyday life information seeking has demonstrated that people often relied on other people to obtain the information they need. Weak ties (i.e., acquaintances) were found to be particularly instrumental to get new information. This study employed social network analysis to examine the characteristics of the ties through which late adolescents (15-17 years old) discover new music. In-depth interviews with 19 adolescents were conducted, which generated a sample of 334 ties. A statistical analysis of the ties showed that these adolescents relied mostly on strong ties to expand their music repertoire, that is, on people to which they felt very close and with whom they had frequent contacts. These ties were predominantly homophilous in terms of age, gender and musical taste. It was also found that parents were more likely than friends or other types of kins to be instrumental for music discovery. These findings suggest that a better knowledge of the characteristics of the ties through which people discover new music could provide useful insights for the design of recommender systems that include social networking features. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Everyday life information seeking; In-depth interviews; Social capitals; Social Network Analysis; Weak ties; Social networking (online)","A. Laplante; ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Canada; email: audrey.laplante@umontreal.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Raphael C.; Wang J.","Raphael, Christopher (7004214964); Wang, Jingya (57216680024)","7004214964; 57216680024","Newapproaches to optical music recognition","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442036&partnerID=40&md5=fd0b3c17bf03b0469a8e9722a04546e0","Indiana University, School of Informatics and Computing, United States","Raphael C., Indiana University, School of Informatics and Computing, United States; Wang J., Indiana University, School of Informatics and Computing, United States","We present the beginnings of a new system for optical music recognition (OMR), aimed toward the score images of the InternationalMusic Score Library Project (IMSLP). Our system focuses on measures as the basic unit of recognition. We identify candidate composite symbols (chords and beamed groups) using grammatically-formulated top-down model-based methods, while employing template matching to find isolated rigid symbols. We reconcile these overlapping symbols by seeking non-overlapping variants of the composite symbols that best account for the pixel data. We present results on a representative score from the IMSLP. © 2011 International Society for Music Information Retrieval.","","Template matching; Basic units; Library projects; Model-based method; Optical music recognition; System focus; Topdown; Information retrieval","","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Macrae R.; Dixon S.","Macrae, Robert (36608414000); Dixon, Simon (7201479437)","36608414000; 7201479437","Guitar tab mining, analysis and ranking","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420306&partnerID=40&md5=bb35dab58df2806784801b4c470474d9","Centre for Digital Music, Queen Mary University, London, United Kingdom","Macrae R., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","With over 4.5 million tablatures and chord sequences (collectively known as tabs), the web holds vast quantities of hand annotated scores in non-standardised text files. These scores are typically error-prone and incomplete, and tab collections contain many duplicates, making retrieval of high quality tabs difficult. Despite this, tabs are by far the most popular means of sharing musical instructions on the internet. We have developed tools that use text analysis and alignment for the automatic retrieval, interpretation and analysis of such tabs in order to filter and estimate the most accurate tabs from the multitude available. We show that the standard means of ranking tabs, such as search engine ranks or user ratings, have little correlation with the accuracy of a tab and that a better ranking method is to use features such as the concurrency between tabs of the same song. We also compare the quality of top-ranked tabs with state-of-the-art chord transcription output and find that the latter provides a more reliable source of chord symbols with an accuracy rate 10% higher than the ranked hand annotations. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Search engines; Accuracy rate; Automatic retrieval; Chord sequence; Error prones; Hand-annotations; High quality; Ranking methods; Text analysis; Text file; User rating; Filtration","R. Macrae; Centre for Digital Music, Queen Mary University, London, United Kingdom; email: robert.macrae@eecs.qmul.ac.uk","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schuller B.; Weninger F.; Dorfner J.","Schuller, Björn (6603767415); Weninger, Felix (13610240900); Dorfner, Johannes (36056207400)","6603767415; 13610240900; 36056207400","Multi-modal non-prototypical music mood analysis in continuous space: Reliability and performances","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871398919&partnerID=40&md5=deea7b314eea3ec4b9cdd4879861efac","Institute for Human-Machine Communication, Technische Universitä München, Germany; Institute for Energy Economy and Application Technology, Technische Universität München, Germany","Schuller B., Institute for Human-Machine Communication, Technische Universitä München, Germany; Weninger F., Institute for Human-Machine Communication, Technische Universitä München, Germany; Dorfner J., Institute for Energy Economy and Application Technology, Technische Universität München, Germany","Music Mood Classification is frequently turned into 'Music Mood Regression' by using a continuous dimensional model rather than discrete mood classes. In this paper we report on automatic analysis of performances in a mood space spanned by arousal and valence on the 2.6 k songs NTWICM corpus of popular UK chart music in full realism, i. e., by automatic web-based retrieval of lyrics and diverse acoustic features without pre-selection of prototypical cases. We discuss optimal modeling of the gold standard by introducing the evaluator weighted estimator principle, group-wise feature relevance, 'tuning' of the regressor, and compare early and late fusion strategies. In the result, correlation coefficients of .736 (valence) and .601 (arousal) are reached on previously unseen test data. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Acoustic features; Automatic analysis; Continuous spaces; Correlation coefficient; Dimensional model; Feature relevance; Gold standards; Late fusion; Multi-modal; Music mood analysis; Optimal modeling; Pre-selection; Test data; Weighted estimator; Reliability analysis","B. Schuller; Institute for Human-Machine Communication, Technische Universitä München, Germany; email: schuller@tum.de","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mathieu B.; Essid S.; Fillon T.; Prado J.; Richard G.","Mathieu, Benoit (57196757784); Essid, Slim (16033218700); Fillon, Thomas (14631798700); Prado, Jacques (8046413800); Richard, Gaël (57195915952)","57196757784; 16033218700; 14631798700; 8046413800; 57195915952","Yaafe, an easy to use and efficient audio feature extraction software","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","148","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597271&partnerID=40&md5=4a81ef44e84ea15c56ed6935e84afc2e","CNRS/LTCI, Telecom ParisTech, Institut Telecom, France","Mathieu B., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Essid S., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Fillon T., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Prado J., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; Richard G., CNRS/LTCI, Telecom ParisTech, Institut Telecom, France","Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involving automatic classification (e.g. speech/music discrimination, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used libraries. The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation. YAAFE remains easy to configure and each feature can be parameterized independently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License. © 2010 International Society for Music Information Retrieval.","","Information retrieval systems; Speech recognition; Audio feature extraction; Audio features; Automatic classification; Core features; General Public License; Large dataset; Lower complexity; Music genre; Music information retrieval; Parameterized; Source codes; Speech/music discrimination; Traditional approaches; Feature extraction","B. Mathieu; CNRS/LTCI, Telecom ParisTech, Institut Telecom, France; email: benoit.mathieu@telecom-paristech.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"McKay C.; Bainbridge D.","McKay, Cory (14033215600); Bainbridge, David (8756864800)","14033215600; 8756864800","A musical web mining and audio feature extraction extension to the greenstone digital library software","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584527&partnerID=40&md5=174c2a40fec76b9972db77a9829af3de","CIRMMT, Marianopolis College, Montréal, Canada; University of Waikato, Hamilton, New Zealand","McKay C., CIRMMT, Marianopolis College, Montréal, Canada; Bainbridge D., University of Waikato, Hamilton, New Zealand","This paper describes updates to the Greenstone open source digital library software that significantly expand its functionality with respect to music. The first of the two major improvements now allows Greenstone to extract and store classification-oriented features from audio files using a newly updated version of the jAudio software. The second major improvement involves the implementation and integration of the new jSongMiner software, which provides Greenstone with a framework for automatically identifying audio recordings using audio fingerprinting and then extracting extensive metadata about them from a variety of resources available on the Internet. Several illustrative use cases and case studies are discussed. © 2011 International Society for Music Information Retrieval.","","Digital libraries; Feature extraction; Information retrieval; Metadata; Audio feature extraction; Audio files; Audio fingerprinting; Open sources; Web Mining; Open systems","C. McKay; CIRMMT, Marianopolis College, Montréal, Canada; email: cory.mckay@mail.mcgill.ca","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Mora J.; Gómez F.; Gómez E.; Escobar-Borrego F.; Díaz-Báñez J.M.","Mora, Joaquín (55582857600); Gómez, Francisco (57196622778); Gómez, Emilia (14015483200); Escobar-Borrego, Francisco (53563105800); Díaz-Báñez, José Miguel (8847635500)","55582857600; 57196622778; 14015483200; 53563105800; 8847635500","Characterization and melodic similarity of a cappella flamenco cantes","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592148&partnerID=40&md5=821f7f2ccd7cf03d7990b72a77036e5a","Department of Evolutive and Educational Psychology, University of Seville, Spain; Applied Mathematics Department, School of Computer Science, Polytechnic University of Madrid, Spain; Music Technology Group, Universitat Pompeu Fabra, Spain; Department of Audiovisual Communication, Publicity and Literature, University of Seville, Spain; Department of Applied Mathematics II, University of Seville, Spain","Mora J., Department of Evolutive and Educational Psychology, University of Seville, Spain; Gómez F., Applied Mathematics Department, School of Computer Science, Polytechnic University of Madrid, Spain; Gómez E., Music Technology Group, Universitat Pompeu Fabra, Spain; Escobar-Borrego F., Department of Audiovisual Communication, Publicity and Literature, University of Seville, Spain; Díaz-Báñez J.M., Department of Applied Mathematics II, University of Seville, Spain","This paper intends to research on the link between musical similarity and style and sub-style (variant) classification in the context of flamenco a cappella singing styles. Given the limitation of standard computational models for melodic characterization and similarity computation in this particular context, we have proposed a specific set of melodic features adapted to flamenco singing styles. In order to evaluate them, we have gathered a collection of music recordings from the most representative singers and have manually extracted those proposed features. Based on those features, we have defined a similarity measure between two performances and have validated their usefulness in differentiating several styles and variants. The main conclusion of this work is the need to incorporate specific musical features to the design of similarity measures for flamenco music so that flamencoadapted MIR systems can be developed. © 2010 International Society for Music Information Retrieval.","","Computational model; Melodic similarity; Music recording; Musical features; Musical similarity; Similarity computation; Similarity measure; Singing styles; Information retrieval","J. Mora; Department of Evolutive and Educational Psychology, University of Seville, Spain; email: mora@us.es","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Bimbot F.; Deruty E.; Sargent G.; Vincent E.","Bimbot, Frédéric (6701567957); Deruty, Emmanuel (35503270700); Sargent, Gabriel (55583665800); Vincent, Emmanuel (14010158800)","6701567957; 35503270700; 55583665800; 14010158800","Methodology and resources for the structural segmentation of music pieces into autonomous and comparable blocks","2011","Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576872&partnerID=40&md5=6d4e342413c9e1d915ab2c4aafebefb3","METISS - Speech and Audio Research Group, IRISA, CNRS - UMR 6074, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; METISS - Speech and Audio Research Group, INRIA, Rennes Bretagne Atlantique, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; METISS - Speech and Audio Research Group, Université de Rennes 1, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France","Bimbot F., METISS - Speech and Audio Research Group, IRISA, CNRS - UMR 6074, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Deruty E., METISS - Speech and Audio Research Group, INRIA, Rennes Bretagne Atlantique, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Sargent G., METISS - Speech and Audio Research Group, Université de Rennes 1, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; Vincent E., METISS - Speech and Audio Research Group, INRIA, Rennes Bretagne Atlantique, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France","The approach called decomposition into autonomous and comparable blocks specifies a methodology for producing music structure annotation by human listeners based on a set of criteria relying on the listening experience of the human annotator [12]. The present article develops further a number of fundamental notions and practical issues, so as to facilitate the usability and the reproducibility of the approach. We formalize the general methodology as an iterative process which aims at estimating both a structural metric pattern and its realization, by searching empirically for an optimal compromise describing the organization of the content of the music piece in the most economical way, around a typical timescale. Based on experimental observations, we detail some practical considerations and we illustrate the method by an extensive case study. We introduce a set of 500 songs for which we are releasing freely the structural annotations to the research community, for examination, discussion and utilization. © 2011 International Society for Music Information Retrieval.","","Information retrieval; Experimental observation; General methodologies; Human listeners; Iterative process; Music structures; Practical issues; Reproducibilities; Research communities; Time-scales; Iterative methods","F. Bimbot; METISS - Speech and Audio Research Group, IRISA, CNRS - UMR 6074, 35042 Rennes cedex, Campus Universitaire de Beaulieu, France; email: frederic.bimbot@irisa.fr","","12th International Society for Music Information Retrieval Conference, ISMIR 2011","24 October 2011 through 28 October 2011","Miami, FL","95397"
"Schmidt E.M.; West K.; Kim Y.E.","Schmidt, Erik M. (36053813000); West, Kris (15766636800); Kim, Youngmoo E. (24724623000)","36053813000; 15766636800; 24724623000","Efficient acoustic feature extraction for music information retrieval using programmable gate arrays","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873673427&partnerID=40&md5=37c89946217f584c0a20c381ac1c346e","MET-lab, Drexel University, United States; IMIRSEL, University of Illinois, United States","Schmidt E.M., MET-lab, Drexel University, United States; West K., IMIRSEL, University of Illinois, United States; Kim Y.E., MET-lab, Drexel University, United States","Many of the recent advances in music information retrieval from audio signals have been data-driven, i.e., resulting from the analysis of very large data sets. Widespread performance evaluations on common data sets, such as the annual MIREX events, have also been instrumental in advancing the field. These endeavors incur a large computational cost, and could potentially benefit greatly from more rapid calculation of acoustic features. Traditional, clusterbased solutions for large-scale feature extraction are expensive and space- and power-inefficient. Using the massively parallel architecture of the field programmable gate array (FPGA), it is possible to design an application specific chip rivaling the speed of a cluster for large-scale acoustic feature computation at lower cost. Recent advances in development tools, such as the Xilinx Blockset in Simulink, allow rapid prototyping, simulation, and implementation on actual hardware. Such devices also show potential for the implementation of MIR systems on embedded devices such as cell phones and PDAs where hardware acceleration would be an absolute necessity. We present a prototype library for acoustic feature calculation for implementation on Xilinx FPGA hardware. Furthermore, using a genre classification task we compare the performance of simulated hardware features to those computed using standard methods, demonstrating a nearly negligible drop in classification performance with the potential for large reductions in computation time. © 2009 International Society for Music Information Retrieval.","","Embedded systems; Feature extraction; Field programmable gate arrays (FPGA); Hardware; Parallel architectures; Rapid prototyping; Acoustic feature extraction; Acoustic features; Application specific; Audio signal; Cell phone; Classification performance; Cluster-based; Common datum; Computation time; Computational costs; Development tools; Embedded device; Genre classification; Hardware acceleration; Hardware features; Music information retrieval; Performance evaluation; Programmable gate array; Simulink; Standard method; Very large datum; Xilinx FPGA; Information retrieval","E.M. Schmidt; MET-lab, Drexel University, United States; email: eschmidt@drexel.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Jeon W.; Ma C.; Cheng Y.M.","Jeon, Woojay (13607940500); Ma, Changxue (7402924283); Cheng, Yan Ming (57213306704)","13607940500; 7402924283; 57213306704","An efficient signal-matching approach to melody indexing and search using continuous pitch contours andwavelets","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873692182&partnerID=40&md5=5bb9e85912dfe93bacf66081304caedb","Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States","Jeon W., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States; Ma C., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States; Cheng Y.M., Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States","We describe a method of indexing and efficiently searching music melodies based on their continuous dominant fundamental frequency (f0) contours without obtaining notelevel transcriptions. Each f0 contour is encoded by a redundant set of wavelet coefficients that represent its shape in level-normalized form at various locations and time scales. This allows a query melody to be exhaustively compared with variable-length portions of a target melody at arbitrary locations while accounting for differences in key and tempo. Themethod is applied in a Query-by-Humming (QBH) systemwhere usersmay search a database of recorded pop songs by humming or singing an arbitrary part of the melody of an intended song. The system has fast retrieval times because the wavelet coefficients can be effectively indexed in a binary tree and a vector distance measure instead of dynamic programming is used for comparisons. Using automatic pitch extraction to obtain all f0 contours from acoustic data, the method demonstrates practical performance in an experiment with an existing monophonic data set and in a preliminary experiment with real-world polyphonic music. © 2009 International Society for Music Information Retrieval.","","Binary trees; Information retrieval; Query processing; Wavelet transforms; Acoustic data; Data set; F0 contours; Fast retrievals; Fundamental frequencies; Pitch contours; Pitch extraction; Polyphonic music; Query-by-humming; Real-world; Time-scales; Vector distance; Wavelet coefficients; Indexing (of information)","W. Jeon; Applied Research and Technology Center, Motorola, Inc., Schaumburg, IL, United States; email: woojay@motorola.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lidy T.; Mayer R.; Rauber A.; Ponce De León P.J.; Pertusa A.; Iñesta J.M.","Lidy, T. (23035315800); Mayer, R. (23397787500); Rauber, A. (57074846700); Ponce De León, P.J. (14042292100); Pertusa, A. (8540257800); Iñesta, J.M. (6701387099)","23035315800; 23397787500; 57074846700; 14042292100; 8540257800; 6701387099","A cartesian ensemble of feature subspace classifiers for music categorization","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586694&partnerID=40&md5=822dcd6825fe1e8a78eae3f5c6d48d9a","Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain","Lidy T., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Mayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Ponce De León P.J., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain; Pertusa A., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain; Iñesta J.M., Departamento de Lenguajes, Sistemas Informaticos, University of Alicante, Spain","We present a cartesian ensemble classification system that is based on the principle of late fusion and feature subspaces. These feature subspaces describe different aspects of the same data set. The framework is built on the Weka machine learning toolkit and able to combine arbitrary feature sets and learning schemes. In our scenario, we use it for the ensemble classification of multiple feature sets from the audio and symbolic domains. We present an extensive set of experiments in the context of music genre classification, based on numerous Music IR benchmark datasets, and evaluate a set of combination/voting rules. The results show that the approach is superior to the best choice of a single algorithm on a single feature set. Moreover, it also releases the user from making this choice explicitly. © 2010 International Society for Music Information Retrieval.","","Data processing; Information retrieval; Learning algorithms; Benchmark datasets; Best choice; Cartesians; Data set; Ensemble classification; Feature sets; Feature subspace; Late fusion; Learning schemes; Multiple features; Music genre classification; Classification (of information)","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"McKay C.; Burgoyne J.A.; Hockman J.; Smith J.B.L.; Vigliensoni G.; Fujinaga I.","McKay, Cory (14033215600); Burgoyne, John Ashley (23007865600); Hockman, Jason (36730968100); Smith, Jordan B. L. (55582613300); Vigliensoni, Gabriel (55217696900); Fujinaga, Ichiro (9038140900)","14033215600; 23007865600; 36730968100; 55582613300; 55217696900; 9038140900","Evaluating the genre classification performance of lyrical features relative to audio, symbolic and cultural features","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866621013&partnerID=40&md5=f6a76e110f560060ef97a6d77afcd175","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","McKay C., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Hockman J., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Smith J.B.L., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Vigliensoni G., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada","This paper describes experimental research investigating the genre classification utility of combining features extracted from lyrical, audio, symbolic and cultural sources of musical information. It was found that cultural features consisting of information extracted from both web searches and mined listener tags were particularly effective, with the result that classification accuracies were achieved that compare favorably with the current state of the art of musical genre classification. It was also found that features extracted from lyrics were less effective than the other feature types. Finally, it was found that, with some exceptions, combining feature types does improve classification performance. The new lyricFetcher and jLyrics software are also presented as tools that can be used as a framework for developing more effective classification methodologies based on lyrics in the future. © 2010 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Websites; Classification accuracy; Classification methodologies; Classification performance; Experimental research; Feature types; Genre classification; Musical genre classification; Musical information; State of the art; Web searches; Classification (of information)","C. McKay; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, QC, Canada; email: cory.mckay@mail.mcgill.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Joder C.; Essid S.; Richard G.","Joder, Cyril (35107208400); Essid, Slim (16033218700); Richard, Gaël (57195915952)","35107208400; 16033218700; 57195915952","An improved hierarchical approach for music-to-symbolic score alignment","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051648102&partnerID=40&md5=be6ed89cf85e07f477e74261bb1dcf77","CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France","Joder C., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France; Essid S., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France; Richard G., CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France","We present an efficient approach for an off-line alignment of a symbolic score to a recording of the same piece, using a statistical model. A hidden state model is built from the score, which allows for the use of two different kinds of features, namely chroma vectors and an onset detection function (spectral flux) with specific production models, in a simple manner. We propose a hierarchical pruning method for an approximate decoding of this statistical model. This strategy reduces the search space in an adaptive way, yielding a better overall efficiency than the tested state-of-the art method. Experiments run on a large database of 94 pop songs show that the resulting system obtains higher recognition rates than the dynamic programming algorithm (DTW), with a significantly lower complexity, even though the rhythmic information is not used for the alignment. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Approximate decoding; Dynamic programming algorithm; Hidden state; Hierarchical approach; Large database; Lower complexity; Onset detection; Overall efficiency; Production models; Pruning methods; Recognition rates; Search spaces; Spectral flux; State of the art; Statistical models; Alignment","C. Joder; CNRS LTCI, Institut TELECOM, TELECOM ParisTech, France; email: cyril.joder@telecom-paristech.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Marsden A.","Marsden, Alan (15054644600)","15054644600","Recognition of variations using automatic schenkerian reduction","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578828&partnerID=40&md5=01d04ec49c14a8bf6289e6a7f9b1642e","Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","Marsden A., Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","Experiments on techniques to automatically recognise whether or not an extract of music is a variation of a given theme are reported, using a test corpus derived from ten of Mozart's sets of variations for piano. Methods which examine the notes of the surface are compared with methods which make use of an automatically derived quasi-Schenkerian reduction of the theme and the extract in question. The maximum average F-measure achieved was 0.87. Unexpectedly, this was for a method of matching based on the surface alone, and in general the results for matches based on the surface were marginally better than those based on reduction, though the small number of possible test queries means that this result cannot be regarded as conclusive. Other inferences on which factors seem to be important in recognising variations are discussed. Possibilities for improved recognition of matching using reduction are outlined. © 2010 International Society for Music Information Retrieval.","","F-measure; Test corpus; Information retrieval","A. Marsden; Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom; email: A.Marsden@lancaster.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Angeles B.; McKay C.; Fujinaga I.","Angeles, Bruno (55585873600); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","55585873600; 14033215600; 9038140900","Discovering metadata inconsistencies","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574983&partnerID=40&md5=27c24566ce5a2817677015a1fb36ae97","CIRMMT, Schulich School of Music, McGill University, Canada","Angeles B., CIRMMT, Schulich School of Music, McGill University, Canada; McKay C., CIRMMT, Schulich School of Music, McGill University, Canada; Fujinaga I., CIRMMT, Schulich School of Music, McGill University, Canada","This paper describes the use of fingerprinting-based querying in identifying metadata inconsistencies in music libraries, as well as the updates to the jMusicMeta- Manager software in order to perform the analysis. Test results are presented for both the Codaich database and a generic library of unprocessed metadata. Statistics were computed in order to evaluate the differences between a manually-maintained library and an unprocessed collection when comparing metadata with values on a MusicBrainz server queried by fingerprinting. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Query processing; Generic libraries; Music library; Metadata","B. Angeles; CIRMMT, Schulich School of Music, McGill University, Canada; email: bruno.angeles@mail.mcgill.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Grindlay G.; Ellis D.P.W.","Grindlay, Graham (56950633500); Ellis, Daniel P.W. (13609089200)","56950633500; 13609089200","A probabilistic subspace model for multi-instrument polyphonic transcription","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053048831&partnerID=40&md5=67346c8dee814844d37d871af5e08382","Dept. of Electrical Engineering, LabROSA, Columbia University, United States","Grindlay G., Dept. of Electrical Engineering, LabROSA, Columbia University, United States; Ellis D.P.W., Dept. of Electrical Engineering, LabROSA, Columbia University, United States","In this paper we present a general probabilistic model suitable for transcribing single-channel audio recordings containing multiple polyphonic sources. Our system requires no prior knowledge of the instruments in the mixture, although it can benefit from this information if available. In contrast to many existing polyphonic transcription systems, our approach explicitly models the individual instruments and is thereby able to assign detected notes to their respective sources. We use a set of training instruments to learn a model space which is then used during transcription to constrain the properties of models fit to the target mixture. In addition, we encourage model sparsity using a simple approach related to tempering. We evaluate our method on both recorded and synthesized two-instrument mixtures, obtaining average framelevel F-measures of up to 0.60 for synthesized audio and 0.53 for recorded audio. If knowledge of the instrument types in the mixture is available, we can increase these measures to 0.68 and 0.58, respectively, by initializing the model with parameters from similar instruments. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Mixtures; Transcription; Model spaces; Polyphonic transcriptions; Prior knowledge; Probabilistic models; Simple approach; Single-channel; Subspace models; Instruments","G. Grindlay; Dept. of Electrical Engineering, LabROSA, Columbia University, United States; email: grindlay@ee.columbia.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Gkiokas A.; Katsouros V.; Carayannis G.","Gkiokas, Aggelos (14035320300); Katsouros, Vassilis (6507518296); Carayannis, George (7003979360)","14035320300; 6507518296; 7003979360","Tempo induction using filterbank analysis and tonal features","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585927&partnerID=40&md5=3058b38982dc942fe287b8407c6f2fb4","Institute for Language and Speech Processing, Greece; National Technical University of Athens, Greece","Gkiokas A., Institute for Language and Speech Processing, Greece, National Technical University of Athens, Greece; Katsouros V., Institute for Language and Speech Processing, Greece; Carayannis G., National Technical University of Athens, Greece","This paper presents an algorithm that extracts the tempo of a musical excerpt. The proposed system assumes a constant tempo and deals directly with the audio signal. A sliding window is applied to the signal and two feature classes are extracted. The first class is the log-energy of each band of a mel-scale triangular filterbank, a common feature vector used in various MIR applications. For the second class, a novel feature for the tempo induction task is presented; the strengths of the twelve western musical tones at all octaves are calculated for each audio frame, in a similar fashion with Pitch Class Profile. The timeevolving feature vectors are convolved with a bank of resonators, each resonator corresponding to a target tempo. Then the results of each feature class are combined to give the final output. The algorithm was evaluated on the popular ISMIR 2004 Tempo Induction Evaluation Exchange Dataset. Results demonstrate that the superposition of the different types of features enhance the performance of the algorithm, which is in the current state-of-the-art algorithms of the tempo induction task. © 2010 International Society for Music Information Retrieval.","","Filter banks; Information retrieval; Resonators; Audio frames; Audio signal; Common features; Feature vectors; Filter-bank analysis; Musical tones; Second class; Sliding Window; State-of-the-art algorithms; Algorithms","A. Gkiokas; Institute for Language and Speech Processing, Greece; email: agkiokas@ilsp.gr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Bimbot F.; Le Blouch O.; Sargent G.; Vincent E.","Bimbot, Frédéric (6701567957); Le Blouch, Olivier (54882457800); Sargent, Gabriel (55583665800); Vincent, Emmanuel (14010158800)","6701567957; 54882457800; 55583665800; 14010158800","Decomposition into autonomous and comparable blocks : A structural description of music pieces","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591001&partnerID=40&md5=e00978b5b29ee918532a9e78ad7875e8","IRISA, CNRS - UMR 6074, Campus Universitaire de Beaulieu, 35042 Rennes cedex, France; INRIA, Campus Universitaire de Beaulieu, 35042 Rennes cedex, Rennes Bretagne Atlantique, France; Campus Universitaire de Beaulieu, Université de Rennes 1, 35042 Rennes cedex, France","Bimbot F., IRISA, CNRS - UMR 6074, Campus Universitaire de Beaulieu, 35042 Rennes cedex, France; Le Blouch O., INRIA, Campus Universitaire de Beaulieu, 35042 Rennes cedex, Rennes Bretagne Atlantique, France; Sargent G., Campus Universitaire de Beaulieu, Université de Rennes 1, 35042 Rennes cedex, France; Vincent E., INRIA, Campus Universitaire de Beaulieu, 35042 Rennes cedex, Rennes Bretagne Atlantique, France","The structure of a music piece is a concept which is often referred to in various areas of music sciences and technologies, but for which there is no commonly agreed definition. This raises a methodological issue in MIR, when designing and evaluating automatic structure inference algorithms. It also strongly limits the possibility to produce consistent large-scale annotation datasets in a cooperative manner. This article proposes an approach called decomposition into autonomous and comparable blocks, based on principles inspired from structuralism and generativism. It specifies a methodology for producing music structure annotation by human listeners based on simple criteria and resorting solely to the listening experience of the annotator. We show on a development set that the proposed approach can provide a reasonable level of concordance across annotators and we introduce a set of annotations on the RWC database, intended to be released to the MIR community. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Automatic structures; Human listeners; Music structures; Structural descriptions; Inference engines","F. Bimbot; IRISA, CNRS - UMR 6074, Campus Universitaire de Beaulieu, 35042 Rennes cedex, France; email: frederic.bimbot@irisa.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Oliveira J.L.; Gouyon F.; Martins L.G.; Reis L.P.","Oliveira, João Lobato (49864144600); Gouyon, Fabien (8373002800); Martins, Luis Gustavo (16245475200); Reis, Luis Paulo (13907511600)","49864144600; 8373002800; 16245475200; 13907511600","IBT: A real-time tempo and beat tracking system","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864672836&partnerID=40&md5=cd83552150c924270d44225c22c4f512","Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal; Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal; Research Center for Science and Technology in Art (CITAR), UCP, Porto, Portugal","Oliveira J.L., Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal, Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal; Gouyon F., Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal; Martins L.G., Research Center for Science and Technology in Art (CITAR), UCP, Porto, Portugal; Reis L.P., Artificial Intelligence and Computer Science Laboratory (LIACC), FEUP, Porto, Portugal","This paper describes a tempo induction and beat tracking system based on the efficient strategy (initially introduced in the BeatRoot system [Dixon S., ""Automatic extraction of tempo and beat from expressive performances."" Journal of New Music Research, 30(1):39-58, 2001]) of competing agents processing musical input sequentially and considering parallel hypotheses regarding tempo and beats. In this paper, we propose to extend this strategy to the causal processing of continuous input data. The main reasons for this are threefold: providing more robustness to potentially noisy input data, permitting the parallel consideration of a number of low-level frame-based features as input, and opening the way to real-time uses of the system (as e.g. for a mobile robotic platform). The system is implemented in C++, permitting faster than real-time processing of audio data. It is integrated in the MARSYAS framework, and is therefore available under GPL for users and/or researchers. Detailed evaluation of the causal and non-causal versions of the system on common benchmark datasets show performances reaching those of state-of-the-art beat trackers. We propose a series of lines for future work based on careful analysis of the results. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Input output programs; Tracking (position); Audio data; Automatic extraction; Beat tracking; Benchmark datasets; Continuous input; Efficient strategy; Expressive performance; Frame-based; Input datas; Mobile robotic; Realtime processing; Data handling","J.L. Oliveira; Institute for Systems and Computer Engineering of Porto (INESC Porto), Porto, Portugal; email: jmso@inescporto.pt","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hamel P.; Eck D.","Hamel, Philippe (8402361900); Eck, Douglas (12141444300)","8402361900; 12141444300","Learning features from music audio with deep belief networks","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","206","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584268&partnerID=40&md5=ff934d48d288a327dcd225176cf7e171","DIRO, CIRMMT, Université de Montréal, Canada","Hamel P., DIRO, CIRMMT, Université de Montréal, Canada; Eck D., DIRO, CIRMMT, Université de Montréal, Canada","Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically extract relevant features from audio for a given task. The feature extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the audio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features perform significantly better than MFCCs. Moreover, we obtain a classification accuracy of 84.3% on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also applied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features. © 2010 International Society for Music Information Retrieval.","","Bayesian networks; Classification (of information); Discrete Fourier transforms; Feature extraction; Information retrieval; Support vector machines; Classification accuracy; Deep belief network (DBN); Deep belief networks; Discrete fourier transform (DFTs); Frame-based; Relevant features; Temporal features; Audio acoustics","P. Hamel; DIRO, CIRMMT, Université de Montréal, Canada; email: hamelphi@iro.umontreal.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Crawford T.; Mauch M.; Rhodes C.","Crawford, Tim (15054056900); Mauch, Matthias (36461512900); Rhodes, Christophe (57196565939)","15054056900; 36461512900; 57196565939","Recognizing classical works in historical recordings","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955383518&partnerID=40&md5=02f31eee9cdfcee462b511bf12b3736c","Centre for Cognition, Computation and Culture, Goldsmiths, University of London, United Kingdom; Centre for Digital Music, Queen Mary University of London, United Kingdom; Department of Computing, Goldsmiths, University of London, United Kingdom","Crawford T., Centre for Cognition, Computation and Culture, Goldsmiths, University of London, United Kingdom; Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Rhodes C., Department of Computing, Goldsmiths, University of London, United Kingdom","In collections of recordings of classical music, it is normal to find multiple performances, usually by different artists, of the same pieces of music. While there may be differences in many dimensions of musical similarity, such as timbre, pitch or structural detail, the underlying musical content is essentially and recognizably the same. The degree of divergence is generally less than that found between 'cover songs' in the domain of popular music, and much less than in typical performances of jazz standards. MIR methods, based around variants of the chroma representation, can be useful in tasks such as work identification especially where disco/bibliographical metadata is absent or incomplete as well as for access, curation and management of collections. We describe some initial experiments in work-recognition on a test-collection comprising c. 2000 digital transfers of historical recordings, and show that the use of NNLS chroma, a new, musically-informed chroma feature, dramatically improves recognition. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Chroma features; Classical musics; Cover songs; Curation; Digital transfers; Jazz standards; Musical similarity; Popular music; Structural details; Audio recordings","T. Crawford; Centre for Cognition, Computation and Culture, Goldsmiths, University of London, United Kingdom; email: t.crawford@gold.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hoashi K.; Hamawaki S.; Ishizaki H.; Takishima Y.; Katto J.","Hoashi, Keiichiro (6603899930); Hamawaki, Shuhei (26025043300); Ishizaki, Hiromi (16068647300); Takishima, Yasuhiro (6602909921); Katto, Jiro (55903062000)","6603899930; 26025043300; 16068647300; 6602909921; 55903062000","Usability evaluation of visualization interfaces for content-based music retrieval systems","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873672344&partnerID=40&md5=6c69bba389a9514c114667a0d077041b","KDDI R and D Laboratories Inc., Japan; Graduate School of Science and Engineering, Waseda University, Japan","Hoashi K., KDDI R and D Laboratories Inc., Japan; Hamawaki S., Graduate School of Science and Engineering, Waseda University, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan; Katto J., Graduate School of Science and Engineering, Waseda University, Japan","This research presents a formal user evaluation of a typical visualization method for content-based music information retrieval (MIR) systems, and also proposes a novel interface to improve MIR usability. Numerous interfaces to visualize content-based MIR systems have been proposed, but reports on user evaluations of such proposed GUIs are scarce. This research aims to evaluate the effectiveness of a typical 2-D visualization method for content-based MIR systems, by conducting comparative user evaluations against the traditional list-based format to present MIR results to the user. Based on the observations of the experimental results, we next propose a 3-D visualization system, which features a function to specify sub-regions of the feature space based on genre classification results, and a function which allows users to select features that are assigned to the axes of the 3-D space. Evaluation of this GUI conclude that the functions of the 3-D system can significantly improve both the efficiency and usability of MIR systems. © 2009 International Society for Music Information Retrieval.","","Function evaluation; Graphical user interfaces; Information retrieval; Visualization; 2-D visualizations; 3-D space; 3-D visualization systems; Content-based; Content-based music retrieval; Feature space; Genre classification; Music information retrieval; Sub-regions; Usability evaluation; User evaluations; Visualization method; Three dimensional computer graphics","K. Hoashi; KDDI R and D Laboratories Inc., Japan; email: hoashi@kddilabs.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Ferrer R.; Eerola T.","Ferrer, Rafael (23488472500); Eerola, Tuomas (6602209042)","23488472500; 6602209042","Timbral qualities of semantic structures of music","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583216&partnerID=40&md5=4b3f5131a053ac6eed730f8300c0c2fe","Finnish Centre of Excellence in Interdisciplinary Music Research, Finland","Ferrer R., Finnish Centre of Excellence in Interdisciplinary Music Research, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, Finland","The rapid expansion of social media in music has provided the field with impressive datasets that offer insights into the semantic structures underlying everyday uses and classification of music. We hypothesize that the organization of these structures are rather directly linked with the ""qualia"" of the music as sound. To explore the ways in which these structures are connected with the qualities of sounds, a semantic space was extracted from a large collection of musical tags with latent semantic and cluster analysis. The perceptual and musical properties of 19 clusters were investigated by a similarity rating task that used spliced musical excerpts representing each cluster. The resulting perceptual space denoting the clusters correlated high with selected acoustical features extracted from the stimuli. The first dimension related to the high-frequency energy content, the second to the regularity of the spectrum, and the third to the fluctuations within the spectrum. These findings imply that meaningful organization of music may be derived from low-level descriptions of the excerpts. Novel links with the functions of music embedded into the tagging information included within the social media are proposed. © 2010 International Society for Music Information Retrieval.","","Classification (of information); Cluster analysis; Information retrieval; Energy content; High frequency HF; Latent semantics; Rapid expansion; Semantic Space; Semantic structures; Social media; Semantics","R. Ferrer; Finnish Centre of Excellence in Interdisciplinary Music Research, Finland; email: rafael.ferrer-flores@jyu.fi","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Paulus J.","Paulus, Jouni (16068963200)","16068963200","Improving markov model-based music piece structure labelling with acoustic information","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583694&partnerID=40&md5=bd2e218276e70ecba6403abcbdfe905f","Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","Paulus J., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany","This paper proposes using acoustic information in the labelling of music piece structure descriptions. Here, music piece structure means the sectional form of the piece: temporal segmentation and grouping to parts such as chorus or verse. The structure analysis methods rarely provide the parts with musically meaningful names. The proposed method labels the parts in a description. The baseline method models the sequential dependencies between musical parts with N-grams and uses themfor the labelling. The acoustic model proposed in this paper is based on the assumption that the parts with the same label even in different pieces share some acoustic properties compared to other parts in the same pieces. The proposed method uses mean and standard deviation of relative loudness in a part as the feature which is then modelled with a single multivariate Gaussian distribution. The method is evaluated on three data sets of popular music pieces, and in all of them the inclusion of the acoustic model improves the labelling accuracy over the baseline method. © 2010 International Society for Music Information Retrieval.","","Acoustic properties; Markov processes; Acoustic information; Acoustic model; Baseline methods; Mean and standard deviations; Multivariate Gaussian Distributions; N-grams; Piece structures; Popular music; Sequential dependencies; Structure analysis; Temporal segmentations; Information retrieval","J. Paulus; Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany; email: jouni.paulus@iis.fraunhofer.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Miotto R.; Orio N.","Miotto, Riccardo (57202034402); Orio, Nicola (6507928255)","57202034402; 6507928255","A probabilistic approach to merge context and content information for music retrieval","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857180763&partnerID=40&md5=fc69ea25948f0ad9bdde99529722f052","University of Padova, Italy","Miotto R., University of Padova, Italy; Orio N., University of Padova, Italy","An interesting problem in music information retrieval is how to combine the information from different sources in order to improve retrieval effectiveness. This paper introduces an approach to represent a collection of tagged songs through an hidden Markov model with the purpose to develop a system that merges in the same framework both acoustic similarity and semantic descriptions. The former provides content-based information on song similarity, the latter provides context-aware information about individual songs. Experimental results show how the proposed model leads to better performances than approaches that rank songs using both a single information source and a their linear combination. © 2010 International Society for Music Information Retrieval.","","Hidden Markov models; Semantics; Acoustic similarities; Better performance; Content-based information; Context and content; Context-aware informations; Information sources; Linear combinations; Music information retrieval; Music retrieval; Probabilistic approaches; Retrieval effectiveness; Semantic descriptions; Information retrieval","R. Miotto; University of Padova, Italy; email: miottori@dei.unipd.it","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hirjee H.; Brown D.G.","Hirjee, Hussein (55586282300); Brown, Daniel G. (55738804200)","55586282300; 55738804200","Solving misheard lyric search queries using a probabilistic model of speech sounds","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872866272&partnerID=40&md5=75d954cfb2f31fbc185ba1cedf0409b3","Cheriton School of Computer Science, University of Waterloo, Canada","Hirjee H., Cheriton School of Computer Science, University of Waterloo, Canada; Brown D.G., Cheriton School of Computer Science, University of Waterloo, Canada","Music listeners often mishear the lyrics to unfamiliar songs heard from public sources, such as the radio. Since standard text search engines will find few relevant results when they are entered as a query, these misheard lyrics require phonetic pattern matching techniques to identify the song. We introduce a probabilistic model of mishear- ing trained on examples of actual misheard lyrics, and develop a phoneme similarity scoring matrix based on this model. We compare this scoring method to simpler pattern matching algorithms on the task of finding the correct lyric from a collection given a misheard query. The probabilistic method significantly outperforms all other methods, finding 5-8% more correct lyrics within the first five hits than the previous best method. © 2010 International Society for Music Information Retrieval.","","Pattern matching; Search engines; Lyric searches; Pattern matching algorithms; Pattern-matching technique; Probabilistic methods; Probabilistic models; Scoring matrices; Scoring methods; Speech sounds; Information retrieval","H. Hirjee; Cheriton School of Computer Science, University of Waterloo, Canada; email: hahirjee@uwaterloo.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Rump H.; Miyabe S.; Tsunoo E.; Ono N.; Sagama S.","Rump, Halfdan (55586139600); Miyabe, Shigeki (15840326400); Tsunoo, Emiru (35068648900); Ono, Nobukata (7202472899); Sagama, Shigeki (55586495700)","55586139600; 15840326400; 35068648900; 7202472899; 55586495700","Autoregressive MFCC models for genre classification improved by harmonic-percussion separation","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053014937&partnerID=40&md5=ad75c3ea1bdfec126bae1b0e8cae16c7","School of Information Science and Technology, University of Tokyo, Japan","Rump H., School of Information Science and Technology, University of Tokyo, Japan; Miyabe S., School of Information Science and Technology, University of Tokyo, Japan; Tsunoo E., School of Information Science and Technology, University of Tokyo, Japan; Ono N., School of Information Science and Technology, University of Tokyo, Japan; Sagama S., School of Information Science and Technology, University of Tokyo, Japan","In this work we improve accuracy of MFCC-based genre classification by using the Harmonic-Percussion Signal Separation (HPSS) algorithm on the music signal, and then calculate the MFCCs on the separated signals. The choice of the HPSS algorithm was mainly based on the observation that the presence of harmonics causes the high MFCCs to be noisy. A multivariate autoregressive (MAR) model was trained on the improved MFCCs, and performance in the task of genre classification was evaluated. By combining features calculated on the separated signals, relative error rate reductions of 20% and 16.2% were obtained when an SVM classifier was trained on the MFCCs and MAR features respectively. Next, by analyzing the MAR features calculated on the separated signals, it was concluded that the original signal contained some information which the MAR model was capable of handling, and that the best performance was obtained when all three signals were used. Finally, by choosing the number of MFCCs from each signal type to be used in the autoregressive modelling, it was verified that the best performance was reached when the high MFCCs calculated on the harmonic signal were discarded. © 2010 International Society for Music Information Retrieval.","","Algorithms; Harmonic analysis; Information retrieval; Separation; Speech recognition; Auto-regressive; Autoregressive modelling; Genre classification; Harmonic signals; Multivariate autoregressive (mAR) model; Music signals; Original signal; Relative error rates; Signal separation; SVM classifiers; Computer music","H. Rump; School of Information Science and Technology, University of Tokyo, Japan; email: rump@hil.t.u-tokyo.ac.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Laplante A.","Laplante, Audrey (37110930300)","37110930300","Users' relevance criteria in music retrieval in everyday life: An exploratory study","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447381&partnerID=40&md5=3487fec29409d25660f773db9c591a0b","ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Montréal, QC, Canada","Laplante A., ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Montréal, QC, Canada","The paper presents the findings of a qualitative study on the way young adults make relevance inferences about music items when searching for music for recreational purposes. Data were collected through in-depth interviews and analyzed following the constant comparative method. Content analysis revealed that participants used four types of clues to make relevance inferences: bibliographic metadata (e.g., names of contributors, labels), relational metadata (e.g., genres, similar artists), associative metadata (e.g., cover arts), and recommendations/reviews. Relevance judgments were also found to be influenced by the external context (i.e., the functions music plays in one's life) and the internal context (i.e., individual tastes and beliefs, state of mind). © 2010 International Society for Music Information Retrieval.","","Information retrieval; Comparative methods; Content analysis; Exploratory studies; In-depth interviews; Music retrieval; Qualitative study; Relevance criteria; Relevance judgment; Young adults; Metadata","A. Laplante; ÉCole de Bibliothéconomie et des Sciences de L'information, Université de Montréal, Montréal, QC, Canada; email: audrey.laplante@umontreal.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Yoshii K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","7103400120; 7403505330","Infinite latent harmonic allocation: A nonparametric bayesian approach to multipitch analysis","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586598&partnerID=40&md5=c43105f4270d0f03ba115f359f27170f","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a statistical method called Infinite Latent Harmonic Allocation (iLHA) for detecting multiple fundamental frequencies in polyphonic audio signals. Conventional methods face a crucial problem known as model selection because they assume that the observed spectra are superpositions of a certain fixed number of bases (sound sources and/or finer parts). iLHA avoids this problem by assuming that the observed spectra are superpositions of a stochastically-distributed unbounded (theoretically infinite) number of bases. Such uncertainty can be treated in a principled way by leveraging the state-of-the-art paradigm of machine-learning called Bayesian nonparametrics. To represent a set of time-sliced spectral strips, we formulated nested infinite Gaussian mixture models (GMMs) based on hierarchical and generalized Dirichlet processes. Each strip is allowed to contain an unbounded number of sound sources (GMMs), each of which is allowed to contain an unbounded number of harmonic partials (Gaussians). To train the nested infinite GMMs efficiently, we used a modern inference technique called collapsed variational Bayes (CVB). Our experiments using audio recordings of real piano and guitar performances showed that fully automated iLHA based on noninformative priors performed as well as optimally tuned conventional methods. © 2010 International Society for Music Information Retrieval.","","Acoustic generators; Bayesian networks; Information retrieval; Audio signal; Bayesian nonparametrics; Conventional methods; Fixed numbers; Fully automated; Fundamental frequencies; Gaussians; Generalized Dirichlet; Inference techniques; Infinite Gaussian mixture models; Machine-learning; Model Selection; Multi pitches; Non-informative prior; Non-parametric Bayesian; Sound source; Variational bayes; Harmonic analysis","K. Yoshii; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: k.yoshii@aist.go.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hsu C.-L.; Jang J.-S.R.","Hsu, Chao-Ling (47061226300); Jang, Jyh-Shing Roger (7402965041)","47061226300; 7402965041","Singing pitch extraction by voice vibrato/tremolo estimation and instrument partial deletion","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051657795&partnerID=40&md5=b596c7118468c2598983c786163ec30f","Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan","Hsu C.-L., Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan; Jang J.-S.R., Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan","This paper proposes a novel and effective approach to extract the pitches of the singing voice from monaural polyphonic songs. The sinusoidal partials of the musical audio signals are first extracted. The Fourier transform is then applied to extract the vibrato/tremolo information of each partial. Some criteria based on this vibrato/tremolo information are employed to discriminate the vocal partials from the music accompaniment partials. Besides, a singing pitch trend estimation algorithm which is able to find the global singing progressing tunnel is also proposed. The singing pitches can then be extracted more robustly via these two processes. Quantitative evaluation shows that the proposed algorithms significantly improve the raw pitch accuracy of our previous approach and are comparable with other state of the art approaches submitted to MIREX. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Effective approaches; Music accompaniments; Musical audio signal; Polyphonic songs; Quantitative evaluation; Singing pitch extractions; Singing voices; State-of-the-art approach; Trend estimation; Algorithms","C.-L. Hsu; Computer Science Department, Multimedia Information Retrieval Laboratory, National Tsing Hua University, Hsinchu, Taiwan; email: leon@mirlab.org","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Maezawa A.; Goto M.; Okuno H.G.","Maezawa, Akira (35753591100); Goto, Masataka (7403505330); Okuno, Hiroshi G. (7102397930)","35753591100; 7403505330; 7102397930","Query-by-conducting: An interface to retrieve classical-music interpretations by real-time tempo input","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051631241&partnerID=40&md5=a81b6f4163f6ca956967cc921ceb9a89","Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","Maezawa A., Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper presents an interface for finding interpretations of a user-specified music, Query-by-Conducting. In classical music, there are many interpretations to a particular piece, and finding ""the"" interpretation that matches the listener's taste allows a listener to further enjoy the piece. The critical issue in finding such an interpretation is the way or interface to allow the listener to listen through different interpretations. Our interface allows a user, by swinging a conducting hardware interface, to conduct the desired global tempo along the playback of a piece, at any time in the piece. The real-time conducting input by the user dynamically switches the interpretation being played back to the one closest to how the user is currently conducting. At the end of the piece, our interface ranks each interpretation according to how close the tempo of each interpretation was to the user input. At the core of our interface is an automated tempo estimation method based on audio-score alignment. We improve tempo estimation by requiring the audio-score alignment of different interpretations to be consistent with each other. We evaluate the tempo estimation method using a solo, chamber, and orchestral repertoire. The proposed tempo estimation decreases the error by as much as 0.94 times the original error. © 2010 International Society for Music Information Retrieval.","","Alignment; Information retrieval; Classical musics; Critical issues; Hardware interfaces; Tempo estimations; User input; Estimation","A. Maezawa; Dept. of Intelligence Science and Technology, School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: amaezaw@kuis.kyoto-u.ac.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Bosteels K.; Pampalk E.; Kerre E.E.","Bosteels, Klaas (21233431000); Pampalk, Elias (6507297042); Kerre, Etienne E. (35509593100)","21233431000; 6507297042; 35509593100","Evaluating and analysing dynamic playlist generation heuristics using radio logs and fuzzy set theory","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695311&partnerID=40&md5=0294ae969febee49afaa94d073db7f2d","Ghent University, Gent, Belgium; Last.fm Ltd., London, United Kingdom","Bosteels K., Ghent University, Gent, Belgium; Pampalk E., Last.fm Ltd., London, United Kingdom; Kerre E.E., Ghent University, Gent, Belgium","In this paper, we analyse and evaluate several heuristics for adding songs to a dynamically generated playlist. We explain how radio logs can be used for evaluating such heuristics, and show that formalizing the heuristics using fuzzy set theory simplifies the analysis. More concretely, we verify previous results by means of a large scale evaluation based on 1.26 million listening patterns extracted from radio logs, and explain why some heuristics perform better than others by analysing their formal definitions and conducting additional evaluations. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Formal definition; Fuzzy set theory","K. Bosteels; Ghent University, Gent, Belgium; email: klaas.bosteels@ugent.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Izmirli Ö.; Dannenberg R.B.","Izmirli, Özgür (6507754258); Dannenberg, Roger B. (7003266250)","6507754258; 7003266250","Understanding features and distance functions for music sequence alignment","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573319&partnerID=40&md5=a026c7434a3ea02674582a9d71cc8a12","Computer Science Department, Center for Arts and Technology, Connecticut College, United States; School of Computer Science, Carnegie Mellon University, United States","Izmirli Ö., Computer Science Department, Center for Arts and Technology, Connecticut College, United States; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States","We investigate the problem of matching symbolic representations directly to audio based representations for applications that use data from both domains. One such application is score alignment, which aligns a sequence of frames based on features such as chroma vectors and distance functions such as Euclidean distance. Good representations are critical, yet current systems use ad hoc constructions such as the chromagram that have been shown to work quite well. We investigate ways to learn chromagram-like representations that optimize the classification of ""matching"" vs. ""non-matching"" frame pairs of audio and MIDI. New representations learned automatically from examples not only perform better than the chromagram representation but they also reveal interesting projection structures that differ distinctly from the traditional chromagram. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Audio-based; Distance functions; Euclidean distance; Sequence alignments; Symbolic representation; Alignment","Ö. Izmirli; Computer Science Department, Center for Arts and Technology, Connecticut College, United States; email: oizm@conncoll.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Honingh A.; Bod R.","Honingh, Aline (14032798400); Bod, Rens (12784000100)","14032798400; 12784000100","Pitch class set categories as analysis tools for degrees of tonality","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573958&partnerID=40&md5=5ca7d6af3998920fe156c7054557843e","Institute for Logic, Language and Computation, University of Amsterdam, Netherlands","Honingh A., Institute for Logic, Language and Computation, University of Amsterdam, Netherlands; Bod R., Institute for Logic, Language and Computation, University of Amsterdam, Netherlands","This is an explorative paper in which we present a new method for music analysis based on pitch class set categories. It has been shown before that pitch class sets can be divided into six different categories. Each category inherits a typical character which can ""tell"" something about the music in which it appears. In this paper we explore the possibilities of using pitch class set categories for 1) classification in major/minor mode, 2) classification in tonal/atonal music, 3) determination of a degree of tonality, and 4) determination of a composer's period. © 2010 International Society for Music Information Retrieval.","","Analysis tools; Music analysis; Information retrieval","A. Honingh; Institute for Logic, Language and Computation, University of Amsterdam, Netherlands; email: A.K.Honingh@uva.nl","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Collins T.; Thurlow J.; Laney R.; Willis A.; Garthwaite P.H.","Collins, Tom (37088212600); Thurlow, Jeremy (53872095200); Laney, Robin (8283074400); Willis, Alistair (23986544600); Garthwaite, Paul H. (6701627235)","37088212600; 53872095200; 8283074400; 23986544600; 6701627235","A comparative evaluation of algorithms for discovering translational patterns in baroque keyboard works","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954502326&partnerID=40&md5=7be4977588e0d5e67751f0963c686fca","Open University, United Kingdom; University of Cambridge, United Kingdom","Collins T., Open University, United Kingdom; Thurlow J., University of Cambridge, United Kingdom; Laney R., Open University, United Kingdom; Willis A., Open University, United Kingdom; Garthwaite P.H., Open University, United Kingdom","We consider the problem of intra-opus pattern discovery, that is, the task of discovering patterns of a specified type within a piece of music. A music analyst undertook this task for works by Domenico Scarlattti and Johann Sebastian Bach, forming a benchmark of 'target' patterns. The performance of two existing algorithms and one of our own creation, called SIACT, is evaluated by comparison with this benchmark. SIACT out-performs the existing algorithms with regard to recall and, more often than not, precision. It is demonstrated that in all but the most carefully selected excerpts of music, the two existing algorithms can be affected by what is termed the 'problem of isolated membership'. Central to the relative success of SIACT is our intention that it should address this particular problem. The paper contrasts string-based and geometric approaches to pattern discovery, with an introduction to the latter. Suggestions for future work are given. © 2010 International Society for Music Information Retrieval.","","Benchmarking; Information retrieval; Comparative evaluations; Geometric approaches; Pattern discovery; Sebastian; Algorithms","T. Collins; Open University, United Kingdom; email: t.e.collins@open.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Kaiser F.; Sikora T.","Kaiser, Florian (57201809842); Sikora, Thomas (36777640600)","57201809842; 36777640600","Music structure discovery in popular music using non-negative matrix factorization","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","43","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581905&partnerID=40&md5=94391ce2dd7e936a7b516cbcda12ee8b","Communication Systems Group, Technische Universiẗ, Berlin, Germany","Kaiser F., Communication Systems Group, Technische Universiẗ, Berlin, Germany; Sikora T., Communication Systems Group, Technische Universiẗ, Berlin, Germany","We introduce a method for the automatic extraction of musical structures in popular music. The proposed algorithm uses non-negative matrix factorization to segment regions of acoustically similar frames in a self-similarity matrix of the audio data. We show that over the dimensions of the NMF decomposition, structural parts can easily be modeled. Based on that observation, we introduce a clustering algorithm that can explain the structure of the whole music piece. The preliminary evaluation we report in the the paper shows very encouraging results. © 2010 International Society for Music Information Retrieval.","","Clustering algorithms; Information retrieval; Audio data; Automatic extraction; Music structures; Musical structures; Nonnegative matrix factorization; Popular music; Self-similarity matrix; Structural parts; Factorization","F. Kaiser; Communication Systems Group, Technische Universiẗ, Berlin, Germany; email: kaiser@nue.tu-berlin.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Weiss R.J.; Bello J.P.","Weiss, Ron J. (58097887700); Bello, Juan Pablo (7102889110)","58097887700; 7102889110","Identifying repeated patterns in music using sparse convolutive non-negative matrix factorization","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573953&partnerID=40&md5=48e8f7d80e4b7269b96d38cd6eac325d","Music and Audio Research Lab (MARL), New York University, United States","Weiss R.J., Music and Audio Research Lab (MARL), New York University, United States; Bello J.P., Music and Audio Research Lab (MARL), New York University, United States","We describe an unsupervised, data-driven, method for automatically identifying repeated patterns in music by analyzing a feature matrix using a variant of sparse convolutive non-negative matrix factorization. We utilize sparsity constraints to automatically identify the number of patterns and their lengths, parameters that would normally need to be fixed in advance. The proposed analysis is applied to beatsynchronous chromagrams in order to concurrently extract repeated harmonic motifs and their locations within a song. Finally, we show how this analysis can be used for longterm structure segmentation, resulting in an algorithm that is competitive with other state-of-the-art segmentation algorithms based on hidden Markov models and self similarity matrices. © 2010 International Society for Music Information Retrieval.","","Hidden Markov models; Image segmentation; Feature matrices; Long-term structures; Nonnegative matrix factorization; Repeated patterns; Segmentation algorithms; Self-similarities; Sparsity constraints; Information retrieval","R.J. Weiss; Music and Audio Research Lab (MARL), New York University, United States; email: ronw@nyu.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Murao K.; Nakano M.; Kitano Y.; Ono N.; Sagayama S.","Murao, Kazuma (57919300800); Nakano, Masahiro (55701875500); Kitano, Yu (36608202600); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","57919300800; 55701875500; 36608202600; 7202472899; 7004859104","Monophonic instrument sound segregation by clustering NMF components based on basis similarity and gain disjointness","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052993673&partnerID=40&md5=77bbf30bf4c2a03d3e8a2b73d87d6ec9","School of Information Science and Technology, University of Tokyo, Japan","Murao K., School of Information Science and Technology, University of Tokyo, Japan; Nakano M., School of Information Science and Technology, University of Tokyo, Japan; Kitano Y., School of Information Science and Technology, University of Tokyo, Japan; Ono N., School of Information Science and Technology, University of Tokyo, Japan; Sagayama S., School of Information Science and Technology, University of Tokyo, Japan","This paper discusses a method for monophonic instrument sound separation based on nonnegative matrix factorization (NMF). In general, it is not easy to classify NMF components into each instrument. By contrast, monophonic instrument sound gives us an important clue to classify them, because no more than one sound would be activated simultaneously. Our approach is to classify NMF components into each instrument based on basis spectrum vector similarity and temporal activity disjointness. Our clustering employs a hierarchical clustering algorithm: group average method (GAM). The efficiency of our approach is evaluated by some experiments. © 2010 International Society for Music Information Retrieval.","","Acoustic waves; Clustering algorithms; Information retrieval; Average method; Disjointness; Hierarchical clustering algorithms; Nonnegative matrix factorization; Sound segregation; Sound separation; Vector similarity; Instruments","K. Murao; School of Information Science and Technology, University of Tokyo, Japan; email: k-murao@hil.t.u-tokyo.ac.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Fremerey C.; Müller M.; Clausen M.","Fremerey, Christian (23396821400); Müller, Meinard (7404689873); Clausen, Michael (56225233200)","23396821400; 7404689873; 56225233200","Handling repeats and jumps in score-performance synchronization","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960505828&partnerID=40&md5=fdc8c63fe77e80158d73a1f9cceb4a6d","Department of Computer Science, Bonn University, Bonn, Germany; MPI Informatik, Saarland University, Saarbrücken, Germany","Fremerey C., Department of Computer Science, Bonn University, Bonn, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Clausen M., Department of Computer Science, Bonn University, Bonn, Germany","Given a score representation and a recorded performance of the same piece of music, the task of score-performance synchronization is to temporally align musical sections such as bars specified by the score to temporal sections in the performance. Most of the previous approaches assume that the score and the performance to be synchronized globally agree with regard to the overall musical structure. In practice, however, this assumption is often violated. For example, a performer may deviate from the score by ignoring a repeat or introducing an additional repeat that is not written in the score. In this paper, we introduce a synchronization approach that can cope with such structural differences. As main technical contribution, we describe a novel variant of dynamic time warping (DTW), referred to as JumpDTW, which allows for handling jumps and repeats in the alignment. Our approach is evaluated for the practically relevant case of synchronizing score data obtained from scanned sheet music via optical music recognition to corresponding audio recordings. Our experiments based on Beethoven piano sonatas show that JumpDTW can robustly identify and handle most of the occurring jumps and repeats leading to an overall alignment accuracy of over 99% on the bar-level. © 2010 International Society for Music Information Retrieval.","","Alignment; Information retrieval; Alignment accuracy; Dynamic time warping; Musical structures; Optical music recognition; Structural differences; Technical contribution; Synchronization","C. Fremerey; Department of Computer Science, Bonn University, Bonn, Germany; email: fremerey@iai.uni-bonn.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Kim Y.E.; Schmidt E.M.; Migneco R.; Morton B.G.; Richardson P.; Scott J.; Speck J.A.; Turnbull D.","Kim, Youngmoo E. (24724623000); Schmidt, Erik M. (36053813000); Migneco, Raymond (35311311700); Morton, Brandon G. (36457242900); Richardson, Patrick (37089206400); Scott, Jeffrey (35312262000); Speck, Jacquelin A. (36457008100); Turnbull, Douglas (8380095700)","24724623000; 36053813000; 35311311700; 36457242900; 37089206400; 35312262000; 36457008100; 8380095700","Music emotion recognition: A state of the art review","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","310","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591302&partnerID=40&md5=4cb6e1ed89144c53f9c1ae8400db638f","Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States","Kim Y.E., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Schmidt E.M., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Migneco R., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Morton B.G., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Richardson P., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Scott J., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Speck J.A., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; Turnbull D., Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States","This paper surveys the state of the art in automatic emotion recognition in music. Music is oftentimes referred to as a ""language of emotion"" [1], and it is natural for us to categorize music in terms of its emotional associations. Myriad features, such as harmony, timbre, interpretation, and lyrics affect emotion, and the mood of a piece may also change over its duration. But in developing automated systems to organize music in terms of emotional content, we are faced with a problem that oftentimes lacks a welldefined answer; there may be considerable disagreement regarding the perception and interpretation of the emotions of a song or ambiguity within the piece itself. When compared to other music information retrieval tasks (e.g., genre identification), the identification of musical mood is still in its early stages, though it has received increasing attention in recent years. In this paper we explore a wide range of research in music emotion recognition, particularly focusing on methods that use contextual text information (e.g., websites, tags, and lyrics) and content-based approaches, as well as systems combining multiple feature domains. © 2010 International Society for Music Information Retrieval.","","Automation; Information retrieval; Automated systems; Automatic emotion recognition; Content-based approach; Genre identification; Multiple features; Music emotions; Music information retrieval; Paper surveys; State of the art; State-of-the art reviews; Text information; Behavioral research","Y.E. Kim; Department of Electrical and Computer Engineering, Ithaca College, Drexel University Computer Science, United States; email: ykim@drexel.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Burgoyne J.A.; Devaney J.; Ouyang Y.; Pugin L.; Himmelman T.; Fujinaga I.","Burgoyne, John Ashley (23007865600); Devaney, Johanna (35766484200); Ouyang, Yue (55587339900); Pugin, Laurent (23009752900); Himmelman, Tristan (36715197000); Fujinaga, Ichiro (9038140900)","23007865600; 35766484200; 55587339900; 23009752900; 36715197000; 9038140900","Lyric extraction and recognition on digital images of early music sources","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873686672&partnerID=40&md5=5ed4ee38eadb54da684a3853b5855c53","Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada","Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Devaney J., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Ouyang Y., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Himmelman T., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada","Optical music recognition (OMR) is one of the most promising tools for generating large-scale, distributable libraries of musical data. Much OMR work has focussed on instrumental music, avoiding a special challenge vocal music poses for OMR: lyric recognition. Lyrics complicate the page layout, making it more difficult to identify the regions of the page that carry musical notation. Furthermore, users expect a complete OMR process for vocal music to include recognition of the lyrics, reunification of syllables when they have been separated, and alignment of these lyrics with the recognised music. Unusual layouts and inconsistent practises for syllabification, however, make lyric recognition more challenging than traditional optical character recognition (OCR). This paper surveys historical approaches to lyric recognition, outlines open challenges, and presents a new approach to extracting text lines in medieval manuscripts, one of the frontiers of OMR research today. © 2009 International Society for Music Information Retrieval.","","Optical character recognition; Digital image; Early musics; Medieval manuscript; Musical notation; New approaches; Optical character recognition (OCR); Optical music recognition; Page layouts; Paper surveys; Text lines; Vocal music; Information retrieval","J.A. Burgoyne; Centre for Interdisciplinary Research in Music and Media Technology, McGill University, Montréal, QC, Canada; email: ashley@music.mcgill.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Panagakis Y.; Kotropoulos C.; Arce G.R.","Panagakis, Yannis (35503932300); Kotropoulos, Constantine (35563688200); Arce, Gonzalo R. (7006653894)","35503932300; 35563688200; 7006653894","Sparse multi-label linear embedding within nonnegative tensor factorization applied to music tagging","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578446&partnerID=40&md5=5be9d56d821d5970668bfe8992c500ea","Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Dept. of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716-3130, United States","Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716-3130, United States","A novel framework for music tagging is proposed. First, each music recording is represented by bio-inspired auditory temporal modulations. Then, a multilinear subspace learning algorithm based on sparse label coding is developed to effectively harness the multi-label information for dimensionality reduction. The proposed algorithm is referred to as Sparse Multi-label Linear Embedding Non- negative Tensor Factorization, whose convergence to a stationary point is guaranteed. Finally, a recently proposed method is employed to propagate the multiple labels of training auditory temporal modulations to auditory temporal modulations extracted from a test music recording by means of the sparse ℓ1 reconstruction coefficients. The overall framework, that is described here, outperforms both humans and state-of-the-art computer audition systems in the music tagging task, when applied to the CAL500 dataset. © 2010 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Learning algorithms; Modulation; Tensors; Bio-inspired; Computer audition; Dimensionality reduction; Linear embedding; Multi-label; Multiple labels; Music recording; Nonnegative tensor factorizations; Stationary points; Temporal modulations; Tensor factorization; Test music; Factorization","Y. Panagakis; Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, GR-54124, Box 451, Greece; email: panagakis@aiia.csd.auth.gr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Tjoa S.K.; Ray Liu K.J.","Tjoa, Steven K. (18042682600); Ray Liu, K.J. (7404200118)","18042682600; 7404200118","Musical instrument recognition using biologically inspired filtering of temporal dictionary atoms","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588006&partnerID=40&md5=840bcb81b55d2e1df21b7b75c791b48b","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States","Tjoa S.K., Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States; Ray Liu K.J., Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States","Most musical instrument recognition systems rely entirely upon spectral information instead of temporal information. In this paper, we test the hypothesis that temporal information can improve upon the accuracy achievable by the state of the art in instrument recognition. Unlike existing temporal classification methods which use traditional features such as temporal moments, we extract novel features from temporal atoms generated by nonnegative matrix factorization by using a multiresolution gamma filterbank. Among isolated sounds taken from twenty-four instrument classes, the proposed system can achieve 92.3% accuracy, thus improving upon the state of the art. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Biologically inspired; Instrument recognition; Multi-resolutions; Musical instrument recognition; Nonnegative matrix factorization; Spectral information; State of the art; Temporal classification; Temporal information; Temporal moments; Musical instruments","S.K. Tjoa; Department of Electrical and Computer Engineering, University of Maryland, College Park, MD 20742, United States; email: kiemyang@umd.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hockman J.A.; Fujinaga I.","Hockman, Jason A. (36730968100); Fujinaga, Ichiro (9038140900)","36730968100; 9038140900","Fast vs slow: Learning tempo octaves from user data","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871379653&partnerID=40&md5=b6ba5a1b58213f6e39f8e5510c940d1a","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada","Hockman J.A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada","The widespread use of beat- and tempo-tracking methods in music information retrieval tasks has been marginalized due to undesirable sporadic results from these algorithms. While sensorimotor and listening studies have demonstrated the subjectivity and variability inherent to human performance of this task, MIR applications such as recommendation require more reliable output than available from present tempo estimation models. In this paper, we present a initial investigation of tempo assessment based on the simple classification of whether the music is fast or slow. Through three experiments, we provide performance results of our method across two datasets, and demonstrate its usefulness in the pursuit of a reliable global tempo estimation. © 2010 International Society for Music Information Retrieval.","","Human performance; Music information retrieval; Tempo estimations; User data; Information retrieval","J.A. Hockman; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; email: jason.hockman@mail.mcgill.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Eyben F.; Böck S.; Schuller B.; Graves A.","Eyben, Florian (18041887200); Böck, Sebastian (55413719000); Schuller, Björn (6603767415); Graves, Alex (56273511600)","18041887200; 55413719000; 6603767415; 56273511600","Universal onset detection with bidirectional long short-term memory neural networks","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","117","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863771261&partnerID=40&md5=07fb54d73b1e6a588270ae8c99956535","Institute for Human-Machine Communication, Technische Universität München, Germany; Institute for Computer Science VI, Technische Universität München, Germany","Eyben F., Institute for Human-Machine Communication, Technische Universität München, Germany; Böck S., Institute for Human-Machine Communication, Technische Universität München, Germany; Schuller B., Institute for Human-Machine Communication, Technische Universität München, Germany; Graves A., Institute for Computer Science VI, Technische Universität München, Germany","Many different onset detection methods have been proposed in recent years. However those that perform well tend to be highly specialised for certain types of music, while those that are more widely applicable give only moderate performance. In this paper we present a new onset detector with superior performance and temporal precision for all kinds of music, including complex music mixes. It is based on auditory spectral features and relative spectral differences processed by a bidirectional Long Short-Term Memory recurrent neural network, which acts as reduction function. The network is trained with a large database of onset data covering various genres and onset types. Due to the data driven nature, our approach does not require the onset detection method and its parameters to be tuned to a particular type of music. We compare results on the Bello onset data set and can conclude that our approach is on par with related results on the same set and outperforms them in most cases in terms of F1-measure. For complex music with mixed onset types, an absolute improvement of 3.6% is reported. © 2010 International Society for Music Information Retrieval.","","Digital storage; Information retrieval; Recurrent neural networks; Data driven; Data set; Large database; Long short-term memories; Onset detection; Reduction function; Spectral differences; Spectral feature; Temporal precision; Brain","F. Eyben; Institute for Human-Machine Communication, Technische Universität München, Germany; email: eyben@tum.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Smith L.M.","Smith, Leigh M. (35175328500)","35175328500","Beat critic: Beat tracking octave error identification by metrical profile analysis","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585095&partnerID=40&md5=7893a35264431d96e4dec4512bcc85cc","IRCAM, France","Smith L.M., IRCAM, France","Computational models of beat tracking of musical audio have been well explored, however, such systems often make ""octave errors"", identifying the beat period at double or half the beat rate than that actually recorded in the music. A method is described to detect if octave errors have occurred in beat tracking. Following an initial beat tracking estimation, a feature vector of metrical profile separated by spectral subbands is computed. A measure of subbeat quaver (1/8th note) alternation is used to compare half time and double time measures against the initial beat track estimation and indicate a likely octave error. This error estimate can then be used to re-estimate the beat rate. The performance of the approach is evaluated against the RWC database, showing successful identification of octave errors for an existing beat tracker. Using the octave error detector together with the existing beat tracking model improved beat tracking by reducing octave errors to 43% of the previous error rate. © 2010 International Society for Music Information Retrieval.","","Audio acoustics; Audio systems; Estimation; Information retrieval; Beat rates; Beat tracking; Computational model; Error detectors; Error estimates; Error identification; Error rate; Feature vectors; Musical audio; Profile analysis; Subbands; Track estimation; Errors","L.M. Smith; IRCAM, France; email: leigh.smith@ircam.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Karydis I.; Radovanović M.; Nanopoulos A.; Ivanović M.","Karydis, Ioannis (36950839300); Radovanović, Milos (14626806500); Nanopoulos, Alexandros (6603555418); Ivanović, Mirjana (7005907326)","36950839300; 14626806500; 6603555418; 7005907326","Looking through the ""Glass ceiling"": A conceptual framework for the problems of spectral similarity","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649416079&partnerID=40&md5=0c3aae790be541df9424c9106bdb2a36","Dept. of Informatics, Ionian University, Greece; Faculty of Science, University of Novi Sad, Serbia; Inst. of Computer Science, University of Hildesheim, Germany","Karydis I., Dept. of Informatics, Ionian University, Greece; Radovanović M., Faculty of Science, University of Novi Sad, Serbia; Nanopoulos A., Inst. of Computer Science, University of Hildesheim, Germany; Ivanović M., Faculty of Science, University of Novi Sad, Serbia","Spectral similarity measures have been shown to exhibit good performance in several Music Information Retrieval (MIR) applications. They are also known, however, to possess several undesirable properties, namely allowing the existence of hub songs (songs which frequently appear in nearest neighbor lists of other songs), ""orphans"" (songs which practically never appear), and difficulties in distinguishing the farthest from the nearest neighbor due to the concentration effect caused by high dimensionality of data space. In this paper we develop a conceptual framework that allows connecting all three undesired properties. We show that hubs and ""orphans"" are expected to appear in high-dimensional data spaces, and relate the cause of their appearance with the concentration property of distance / similarity measures. We verify our conclusions on realmusic data, examining groups of frames generated by Gaussian Mixture Models (GMMs), considering two similarity measures: Earth Mover's Distance (EMD) in combination with Kullback-Leibler (KL) divergence, and Monte Carlo (MC) sampling. The proposed framework can be useful to MIR researchers to address problems of spectral similarity, understand their fundamental origins, and thus be able to develop more robust methods for their remedy. © 2010 International Society for Music Information Retrieval.","","Concentration effects; Concentration properties; Conceptual frameworks; Data space; Earth Mover's distance; Gaussian mixture model (GMMs); Glass ceiling; Groups of frames; High dimensionality; High-dimensional data space; Kullback-Leibler divergence; MONTE CARLO; Music information retrieval; Nearest neighbors; Robust methods; Similarity measure; Spectral similarity; Information retrieval","I. Karydis; Dept. of Informatics, Ionian University, Greece; email: karydis@ionio.gr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hu Y.; Chen X.; Yang D.","Hu, Yajie (57221204354); Chen, Xiaoou (11639519200); Yang, Deshun (24485594500)","57221204354; 11639519200; 24485594500","Lyric-based song emotion detection with affective lexicon and fuzzy clustering method","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","95","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873686728&partnerID=40&md5=f27a7a9d51a1be05c45a540a8244c489","Institute of Computer Science and Technology, Peking University, China","Hu Y., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China","A method is proposed for detecting the emotions of Chinese song lyrics based on an affective lexicon. The lexicon is composed of words translated from ANEW and words selected by other means. For each lyric sentence, emotion units, each based on an emotion word in the lexicon, are found out, and the influences of modifiers and tenses on emotion units are taken into consideration. The emotion of a sentence is calculated from its emotion units. To figure out the prominent emotions of a lyric, a fuzzy clustering method is used to group the lyric's sentences according to their emotions. The emotion of a cluster is worked out from that of its sentences considering the individual weight of each sentence. Clusters are weighted according to the weights and confidences of their sentences and singing speeds of sentences are considered as the adjustment of the weights of clusters. Finally, the emotion of the cluster with the highest weight is selected from the prominent emotions as the main emotion of the lyric. The performance of our approach is evaluated through an experiment of emotion classification of 500 Chinese song lyrics. © 2009 International Society for Music Information Retrieval.","","Fuzzy systems; Information retrieval; Emotion classification; Emotion detection; Fuzzy clustering method; Fuzzy clustering","Y. Hu; Institute of Computer Science and Technology, Peking University, China; email: huyajie@icst.pku.edu.cn","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Chordia P.; Sastry A.; Malikarjuna T.; Albin A.","Chordia, Parag (24723695700); Sastry, Avinash (57196872874); Malikarjuna, Trishul (27867842700); Albin, Aaron (36719824000)","24723695700; 57196872874; 27867842700; 36719824000","Multiple viewpoints modeling of tabla sequences","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960496938&partnerID=40&md5=41743e0f2482ce6801bb6ea58ca67f13","Center for Music Technology, Georgia Tech, Atlanta, GA, United States","Chordia P., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Sastry A., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Malikarjuna T., Center for Music Technology, Georgia Tech, Atlanta, GA, United States; Albin A., Center for Music Technology, Georgia Tech, Atlanta, GA, United States","We describe a system that attempts to predict the continuation of a symbolically encoded tabla composition at each time step using a variable-length n-gram model. Using cross-entropy as a measure of model fit, the best model attained an entropy rate of 0.780 in a cross-validation experiment, showing that symbolic tabla compositions can be effectively encoded using such a model. The choice of smoothing algorithm, which determines how information from different-order models is combined, is found to be an important factor in the models performance. We extend the basic n-gram model by adding viewpoints, other streams of information that can be used to improve predictive performance. First, we show that adding a short-term model, built on the current composition and not the entire corpus, leads to substantial improvements. Additional experiments were conducted with derived types, representations derived from the basic data type (stroke names), and cross-types, which model dependencies between parameters, such as duration and stroke name. For this database, such extensions improved performance only marginally, although this may have been due to the low entropy rate attained by the basic model. © 2010 International Society for Music Information Retrieval.","","Computer aided language translation; Experiments; Signal theory; Additional experiments; Basic models; Best model; Cross entropy; Cross validation; Data type; Entropy rates; Low entropy; Model dependencies; Model fit; Multiple viewpoints; N-gram models; Predictive performance; Smoothing algorithms; Time step; Information retrieval","P. Chordia; Center for Music Technology, Georgia Tech, Atlanta, GA, United States; email: ppc@gatech.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Vincent E.; Raczynski S.A.; Ono N.; Sagayama S.","Vincent, Emmanuel (14010158800); Raczynski, Stanisław A. (57190605458); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","14010158800; 57190605458; 7202472899; 7004859104","A roadmap towards versatile MIR","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582945&partnerID=40&md5=a3fc4e8ca6409b25557289e34a19394e","INRIA, France; University of Tokyo, Japan","Vincent E., INRIA, France; Raczynski S.A., University of Tokyo, Japan; Ono N., University of Tokyo, Japan; Sagayama S., University of Tokyo, Japan","Most MIR systems are specifically designed for one application and one cultural context and suffer from the semantic gap between the data and the application. Advances in the theory of Bayesian language and information processing enable the vision of a versatile, meaningful and accurate MIR system integrating all levels of information. We propose a roadmap to collectively achieve this vision. © 2010 International Society for Music Information Retrieval.","","Data processing; Semantics; Bayesian; Cultural context; Roadmap; Semantic gap; Information retrieval","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Marolt M.; Lefeber M.","Marolt, Matija (6603601816); Lefeber, Marieke (55240494500)","6603601816; 55240494500","It's time for a song - Transcribing recordings of bell-playing clocks","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856146718&partnerID=40&md5=d66e904a10763c7726d2ace9fc7a5228","Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Meertens Instituut and Museum Speelklok, Netherlands","Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Lefeber M., Meertens Instituut and Museum Speelklok, Netherlands","The paper presents an algorithm for automatic transcription of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing mechanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probability of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks. © 2010 International Society for Music Information Retrieval.","","Algorithms; Audio recordings; Clocks; Information retrieval; Transcription; Acoustical properties; Automatic transcription; Joint probability; Probabilistic methods; Bells","M. Marolt; Faculty of Computer and Information Science, University of Ljubljana, Slovenia; email: matija.marolt@fri.uni-lj.si","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Ahonen T.E.","Ahonen, Teppo E. (36719987900)","36719987900","Combining chroma features for cover version identification","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864189055&partnerID=40&md5=d9a6bce3427d2862794a63a040f78bd3","Department of Computer Science, University of Helsinki, Finland","Ahonen T.E., Department of Computer Science, University of Helsinki, Finland","We present an approach for cover version identification which is based on combining different discretized features derived from the chromagram vectors extracted from the audio data. For measuring similarity between features, we use a parameter-free quasi-universal similarity metric which utilizes data compression. Evaluation proves that combined feature distances increase the accuracy in cover version identification. © 2010 International Society for Music Information Retrieval.","","Data compression; Audio data; Chroma features; Combined features; Measuring similarities; Similarity metrics; Version identification; Information retrieval","T.E. Ahonen; Department of Computer Science, University of Helsinki, Finland; email: teahonen@cs.helsinki.fi","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hoffman M.D.; Blei D.M.; Cook P.R.","Hoffman, Matthew D. (17434675700); Blei, David M. (55914504500); Cook, Perry R. (57203105418)","17434675700; 55914504500; 57203105418","Easy as CBA: A simple probabilistic model for tagging music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","65","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873687590&partnerID=40&md5=779bb731f413d1d72461bba87cf488dc","Dept. of Computer Science, Princeton University, United States","Hoffman M.D., Dept. of Computer Science, Princeton University, United States; Blei D.M., Dept. of Computer Science, Princeton University, United States; Cook P.R., Dept. of Computer Science, Princeton University, United States","Many songs in large music databases are not labeled with semantic tags that could help users sort out the songs they want to listen to from those they do not. If the words that apply to a song can be predicted from audio, then those predictions can be used both to automatically annotate a song with tags, allowing users to get a sense of what qualities characterize a song at a glance. Automatic tag prediction can also drive retrieval by allowing users to search for the songs most strongly characterized by a particular word. We present a probabilistic model that learns to predict the probability that a word applies to a song from audio. Our model is simple to implement, fast to train, predicts tags for new songs quickly, and achieves state-of-the-art performance on annotation and retrieval tasks. © 2009 International Society for Music Information Retrieval.","","Music database; Probabilistic models; Semantic tags; State-of-the-art performance; Information retrieval","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Schuller B.; Kozielski C.; Weninger F.; Eyben F.; Rigoll G.","Schuller, Björn (6603767415); Kozielski, Christoph (55585847300); Weninger, Felix (13610240900); Eyben, Florian (18041887200); Rigoll, Gerhard (7004571019)","6603767415; 55585847300; 13610240900; 18041887200; 7004571019","Vocalist gender recognition in recorded popular music","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585840&partnerID=40&md5=7caae6cc732a62b0aed92ca845fbcdca","Institute for Human-Machine Communication, Technische Universität München, Munich, Germany","Schuller B., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Kozielski C., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Weninger F., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Eyben F., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; Rigoll G., Institute for Human-Machine Communication, Technische Universität München, Munich, Germany","We introduce the task of vocalist gender recognition in popular music and evaluate the benefit of Non-Negative Matrix Factorization based enhancement of melodic components to this aim. The underlying automatic separation of drum beats is described in detail, and the obtained significant gain by its use is verified in extensive test-runs on a novel database of 1.5 days of MP3 coded popular songs based on transcriptions of the Karaoke-game UltraStar. As classifiers serve Support Vector Machines and Hidden Naive Bayes. Overall, the suggested methods lead to fully automatic recognition of the pre-dominant vocalist gender at 87.31% accuracy on song level for artists unkown to the system in originally recorded music. © 2010 International Society for Music Information Retrieval.","","Automatic recognition; Automatic separations; Gender recognition; Naive bayes; Nonnegative matrix factorization; Popular music; Popular song; Information retrieval","B. Schuller; Institute for Human-Machine Communication, Technische Universität München, Munich, Germany; email: schuller@tum.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Koenigstein N.; Lanckriet G.; McFee B.; Shavitt Y.","Koenigstein, Noam (26534475900); Lanckriet, Gert (7801431767); McFee, Brian (34875379700); Shavitt, Yuval (7004130839)","26534475900; 7801431767; 34875379700; 7004130839","Collaborative filtering based on P2P networks","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588717&partnerID=40&md5=d30f716fb1c91aff41c78716d4e50db7","School of Electrical Engineering, Tel Aviv University, Israel; Department of Electrical and Computer Engineering, University of California, San Diego, United States; Department of Computer Science and Engineering, University of California, San Diego, United States","Koenigstein N., School of Electrical Engineering, Tel Aviv University, Israel; Lanckriet G., Department of Electrical and Computer Engineering, University of California, San Diego, United States; McFee B., Department of Computer Science and Engineering, University of California, San Diego, United States; Shavitt Y., School of Electrical Engineering, Tel Aviv University, Israel","Peer-to-Peer (P2P) networks are used by millions of people for sharing music files. As these networks become ever more popular, they also serve as an excellent source for Music Information Retrieval (MIR) tasks. This paper reviews the latest MIR studies based on P2P data-sets, and presents a new file sharing data collection system over the Gnutella. We discuss several advantages of P2P based data-sets over some of the more ""traditional"" data sources, and evaluate the information quality of our data-set in comparison to other data sources (Last.fm, social tags, biography data, and MFCCs). The evaluation is based on an artists similarity task using Partial Order Embedding (POE). We show that a P2P based Collaborative Filtering dataset performs at least as well as ""traditional"" data-sets, yet maintains some inherent advantages such as scale, availability and additional information features such as ID3 tags and geographical location. © 2010 International Society for Music Information Retrieval.","","Collaborative filtering; Distributed computer systems; Information retrieval; Data collection system; Data-sources; File Sharing; Geographical locations; Gnutella; Information feature; Information quality; Last.fm; Music files; Music information retrieval; P2P network; P2P-based; Partial order; Peer to Peer (P2P) network; Social Tags; Peer to peer networks","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Lu Q.; Chen X.; Yang D.; Wang J.","Lu, Qi (55585884300); Chen, Xiaoou (11639519200); Yang, Deshun (24485594500); Wang, Jun (57200017808)","55585884300; 11639519200; 24485594500; 57200017808","Boosting for multi-modal music emotion classification","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871292332&partnerID=40&md5=9d2064fd7d757fbdd60b1edf3d8fb4d4","Institute of Computer Science and Technology, Peking University, China","Lu Q., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Yang D., Institute of Computer Science and Technology, Peking University, China; Wang J., Institute of Computer Science and Technology, Peking University, China","With the explosive growth of music recordings, automatic classification of music emotion becomes one of the hot spots on research and engineering. Typical music emotion classification (MEC) approaches apply machine learning methods to train a classifier based on audio features. In addition to audio features, the MIDI and lyrics features of music also contain useful semantic information for predicting the emotion of music. In this paper we apply AdaBoost algorithm to integrate MIDI, audio and lyrics information and propose a two-layer classifying strategy called Fusion by Subtask Merging for 4-class music emotion classification. We evaluate each modality respectively using SVM, and then combine any two of the three modalities, using AdaBoost algorithm (MIDI+audio, MIDI+lyrics, audio+lyrics). Moreover, integrating this in a multimodal system (MIDI+audio+lyrics) allows an improvement in the overall performance. The experimental results show that MIDI, audio and lyrics information are complementary, and can be combined to improve a classification system. © 2010 International Society for Music Information Retrieval.","Adaboost; Fusion by subtask merging; Multi- modal; Music emotion classification","Adaptive boosting; Algorithms; Classification (of information); Information retrieval; Learning systems; Merging; AdaBoost algorithm; Audio features; Automatic classification; Classification system; Explosive growth; Hot spot; IMPROVE-A; Machine learning methods; Multi- modal; Multimodal system; Music emotion classifications; Music emotions; Music recording; Semantic information; Subtask; Two-layer; Audio acoustics","Q. Lu; Institute of Computer Science and Technology, Peking University, China; email: luqi@icst.pku.edu.cn","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Chuan C.-H.; Chew E.","Chuan, Ching-Hua (15050129500); Chew, Elaine (8706714000)","15050129500; 8706714000","Quantifying the benefits of using an interactive decision support tool for creating musical accompaniment in a particular style","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856418678&partnerID=40&md5=d060598af74b7851056b8d4f074a07f6","School of Computing, University of North Florida, Jacksonville, FL, United States; Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States","Chuan C.-H., School of Computing, University of North Florida, Jacksonville, FL, United States; Chew E., Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States","We present a human-centered experiment designed to measure the degree of support for creating musical accompaniment provided by an interactive composition decision- support system. We create an interactive system with visual and audio cues to assist users in the choosing of chords to craft an accompaniment in a desired style. We propose general measures for objectively evaluating the effectiveness and usability of such systems. We use melodies of existing songs by Radiohead as tests. Quantitative measures of musical distance - percentage correct and closely related chords, and average neo-Riemannian distance - compare the user-created accompaniment with the original, with and without decision support. Numbers of backward edits, unique chords explored, and repeated chord choices during composition help quantify composition behavior. We present experimental data from musicians and non-musicians. We observe that decision support reduces the time spent in composition, the number of revisions of earlier choices, redundant behavior such as repeated chord choices, and the gap between musicians' and non-musicians' work, without significantly limiting the range of users' choices. © 2010 International Society for Music Information Retrieval.","","Human computer interaction; Information retrieval; Audio cues; Decision support tools; Decision supports; Degree of support; Experimental datum; In compositions; Interactive composition; Interactive system; Quantitative measures; Support systems; Time spent; Decision support systems","C.-H. Chuan; School of Computing, University of North Florida, Jacksonville, FL, United States; email: chchuan@mail.barry.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Niedermayer B.; Widmer G.","Niedermayer, Bernhard (37113938000); Widmer, Gerhard (7004342843)","37113938000; 7004342843","A multi-pass algorithm for accurate audio-to-score alignment","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051657095&partnerID=40&md5=28db7a4271388884469a185116f2cfc4","Department for Computational Perception, Johannes Kepler University Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Niedermayer B., Department for Computational Perception, Johannes Kepler University Linz, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Most current audio-to-score alignment algorithms work on the level of score time frames; i.e., they cannot differentiate between several notes occurring at the same discrete time within the score. This level of accuracy is sufficient for a variety of applications. However, for those that deal with, for example, musical expression analysis such microtimings might also be of interest. Therefore, we propose a method that estimates the onset times of individual notes in a post-processing step. Based on the initial alignment and a feature obtained by matrix factorization, those notes for which the confidence in the alignment is high are chosen as anchor notes. The remaining notes in between are revised, taking into account the additional information about these anchors and the temporal relations given by the score. We show that this method clearly outperforms a reference method that uses the same features but does not differentiate between anchor and non-anchor notes. © 2010 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Alignment algorithms; Discrete time; Initial alignment; Matrix factorizations; Multi-pass; Musical expression; Post processing; Reference method; Temporal relation; Time frame; Alignment","B. Niedermayer; Department for Computational Perception, Johannes Kepler University Linz, Austria; email: music@jku.at","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Flexer A.; Schnitzer D.; Gasser M.; Pohle T.","Flexer, Arthur (7004555682); Schnitzer, Dominik (23996271700); Gasser, Martin (18037419600); Pohle, Tim (14036302300)","7004555682; 23996271700; 18037419600; 14036302300","Combining features reduces hubness in audio similarity","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960546359&partnerID=40&md5=5f2e668cb465d1f455beb234b11ea5a2","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In audio based music similarity, a well known effect is the existence of hubs, i.e. songs which appear similar to many other songs without showing any meaningful perceptual similarity. We verify that this effect also exists in very large databases (> 250000 songs) and that it even gets worse with growing size of databases. By combining different aspects of audio similarity we are able to reduce the hub problem while at the same time maintaining a high overall quality of audio similarity. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Audio similarities; Audio-based; Hubness; Music similarity; Overall quality; Perceptual similarity; Very large database; Audio acoustics","A. Flexer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: arthur.flexer@ofai.at","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Schnitzer D.; Flexer A.; Widmer G.; Gasser M.","Schnitzer, Dominik (23996271700); Flexer, Arthur (7004555682); Widmer, Gerhard (7004342843); Gasser, Martin (18037419600)","23996271700; 7004555682; 7004342843; 18037419600","Islands of gaussians: The self organizing map and gaussian music similarity features","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155154171&partnerID=40&md5=30aff0ff52751e7f7eadfd7a35366f09","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Multivariate Gaussians are of special interest in the MIR field of automatic music recommendation. They are used as the de facto standard representation of music timbre to compute music similarity. However, standard algorithms for clustering and visualization are usually not designed to handle Gaussian distributions and their attached metrics (e.g. the Kullback-Leibler divergence). Hence to use these features the algorithms generally handle them indirectly by first mapping them to a vector space, for example by deriving a feature vector representation from a similarity matrix. This paper uses the symmetrized Kullback-Leibler centroid of Gaussians to show how to avoid the vectorization detour for the Self Organizing Maps (SOM) data visualization algorithm. We propose an approach so that the algorithm can directly and naturally work on Gaussian music similarity features to compute maps of music collections. We show that by using our approach we can create SOMs which (1) better preserve the original similarity topology and (2) are far less complex to compute, as the often costly vectorization step is eliminated. © 2010 International Society for Music Information Retrieval.","","Data visualization; Information retrieval; Self organizing maps; De facto standard; Feature vectors; Gaussians; Kullback Leibler divergence; Kullback-Leibler; Music collection; Music recommendation; Music similarity; Self-organizing map (SOM); Similarity matrix; Standard algorithms; Vectorization; Visualization algorithms; Clustering algorithms","D. Schnitzer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: dominik.schnitzer@ofai.at","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Mandel M.I.; Eck D.; Bengio Y.","Mandel, Michael I (14060460000); Eck, Douglas (12141444300); Bengio, Yoshua (7003958245)","14060460000; 12141444300; 7003958245","Learning tags that vary within a song","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053156439&partnerID=40&md5=36b11421038c43f7342798b5caf267ea","LISA Lab, Université de Montréal, Canada","Mandel M.I., LISA Lab, Université de Montréal, Canada; Eck D., LISA Lab, Université de Montréal, Canada; Bengio Y., LISA Lab, Université de Montréal, Canada","This paper examines the relationship between human generated tags describing different parts of the same song. These tags were collected using Amazon's Mechanical Turk service. We find that the agreement between different people's tags decreases as the distance between the parts of a song that they heard increases. To model these tags and these relationships, we describe a conditional restricted Boltzmann machine. Using this model to fill in tags that should probably be present given a context of other tags, we train automatic tag classifiers (autotaggers) that outperform those trained on the original data. © 2010 International Society for Music Information Retrieval.","","Amazon's mechanical turks; Conditional restricted boltzmann machines; Information retrieval","M.I. Mandel; LISA Lab, Université de Montréal, Canada; email: mandelm@iro.umontreal.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Konz V.; Müller M.; Ewert S.","Konz, Verena (36459884900); Müller, Meinard (7404689873); Ewert, Sebastian (32667575400)","36459884900; 7404689873; 32667575400","A multi-perspective evaluation framework for chord recognition","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053003979&partnerID=40&md5=14247cca58c735e69a2a4eb6a7b5cf56","MPI Informatik, Saarland University, Germany; Department of Computer Science III, University of Bonn, Germany","Konz V., MPI Informatik, Saarland University, Germany; Müller M., MPI Informatik, Saarland University, Germany; Ewert S., Department of Computer Science III, University of Bonn, Germany","The automated extraction of chord labels from audio recordings constitutes a major task in music information retrieval. To evaluate computer-based chord labeling procedures, one requires ground truth annotations for the underlying audio material. However, the manual generation of such annotations on the basis of audio recordings is tedious and time-consuming. On the other hand, trained musicians can easily derive chord labels from symbolic score data. In this paper, we bridge this gap by describing a procedure that allows for transferring annotations and chord labels from the score domain to the audio domain and vice versa. Using music synchronization techniques, the general idea is to locally warp the annotations of all given data streams onto a common time axis, which then allows for a cross-domain evaluation of the various types of chord labels. As a further contribution of this paper, we extend this principle by introducing amulti-perspective evaluation framework for simultaneously comparing chord recognition results over multiple performances of the same piece of music. The revealed inconsistencies in the results do not only indicate limitations of the employed chord labeling strategies but also deepen the understanding of the underlying music material. © 2010 International Society for Music Information Retrieval.","","Audio recordings; Automated extraction; Chord recognition; Cross-domain evaluations; Data stream; Evaluation framework; Ground truth; Labeling procedures; Labeling strategy; Multi-perspective; Music information retrieval; Music materials; Music synchronizations; Time axis; Information retrieval","V. Konz; MPI Informatik, Saarland University, Germany; email: vkonz@mpi-inf.mpg.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Le Coz M.; Lachambre H.; Koenig L.; Andre-Obrecht R.","Le Coz, Maxime (55365538400); Lachambre, Helene (35105371800); Koenig, Lionel (55311568900); Andre-Obrecht, Regine (6603620564)","55365538400; 35105371800; 55311568900; 6603620564","A segmentation-based tempo induction method","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866673349&partnerID=40&md5=3b916e6e8d5c87447475a8882853b962","IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France","Le Coz M., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Lachambre H., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Koenig L., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; Andre-Obrecht R., IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France","The automatized beat detection and localization have been the subject of multiple research in the field of music information retrieval. Most of the methods are based on onset detection. We propose an alternative approach: Our method is based on the ""Forward-Backward segmentation"": the segments may be interpreted as attacks, decays, sustains and releases of notes. We process the segment boundaries as a weighted Dirac signal. Three methods devived from its spectral analysis are proposed to find a periodicity which corresponds to the tempo. The experiments are carried out on a corpus of 100 songs of the RWC database. The performances of our system on this base demonstrate a potential in the use of a "" Forward- Backward Segmentation"" for temporal information retrieval in musical signals. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Spectrum analysis; Alternative approach; Beat detection; Induction method; Multiple research; Music information retrieval; Musical signals; Onset detection; Temporal information retrievals; Signal detection","M. Le Coz; IRIT, Universite Paul Sabatier, F-31062 Toulouse cedex 9, 118 Route de Narbonne, France; email: lecoz@irit.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Ramirez C.; Ohya J.","Ramirez, Carolina (37120884000); Ohya, Jun (56271441000)","37120884000; 56271441000","Symbol classification approach for OMR of square notation manuscripts","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955723424&partnerID=40&md5=f53882bd3148408e8641415072b43c70","Waseda University, Japan","Ramirez C., Waseda University, Japan; Ohya J., Waseda University, Japan","Researchers in the field of OMR (Optical Music Recognition) have acknowledged that the automatic transcription of medieval musical manuscripts is still an open problem [2, 3], mainly due to lack of standards in notation and the physical quality of the documents. Nonetheless, the amount of medieval musical manuscripts is so vast that the consensus seems to be that OMR can be a vital tool to help in the preserving and sharing of this information in digital format. In this paper we report our results on a preliminary approach to OMR of medieval plainchant manuscripts in square notation, at the symbol classification level, which produced good results in the recognition of eight basic symbols. Our preliminary approach consists of the preprocessing, segmentation, and classification stages. © 2010 International Society for Music Information Retrieval.","","Automatic transcription; Basic symbols; Classification approach; Digital format; Optical music recognition; Physical quality; Preliminary approach; Information retrieval","C. Ramirez; Waseda University, Japan; email: ramirez@akane.waseda.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Barrington L.; Oda R.; Lanckriet G.","Barrington, Luke (14041197300); Oda, Reid (58289946000); Lanckriet, Gert (7801431767)","14041197300; 58289946000; 7801431767","Smarter than genius? Human evaluation of music recommender systems","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","91","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873683421&partnerID=40&md5=4aced3b02495612b8e0c8d87d5dcdb00","Electrical and Computer Engineering, University of California, San Diego, United States; Cognitive Science, University of California, San Diego, United States","Barrington L., Electrical and Computer Engineering, University of California, San Diego, United States; Oda R., Cognitive Science, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","Genius is a popular commercial music recommender system that is based on collaborative filtering of huge amounts of user data. To understand the aspects of music similarity that collaborative filtering can capture, we compare Genius to two canonical music recommender systems: one based purely on artist similarity, the other purely on similarity of acoustic content. We evaluate this comparison with a user study of 185 subjects. Overall, Genius produces the best recommendations. We demonstrate that collaborative filtering can actually capture similarities between the acoustic content of songs. However, when evaluators can see the names of the recommended songs and artists, we find that artist similarity can account for the performance of Genius. A system that combines these musical cues could generate music recommendations that are as good as Genius, even when collaborative filtering data is unavailable. © 2009 International Society for Music Information Retrieval.","","Collaborative filtering; Artist similarities; Human evaluation; Music recommendation; Music recommender systems; Music similarity; User data; User study; Recommender systems","L. Barrington; Electrical and Computer Engineering, University of California, San Diego, United States; email: lukeinusa@gmail.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Vigliensoni G.; McKay C.; Fujinaga I.","Vigliensoni, Gabriel (55217696900); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","55217696900; 14033215600; 9038140900","Using jWEBminer 2.0 to improve music classification performance by combining different types of features mined from the web","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871992019&partnerID=40&md5=294f910557cdd760d33efdfd400e784b","CIRMMT, McGill University, Canada","Vigliensoni G., CIRMMT, McGill University, Canada; McKay C., CIRMMT, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper presents the jWebMiner 2.0 cultural feature extraction software and describes the results of several musical genre classification experiments performed with it. jWebMiner 2.0 is an easy-to-use and open-source tool that allows users to mine the Internet in order to extract features based on both Last.fm social tags and general web search string co-occurrences extracted using the Yahoo! API. The experiments performed found that the features based on social tags were more effective at classifying music into a small (5-genre) genre ontology, but the features based on general web co-occurrences were more effective at classifying a moderate (10-genre) ontology. It was also found that combining the two types of features resulted in improved performance overall. © 2010 International Society for Music Information Retrieval.","","Experiments; Feature extraction; Last.fm; Music classification; Musical genre classification; Open source tools; Social Tags; Web searches; Information retrieval","G. Vigliensoni; CIRMMT, McGill University, Canada; email: gabriel@music.mcgill.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Pinto A.","Pinto, Alberto (55937244900)","55937244900","Eigenvector-based relational motif discovery","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576440&partnerID=40&md5=1aab043e6e702544f9269efa5b359bd3","Dipartimento di Informatica E Comunicazione, Universitá degli Studi di Milano, I-20135 Milano, Via Comelico 39/41, Italy","Pinto A., Dipartimento di Informatica E Comunicazione, Universitá degli Studi di Milano, I-20135 Milano, Via Comelico 39/41, Italy","The development of novel analytical tools to investigate the structure of music works is central in current music information retrieval research. In particular, music summarization aims at finding the most representative parts of a music piece (motifs) that can be exploited for an efficient music database indexing system. Here we present a novel approach for motif discovery in music pieces based on an eigenvector method. Scores are segmented into a network of bars and then ranked depending on their centrality. Bars with higher centrality are more likely to be relevant for music summarization. Results on the corpus of J.S.Bach's 2-part Inventions demonstrate the effectiveness of the method and suggest that different musical metrics might be more suitable than others for different applications. © 2010 International Society for Music Information Retrieval.","","Analytical tool; Eigenvector methods; Motif discovery; Music database; Music information retrieval; Music summarization; Information retrieval","A. Pinto; Dipartimento di Informatica E Comunicazione, Universitá degli Studi di Milano, I-20135 Milano, Via Comelico 39/41, Italy; email: pinto@dico.unimi.it","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Van Zaanen M.; Kanters P.","Van Zaanen, Menno (6507482370); Kanters, Pieter (55586401700)","6507482370; 55586401700","Automatic mood classification using TF*IDF based on lyrics","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","62","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862936491&partnerID=40&md5=2b417c4854be9f4d41c1d88ce12a118a","Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands","Van Zaanen M., Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands; Kanters P., Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands","This paper presents the outcomes of research into using lingual parts of music in an automatic mood classification system. Using a collection of lyrics and corresponding user-tagged moods, we build classifiers that classify lyrics of songs into moods. By comparing the performance of different mood frameworks (or dimensions), we examine to what extent the linguistic part of music reveals adequate information for assigning a mood category and which aspects of mood can be classified best. Our results show that word oriented metrics provide a valuable source of information for automatic mood classification of music, based on lyrics only. Metrics such as term frequencies and tf*idf values are used to measure relevance of words to the different mood classes. These metrics are incorporated in a machine learning classifier setup. Different partitions of the mood plane are investigated and we show that there is no large difference in mood prediction based on the mood division. Predictions on the valence, tension and combinations of aspects lead to similar performance. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Learning systems; Classification system; Learning classifiers; Prediction-based; Term Frequency; Classification (of information)","M. Van Zaanen; Tilburg Center for Cognition and Communication, Tilburg University, Tilburg, Netherlands; email: mvzaanen@uvt.nl","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Paulus J.; Müller M.; Klapuri A.","Paulus, Jouni (16068963200); Müller, Meinard (7404689873); Klapuri, Anssi (6602945099)","16068963200; 7404689873; 6602945099","Audio-based music structure analysis","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","187","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863552197&partnerID=40&md5=ea67d8da0ed5d8e42bd4f5a30d4c7440","Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany; MPI Informatik, Saarland University, Saarbrücken, Germany; Centre for Digital Music, Queen Mary Univ. of London, London, United Kingdom","Paulus J., Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Klapuri A., Centre for Digital Music, Queen Mary Univ. of London, London, United Kingdom","Humans tend to organize perceived information into hierarchies and structures, a principle that also applies to music. Even musically untrained listeners unconsciously analyze and segment music with regard to various musical aspects, for example, identifying recurrent themes or detecting temporal boundaries between contrasting musical parts. This paper gives an overview of state-of-theart methods for computational music structure analysis, where the general goal is to divide an audio recording into temporal segments corresponding to musical parts and to group these segments into musically meaningful categories. There are many different criteria for segmenting and structuring music audio. In particular, one can identify three conceptually different approaches, which we refer to as repetition-based, novelty-based, and homogeneitybased approaches. Furthermore, one has to account for different musical dimensions such as melody, harmony, rhythm, and timbre. In our state-of-the-art report, we address these different issues in the context of music structure analysis, while discussing and categorizing the most relevant and recent articles in this field. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Audio-based; Music structure analysis; Temporal segments; Audio acoustics","J. Paulus; Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany; email: jouni.paulus@iis.fraunhofer.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Schmidt E.M.; Kim Y.E.","Schmidt, Erik M. (36053813000); Kim, Youngmoo E. (24724623000)","36053813000; 24724623000","Prediction of time-varying musical mood distributions from audio","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","49","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555174290&partnerID=40&md5=34157cfffc550abc1377b1b806ad577e","Department of Electrical and Computer Engineering, Drexel University, United States","Schmidt E.M., Department of Electrical and Computer Engineering, Drexel University, United States; Kim Y.E., Department of Electrical and Computer Engineering, Drexel University, United States","The appeal of music lies in its ability to express emotions, and it is natural for us to organize music in terms of emotional associations. But the ambiguities of emotions make the determination of a single, unequivocal response label for the mood of a piece of music unrealistic. We address this lack of specificity by modeling human response labels to music in the arousal-valence (A-V) representation of affect as a stochastic distribution. Based upon our collected data, we present and evaluate methods using multiple sets of acoustic features to estimate these mood distributions parametrically using multivariate regression. Furthermore, since the emotional content of music often varies within a song, we explore the estimation of these A-V distributions in a time-varying context, demonstrating the ability of our system to track changes on a short-time basis. © 2010 International Society for Music Information Retrieval.","","Computer music; Information retrieval; Regression analysis; Acoustic features; Express emotions; Human response; Multiple set; Multivariate regression; Stochastic distribution; Time varying; Track changes; Audio acoustics","E.M. Schmidt; Department of Electrical and Computer Engineering, Drexel University, United States; email: eschmidt@drexel.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Ganseman J.; Scheunders P.; Mysore G.J.; Abel J.S.","Ganseman, Joachim (42061332400); Scheunders, Paul (7003845862); Mysore, Gautham J. (24465525500); Abel, Jonathan S. (7103340370)","42061332400; 7003845862; 24465525500; 7103340370","Evaluation of a score-informed source separation system","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053006437&partnerID=40&md5=0990aa343e1f9ee9ed6b3b5abc23b54c","Department of Physics, IBBT - Visielab, University of Antwerp, 2000 Antwerp, Belgium; Department of Music, CCRMA, Stanford University, Stanford, CA 94305, United States","Ganseman J., Department of Physics, IBBT - Visielab, University of Antwerp, 2000 Antwerp, Belgium; Scheunders P., Department of Physics, IBBT - Visielab, University of Antwerp, 2000 Antwerp, Belgium; Mysore G.J., Department of Music, CCRMA, Stanford University, Stanford, CA 94305, United States; Abel J.S., Department of Music, CCRMA, Stanford University, Stanford, CA 94305, United States","In this work, we investigate a method for score-informed source separation using Probabilistic Latent Component Analysis (PLCA). We present extensive test results that give an indication of the performance of the method, its strengths and weaknesses. For this purpose, we created a test database that has been made available to the public, in order to encourage comparisons with alternative methods. © 2010 International Society for Music Information Retrieval.","","Alternative methods; Probabilistic latent component analysis; Score-informed source separations; Information retrieval","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Vatolkin I.; Theimer W.; Botteck M.","Vatolkin, Igor (25652236100); Theimer, Wolfgang (6602930202); Botteck, Martin (16309125200)","25652236100; 6602930202; 16309125200","Amuse (Advanced music explorer) - A multitool framework for music data analysis","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959404927&partnerID=40&md5=bcf2454b5e388ae6f2c20c32be71d42a","Department of Algorithm Engineering, TU Dortmund, Germany; Research in Motion, Bochum, Germany","Vatolkin I., Department of Algorithm Engineering, TU Dortmund, Germany; Theimer W., Research in Motion, Bochum, Germany; Botteck M.","A large variety of research tools is available now for music information retrieval tasks. In this paper we present a further framework which aims to facilitate the interaction between these applications. Since the available tools are very different in target domain, range of available methods, learning efforts, installation and runtime characteristics etc., it is not easy to find software which is optimal for certain research goals. Another problematic issue is that many incompatible data formats exist, so it is not always possible to use output from one tool just as input for another one. At first we describe some of the available projects and outline our motivation starting the development of AMUSE framework for audio data analysis. Requirements and application purposes are given. The structure of our framework is introduced in detail and the information for efficient application is provided. Finally we discuss several ideas for further work. © 2010 International Society for Music Information Retrieval.","","Further works; Learning efforts; Music information retrieval; Problematic issues; Research goals; Research tools; Runtimes; Target domain; Information retrieval","I. Vatolkin; Department of Algorithm Engineering, TU Dortmund, Germany; email: igor.vatolkin@udo.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Collins N.","Collins, Nick (24478398900)","24478398900","Computational analysis of musical influence: A musicological case study using MIR tools","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955393344&partnerID=40&md5=dc421e173250ea53b129f2c22fac26f9","Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QJ, United Kingdom","Collins N., Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QJ, United Kingdom","Are there new insights through computational methods to the thorny problem of plotting the flow of musical influence? This project, motivated by a musicological study of early synth pop, applies MIR tools as an aid to the investigator. Web scraping and web services provide one angle, sourcing data from allmusic.com, and utilising python APIs for last.fm, EchoNest, and MusicBrainz. Charts of influence are constructed in GraphViz combining artist similarity and dates. Content based music similarity is the second approach, based around a core collection of synth pop albums. The prospect for new musical analyses are discussed with respect to these techniques. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Web services; Artist similarities; Computational analysis; Content-based; Last.fm; Music similarity; Musical analysis; Web scrapings; Computational methods","N. Collins; Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QJ, United Kingdom; email: N.Collins@sussex.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Wang D.; Li T.; Ogihara M.","Wang, Dingding (23037039100); Li, Tao (55553727585); Ogihara, Mitsunori (54420747900)","23037039100; 55553727585; 54420747900","Are tags better than audio features? The effect of joint use of tags and audio content features for artistic style clustering","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586245&partnerID=40&md5=0d4001bd77be32b8d9fbe2754299d83a","School of Computer Science, Florida International University, Miami, FL, United States; Department of Computer Science, University of Miami, Coral Gables, FL, United States","Wang D., School of Computer Science, Florida International University, Miami, FL, United States; Li T., School of Computer Science, Florida International University, Miami, FL, United States; Ogihara M., Department of Computer Science, University of Miami, Coral Gables, FL, United States","Social tags are receiving growing interests in information retrieval. In music information retrieval previous research has demonstrated that tags can assist in music classification and clustering. This paper studies the problem of combining tags and audio contents for artistic style clustering. After studying the effectiveness of using tags and audio contents separately for clustering, this paper proposes a novel language model that makes use of both data sources. Experiments with various methods for combining feature sets demonstrate that tag features are more useful than audio content features for style clustering and that the proposed model can marginally improve clustering performance by combing tags and audio contents. © 2010 International Society for Music Information Retrieval.","","Computational linguistics; Websites; Audio content; Audio features; Data-sources; Feature sets; Language model; Music classification; Music information retrieval; Social Tags; Information retrieval","D. Wang; School of Computer Science, Florida International University, Miami, FL, United States; email: dwang003@cs.fiu.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Jewell M.O.; Rhodes C.; D'Inverno M.","Jewell, Michael O. (14016052900); Rhodes, Christophe (57196565939); D'Inverno, Mark (55946449000)","14016052900; 57196565939; 55946449000","Querying improvised music: Do you sound like yourself?","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955412346&partnerID=40&md5=2647d066b855ee2beeef255a90eb88ff","Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom","Jewell M.O., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom; Rhodes C., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom; D'Inverno M., Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom","Improvisers are often keen to assess how their performance practice stands up to an ideal: whether that ideal is of technical accuracy or instant composition of material meeting complex harmonic constraints at speed. This paper reports on the development of an interface for querying and navigating a collection of recorded material for the purpose of presenting information on musical similarity, and the application of this interface to the investigation of a set of recordings by jazz performers. We investigate the retrieval performance of our tool, and in analysing the 'hits' and particularly the 'misses', provide information suggesting a change in one of the authors' improvisation style. © 2010 International Society for Music Information Retrieval.","","Information retrieval; At-speed; Harmonic constraint; Musical similarity; Presenting informations; Retrieval performance; Interfaces (materials)","M.O. Jewell; Department of Computing, Goldsmiths, University of London, London, SE14 6NW, New Cross, United Kingdom; email: m.jewell@gold.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Lemstrom K.","Lemstrom, Kjell (7006564183)","7006564183","Towards more robust geometric content-based music retrieval","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053097401&partnerID=40&md5=00f4a4bfabfd7686d69dcac881bc6ee3","Department of Computer Science, University of Helsinki, Finland","Lemstrom K., Department of Computer Science, University of Helsinki, Finland","This paper studies the problem of transposition and time-scale invariant (ttsi) polyphonic music retrieval in symbolically encoded music. In the setting, music is represented by sets of points in plane. We give two new algorithms. Applying a search window of size w and given a query point set, of size m, to be searched for in a database point set, of size n, our algorithm for exact ttsi occurrences runs in O(mwn log n) time; for partial occurrences we have an O(mnw2 log n) algorithm. The framework used is flexible allowing development towards even more robust geometric retrieval. © 2010 International Society for Music Information Retrieval.","","Geometry; Information retrieval; Query processing; Content-based music retrieval; Point set; Polyphonic music; Query points; Search windows; Time-scales; Algorithms","","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Macrae R.; Dixon S.","Macrae, Robert (36608414000); Dixon, Simon (7201479437)","36608414000; 7201479437","Accurate real-time windowed time warping","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578359&partnerID=40&md5=d494982adb0b3e6d4a76ad3c17454996","Centre for Digital Music, Queen Mary University, London, United Kingdom","Macrae R., Centre for Digital Music, Queen Mary University, London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University, London, United Kingdom","Dynamic Time Warping (DTW) is used to find alignments between two related streams of information and can be used to link data, recognise patterns or find similarities. Typically, DTW requires the complete series of both input streams in advance and has quadratic time and space requirements. As such DTW is unsuitable for real-time applications and is inefficient for aligning long sequences. We present Windowed Time Warping (WTW), a variation on DTW that, by dividing the path into a series of DTW windows and making use of path cost estimation, achieves alignments with an accuracy and efficiency superior to other leading modifications and with the capability of synchronising in real-time. We demonstrate this method in a score following application. Evaluation of the WTW score following system found 97.0% of audio note onsets were correctly aligned within 2000 ms of the known time. Results also show reductions in execution times over state-of-theart efficient DTW modifications. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Cost estimations; Dynamic time warping; Execution time; Input streams; Long sequences; Quadratic time; Real-time application; Score-following; Space requirements; Time warping; Alignment","R. Macrae; Centre for Digital Music, Queen Mary University, London, United Kingdom; email: robert.macrae@elec.qmul.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Molina-Solana M.; Grachten M.; Widmer G.","Molina-Solana, Miguel (24385556000); Grachten, Maarten (8974600000); Widmer, Gerhard (7004342843)","24385556000; 8974600000; 7004342843","Evidence for pianist-specific rubato style in chopin nocturnes","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051604329&partnerID=40&md5=a68cb17c78d24252341cdd2aad0b3677","Dpt. Computer Science and AI, University of Granada, Spain; IPEM - Dept. of Musicology, Ghent University, Belgium; Dpt. of Computational Perception, Johannes Kepler Univ., Austria","Molina-Solana M., Dpt. Computer Science and AI, University of Granada, Spain; Grachten M., IPEM - Dept. of Musicology, Ghent University, Belgium; Widmer G., Dpt. of Computational Perception, Johannes Kepler Univ., Austria","The performance of music usually involves a great deal of interpretation by the musician. In classical music, the final ritardando is a good example of the expressive aspect of music performance. Even though expressive timing data is expected to have a strong component that is determined by the piece itself, in this paper we investigate to what degree individual performance style has an effect on the timing of final ritardandi. The particular approach taken here uses Friberg and Sundberg's kinematic rubato model in order to characterize performed ritardandi. Using a machine- learning classifier, we carry out a pianist identification task to assess the suitability of the data for characterizing the in- dividual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, when cancelling the piece-specific aspects, pianists can often be identified with accuracy above baseline. This fact suggests the existence of a performer-specific style of playing ritardandi. © 2010 International Society for Music Information Retrieval.","","Classical musics; Individual performance; Learning classifiers; Music performance; Playing style; Reduced data; Information retrieval","M. Molina-Solana; Dpt. Computer Science and AI, University of Granada, Spain; email: miguelmolina@ugr.es","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Conklin D.; Bergeron M.","Conklin, Darrell (57220096325); Bergeron, Mathieu (23992413700)","57220096325; 23992413700","Discovery of contrapuntal patterns","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959609428&partnerID=40&md5=e866145476371110c1981fbef3b19969","Department of Computer Science and AI, Basque Foundation for Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain; CIRMMT, McGill University, Montreal, Canada","Conklin D., Department of Computer Science and AI, Basque Foundation for Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain; Bergeron M., CIRMMT, McGill University, Montreal, Canada","This paper develops and applies a new method for the discovery of polyphonic patterns. The method supports the representation of abstract relations that are formed between notes that overlap in time without being simultaneous. Such relations are central to understanding species counterpoint. The method consists of an application of the vertical viewpoint technique, which relies on a vertical slicing of the musical score. It is applied to two-voice contrapuntal textures extracted from the Bach chorale harmonizations. Results show that the new method is powerful enough to represent and discover distinctive modules of species counterpoint, including remarkably the suspension principle of fourth species counterpoint. In addition, by focusing on two voices in particular and setting them against all other possible voice pairs, the method can elicit patterns that illustrate well the unique treatment of the voices under investigation, e.g. the inner and outer voices. The results are promising and indicate that the method is suitable for computational musicology research. © 2010 International Society for Music Information Retrieval.","","Musical score; Information retrieval","D. Conklin; Department of Computer Science and AI, Basque Foundation for Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain; email: darrellconklin@ehu.es","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Gärtner D.","Gärtner, Daniel (18041853400)","18041853400","Singing / RAP classification of isolated vocal tracks","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053108452&partnerID=40&md5=f21a6d7483de5c2798dc4b143389cb31","Fraunhofer Institute for Digital Media Technology IDMT, Germany","Gärtner D., Fraunhofer Institute for Digital Media Technology IDMT, Germany","In this paper, a system for the classification of the vocal characteristics in HipHop / R&B music is presented. Isolated vocal track segments, taken from acapella versions of commercial recordings, are classified into classes singing and rap. A feature-set motivated by work from song / speech classification, speech emotion recognition, and from differences that humans perceive and utilize, is presented. An SVM is used as classifier, accuracies of about 90% are achieved. In addition, the features are analyzed according to their contribution, using the IRMFSP feature selection algorithm. In another experiment, it is shown that the features are robust against utterance-specific characteristics. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Feature selection algorithm; Speech classification; Speech emotion recognition; Track segments; Behavioral research","D. Gärtner; Fraunhofer Institute for Digital Media Technology IDMT, Germany; email: daniel.gaertner@idmt.fraunhofer.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Jo S.; Yoo C.D.","Jo, Seokhwan (18042330400); Yoo, Chang D. (7201746384)","18042330400; 7201746384","Melody extraction from polyphonic audio based on particle filter","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572703&partnerID=40&md5=415da956a5b89847bfe56ab0bb7e7456","Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","Jo S., Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; Yoo C.D., Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea","This paper considers a particle filter based algorithm to ex- tract melody from a polyphonic audio in the short-time Fourier transforms (STFT) domain. The extraction is focused on overcoming the difficulties due to harmonic / percussive sound interferences, possibility of octave mismatch, and dynamic variation in melody. The main idea of the algorithm is to consider probabilistic relations between melody and polyphonic audio. Melody is assumed to follow a Markov process, and the framed segments of polyphonic audio are assumed to be conditionally independent given the parameters that represent the melody. The melody parameters are estimated using sequential importance sampling (SIS) which is a conventional particle filter method. In this paper, the likelihood and state transition are defined to overcome the aforementioned difficulties. The SIS algorithm relies on sequential importance density, and this density is designed using multiple pitches which are estimated by a simple multi-pitch extraction algorithm. Experimental results show that the considered algorithm outperforms other famous melody extraction algorithms in terms of the raw pitch accuracy (RPA) and the raw chroma accuracy (RCA). © 2010 International Society for Music Information Retrieval.","","Extraction; Fourier transforms; Information retrieval; Markov processes; Monte Carlo methods; A-particles; Audio-based; Dynamic variations; Extraction algorithms; Importance densities; Melody extractions; Multi pitches; Particle filter; Probabilistic relations; Sequential importance sampling; Short time Fourier transforms; State transitions; Algorithms","S. Jo; Department of Electrical Engineering, Korea Advanced Institute of Science Technology, Yuseong-gu, Daejeon, 305-701, 373-1 Guseong-dong, South Korea; email: antiland00@kaist.ac.kr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Rocher T.; Robine M.; Hanna P.; Oudre L.","Rocher, Thomas (6507641368); Robine, Matthias (24559139300); Hanna, Pierre (23134483000); Oudre, Laurent (35766614800)","6507641368; 24559139300; 23134483000; 35766614800","Concurrent estimation of chords and keys from audio","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572944&partnerID=40&md5=8a8a61cf198f717fe905af9ae0169e55","LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; TELECOM ParisTech, Institut TELECOM, 75014 Paris, 37-39 rue Dareau, France","Rocher T., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Robine M., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; Oudre L., TELECOM ParisTech, Institut TELECOM, 75014 Paris, 37-39 rue Dareau, France","This paper proposes a new method for local key and chord estimation from audio signals. A harmonic content of the musical piece is first extracted by computing a set of chroma vectors. Correlation with fixed chord and key templates then selects a set of key/chord pairs for every frame. A weighted acyclic harmonic graph is then built with these pairs as vertices, and the use of a musical distance to weigh its edges. Finally, the output sequences of chords and keys are obtained by finding the best path in the graph. The proposed system allows a mutual and beneficial chord and key estimation. It is evaluated on a corpus composed of Beatles songs for both the local key estimation and chord recognition tasks. Results show that it performs better than state-of-the art chord analysis algorithms while providing a more complete harmonic analysis. © 2010 International Society for Music Information Retrieval.","","Graph theory; Information retrieval; Analysis algorithms; Audio signal; Best paths; Chord recognition; Harmonic contents; Harmonic graphs; Musical pieces; Output sequences; State of the art; Estimation","T. Rocher; LaBRI, University of Bordeaux, 33405 Talence Cedex, 351 cours de la Libration, France; email: rocher@labri.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Schepens W.","Schepens, Wijnand (26424748900)","26424748900","Chronicle: Representation of complex time structures","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873681901&partnerID=40&md5=256faf9baac430445da448b1f56fd1a6","University College Ghent, Belgium","Schepens W., University College Ghent, Belgium","Chronicle is a novel open source system for representing structured data involving time, such as music. It offers an XML-based file format, object models for internal representation in various programming languages, and software libraries and tools for reading and writing XML and for data transformations. Chronicle defines basic blocks for representing timebased information using events, a hierarchy of groups and instantiable templates. It supports two modes of timing: local timing within a group and association with other elements. The built-in mechanism for resolving time references can be used to implement both timescale mappings and tagging of information. Chronicle aims to be a powerful and flexible foundation on which new file formats and software can be built. Chronicle focuses on structure and timing, but leaves the actual content free to choose. Thus format- or softwaredevelopers can specify their own domain-model. This makes it possible to make representations for different types of musical information (scores, performance data, ...) in different styles or cultures (CMN, non-western, contemporary, ...), but also for other domains like choreography, scheduling, task management, and so on. It is also ideal for structured tagging of audio and multimedia (movie subtitles, karaoke, synchronisation, ...) and for representing ""internal"" data used in music algorithms. The system is organized in four levels of increasing complexity. Software developed for a specific level and domain will also accept lower level data, while users can choose to represent data in a higher level and use Chronicle tools to reduce the level. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Scheduling; Wavelet analysis; XML; Basic blocks; Data transformation; File formats; Flexible foundations; Internal representation; It supports; Karaoke; MUSIC algorithms; Musical information; Object model; Open source system; Performance data; Software libraries; Structured data; Task management; Time-based information; Time-scales; XML-based files; Open systems","W. Schepens; University College Ghent, Belgium; email: wijnand.schepens@hogent.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Cuthbert M.S.; Ariza C.","Cuthbert, Michael Scott (15724599200); Ariza, Christopher (57204340082)","15724599200; 57204340082","Music21: A toolkit for computer-aided musicology and symbolic music data","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","205","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479813&partnerID=40&md5=3a16428b90cf2f918794462d048ca682","Department of Music and Theater Arts, Massachusetts Institute of Technology, United States","Cuthbert M.S., Department of Music and Theater Arts, Massachusetts Institute of Technology, United States; Ariza C., Department of Music and Theater Arts, Massachusetts Institute of Technology, United States","Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills. This paper introduces the music21 system, demonstrating how to use it and the types of problems it is wellsuited toward advancing. We include numerous examples of its power and flexibility, including demonstrations of graphing data and generating annotated musical scores. © 2010 International Society for Music Information Retrieval.","","Computer programming; Computer-aided; Modular approach; Music data; Music theory; Musical score; Object oriented; Powerful software tools; Programming experience; Information retrieval","M.S. Cuthbert; Department of Music and Theater Arts, Massachusetts Institute of Technology, United States; email: cuthbert@mit.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Barbieri G.; Pachet F.; Esposti M.D.; Roy P.","Barbieri, Gabriele (55586128700); Pachet, François (6701441655); Esposti, Mirko Degli (57202535306); Roy, Pierre (55506419000)","55586128700; 6701441655; 57202535306; 55506419000","Is there a relation between the syntax and the fitness of an audio feature?","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600345&partnerID=40&md5=2e67b57ce4be860f8e965541e176e9d4","Dip. di Matematica, Università di Bologna, Italy; Sony CSL, Paris, France","Barbieri G., Dip. di Matematica, Università di Bologna, Italy; Pachet F., Sony CSL, Paris, France; Esposti M.D., Dip. di Matematica, Università di Bologna, Italy; Roy P., Sony CSL, Paris, France","Feature generation has been proposed recently to generate feature sets automatically, as opposed to human-designed feature sets. This technique has shown promising results in many areas of supervised classification, in particular in the audio domain. However, feature generation is usually performed blindly, with genetic algorithms. As a result search performance is poor, thereby limiting its practical use. We propose a method to increase the search performance of feature generation systems. We focus on analytical features, i.e. features determined by their syntax. Our method consists in first extracting statistical proper- ties of the feature space called spin patterns, by analogy with statistical physics. We show that spin patterns carry information about the topology of the feature space. We exploit these spin patterns to guide a simulated annealing algorithm specifically designed for feature generation. We evaluate our approach on three audio classification problems, and show that it increases performance by an order of magnitude. More generally this work is a first step in using tools from statistical physics for the supervised classification of complex audio signals. © 2010 International Society for Music Information Retrieval.","","Audio acoustics; Genetic algorithms; Information retrieval; Simulated annealing; Supervised learning; Audio classification; Audio features; Feature generation; Feature space; Search performance; Simulated annealing algorithms; Statistical physics; Supervised classification; Syntactics","G. Barbieri; Dip. di Matematica, Università di Bologna, Italy; email: gbarbieri@dm.unibo.it","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Inskip C.; MacFarlane A.; Rafferty P.","Inskip, Charlie (23570463400); MacFarlane, Andy (7102685938); Rafferty, Pauline (24923964800)","23570463400; 7102685938; 24923964800","Upbeat and quirky, with a bit of a build: Interpretive repertoires in creative music search","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581828&partnerID=40&md5=1f5b0e4ea84c71b0b743680ad8fc56a0","Dept of Info Science, City University London, United Kingdom; Dept of Info Studies, University of Aberystwyth, United Kingdom","Inskip C., Dept of Info Science, City University London, United Kingdom; MacFarlane A., Dept of Info Science, City University London, United Kingdom; Rafferty P., Dept of Info Studies, University of Aberystwyth, United Kingdom","Pre-existing commercial music is widely used to accompany moving images in films, TV commercials and computer games. This process is known as music synchronisation. Professionals are employed by rights holders and film makers to perform creative music searches on large catalogues to find appropriate pieces of music for synchronisation. This paper discusses a Discourse Analysis of thirty interview texts related to the process. Coded examples are presented and discussed. Four interpretive repertoires are identified: the Musical Repertoire, the Soundtrack Repertoire, the Business Repertoire and the Cultural Repertoire. These ways of talking about music are adopted by all of the community regardless of their interest as Music Owner or Music User. Music is shown to have multi-variate and sometimes conflicting meanings within this community which are dynamic and negotiated. This is related to a theoretical feedback model of communication and meaning making which proposes that Owners and Users employ their own and shared ways of talking and thinking about music and its context to determine musical meaning. The value to the music information retrieval community is to inform system design from a user information needs perspective. © 2010 International Society for Music Information Retrieval.","","Community IS; Discourse analysis; Feedback model; Film-makers; Meaning makings; Moving image; Music information retrieval; TV commercial; User information need; Information retrieval","C. Inskip; Dept of Info Science, City University London, United Kingdom; email: c.inskip@city.ac.uk","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Coviello E.; Barrington L.; Chan A.B.; Lanckriet Gert.R.G.","Coviello, Emanuele (35147534800); Barrington, Luke (14041197300); Chan, Antoni B. (14015159100); Lanckriet, Gert.R.G. (7801431767)","35147534800; 14041197300; 14015159100; 7801431767","Automatic music tagging with time series models","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579339&partnerID=40&md5=74095ba7f158918218ce21a96c9c5b22","Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Dept. of Computer Science, University of Hong Kong, Hong Kong","Coviello E., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Chan A.B., Dept. of Computer Science, University of Hong Kong, Hong Kong; Lanckriet Gert.R.G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","State-of-the-art systems for automatic music tagging model music based on bag-of-feature representations which give little or no account of temporal dynamics, a key characteristic of the audio signal. We describe a novel approach to automatic music annotation and retrieval that captures temporal (e.g., rhythmical) aspects as well as timbral content. The proposed approach leverages a recently proposed song model that is based on a generative time series model of the musical content - the dynamic texture mixture (DTM) model - that treats fragments of audio as the output of a linear dynamical system. To model characteristic temporal dynamics and timbral content at the tag level, a novel, efficient hierarchical EM algorithm for DTM (HEM-DTM) is used to summarize the common information shared by DTMs modeling individual songs associated with a tag. Experiments show learning the semantics of music benefits from modeling temporal dynamics. © 2010 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Linear control systems; Semantics; Temperature control; Time series; Audio signal; Automatic Music Tagging; Dynamic textures; EM algorithms; Information shared; Key characteristics; Linear dynamical systems; State-of-the-art system; Temporal dynamics; Time series models; Audio acoustics","E. Coviello; Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; email: ecoviell@ucsd.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Wang J.; Chen X.; Hu Y.; Feng T.","Wang, Jun (57200017808); Chen, Xiaoou (11639519200); Hu, Yajie (57221204354); Feng, Tao (58331965100)","57200017808; 11639519200; 57221204354; 58331965100","Predicting high-level music semantics using social tags via ontology-based reasoning","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955369613&partnerID=40&md5=b59fb4ee534e358e33afb004c16a3720","Institute of Computer Science and Technology, Peking University, China","Wang J., Institute of Computer Science and Technology, Peking University, China; Chen X., Institute of Computer Science and Technology, Peking University, China; Hu Y., Institute of Computer Science and Technology, Peking University, China; Feng T., Institute of Computer Science and Technology, Peking University, China","High-level semantics such as ""mood"" and ""usage"" are very useful in music retrieval and recommendation but they are normally hard to acquire. Can we predict them from a cloud of social tags? We propose a semantic identification and reasoning method: Given a music taxonomy system, we map it to an ontology's terminology, map its finite set of terms to the ontology's assertional axioms, and then map tags to the closest conceptual level of the referenced terms in WordNet to enrich the knowledge base, then we predict richer high-level semantic information with a set of reasoning rules. We find this method predicts mood annotations for music with higher accuracy, as well as giving richer semantic association information, than alternative SVM-based methods do. © 2010 International Society for Music Information Retrieval.","","Forecasting; Information retrieval; Knowledge based systems; Conceptual levels; Finite set; High level semantics; Knowledge base; Music retrieval; Ontology's; Ontology-based; Reasoning methods; Reasoning rules; Semantic associations; Semantic identification; Social Tags; SVM-based methods; Wordnet; Semantics","J. Wang; Institute of Computer Science and Technology, Peking University, China; email: wangjun@icst.pku.edu.cn","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Holzapfel A.; Stylianou Y.","Holzapfel, Andre (18041818000); Stylianou, Yannis (6601991415)","18041818000; 6601991415","Parataxis: Morphological similarity in traditional music","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865698922&partnerID=40&md5=52ff09c47f0de2cb88264fb56557507c","FORTH, Institute of Computer Science, University of Crete, Greece","Holzapfel A., FORTH, Institute of Computer Science, University of Crete, Greece; Stylianou Y., FORTH, Institute of Computer Science, University of Crete, Greece","In this paper an automatic system for the detection of similar phrases in music of the Eastern Mediterranean is proposed. This music follows a specific structure, which is referred to as parataxis. The proposed system can be applied to audio signals of complex mixtures that contain the lead melody together with instrumental accompaniment. It is shown that including a lead melody estimation into a stateof- the-art system for cover song detection leads to promising results on a dataset of transcribed traditional dances from the island of Crete in Greece. Furthermore, a general framework that includes also rhythmic aspects is proposed. The proposed method represents a simple framework for the support of ethnomusicological studies on related forms of traditional music. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Audio signal; Automatic systems; Complex mixture; Cover songs; Eastern Mediterranean; Chemical detection","A. Holzapfel; FORTH, Institute of Computer Science, University of Crete, Greece; email: hannover@csd.uoc.gr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hillewaere R.; Manderick B.; Conklin D.","Hillewaere, Ruben (36720698100); Manderick, Bernard (6602920805); Conklin, Darrell (57220096325)","36720698100; 6602920805; 57220096325","String quartet classification with monophonic models","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579972&partnerID=40&md5=79fb3913a6e04bd4920f1f8ca3f57f13","Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Department of Computer Science and AI, Basque Foundation or Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain","Hillewaere R., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Department of Computer Science and AI, Basque Foundation or Science, Universidad del Páis Vasco, Bilbao, San Sebastían, Spain","Polyphonic music classification remains a very challenging area in the field of music information retrieval. In this study, we explore the performance of monophonic models on single parts that are extracted from the polyphony. The presented method is specifically designed for the case of voiced polyphony, but can be extended to any type of music with multiple parts. On a dataset of 207 Haydn and Mozart string quartet movements, global feature models with standard machine learning classifiers are compared with a monophonic n-gram model for the task of composer recognition. Global features emerging from feature selection are presented, and future guidelines for the research of polyphonic music are outlined. © 2010 International Society for Music Information Retrieval.","","Learning systems; Global feature; Learning classifiers; Multiple parts; Music information retrieval; N-gram models; Polyphonic music; Single parts; Standard machines; Information retrieval","R. Hillewaere; Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; email: rhillewa@vub.ac.be","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Han Y.; Raphael C.","Han, Yushen (24477451100); Raphael, Christopher (7004214964)","24477451100; 7004214964","Informed source separation of orchestra and soloist","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862112409&partnerID=40&md5=20c1979e776154ba690cac4724a5bf5a","School of Informatics and Computing, Indiana University, Bloomington, United States","Han Y., School of Informatics and Computing, Indiana University, Bloomington, United States; Raphael C., School of Informatics and Computing, Indiana University, Bloomington, United States","A novel technique of unmasking to repair the degradation in sources separated by spectrogram masking is proposed. Our approach is based on explicit knowledge of the musical audio at note level from a score-audio alignment, which we termed Informed Source Separation (ISS). Such knowledge allows the spectrogram energy to be decomposed into note-based models. We assume that a spectrogram mask for the solo is obtained and focus on the problem of repairing audio resulting from applying the mask. We evaluate the spectrogram as well as the harmonic structure of the music. We either search for unmasked (orchestra) partials of the orchestra to be transposed onto a masked (solo) region or reshape a solo partial with phase and amplitude imputed from unmasked regions. We describe a Kalman smoothing technique to decouple the phase and amplitude of a musical partial that enables the modification to the spectrogram. Audio examples from a piano concerto are available for evaluation. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Spectrographs; Explicit knowledge; Harmonic structures; Kalman smoothing; Musical audio; Novel techniques; Spectrograms; Audio acoustics","Y. Han; School of Informatics and Computing, Indiana University, Bloomington, United States; email: yushan@indiana.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Moelants D.; Cornelis O.; Leman M.","Moelants, Dirk (6507495086); Cornelis, Olmo (27267474100); Leman, Marc (6603703642)","6507495086; 27267474100; 6603703642","Exploring African tone scales","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873687321&partnerID=40&md5=ce13827b29d7dbca3ed049b5290cfc61","Ghent University, Belgium; University College, Ghent, Belgium","Moelants D., Ghent University, Belgium; Cornelis O., University College, Ghent, Belgium; Leman M., Ghent University, Belgium","Key-finding is a central topic in Western music analysis and development of MIR tools. However, most approaches rely on the Western 12-tone scale, which is not universally used. African music does not follow a fixed tone scale. In order to classify and study African tone scales, we developed a system in which the pitch is first analyzed on a continuous scale. Peak analysis is then applied on these data to extract the actual scale used. This system has been applied to a selection of African music, it allows us to look for similarities using cross-correlation. Thus it provides an interesting tool for query-by-example and database management in collections of ethnic music which can not be simply classified according to keys. Next to this the data can be used for ethnomusicological research. The study of the intervals used in this collection, e.g., gives us evidence for Western influence, with recent recordings having a tendency to use more regular intervals. © 2009 International Society for Music Information Retrieval.","","Continuous scale; Cross correlations; Database management; Music analysis; Peak analysis; Query-by-example; Tone scale; Information retrieval","D. Moelants; Ghent University, Belgium; email: Dirk.Moelants@UGent.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hu X.; Stephen Downie J.","Hu, Xiao (55496358400); Stephen Downie, J. (7102932568)","55496358400; 7102932568","When lyrics outperform audio for music mood classification: A feature analysis","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","57","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555199091&partnerID=40&md5=db4f91a47d3c2b8f5b16a5f915e18286","School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Hu X., School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper builds upon and extends previous work on multi-modal mood classification (i.e., combining audio and lyrics) by analyzing in-depth those feature types that have shown to provide statistically significant improvements in the classification of individual mood categories. The dataset used in this study comprises 5,296 songs (with lyrics and audio for each) divided into 18 mood categories derived from user-generated tags taken from last.fm. These 18 categories show remarkable consistency with the popular Russell's mood model. In seven categories, lyric features significantly outperformed audio spectral features. In one category only, audio outperformed all lyric features types. A fine grained analysis of the significant lyric feature types indicates a strong and obvious semantic association between extracted terms and the categories. No such obvious semantic linkages were evident in the case where audio spectral features proved superior. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Semantics; Feature analysis; Feature types; Fine-grained analysis; Last.fm; Multi-modal; Semantic associations; Spectral feature; User-generated; Audio acoustics","X. Hu; School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; email: xiaohu@illinois.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Raczynski S.A.; Vincent E.; Bimbot F.; Sagayama S.","Raczynski, Stanisław A. (57190605458); Vincent, Emmanuel (14010158800); Bimbot, Frédéric (6701567957); Sagayama, Shigeki (7004859104)","57190605458; 14010158800; 6701567957; 7004859104","Multiple pitch transcription using DBN-based musicological models","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573353&partnerID=40&md5=6bdae665cc64fef40510a3a50420cee5","University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan; INRIA Rennes, 35042 Rennes Cedex, Britagne Atlantique, France","Raczynski S.A., University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan; Vincent E., INRIA Rennes, 35042 Rennes Cedex, Britagne Atlantique, France; Bimbot F., INRIA Rennes, 35042 Rennes Cedex, Britagne Atlantique, France; Sagayama S., University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan","We propose a novel approach to solve the problem of estimating pitches of notes present in an audio signal. We have developed a probabilistically rigorous model that takes into account temporal dependencies between musical notes and between the underlying chords, as well as the instantaneous dependencies between chords, notes and the observed note saliences. We investigated its modeling ability by measuring the cross-entropy with symbolic (MIDI) data and then proceed to observe the model's performance in multiple pitch estimation of audio data. © 2010 International Society for Music Information Retrieval.","","Audio data; Audio signal; Cross entropy; Modeling abilities; Musical notes; Pitch estimation; Rigorous model; Information retrieval","S.A. Raczynski; University of Tokyo, Bunkyo-ku, Tokyo 133-8656, 7-3-1 Hongo, Japan; email: raczynski@hil.t.u-tokyo.ac.jp","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Abeßer J.; Bräuer P.; Lukashevich H.; Schuller G.","Abeßer, Jakob (36607532300); Bräuer, Paul (55365375500); Lukashevich, Hanna (6508121862); Schuller, Gerald (7005659200)","36607532300; 55365375500; 6508121862; 7005659200","Bass playing style detection based on high-level features and pattern similarity","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053125108&partnerID=40&md5=db3777dde03905527c8967922f8889be","Fraunhofer IDMT, Ilmenau, Germany; Piranha Musik and IT, Berlin, Germany","Abeßer J., Fraunhofer IDMT, Ilmenau, Germany; Bräuer P., Piranha Musik and IT, Berlin, Germany; Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany; Schuller G., Fraunhofer IDMT, Ilmenau, Germany","In this paper, we compare two approaches for automatic classification of bass playing styles, one based on highlevel features and another one based on similarity measures between bass patterns. For both approaches,we compare two different strategies: classification of patterns as a whole and classification of all measures of a pattern with a subsequent accumulation of the classification results. Furthermore, we investigate the influence of potential transcription errors on the classification accuracy, which tend to occur when real audio data is analyzed. We achieve best classification accuracy values of 60.8% for the feature-based classification and 68.5% for the classification based on pattern similarity based on a taxonomy consisting of 8 different bass playing styles. © 2010 International Society for Music Information Retrieval.","","Audio data; Automatic classification; Classification accuracy; Classification results; Feature-based classification; High-level features; Pattern similarity; Playing style; Similarity measure; Two Approaches; Information retrieval","J. Abeßer; Fraunhofer IDMT, Ilmenau, Germany; email: abr@idmt.fraunhofer.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Chen Y.-X.; Klüber R.","Chen, Ya-Xi (24830156400); Klüber, René (14056090800)","24830156400; 14056090800","ThumbnailDJ: Visual thumbnails of music content","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873586667&partnerID=40&md5=438a2b9bb8e723bfdb318f3bc7286936","Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany","Chen Y.-X., Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany; Klüber R., Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany","Musical perception is non-visual and people cannot describe what a song sounds like without listening to it. To facilitate music browsing and searching, we explore the automatic generation of visual thumbnails for music. Targeting an expert user groups, DJs, we developed a concept named ThumbnailDJ: Based on a metaphor of music notation, a visual thumbnail can be automatically generated for an audio file, including information of tempo, volume, genre, aggressiveness and bass. We discussed ThumbnailDJ and other 3 selected concepts with DJs, and our concept was preferred most. Based on the results of this interview, we refined ThumbnailDJ and conducted an evaluation with DJs. The results confirmed that ThumbnailDJ can facilitate expert users browsing and searching within their music collection. © 2010 International Society for Music Information Retrieval.","","Audio files; Automatic Generation; Automatically generated; Expert users; Music collection; Music contents; Music notation; Non visuals; Information retrieval","Y.-X. Chen; Media Informatics, University of Munich, 80333 Munich, Amalienstr. 17, Germany; email: yaxi.chen@ifi.lmu.de","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Urbano J.; Marrero M.; Martín D.; Lloréns J.","Urbano, Julián (36118414700); Marrero, Mónica (36117926400); Martín, Diego (13905154700); Lloréns, Juan (7006368289)","36118414700; 36117926400; 13905154700; 7006368289","Improving the generation of ground truths based on partially ordered lists","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053112542&partnerID=40&md5=c9416e55aaa2b06b6fefc06b42498bc4","Department of Computer Science, University Carlos III, Madrid, Spain","Urbano J., Department of Computer Science, University Carlos III, Madrid, Spain; Marrero M., Department of Computer Science, University Carlos III, Madrid, Spain; Martín D., Department of Computer Science, University Carlos III, Madrid, Spain; Lloréns J., Department of Computer Science, University Carlos III, Madrid, Spain","Ground truths based on partially ordered lists have been used for some years now to evaluate the effectiveness of Music Information Retrieval systems, especially in tasks related to symbolic melodic similarity. However, there has been practically no meta-evaluation to measure or improve the correctness of these evaluations. In this paper we revise the methodology used to generate these ground truths and disclose some issues that need to be addressed. In particular, we focus on the arrangement and aggregation of the relevant results, and show that it is not possible to ensure lists completely consistent. We develop a measure of consistency based on Average Dynamic Recall and propose several alternatives to arrange the lists, all of which prove to be more consistent than the original method. The results of the MIREX 2005 evaluation are revisited using these alternative ground truths. © 2010 International Society for Music Information Retrieval.","","Ground truth; Melodic similarity; Music information retrieval; Information retrieval systems","J. Urbano; Department of Computer Science, University Carlos III, Madrid, Spain; email: jurbano@inf.uc3m.es","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Grachten M.; Widmer G.","Grachten, Maarten (8974600000); Widmer, Gerhard (7004342843)","8974600000; 7004342843","Who is who in the end? Recognizing pianists by their final ritardandi","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873673455&partnerID=40&md5=25335c5623244b201fa3e083ee434131","Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Grachten M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","The performance of music usually involves a great deal of interpretation by the musician. In classical music, final ritardandi are emblematic for the expressive aspect of music performance. In this paper we investigate to what degree individual performance style has an effect on the form of final ritardandi. To this end we look at interonset-interval deviations from a performance norm. We define a criterion for filtering out deviations that are likely to be due to measurement error. Using a machine-learning classifier, we evaluate an automatic pairwise pianist identification task as an initial assessment of the suitability of the filtered data for characterizing the individual playing style of pianists. The results indicate that in spite of an extremely reduced data representation, pianists can often be identified with accuracy significantly above baseline. © 2009 International Society for Music Information Retrieval.","","Classical musics; Individual performance; Initial assessment; Machine-learning; Music performance; Playing style; Reduced data; Information retrieval","M. Grachten; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; email: maarten.grachten@jku.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Cunningham S.J.; Nichols D.M.","Cunningham, Sally Jo (7201937110); Nichols, David M. (10044366000)","7201937110; 10044366000","Exploring social music behavior: An investigation of music selection at parties","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873674146&partnerID=40&md5=9537100c310ccf9aef8a2534ee95c60d","Department of Computer Science, University of Waikato, Hamilton, New Zealand","Cunningham S.J., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Nichols D.M., Department of Computer Science, University of Waikato, Hamilton, New Zealand","This paper builds an understanding how music is currently listened to by small (fewer than 10 individuals) to medium-sized (10 to 40 individuals) gatherings of people- how songs are chosen for playing, how the music fits in with other activities of group members, who supplies the music, the hardware/software that supports song selection and presentation. This fine-grained context emerges from a qualitative analysis of a rich set of participant observations and interviews focusing on the selection of songs to play at social gatherings. We suggest features for software to support music playing at parties. © 2009 International Society for Music Information Retrieval.","","Group members; Hardware/software; Participant observations; Qualitative analysis; Social gatherings; Information retrieval","S.J. Cunningham; Department of Computer Science, University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Han B.-J.; Rho S.; Dannenberg R.B.; Hwang E.","Han, Byeong-Jun (23094368600); Rho, Seungmin (10738984000); Dannenberg, Roger B. (7003266250); Hwang, Eenjun (7101826788)","23094368600; 10738984000; 7003266250; 7101826788","SMERS: Music emotion recognition using support vector regression","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","84","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873695101&partnerID=40&md5=5870e81c1ec255c9e49da837f162a777","School of Electrical Engineering, Korea University, South Korea; School of Computer Science, Carnegie Mellon University, United States","Han B.-J., School of Electrical Engineering, Korea University, South Korea; Rho S., School of Electrical Engineering, Korea University, South Korea; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Hwang E., School of Electrical Engineering, Korea University, South Korea","Music emotion plays an important role in music retrieval, mood detection and other music-related applications. Many issues for music emotion recognition have been addressed by different disciplines such as physiology, psychology, cognitive science and musicology. We present a support vector regression (SVR) based music emotion recognition system. The recognition process consists of three steps: (i) seven distinct features are extracted from music; (ii) those features are mapped into eleven emotion categories on Thayer's two-dimensional emotion model; (iii) two regression functions are trained using SVR and then arousal and valence values are predicted. We have tested our SVR-based emotion classifier in both Cartesian and polar coordinate system empirically. The result indicates the SVR classifier in the polar representation produces satisfactory result which reaches 94.55% accuracy superior to the SVR (in Cartesian) and other machine learning classification algorithms such as SVM and GMM. © 2009 International Society for Music Information Retrieval.","","Cartesians; Cognitive science; Emotion models; Machine learning classification; Mood detection; Music emotions; Music retrieval; Polar coordinate systems; Polar representation; Recognition process; Regression function; Support vector regression (SVR); Information retrieval","B.-J. Han; School of Electrical Engineering, Korea University, South Korea; email: hbj1147@korea.ac.kr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Miotto R.; Barrington L.; Lanckriet G.","Miotto, Riccardo (57202034402); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","57202034402; 14041197300; 7801431767","Improving auto-tagging by modeling semantic co-occurrences","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584760&partnerID=40&md5=c84e004205bb2ed7e04587657f5a6ede","University of Padova, Italy; UC San Diego, United States","Miotto R., University of Padova, Italy; Barrington L., UC San Diego, United States; Lanckriet G., UC San Diego, United States","Automatic taggers describe music in terms of a multinomial distribution over relevant semantic concepts. This paper presents a framework for improving automatic tagging of music content by modeling contextual relationships between these semantic concepts. The framework extends existing auto-tagging methods by adding a Dirichlet mixture to model the contextual co-occurrences between semantic multinomials. Experimental results show that adding context improves automatic annotation and retrieval of music and demonstrate that the Dirichlet mixture is an appropriate model for capturing co-occurrences between semantics. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Appropriate models; Automatic annotation; Automatic tagging; Contextual relationships; Dirichlet mixture; Multinomial distributions; Multinomials; Music contents; Semantic concept; Semantics","R. Miotto; University of Padova, Italy; email: miottori@dei.unipd.it","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Wołkowicz J.; Kešelj V.","Wołkowicz, Jacek (24472215300); Kešelj, Vlado (6507196408)","24472215300; 6507196408","Predicting development of research in music based on parallels with natural language processing","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952011219&partnerID=40&md5=a6ca65de8993f2e7a0003503f13acabe","Faculty of Computer Science, Dalhousie University, Canada","Wołkowicz J., Faculty of Computer Science, Dalhousie University, Canada; Kešelj V., Faculty of Computer Science, Dalhousie University, Canada","The hypothesis of the paper is that the domain of Natural Languages Processing (NLP) resembles current research in music so one could benefit from this by employing NLP techniques to music. In this paper the similarity between both domains is described. The levels of NLP are listed with pointers to respective tasks within the research of computational music. A brief introduction to history of NLP enables locating music research in this history. Possible directions of research in music, assuming its affinity to NLP, are introduced. Current research in generational and statistical music modeling is compared to similar NLP theories. The paper is concluded with guidelines for music research and information retrieval. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Natural language processing systems; Research; NAtural language processing; Natural languages; Nlp techniques; Computer music","J. Wołkowicz; Faculty of Computer Science, Dalhousie University, Canada; email: jacek@cs.dal.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Schedl M.","Schedl, Markus (8684865900)","8684865900","On the use of microblogging posts for similarity estimation and artist labeling","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869153307&partnerID=40&md5=e58d3fcae55a7c25fd40452b3020d867","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria","Microblogging services, such as Twitter, have risen enormously in popularity during the past years. Despite their popularity, such services have never been analyzed for MIR purposes, to the best of our knowledge. We hence present first investigations of the usability of music artist-related microblogging posts to perform artist labeling and similarity estimation tasks. To this end, we look into different text-based indexing models and term weighting measures. Two artist collections are used for evaluation, and the different methods are evaluated against data from last.fm. We show that microblogging posts are a valuable source for musical meta-data. © 2010 International Society for Music Information Retrieval.","","Indexing models; Last.fm; Micro-blogging services; Microblogging; Similarity estimation; Term weighting; Information retrieval","M. Schedl; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: markus.schedl@jku.at","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hankinson A.; Pugin L.; Fujinaga I.","Hankinson, Andrew (54970482500); Pugin, Laurent (23009752900); Fujinaga, Ichiro (9038140900)","54970482500; 23009752900; 9038140900","An interchange format for optical music recognition applications","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591058&partnerID=40&md5=475d20ea40c63c2b2b1818c1511e0783","Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; RISM Switzerland, Geneva University, Switzerland","Hankinson A., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; Pugin L., RISM Switzerland, Geneva University, Switzerland; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada","Page appearance and layout for music notation is a critical component of the overall musical information contained in a document. To capture and transfer this information, we outline an interchange format for OMR applications, the OMR Interchange Package (OIP) format, which is designed to allow layout information and page images to be preserved and transferred along with semantic musical content. We identify a number of uses for this format that can enhance digital representations of music, and introduce a novel idea for distributed optical music recognition system based on this format. © 2010 International Society for Music Information Retrieval.","","Semantics; Critical component; Digital representations; Interchange formats; Layout information; Music notation; Musical information; Optical music recognition; Information retrieval","A. Hankinson; Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Canada; email: andrew.hankinson@mail.mcgill.ca","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Sammartino S.; Tardón L.J.; De La Bandera C.; Barbancho I.; Barbancho A.M.","Sammartino, Simone (55204516100); Tardón, Lorenzo J. (6602405058); De La Bandera, Cristina (35069181300); Barbancho, Isabel (6602638932); Barbancho, Ana M. (6602362530)","55204516100; 6602405058; 35069181300; 6602638932; 6602362530","The standardized variogram as a novel tool for audio similarity measure","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865508167&partnerID=40&md5=a3641cab68489aa9155423763e91d0a4","Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain","Sammartino S., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Tardón L.J., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; De La Bandera C., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Barbancho I., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; Barbancho A.M., Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain","Most of methods for audio similarity evaluation are based on the Mel frequency cepstral coefficients, employed as main tool for the characterization of audio contents. Such approach needs some way of data compression aimed to optimize the information retrieval task and to reduce the computational costs derived from the usage of cluster analysis tools and probabilistic models. A novel approach is presented in this paper, based on the standardized variogram. This tool, inherited from Geostatistics, is applied to MFCCs matrices to reduce their size and compute compact representations of the audio contents (song signatures), aimed to evaluate audio similarity. The performance of the proposed approach is analyzed in comparison with other alternative methods and on the base of human responses. © 2010 International Society for Music Information Retrieval.","","Cluster analysis; Data compression; Alternative methods; Audio content; Audio similarities; Compact representation; Computational costs; Geo-statistics; Human response; Mel frequency cepstral co-efficient; Probabilistic models; Variograms; Information retrieval","S. Sammartino; Dept. Ingeniería de Comunicaciones, E.T.S. Ingenieŕia de Telecomunicacíon, Universidad de Málaga, 29071, Málaga, Campus Universitario de Teatinos S/n, Spain; email: ssammartino@ic.uma.es","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Lagrange M.; Serrà J.","Lagrange, Mathieu (18042165200); Serrà, Joan (35749172500)","18042165200; 35749172500","Unsupervised accuracy improvement for cover song detection using spectral connectivity network","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859883644&partnerID=40&md5=2031e2fc302f57035943de13f2b8ac7c","IRCAM-CNRS UMR 9912, 75004 Paris, 1 place Igor Stravinsky, France; Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Roc Boronat 138, Spain","Lagrange M., IRCAM-CNRS UMR 9912, 75004 Paris, 1 place Igor Stravinsky, France; Serrà J., Music Technology Group, Universitat Pompeu Fabra, 08018 Barcelona, Roc Boronat 138, Spain","This paper introduces a new method for improving the accuracy in medium scale music similarity problems. Recently, it has been shown that the raw accuracy of query by example systems can be enhanced by considering priors about the distribution of its output or the structure of the music collection being considered. The proposed approach focuses on reducing the dependency to those priors by considering an eigenvalue decomposition of the aforementioned system's output. Experiments carried out in the framework of cover song detection show that the proposed approach has good performance for enhancing a high accuracy system. Furthermore, it maintains the accuracy level for lower performing systems. © 2010 International Society for Music Information Retrieval.","","Eigenvalues and eigenfunctions; Accuracy Improvement; Accuracy level; Cover songs; Eigenvalue decomposition; Medium-scale; Music collection; Music similarity; Query-by example; Information retrieval","M. Lagrange; IRCAM-CNRS UMR 9912, 75004 Paris, 1 place Igor Stravinsky, France; email: mathieu.lagrange@ircam.fr","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Hrybyk A.; Kim Y.","Hrybyk, Alex (55586721000); Kim, Youngmoo (24724623000)","55586721000; 24724623000","Combined audio and video analysis for guitar chord identification","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574298&partnerID=40&md5=ac5f5888add16d3866d6e75c1bbdde36","Department of Electrical and Computer Engineering, Drexel University, United States","Hrybyk A., Department of Electrical and Computer Engineering, Drexel University, United States; Kim Y., Department of Electrical and Computer Engineering, Drexel University, United States","This paper presents a multi-modal approach to automatically identifying guitar chords using audio and video of the performer. Chord identification is typically performed by analyzing the audio, using a chroma based feature to extract pitch class information, then identifying the chord with the appropriate label. Even if this method proves perfectly accurate, stringed instruments add extra ambiguity as a single chord or melody may be played in different positions on the fretboard. Preserving this information is important, because it signifies the original fingering, and implied ""easiest"" way to perform the selection. This chord identification system combines analysis of audio to determine the general chord scale (i.e. A major, G minor), and video of the guitarist to determine chord voicing (i.e. open, barred, inversion), to accurately identify the guitar chord. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Audio and video; Chord identification; Class information; Multi-modal approach; Musical instruments","A. Hrybyk; Department of Electrical and Computer Engineering, Drexel University, United States; email: ahrybyk@drexel.edu","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Kelly C.; Gainza M.; Dorran D.; Coyle E.","Kelly, Cillian (24381593800); Gainza, Mikel (6506332560); Dorran, David (6506979483); Coyle, Eugene (12345048800)","24381593800; 6506332560; 6506979483; 12345048800","Locating tune changes and providing a semantic labelling of sets of irish traditional tunes","2010","Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576596&partnerID=40&md5=276b1d4b885633ff27991518e4e4cd81","Audio Research Group, Dublin 8, DIT Kevin St., Ireland","Kelly C., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Gainza M., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Dorran D., Audio Research Group, Dublin 8, DIT Kevin St., Ireland; Coyle E., Audio Research Group, Dublin 8, DIT Kevin St., Ireland","An approach is presented which provides the tune change locations within a set of Irish Traditional tunes. Also provided are semantic labels for each part of each tune within the set. A set in Irish Traditional music is a number of individual tunes played segue. Each of the tunes in the set are made up of structural segments called parts. Musical variation is a prominent characteristic of this genre. However, a certain set of notes known as 'set accented tones' are considered impervious to musical variation. Chroma information is extracted at 'set accented tone' locations within the music. The resulting chroma vectors are grouped to represent the parts of the music. The parts are then compared with one another to form a part similarity matrix. Unit kernels which represent the possible structures of an Irish Traditional tune are matched with the part similarity matrix to determine the tune change locations and semantic part labels. © 2010 International Society for Music Information Retrieval.","","Information retrieval; Semantic labels; Semantic parts; Similarity matrix; Semantics","C. Kelly; Audio Research Group, Dublin 8, DIT Kevin St., Ireland; email: cillian.kelly@dit.ie","","11th International Society for Music Information Retrieval Conference, ISMIR 2010","9 August 2010 through 13 August 2010","Utrecht","95396"
"Degara-Quintela N.; Pena A.; Torres-Guijarro S.","Degara-Quintela, Norberto (6507227061); Pena, Antonio (7202521336); Torres-Guijarro, Soledad (6507680179)","6507227061; 7202521336; 6507680179","A comparison of score-level fusion rules for onset detection in music signals","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873591406&partnerID=40&md5=c586e95ca5889dd9e991dbcdf7155681","Department of Signal Theory and Communications, University of Vigo, Spain; Laboratorio Oficial de Metroloxía de Galicia (LOMG), Spain","Degara-Quintela N., Department of Signal Theory and Communications, University of Vigo, Spain; Pena A., Department of Signal Theory and Communications, University of Vigo, Spain; Torres-Guijarro S., Laboratorio Oficial de Metroloxía de Galicia (LOMG), Spain","Finding automatically the starting time of audio events is a difficult process. A promising approach for onset detection lies in the combination of multiple algorithms. The goal of this paper is to compare score-level fusion rules that combine signal processing algorithms in a problem of automatic detection of onsets. Previous approaches usually combine detection functions by adding these functions in the time domain. The combination methods explored in this work fuse, at score-level, the peak score information (peak time and onset probability) in order to obtain a better estimate of the probability of having an onset given the probability estimates of multiple experts. Three state-ofthe- art spectral-based onset detection functions are used: a spectral flux detection function, a weighted phase deviation function, and a complex domain detection function. Both untrained and trained fusion rules will be compared using a standard data set of music excerpts. © 2009 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Probability; Audio events; Automatic Detection; Combination method; Complex domains; Data set; Detection functions; Multiple algorithms; Music signals; Onset detection; Phase deviations; Probability estimate; Score-level fusion; Signal processing algorithms; Spectral flux; Starting time; Time domain; Trained fusion; Signal detection","N. Degara-Quintela; Department of Signal Theory and Communications, University of Vigo, Spain; email: ndegara@gts.tsc.uvigo.es","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Oudre L.; Grenier Y.; Févotte C.","Oudre, Laurent (35766614800); Grenier, Yves (55496021000); Févotte, Cédric (14031507800)","35766614800; 55496021000; 14031507800","Template-based chord recognition: Influence of the chord types","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873639352&partnerID=40&md5=6c09874c43fb4c99464dc869fd947dbf","Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; CNRS LTCI, TELECOM ParisTech, 75014 Paris, 37-39 rue Dareau, France","Oudre L., Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; Grenier Y., Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; Févotte C., CNRS LTCI, TELECOM ParisTech, 75014 Paris, 37-39 rue Dareau, France","This paper describes a fast and efficient template-based chord recognition method. We introduce three chord models taking into account one or more harmonics for the notes of the chord. The use of pre-determined chord models enables to consider several types of chords (major, minor, dominant seventh, minor seventh, augmented, diminished...). After extracting a chromagram from the signal, the detected chord over a frame is the one minimizing a measure of fit between the chromagramframe and the chord templates. Several popular measures in the probability and signal processing field are considered for our task. In order to take into account the time persistence, we perform a post-processing filtering over the recognition criteria. The transcription tool is evaluated on the 13 Beatles albums with different chord types and compared to state-of-theart chord recognition methods. We particularly focus on the influence of the chord types considered over the performances of the system. Experimental results show that our method outperforms the state-of-the-art and more importantly is less computationally demanding than the other evaluated systems. © 2009 International Society for Music Information Retrieval.","","Signal processing; Chord models; Chord recognition; Post processing; Signal processing fields; Template-based; Information retrieval","L. Oudre; Institut TELECOM, TELECOM ParisTech, CNRS LTCI, 75014 Paris, 37-39 rue Dareau, France; email: oudre@telecom-paristech.fr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Gross-Amblard D.; Rigaux P.; Abrouk L.; Cullot N.","Gross-Amblard, David (6506129961); Rigaux, Philippe (57204372163); Abrouk, Lylia (27567550300); Cullot, Nadine (56007405300)","6506129961; 57204372163; 27567550300; 56007405300","Fingering watermarking in symbolic digital scores","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649830&partnerID=40&md5=0f5e731991ea91d41ca21614d4ce6691","Le2i-CNRS Lab, Universit́e de Bourgogne, France; Lamsade-CNRS Lab, Universit́e de Dauphine, Paris IX, France","Gross-Amblard D., Le2i-CNRS Lab, Universit́e de Bourgogne, France; Rigaux P., Lamsade-CNRS Lab, Universit́e de Dauphine, Paris IX, France; Abrouk L., Le2i-CNRS Lab, Universit́e de Bourgogne, France; Cullot N., Le2i-CNRS Lab, Universit́e de Bourgogne, France","We propose a new watermarking method that hides the writer's identity into symbolic musical scores featuring fingering annotations. These annotations constitute a valuable part of the symbolic representation, yet they can be slightly modified without altering the quality of the musical information. The method applies a controlled distortion of the existing fingerings so that unauthorized copies can be identified. The proposed watermarkingmethod is robust against attacks like random fingering alterations and score cropping, and its detection does not require the original fingering, but only the suspect one. The method is general and applies to various fingering contexts and instruments. Keywords. Watermarking, fingering. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Digital score; Musical information; Musical score; Symbolic representation; Watermarking methods; Watermarking","D. Gross-Amblard; Le2i-CNRS Lab, Universit́e de Bourgogne, France; email: David.Gross-Amblard@u-bourgogne.fr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Heittola T.; Klapuri A.; Virtanen T.","Heittola, Toni (24474639900); Klapuri, Anssi (6602945099); Virtanen, Tuomas (35504028500)","24474639900; 6602945099; 35504028500","Musical instrument recognition in polyphonic audio using source-filter model for sound separation","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","102","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616077&partnerID=40&md5=02395740c38008640c5418a9ce499ca6","Deparment of Signal Processing, Tampere University of Technology, Finland","Heittola T., Deparment of Signal Processing, Tampere University of Technology, Finland; Klapuri A., Deparment of Signal Processing, Tampere University of Technology, Finland; Virtanen T., Deparment of Signal Processing, Tampere University of Technology, Finland","This paper proposes a novel approach to musical instrument recognition in polyphonic audio signals by using a source-filter model and an augmented non-negative matrix factorization algorithm for sound separation. The mixture signal is decomposed into a sum of spectral bases modeled as a product of excitations and filters. The excitations are restricted to harmonic spectra and their fundamental frequencies are estimated in advance using a multipitch estimator, whereas the filters are restricted to have smooth frequency responses by modeling them as a sum of elementary functions on the Mel-frequency scale. The pitch and timbre information are used in organizing individual notes into sound sources. In the recognition, Mel-frequency cepstral coefficients are used to represent the coarse shape of the power spectrum of sound sources and Gaussian mixture models are used to model instrument-conditional densities of the extracted features. The method is evaluated with polyphonic signals, randomly generated from 19 instrument classes. The recognition rate for signals having six note polyphony reaches 59%. © 2009 International Society for Music Information Retrieval.","","Frequency estimation; Frequency response; Information retrieval; Musical instruments; Separation; And filters; Audio signal; Elementary function; Fundamental frequencies; Gaussian Mixture Model; Harmonic spectrum; Mel-frequency cepstral coefficients; Mixture signals; Multi pitches; Musical instrument recognition; Non-negative matrix factorization algorithms; Polyphonic signals; Recognition rates; Sound separation; Sound source; Source-filter models; Source separation","T. Heittola; Deparment of Signal Processing, Tampere University of Technology, Finland; email: toni.heittola@tut.fi","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Grosche P.; Müller M.","Grosche, Peter (55413290700); Müller, Meinard (7404689873)","55413290700; 7404689873","A mid-level representation for capturing dominant tempo and pulse information in music recordings","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873626279&partnerID=40&md5=4ebef0063268a64ec91b2b732432890b","MPI Informatik, Saarland University, Saarbrücken, Germany","Grosche P., MPI Informatik, Saarland University, Saarbrücken, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany","Automated beat tracking and tempo estimation from music recordings become challenging tasks in the case of nonpercussive music with soft note onsets and time-varying tempo. In this paper, we introduce a novel mid-level representation which captures predominant local pulse information. To this end, we first derive a tempogram by performing a local spectral analysis on a previously extracted, possibly very noisy onset representation. From this, we derive for each time position the predominant tempo as well as a sinusoidal kernel that best explains the local periodic nature of the onset representation. Then, our main idea is to accumulate the local kernels over time yielding a single function that reveals the predominant local pulse (PLP). We show that this function constitutes a robust mid-level representation from which one can derive musically meaningful tempo and beat information for non-percussive music even in the presence of significant tempo fluctuations. Furthermore, our representation allows for incorporating prior knowledge on the expected tempo range to exhibit information on different pulse levels. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Spectrum analysis; Beat tracking; Incorporating prior knowledge; Local kernel; Local spectral analysis; Mid-level representation; Music recording; Tempo estimations; Time varying; Audio recordings","P. Grosche; MPI Informatik, Saarland University, Saarbrücken, Germany; email: pgrosche@mpi-inf.mpg.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Duan Z.; Han J.; Pardo B.","Duan, Zhiyao (24450312900); Han, Jinyu (55938782100); Pardo, Bryan (10242155400)","24450312900; 55938782100; 10242155400","Harmonically informed multi-pitch tracking","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052983176&partnerID=40&md5=d24843e3f1518db9d8a0153a9db6d5f4","Northwestern University, Evanston IL, United States","Duan Z., Northwestern University, Evanston IL, United States; Han J., Northwestern University, Evanston IL, United States; Pardo B., Northwestern University, Evanston IL, United States","This paper presents a novel system for multi-pitch tracking, i.e. estimate the pitch trajectory of each monophonic source in a mixture of harmonic sounds. The system consists of two stages: multi-pitch estimation and pitch trajectory formation. In the first stage, we propose a new approach based on modeling spectral peaks and non-peak regions to estimate pitches and polyphony in each single frame. In the second stage, we view the pitch trajectory formation problem as a constrained clustering problem of pitch estimates in all the frames. Constraints are imposed on some pairs of pitch estimates, according to time and frequency proximity. In clustering, harmonic structure is employed as the feature. The proposed system is tested on 10 recorded four-part J. S. Bach chorales. Both multi-pitch estimation and tracking results are very promising. In addition, for multi-pitch estimation, the proposed system is shown to outperform a state-of-the-art multi-pitch estimation approach. © 2009 International Society for Music Information Retrieval.","","Acoustic variables measurement; Harmonic analysis; Information retrieval; Trajectories; Constrained clustering; Harmonic structures; Multi pitches; Multi-pitch estimations; New approaches; Single frames; Spectral peak; Time and frequencies; Trajectory formation; Estimation","Z. Duan; Northwestern University, Evanston IL, United States; email: zhiyaoduan00@gmail.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Battenberg E.; Wessel D.","Battenberg, Eric (55567028300); Wessel, David (7005264358)","55567028300; 7005264358","Accelerating non-negative matrix factorization for audio source separation on multi-core and many-core architectures","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955589746&partnerID=40&md5=6bd1e50274b66d2961d781f2aa7a97d2","Parallel Computing Laboratory, University of California, Berkeley, United States; Center for New Music and Audio Technologies, University of California, Berkeley, United States","Battenberg E., Parallel Computing Laboratory, University of California, Berkeley, United States; Wessel D., Center for New Music and Audio Technologies, University of California, Berkeley, United States","Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multicore systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software. © 2009 International Society for Music Information Retrieval.","","Application programming interfaces (API); Factorization; Information retrieval; Iterative methods; Personal computers; Source separation; Audio source separation; Graphics processor; Many-core; Multi core and many cores; Multi-core systems; Music information retrieval; Nonnegative matrix factorization; Parallel implementations; Running time; Shared memories; Computer architecture","E. Battenberg; Parallel Computing Laboratory, University of California, Berkeley, United States; email: ericb@eecs.berkeley.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hamanaka M.; Tojo S.","Hamanaka, Masatoshi (35253968400); Tojo, Satoshi (7103319884)","35253968400; 7103319884","Interactive GTTM analyzer","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867621903&partnerID=40&md5=8ed3453e1840a1d0571196287ab7e7fe","University of Tsukuba, Japan; Japan Advanced Institute of Science and Technology, Japan","Hamanaka M., University of Tsukuba, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Japan","We describe an interactive analyzer for the generative theory of tonal music (GTTM). Generally, a piece of music has more than one interpretation, and dealing with such ambiguity is one of the major problems when constructing a music analysis system. To solve this problem, we propose an interactive GTTM analyzer, called an automatic time-span tree analyzer (ATTA), with a GTTM manual editor. The ATTA has adjustable parameters that enable the analyzer to generate multiple analysis results. As the ATTA cannot output all the analysis results that correspond to all the interpretations of a piece of music, we designed a GTTM manual editor, which generates all the analysis results. Experimental results showed that our interactive GTTM analyzer outperformed the GTTM manual editor without an ATTA. Since we hope to contribute to the research of music analysis, we publicize our interactive GTTM analyzer and a dataset of three hundred pairs of a score and analysis results by musicologist on our website http://music.iit.tsukuba.ac.jp/hamanaka/gttm.htm, which is the largest database of analyzed results from the GTTM to date. © 2009 International Society for Music Information Retrieval.","","Data processing; Adjustable parameters; Music analysis; Tonal music; Information retrieval","M. Hamanaka; University of Tsukuba, Japan; email: hamanaka@iit.tsukuba.ac.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Stolzenburg F.","Stolzenburg, Frieder (55887001400)","55887001400","A periodicity-based theory for harmony perception and scales","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871313430&partnerID=40&md5=a72b1868b2a0a825ce21f53f68d1d2d9","Automation and Computer Sciences Department, Hochschule Harz, 38855 Wernigerode, Germany","Stolzenburg F., Automation and Computer Sciences Department, Hochschule Harz, 38855 Wernigerode, Germany","Empirical results demonstrate, that human subjects rate harmonies, e.g. major and minor triads, differently with respect to their sonority. These judgements of listeners have a strong psychophysical basis. Therefore, harmony perception often is explained by the notions of dissonance and tension, computing the consonance of one or two intervals. In this paper, a theory on harmony perception based on the notion of periodicity is introduced. Mathematically, periodicity is derivable from the frequency ratios of the tones in the chord with respect to its lowest tone. The used ratios can be computed by continued fraction expansion and are psychophysically motivated by the just noticeable differences in pitch perception. The theoretical results presented here correlate well to experimental results and also explain the origin of complex chords and common musical scales. © 2009 International Society for Music Information Retrieval.","","Continued fraction expansion; Frequency ratios; Human subjects; Just-noticeable difference; Musical scale; Perception-based; Pitch perception; Psychophysical; Theoretical result; Information retrieval","F. Stolzenburg; Automation and Computer Sciences Department, Hochschule Harz, 38855 Wernigerode, Germany; email: fstolzenburg@hs-harz.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Izmirli Ö.","Izmirli, Özgür (6507754258)","6507754258","Tonal-atonal classification of music audio using diffusion maps","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873650382&partnerID=40&md5=9bf4a0b1a913ff5f5431291dce18c153","Computer Science Department, Center for Arts and Technology, Connecticut College, United States","Izmirli Ö., Computer Science Department, Center for Arts and Technology, Connecticut College, United States","In this paper we look at the problem of classifying music audio as tonal or atonal by learning a low-dimensional structure representing tonal relationships among keys. We use a training set composed of tonal pieces which includes all major and minor keys. A kernel eigenmap based method is used for structure learning and discovery. Specifically, a Diffusion Maps (DM) framework is used and its parameter tuning is discussed. Since these methods do not scale well with increasing data size, it becomes infeasible to use these methods in online applications. In order to facilitate on-line classification an outof- sample extension to the DM framework is given. The learned structure of tonal relationships is presented and a simple scheme for classification of tonal-atonal pieces is proposed. Evaluation results show that the method is able to perform at an accuracy above 90% with the current data set. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Current data; Data size; Diffusion maps; Evaluation results; Low-dimensional structures; On-line applications; On-line classification; Parameter-tuning; Simple schemes; Structure-learning; Training sets; Audio acoustics","Ö. Izmirli; Computer Science Department, Center for Arts and Technology, Connecticut College, United States; email: oizm@conncoll.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bay M.; Ehmann A.F.; Stephen Downie J.","Bay, Mert (56259607500); Ehmann, Andreas F. (8988651500); Stephen Downie, J. (7102932568)","56259607500; 8988651500; 7102932568","Evaluation of multiple-F0 estimation and tracking systems","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","114","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632675&partnerID=40&md5=0a869fe3508cd3cf9bae1e612949b34c","International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Bay M., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Multi-pitch estimation of sources in music is an ongoing research area that has a wealth of applications in music information retrieval systems. This paper presents the systematic evaluations of over a dozen competing methods and algorithms for extracting the fundamental frequencies of pitched sound sources in polyphonic music. The evaluations were carried out as part of the Music Information Retrieval Evaluation eXchange (MIREX) over the course of two years, from 2007 to 2008. The generation of the dataset and its corresponding ground-truth, the methods by which systems can be evaluated, and the evaluation results of the different systems are presented and discussed. © 2009 International Society for Music Information Retrieval.","","Evaluation results; Fundamental frequencies; Multi-pitch estimations; Multiple-F0 estimations; Music information retrieval; Polyphonic music; Sound source; Systematic evaluation; Information retrieval systems","M. Bay; International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; email: mertbay@illinois.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Serrà J.; Zanin M.; Laurier C.; Sordo M.","Serrà, Joan (35749172500); Zanin, Massimiliano (23991967500); Laurier, Cyril (26031025000); Sordo, Mohamed (43462170300)","35749172500; 23991967500; 26031025000; 43462170300","Unsupervised detection of cover song sets: Accuracy improvement and original identification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859911465&partnerID=40&md5=699e77b7b47758aca0de0637e7c017e7","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Universidad Autó noma de Madrid, Madrid, Spain","Serrà J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Zanin M., Universidad Autó noma de Madrid, Madrid, Spain; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","The task of identifying cover songs has formerly been studied in terms of a prototypical query retrieval framework. However, this framework is not the only one the task allows. In this article, we revise the task of identifying cover songs to include the notion of sets (or groups) of covers. In particular, we study the application of unsupervised clustering and community detection algorithms to detect cover sets. We consider current state-of-the-art algorithms and propose new methods to achieve this goal. Our experiments show that the detection of cover sets is feasible, that it can be performed in a reasonable amount of time, that it does not require extensive parameter tuning, and that it presents certain robustness to inaccurate measurements. Furthermore, we highlight two direct outcomes that naturally arise from the proposed framework revision: increasing the accuracy of query retrieval-based systems and detecting the original song within a set of covers. © 2009 International Society for Music Information Retrieval.","","Accuracy Improvement; Community detection algorithms; Cover songs; Original songs; Parameter-tuning; Query retrieval; State-of-the-art algorithms; Unsupervised clustering; Unsupervised detection; Information retrieval","J. Serrà; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: joan.serraj@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Niitsuma M.; Fujinami T.; Tomita Y.","Niitsuma, Masahiro (36638174100); Fujinami, Tsutomu (14831472200); Tomita, Yo (8535749100)","36638174100; 14831472200; 8535749100","The intersection of computational analysis and music manuscripts: A newmodel for bach source studies of the 21st century","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873635025&partnerID=40&md5=3560a36d1f356a888e957c86631ecb48","School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; Japan Advanced Institute of Science and Technology (JAIST), School of Knowledge Science, Japan","Niitsuma M., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; Fujinami T., Japan Advanced Institute of Science and Technology (JAIST), School of Knowledge Science, Japan; Tomita Y., School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom","This paper addresses the intersection of computational analysis and musicological source studies. In musicology, scholars often find themselves in the situation where their methodologies are inadequate to achieve their goals. Their problems appear to be twofold: (1) the lack of scientific objectivity and (2) the over-reliance on new source discoveries. We propose three stages to resolve these problems, a preliminary result of which is shown. The successful outcome of this work will have a huge impact not only on musicology but also on a wide range of subjects. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Computational analysis; New sources; Scientific objectivity; Computational methods","M. Niitsuma; School of Muisc and Sonic Arts, Queen's University, Belfast, United Kingdom; email: niizuma@nak.ics.keio.ac.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bohak C.; Marolt M.","Bohak, Ciril (55582372100); Marolt, Matija (6603601816)","55582372100; 6603601816","Calculating similarity of folk song variants with melody-based features","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865816822&partnerID=40&md5=5e84d2fe1bcb00c66a6e07b8c9c467a8","Faculty of Computer and Information Science, University of Ljubljana, Slovenia","Bohak C., Faculty of Computer and Information Science, University of Ljubljana, Slovenia; Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia","As folk songs live largely through oral transmission, there usually is no standard form of a song - each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melodybased folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set. © 2009 International Society for Music Information Retrieval.","","Calculating similarities; Data set; Folk songs; Measure of similarities; Melodic similarity; Similarity measure; Symbolic representation; Information retrieval","C. Bohak; Faculty of Computer and Information Science, University of Ljubljana, Slovenia; email: ciril.bohak@fri.uni-lj.si","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Knees P.; Pohle T.; Schedl M.; Schnitzer D.; Seyerlehner K.; Widmer G.","Knees, P. (8219023200); Pohle, T. (14036302300); Schedl, M. (8684865900); Schnitzer, D. (23996271700); Seyerlehner, K. (23996001400); Widmer, G. (7004342843)","8219023200; 14036302300; 8684865900; 23996271700; 23996001400; 7004342843","Augmenting text-based music retrieval with audio similarity","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863768356&partnerID=40&md5=1550062b1be15496e265251d48a324b3","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, OFAI, Vienna, Austria","Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Vienna, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Vienna, Austria","We investigate an approach to a music search engine that indexesmusic pieces based on relatedWeb documents. This allows for searching for relevant music pieces by issuing descriptive textual queries. In this paper, we examine the effects of incorporating audio-based similarity into the text-based ranking process - either by directly modifying the retrieval process or by performing post-hoc audiobased re-ranking of the search results. The aimof this combination is to improve ranking quality by including relevant tracks that are left out by text-based retrieval approaches. Our evaluations show overall improvements but also expose limitations of these unsupervised approaches to combining sources. Evaluations are carried out on two collections, one large real-world collection containing about 35,000 tracks and on the CAL500 set. © 2009 International Society for Music Information Retrieval.","","Audio acoustics; Search engines; Audio similarities; Audio-based; Music retrieval; Ranking process; Re-ranking; Real-world; Retrieval process; Search results; Text-based retrieval; Textual query; Unsupervised approaches; Information retrieval","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Benetos E.; Holzapfel A.; Stylianou Y.","Benetos, Emmanouil (16067946900); Holzapfel, André (18041818000); Stylianou, Yannis (6601991415)","16067946900; 18041818000; 6601991415","Pitched instrument onset detection based on auditory spectra","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668480&partnerID=40&md5=680d7df1d9104c32b8649d59220e213c","Institute of Computer Science, FORTH, Greece; Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece","Benetos E., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece; Holzapfel A., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, Greece, Computer Science Department, Multimedia Informatics Lab, University of Crete, Greece","In this paper, a novel method for onset detection of music signals using auditory spectra is proposed. The auditory spectrogram provides a time-frequency representation that employs a sound processing model resembling the human auditory system. Recent work on onset detection employs DFT-based features, such as the spectral flux and group delay function. The spectral flux and group delay are introduced in the auditory framework and an onset detection algorithm is proposed. Experiments are conducted on a dataset covering 11 pitched instrument types, consisting of 1829 onsets in total. Results indicate the superiority of the auditory representations over the DFT-based ones, with the auditory spectral flux exhibiting an onset detection improvement by 2% in terms of F-measure when compared to the DFT-based feature. © 2009 International Society for Music Information Retrieval.","","Audition; Group delay; Auditory representation; DFT-based; F-measure; Group delay functions; Human auditory system; Music signals; Onset detection; Sound processing; Spectral flux; Spectrograms; Time-frequency representations; Information retrieval","E. Benetos; Institute of Computer Science, FORTH, Greece; email: benetos@csd.uoc.gr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Koenigstein N.; Shavitt Y.","Koenigstein, Noam (26534475900); Shavitt, Yuval (7004130839)","26534475900; 7004130839","Song ranking based on piracy in peer-to-peer networks","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873618528&partnerID=40&md5=a70ca4551815245d194bdba24e7ba712","School of Electrical Engineering, Tel Aviv University, Israel","Koenigstein N., School of Electrical Engineering, Tel Aviv University, Israel; Shavitt Y., School of Electrical Engineering, Tel Aviv University, Israel","Music sales are loosing their role as a means for music dissemination but are still used by the music industry for ranking artist success, e.g., in the Billboard Magazine chart. Thus, it was suggested recently to use social networks as an alternative ranking system; a suggestion which is problematic due to the ease of manipulating the list and the difficulty of implementation. In this work we suggest to use logs of queries from peer-to-peer file-sharing systems for ranking song success. We show that the trend and fluctuations of the popularity of a song in the Billboard list have strong correlation (0.89) to the ones in a list built from the P2P network, and that the P2P list has a week advantage over the Billboard list. Namely, music sales are strongly correlated with music piracy. © 2009 International Society for Music Information Retrieval.","","Crime; Distributed computer systems; Information retrieval; Alternative ranking; Music industry; Music piracy; P2P network; Peer-to-peer file-sharing; Strong correlation; Use social networks; Peer to peer networks","N. Koenigstein; School of Electrical Engineering, Tel Aviv University, Israel; email: noamk@eng.tau.ac.il","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hamel P.; Wood S.; Eck D.","Hamel, Philippe (8402361900); Wood, Sean (57189456167); Eck, Douglas (12141444300)","8402361900; 57189456167; 12141444300","Automatic identification of instrument classes in polyphonic and poly-instrument audio","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649320&partnerID=40&md5=2addf1dc18c3c9b37570b16ff636f8a8","Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada","Hamel P., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada; Wood S., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada; Eck D., Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada","We present and compare several models for automatic identification of instrument classes in polyphonic and poly-instrument audio. The goal is to be able to identify which categories of instrument (Strings, Woodwind, Guitar, Piano, etc.) are present in a given audio example. We use a machine learning approach to solve this task. We constructed a system to generate a large database of musically relevant poly-instrument audio. Our database is generated from hundreds of instruments classified in 7 categories. Musical audio examples are generated by mixing multi-track MIDI files with thousands of instrument combinations. We compare three different classifiers : a Support Vector Machine (SVM), a Multilayer Perceptron (MLP) and a Deep Belief Network (DBN).We show that the DBN tends to outperform both the SVM and the MLP in most cases. © 2009 International Society for Music Information Retrieval.","","Audio acoustics; Automation; Information retrieval; Support vector machines; Automatic identification; Deep belief network (DBN); Large database; Machine learning approaches; MIDI files; Multi layer perceptron; Musical audio; Instruments","P. Hamel; Département d'informatique et de recherche opérationnelle, CIRMMT, Université de Montréal, Canada; email: hamelphi@iro.umontreal.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hirjee H.; Brown D.G.","Hirjee, Hussein (55586282300); Brown, Daniel G. (55738804200)","55586282300; 55738804200","Automatic detection of internal and imperfect rhymes in rap lyrics","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576245&partnerID=40&md5=1a37da783c8ce33e0d11e4d969e850ef","Cheriton School of Computer Science, University of Waterloo, Canada","Hirjee H., Cheriton School of Computer Science, University of Waterloo, Canada; Brown D.G., Cheriton School of Computer Science, University of Waterloo, Canada","Imperfect and internal rhymes are two important features in rap music often ignored in the music information retrieval community. We develop a method of scoring potential rhymes using a probabilistic model based on phoneme frequencies in rap lyrics. We use this scoring scheme to automatically identify internal and line-final rhymes in song lyrics and demonstrate the performance of this method compared to rules-based models. Higher level rhyme features are produced and used to compare rhyming styles in song lyrics from different genres, and for different rap artists. © 2009 International Society for Music Information Retrieval.","","Automatic Detection; Important features; Music information retrieval; Probabilistic models; RAP-MUSIC; Scoring schemes; Information retrieval","H. Hirjee; Cheriton School of Computer Science, University of Waterloo, Canada; email: hahirjee@uwaterloo.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bas De Haas W.; Rohrmeier M.; Veltkamp R.C.; Wiering F.","Bas De Haas, W. (51160955300); Rohrmeier, Martin (6507901506); Veltkamp, Remco C. (7003421646); Wiering, Frans (8976178100)","51160955300; 6507901506; 7003421646; 8976178100","Modeling harmonic similarity using a generative grammar of tonal harmony","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957957955&partnerID=40&md5=b033a39cf7b9009bd18bb3f3b899e628","Utrecht University, Netherlands; University of Cambridge, United Kingdom","Bas De Haas W., Utrecht University, Netherlands; Rohrmeier M., University of Cambridge, United Kingdom; Veltkamp R.C., Utrecht University, Netherlands; Wiering F., Utrecht University, Netherlands","In this paper we investigate a new approach to the similarity of tonal harmony. We create a fully functional remodeling of an earlier version of Rohrmeier's grammar of harmony. With this grammar an automatic harmonic analysis of a sequence of symbolic chord labels is obtained in the form of a parse tree. The harmonic similarity is determined by finding and examining the largest labeled common embeddable subtree (LLCES) of two parse trees. For the calculation of the LLCES a new O(min(n,m)nm) time algorithm is presented, where n and m are the sizes of the trees. For the analysis of the LLCES we propose six distance measures that exploit several structural characteristics of the Combined LLCES. We demonstrate in a retrieval experiment that at least one of these new methods significantly outperforms a baseline string matching approach and thereby show that using additional musical knowledge from music cognitive and music theoreticmodels actually helps improving retrieval performance. © 2009 International Society for Music Information Retrieval.","","Forestry; Information retrieval; Trees (mathematics); Distance measure; New approaches; Parse trees; Retrieval performance; String matching; Structural characteristics; Sub trees; Time algorithms; Formal languages","W. Bas De Haas; Utrecht University, Netherlands; email: Bas.deHaas@cs.uu.nl","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Fremerey C.; Clausen M.; Ewert S.; Müller M.","Fremerey, Christian (23396821400); Clausen, Michael (56225233200); Ewert, Sebastian (32667575400); Müller, Meinard (7404689873)","23396821400; 56225233200; 32667575400; 7404689873","Sheet music-audio identification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865827312&partnerID=40&md5=e20a0b1c87d86e2969fe247f163797a6","Computer Science III, Bonn University, Bonn, Germany; MPI Informatik, Saarland University, Saarbrücken, Germany","Fremerey C., Computer Science III, Bonn University, Bonn, Germany; Clausen M., Computer Science III, Bonn University, Bonn, Germany; Ewert S., Computer Science III, Bonn University, Bonn, Germany; Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany","In this paper, we introduce and discuss the task of sheet music-audio identification. Given a query consisting of a sequence of bars from a sheet music representation, the task is to find corresponding sections within an audio interpretation of the same piece. Two approaches are proposed: a semi-automatic approach using synchronization and a fully automatic approach using matching techniques. A workflow is described that allows for evaluating the matching approach using the results of the more reliable synchronization approach. This workflow makes it possible to handle even complex queries from orchestral scores. Furthermore, we present an evaluation procedure, where we investigate several matching parameters and tempo estimation strategies. Our experiments have been conducted on a dataset comprising pieces of various instrumentations and complexity. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automatic approaches; Complex queries; Matching parameters; Matching techniques; Music representation; Semi-automatics; Tempo estimations; Two Approaches; Audio acoustics","C. Fremerey; Computer Science III, Bonn University, Bonn, Germany; email: fremerey@cs.uni-bonn.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bade K.; Nürnberger A.; Stober S.; Garbers J.; Wiering F.","Bade, Korinna (23391704600); Nürnberger, Andreas (14027288100); Stober, Sebastian (14027561800); Garbers, Jörg (35107028500); Wiering, Frans (8976178100)","23391704600; 14027288100; 14027561800; 35107028500; 8976178100","Supporting folk-song research by automatic metric learning and ranking","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582087&partnerID=40&md5=5576478a2ef7526140250b4be0201cd0","Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Department of Information and Computing Sciences, Utrecht University, Netherlands","Bade K., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Nürnberger A., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; Garbers J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands","In folk song research, appropriate similarity measures can be of great help, e.g. for classification of new tunes. Several measures have been developed so far. However, a particular musicological way of classifying songs is usually not directly reflected by just a single one of these measures. We show how a weighted linear combination of different basic similarity measures can be automatically adapted to a specific retrieval task by learning this metric based on a special type of constraints. Further, we describe how these constraints are derived from information provided by experts. In experiments on a folk song database, we show that the proposed approach outperforms the underlying basic similarity measures and study the effect of different levels of adaptation on the performance of the retrieval system. © 2009 International Society for Music Information Retrieval.","","Folk songs; Metric learning; Retrieval systems; Similarity measure; Weighted linear combinations; Information retrieval","K. Bade; Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany; email: korinna.bade@ovgu.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Liem C.C.S.; Hanjalic A.","Liem, Cynthia C.S. (21733247100); Hanjalic, Alan (6701775211)","21733247100; 6701775211","Cover song retrieval: A comparative study of system component choices","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873639481&partnerID=40&md5=79b9a2a2b5842895698d1d28eea55326","Department of Mediamatics, Delft University of Technology, Netherlands","Liem C.C.S., Department of Mediamatics, Delft University of Technology, Netherlands; Hanjalic A., Department of Mediamatics, Delft University of Technology, Netherlands","The Cover Song Retrieval (CSR) problem has received considerable attention in the MIREX 2006-2008 evaluation sessions. While the reported performance figures provide a general idea about the strengths of the submitted systems, it is not clear what actually causes the reported performance of a certain system. In other words, the question arises whether some system component design choices are more critical for a system's performance results than others. In order to obtain a better understanding of the performance of current CSR approaches and to give recommendations for future research in the field of CSR, we designed and performed a comparative study involving system component design approaches from the best-performing systems in MIREX 2006 and 2007. The datasets used for evaluation were carefully chosen to cover the broad spectrum of the cover song domain, while still providing designated test cases. While the choice of the dissimilarity assessment method was found to cause the largest CSR performance boost and very good retrieval results were obtained on classical opus retrieval cases, results obtained on a new test case, involving recordings originating from different microphone sets, point out new challenges in optimizing the feature representation step. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Insecticides; Assessment methods; Broad spectrum; Comparative studies; Cover songs; Feature representation; System components; System's performance; System-component design; Test case; Audio recordings","C.C.S. Liem; Department of Mediamatics, Delft University of Technology, Netherlands; email: c.c.s.liem@tudelft.nl","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hsu C.-L.; Chen L.-Y.; Jang J.-S.R.; Li H.-J.","Hsu, Chao-Ling (47061226300); Chen, Liang-Yu (36633865100); Jang, Jyh-Shing Roger (7402965041); Li, Hsing-Ji (55587403000)","47061226300; 36633865100; 7402965041; 55587403000","Singing pitch extraction from monaural polyphonic songs by contextual audio modeling and singing harmonic enhancement","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867604169&partnerID=40&md5=3a1f99e5e62e7ab5246f256ff0109044","Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Taiwan","Hsu C.-L., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Chen L.-Y., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Jang J.-S.R., Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; Li H.-J., Innovative Digitech-Enabled Applications and Services Institute (IDEAS), Taiwan","This paper proposes a novel approach to extract the pitches of singing voices from monaural polyphonic songs. The hidden Markov model (HMM) is adopted to model the transition between adjacent singing pitches in time, and the relationships between melody and its chord, which is implicitly represented by features extracted from the spectrum. Moreover, another set of features which represents the energy distribution of the enhanced singing harmonic structure is proposed by applying a normalized sub-harmonic summation technique. By using these two feature sets with complementary characteristics, a 2- stream HMM is constructed for singing pitch extraction. Quantitative evaluation shows that the proposed system outperforms the compared approaches for singing pitch extraction from polyphonic songs. © 2009 International Society for Music Information Retrieval.","","Harmonic analysis; Hidden Markov models; Information retrieval; Audio modeling; Complementary characteristics; Energy distributions; Feature sets; Harmonic enhancement; Harmonic structures; Hidden Markov model(HMM); Polyphonic songs; Quantitative evaluation; Singing pitch extractions; Singing voices; Sub-harmonic summations; Extraction","C.-L. Hsu; Dept. of Computer Science, MediaTek-NTHU Joint Lab, National Tsing Hua Univ., Taiwan; email: leon@mirlab.org","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Diakopoulos D.; Vallis O.; Hochenbaum J.; Murphy J.; Kapur A.","Diakopoulos, Dimitri (54941324500); Vallis, Owen (26422663500); Hochenbaum, Jordan (26430650500); Murphy, Jim (54938514900); Kapur, Ajay (13609187300)","54941324500; 26422663500; 26430650500; 54938514900; 13609187300","21ST century electronica: MIR techniques for classification and performance","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053113331&partnerID=40&md5=47909180c69f5c63e227e302b74aa067","California Institute of Arts, Valencia, CA, United States; New Zealand School of Music, Wellington, New Zealand","Diakopoulos D., California Institute of Arts, Valencia, CA, United States; Vallis O., New Zealand School of Music, Wellington, New Zealand; Hochenbaum J., New Zealand School of Music, Wellington, New Zealand; Murphy J., California Institute of Arts, Valencia, CA, United States; Kapur A., New Zealand School of Music, Wellington, New Zealand","The performance of electronica by Disc Jockys (DJs) presents a unique opportunity to develop interactions between performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classification of electronica is explored. Our research also introduces the use of a multitouch interface to drive a performance-oriented DJ application utilizing the feature set. Furthermore, we present that a multi-touch surface provides an extensible and collaborative interface for browsing and manipulating MIRrelated data in real time. © 2009 International Society for Music Information Retrieval.","Dj; Electronic dance music; Electronica; Genre classification; Multi-touch; User interfaces","Digital storage; Information retrieval; User interfaces; Dj; Electronic dance music; Electronica; Genre classification; Multi-touch; Electronic musical instruments","D. Diakopoulos; California Institute of Arts, Valencia, CA, United States; email: ddiakopoulos@alum.calarts.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Gruhne M.; Dittmar C.; Gaertner D.","Gruhne, Matthias (24461733300); Dittmar, Christian (15051598000); Gaertner, Daniel (55362680800)","24461733300; 15051598000; 55362680800","Improving rhythmic similarity computation by beat histogram transformations","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866006511&partnerID=40&md5=d844bc60d3042f4a41b71ab9385723c6","Bach Technology AS, Germany; Fraunhofer IDMT, Germany","Gruhne M., Bach Technology AS, Germany; Dittmar C., Fraunhofer IDMT, Germany; Gaertner D., Fraunhofer IDMT, Germany","Rhythmic descriptors are often utilized for semantic music classification, such as genre recognition or tempo detection. Several algorithms dealing with the extraction of rhythmic information from music signals were proposed in literature. Most of them derive a so-called beat histogram by auto-correlating a representation of the temporal envelope of the music signal. To circumvent the problem of tempo dependency, post-processing via higher-order statistics has been reported. Tests concluded, that these statistics are still tempo dependent to a certain extent. This paper describes a method, which transforms the original auto-correlated envelope into a tempo-independent rhythmic feature vector by multiplying the lag-axis with a stretch factor. This factor is computed with a new correlation technique which works in the logarithmic domain. The proposed method is evaluated for rhythmic similarity, consisting of two tasks: One test with manually created rhythms as proof of concept and another test using a large realworld music archive. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Semantics; Descriptors; Higher-order statistics; Logarithmic domain; Music classification; Music signals; New correlations; Post processing; Proof of concept; Real-world; Rhythmic features; Similarity computation; Stretch factor; Temporal envelopes; Graphic methods","M. Gruhne; Bach Technology AS, Germany; email: ghe@bachtechnology.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Schnitzer D.; Flexer A.; Widmer G.","Schnitzer, Dominik (23996271700); Flexer, Arthur (7004555682); Widmer, Gerhard (7004342843)","23996271700; 7004555682; 7004342843","A filter-and-refine indexing method for fast similarity search in millions of music tracks","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869799881&partnerID=40&md5=847ebe1fbd6f6a862f84a1c1a0028202","Austrian Research Institute for Artificial Intelligence, Austria; Johannes Kepler University, Linz, Austria","Schnitzer D., Austrian Research Institute for Artificial Intelligence, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence, Austria; Widmer G., Johannes Kepler University, Linz, Austria","We present a filter-and-refine method to speed up acoustic audio similarity queries which use the Kullback-Leibler divergence as similarity measure. The proposed method rescales the divergence and uses a modified FastMap [1] implementation to accelerate nearest-neighbor queries. The search for similar music pieces is accelerated by a factor of 10-30 compared to a linear scan but still offers high recall values (relative to a linear scan) of 95 - 99%. We show how the proposed method can be used to query several million songs for their acoustic neighbors very fast while producing almost the same results that a linear scan over the whole database would return. We present a working prototype implementation which is able to process similarity queries on a 2.5 million songs collection in about half a second on a standard CPU. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Query processing; Audio similarities; FastMap; Indexing methods; Kullback Leibler divergence; Nearest-neighbor query; Process similarities; Prototype implementations; Similarity measure; Similarity search; Speed up; Audio acoustics","D. Schnitzer; Austrian Research Institute for Artificial Intelligence, Austria; email: dominik.schnitzer@ofai.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Orio N.; Rodà A.","Orio, Nicola (6507928255); Rodà, Antonio (7103409569)","6507928255; 7103409569","A measure of melodic similarity based on a graph representation of the music structure","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866726570&partnerID=40&md5=39bec8958ef6c588a27bcfe9cb62075a","Department of Information Engineering, University of Padova, Italy; Lab. AVIRES, University of Udine, Italy","Orio N., Department of Information Engineering, University of Padova, Italy; Rodà A., Lab. AVIRES, University of Udine, Italy","Content-based music retrieval requires to define a similarity measure between music documents. In this paper, we propose a novel similarity measure between melodic content, as represented in symbolic notation, that takes into account musicological aspects on the structural function of the melodic elements. The approach is based on the representation of a collection of music scores with a graph structure, where terminal nodes directly describe the music content, internal nodes represent its incremental generalization, and arcs denote the relationships among them. The similarity between two melodies can be computed by analyzing the graph structure and finding the shortest path between the corresponding nodes inside the graph. Preliminary results in terms of music similarity are presented using a small test collection. © 2009 International Society for Music Information Retrieval.","","Graphic methods; Information retrieval; Content-based music retrieval; Graph representation; Graph structures; Internal nodes; Melodic similarity; Music contents; Music scores; Music similarity; Music structures; Shortest path; Similarity measure; Structural function; Terminal nodes; Test Collection; Graph theory","N. Orio; Department of Information Engineering, University of Padova, Italy; email: nicola.orio@dei.unipd.it","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Montecchio N.; Orio N.","Montecchio, Nicola (25929429200); Orio, Nicola (6507928255)","25929429200; 6507928255","A discrete filter bank approach to audio to score matching for polyphonic music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661147&partnerID=40&md5=974659cbbfe05b94463d2df938f63eff","Department of Information Engineering, University of Padova, Italy","Montecchio N., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","This paper presents a system for tracking the position of a polyphonic music performance in a symbolic score, possibly in real time. The system, based on Hidden Markov Models, is briefly presented, focusing on specific aspects such as observation modeling based on discrete filterbanks, in contrast with traditional FFT-based approaches, and describing the approaches to decoding. Experimental results are provided to assess the validity of the presented model. Proof-of-concept applications are shown, which effectively employ the described approach beyond the traditional automatic accompaniment system. © 2009 International Society for Music Information Retrieval.","","Filter banks; Hidden Markov models; Information retrieval; Automatic accompaniments; Discrete filter; Polyphonic music; Proof of concept; Real time; Score matching; Audio acoustics","N. Montecchio; Department of Information Engineering, University of Padova, Italy; email: nicola.montecchio@dei.unipd.it","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Chordia P.; Rae A.","Chordia, Parag (24723695700); Rae, Alex (24725486000)","24723695700; 24725486000","Using source separation to improve tempo detection","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443702&partnerID=40&md5=204c65f189b13b6e90cf1c0985c880a6","Dept. of Music, Georgia Tech Center for Music Technology, GTCMT, United States","Chordia P., Dept. of Music, Georgia Tech Center for Music Technology, GTCMT, United States; Rae A., Dept. of Music, Georgia Tech Center for Music Technology, GTCMT, United States","We describe a novel tempo estimation method based on decomposing musical audio into sources using principal latent component analysis (PLCA). The approach is motivated by the observation that in rhythmically complex music, some layers may be more rhythmically regular than the overall mix, thus facilitating tempo detection. Each excerpt was analyzed using PLCA and the resulting components were each tempo tracked using a standard autocorrelationbased algorithm. We describe several techniques for aggregating or choosing among the multiple estimates that result from this process to extract a global tempo estimate. The system was evaluated on the MIREX 2006 training database as well as a newly constructed database of rhythmically complex electronic music consisting of 27 examples (IDM DB). For these databases the algorithms improved accuracy by 10% (60% vs 50%) and 22.3% (48.2% vs. 25.9%) respectively. These preliminary results suggest that for some types of music, source-separation may lead to better tempo detection. © 2009 International Society for Music Information Retrieval.","","Algorithms; Audio acoustics; Computer music; Database systems; Estimation; Information retrieval; Source separation; Autocorrelation-based algorithms; Electronic music; Latent component analysis; Multiple estimates; Musical audio; Tempo estimations; Training database; Chemical detection","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Robine M.; Hanna P.; Lagrange M.","Robine, Matthias (24559139300); Hanna, Pierre (23134483000); Lagrange, Mathieu (18042165200)","24559139300; 23134483000; 18042165200","Meter class profiles for music similarity and retrieval","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873647892&partnerID=40&md5=527a42b3f4a69d39dfcd44ca282fd2b3","LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; Telecom ParisTech, 75634 Paris Cedex 13, 46, rue Barrault, France","Robine M., LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; Lagrange M., Telecom ParisTech, 75634 Paris Cedex 13, 46, rue Barrault, France","Rhythm is one of the main properties of Western tonal music. Existing content-based retrieval systems generally deal with melody or style. A few existing ones based on meter or rhythm characteristics have been recently proposed but they require a precise analysis, or they rely on a low-level descriptor. In this paper, we propose a midlevel descriptor: the Meter Class Profile (MCP). The MCP is centered on the tempo and represents the strength of beat multiples, including the measure rate, and the beat subdivisions. TheMCP coefficients are estimated by means of the autocorrelation and the Fourier transform of the onset detection curve. Experiments on synthetic and real databases are presented, and the results demonstrate the efficacy of the MCP descriptor in clustering and retrieval of songs according to their metric properties. © 2009 International Society for Music Information Retrieval.","","Content based retrieval; Descriptors; Metric properties; Music similarity; Onset detection; Precise analysis; Real database; Tonal music; Information retrieval","M. Robine; LaBRI, University of Bordeaux, 33405 Talence cedex, 351, cours de la Libération, France; email: Matthias.Robine@labri.fr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Santoro C.A.; Cheng C.I.","Santoro, Christopher A. (55587939900); Cheng, Corey I. (7404796980)","55587939900; 7404796980","Multiple F0 estimation in the transform domain","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632329&partnerID=40&md5=9ba21f5159c27500274c22afd7864228","LSB Audio, Tampa, FL 33610, United States; Frost School of Music, Music Engineering Technology, University of Miami, Coral Gables, FL 33124, United States; Department of Electrical and Computer Engineering, University of Miami, United States","Santoro C.A., LSB Audio, Tampa, FL 33610, United States, Frost School of Music, Music Engineering Technology, University of Miami, Coral Gables, FL 33124, United States; Cheng C.I., Frost School of Music, Music Engineering Technology, University of Miami, Coral Gables, FL 33124, United States, Department of Electrical and Computer Engineering, University of Miami, United States","A novel algorithm is proposed to estimate the fundamental frequencies present in polyphonic acoustic mixtures expressed in a transform domain. As an example, the algorithm operates on Modified Discrete Cosine Transform (MDCT) coefficients in order to demonstrate the utility of the method in commercially available perceptual audio codecs which use the MDCT. An auditory model is developed along with several optimizations that deal with the constraints of processing in the transform-domain, including an interpolation method, a transform-domain half-wave rectification model, tonal component estimation, and sparse convolution. Test results are separated by instrument and analyzed in detail. The proposed algorithm is shown to perform comparably to state of the art time-domain methods. © 2009 International Society for Music Information Retrieval.","","Algorithms; Discrete cosine transforms; Information retrieval; Time domain analysis; A-transform; Acoustic mixtures; Audio codecs; Auditory models; Component estimation; Fundamental frequencies; Half-wave; Interpolation method; Modified discrete cosine transforms; Multiple-F0 estimations; Novel algorithm; State of the art; Time-domain methods; Transform domain; Estimation","C.A. Santoro; LSB Audio, Tampa, FL 33610, United States; email: chris@lsbaudio.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Kobayashi Y.","Kobayashi, Yoshiyuki (55588529200)","55588529200","Automatic generation of musical instrument detector by using evolutionary learning method","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868666230&partnerID=40&md5=e311cc697ccfd9fb8178176ac40e0ed6","SONY Corporation, Japan","Kobayashi Y., SONY Corporation, Japan","This paper presents a novel way of generating information extractors that obtain high-level information from recorded music such as the presence of a certain musical instrument. Our information extractor is comprised of a feature set and a discrimination or regression formula. We introduce a scheme to generate the entire information extractor given only a large amount of labeled dataset. For example, data could be waveform, and label could be the presence of musical instruments in them. We propose a very flexible description of features that allows various kinds of data other than waveform. Our proposal also includes a modified evolutionary learning method to optimize the feature set. We applied our scheme to automatically generate musical instrument detectors for mixed-down music in stereo. The experiment showed that our scheme could find a suitable set of features for the objective and could generate good detectors. © 2009 International Society for Music Information Retrieval.","","Detectors; Information retrieval; Learning systems; Automatic Generation; Evolutionary Learning; Feature sets; High-level information; Labeled dataset; Large amounts; Regression formulas; Wave forms; Musical instruments","Y. Kobayashi; SONY Corporation, Japan; email: Yoshiyuki.Kobayashi@jp.sony.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Martin B.; Robine M.; Hanna P.","Martin, Benjamin (57198809463); Robine, Matthias (24559139300); Hanna, Pierre (23134483000)","57198809463; 24559139300; 23134483000","Musical structure retrieval by aligning self-similarity matrices","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960695452&partnerID=40&md5=ca823a4f4d46270d9d36cef7c3de84c3","LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France","Martin B., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France; Robine M., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France; Hanna P., LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France","We propose a new retrieval system based on musical structure using symbolic structural queries. The aim is to compare musical form in audio files without extracting explicitly the underlying audio structure. From a given or arbitrary segmentation, an audio file is segmented. Irrespective of the audio feature choice, we then compute a selfsimilarity matrix whose coefficients correspond to the estimation of the similarity between entire parts, obtained by local alignment. Finally, we compute a binary matrix from the symbolic structural query and compare it to the audio segmented matrix, which provides a structural similarity score. We perform experiments using large databases of audio files, and prove robustness to possible imprecisions in the structural query. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Audio files; Binary matrix; Large database; Local alignment; Musical structures; Retrieval systems; Self-similarities; Self-similarity matrix; Structural query; Structural similarity; Audio acoustics","B. Martin; LaBRI, University of Bordeaux, 33405 Talence Cedex, 351, cours de la Libération, France; email: Benjamin.Martin@labri.fr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Xu X.; Naito M.; Kato T.; Kawai H.","Xu, Xin (55587695300); Naito, Masaki (7403261784); Kato, Tsuneo (7405274365); Kawai, Hisashi (35179632500)","55587695300; 7403261784; 7405274365; 35179632500","Robust and fast lyric search based on phonetic confusion matrix","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873622094&partnerID=40&md5=d24c4bd3657c922034f82068b20b6f37","KDDI R and D Laboratories, Inc., Japan; National Institute of Information and Communications Technology, Japan","Xu X., KDDI R and D Laboratories, Inc., Japan; Naito M., KDDI R and D Laboratories, Inc., Japan; Kato T., KDDI R and D Laboratories, Inc., Japan; Kawai H., National Institute of Information and Communications Technology, Japan","This paper proposes a robust and fast lyric search method for music information retrieval. Current lyric search systems by normal text retrieval techniques are severely deteriorated in the case that the queries of lyric phrases contain incorrect parts due to mishearing and misremembering. To solve this problem, the authors apply acoustic distance, which is computed based on a confusion matrix of an ASR experiment, into DP-based phonetic string matching. The experimental results show that the search accuracy is increased by more than 40% compared with the normal text retrieval method; and by 2% ∼4% compared with the conventional phonetic string matching method. Considering the high computation complexity of DP matching, the authors propose a novel two-pass search strategy to shorten the processing time. By pre-selecting the probable candidates by a rapid index-based search for the first pass and executing a DP-based search among these candidates during the second pass, the proposed method reduces processing time by 85.8% and keeps search accuracy at the same level as that of a complete search by DP matching with all lyrics. © 2009 International Society for Music Information Retrieval.","","Image resolution; Information retrieval; Acoustic distance; Complete search; Computation complexity; Confusion matrices; DP matching; Lyric searches; Music information retrieval; Phonetic confusions; Processing time; Search accuracy; Search strategies; String matching; Text retrieval; Text retrieval methods; Linguistics","X. Xu; KDDI R and D Laboratories, Inc., Japan; email: sh-jo@kddilabs.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Abeßer J.; Lukashevich H.; Dittmar C.; Schuller G.","Abeßer, Jakob (36607532300); Lukashevich, Hanna (6508121862); Dittmar, Christian (15051598000); Schuller, Gerald (7005659200)","36607532300; 6508121862; 15051598000; 7005659200","Genre classification using bass-related high-level features and playing styles","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053107868&partnerID=40&md5=101444f3498cf5f129e943130b3dd2f6","Fraunhofer IDMT, Germany","Abeßer J., Fraunhofer IDMT, Germany; Lukashevich H., Fraunhofer IDMT, Germany; Dittmar C., Fraunhofer IDMT, Germany; Schuller G., Fraunhofer IDMT, Germany","Considering itsmediation role between the poles of rhythm, harmony, and melody, the bass plays a crucial role in most music genres. This paper introduces a novel set of transcription-based high-level features that characterize the bass and its interactionwith other participating instruments. Furthermore, a new method to model and automatically retrieve different genre-specific bass playing styles is presented. A genre classification task is used as benchmark to compare common machine learning algorithms based on the presented high-level features with a classification algorithm solely based on detected bass playing styles. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Classification algorithm; Genre classification; High-level features; Music genre; Playing style; Learning algorithms","J. Abeßer; Fraunhofer IDMT, Germany; email: abr@idmt.fraunhofer.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Seyerlehner K.; Knees P.; Schnitzer D.; Widmer G.","Seyerlehner, Klaus (23996001400); Knees, Peter (8219023200); Schnitzer, Dominik (23996271700); Widmer, Gerhard (7004342843)","23996001400; 8219023200; 23996271700; 7004342843","Browsing music recommendation networks","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873649217&partnerID=40&md5=8a41a7e1f5bd372b7c96c5abc9c0e73a","Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for AI, Vienna, Austria","Seyerlehner K., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Austrian Research Institute for AI, Vienna, Austria; Widmer G., Austrian Research Institute for AI, Vienna, Austria","Many music portals offer the possibility to explore music collections via browsing automatically generated music recommendations. In this paper we argue that such music recommender systems can be transformed into an equivalent recommendation graph. We then analyze the recommendation graph of a real-world content-based music recommender systems to find out if users can really explore the underlying song database by following those recommendations. We find that some songs are not recommended at all and are consequently not reachable via browsing. We then take a first attempt to modify a recommendation network in such a way that the resulting network is better suited to explore the respective music space. © 2009 International Society for Music Information Retrieval.","","Automatically generated; Content-based; Music collection; Music recommendation; Music recommender systems; Real-world; Recommender systems","K. Seyerlehner; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; email: klaus.seyerlehner@jku.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Rafii Z.; Pardo B.","Rafii, Zafar (44461970600); Pardo, Bryan (10242155400)","44461970600; 10242155400","Learning to control a reverberator using subjective perceptual descriptors","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873664134&partnerID=40&md5=08322b3fdd56acf093cf204da62c8552","EECS Department, Northwestern University, Evanston IL, United States","Rafii Z., EECS Department, Northwestern University, Evanston IL, United States; Pardo B., EECS Department, Northwestern University, Evanston IL, United States","The complexity of existing tools for mastering audio can be daunting. Moreover, many people think about sound in individualistic terms (such as ""boomy"") that may not have clear mappings onto the controls of existing audio tools. We propose learning to map subjective audio descriptors, such as ""boomy"", onto measures of signal properties in order to build a simple controller that manipulates an audio reverberator in terms of a chosen descriptor. For example, ""make the sound less boomy"". In the learning process, a user is presented with a series of sounds altered in different ways by a reverberator and asked to rate how well each sound represents the audio concept. The system correlates these ratings with reverberator parameters to build a controller that manipulates reverberation in the user's terms. In this paper, we focus on developing the mapping between reverberator controls, measures of qualities of reverberation and user ratings. Results on 22 subjects show the system learns quickly (under 3 minutes of training per concept), predicts users responses well (mean correlation coefficient of system predictiveness 0.75) and meets users' expectations (average human rating of 7.4 out of 10). © 2009 International Society for Music Information Retrieval.","","Audio acoustics; Controllers; Information retrieval; Quality control; Audio tools; Correlation coefficient; Descriptors; Learning process; Signal properties; User rating; Reverberation","Z. Rafii; EECS Department, Northwestern University, Evanston IL, United States; email: ZafarRafii2011@u.northwestern.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bischoff K.; Firan C.S.; Paiu R.; Nejdl W.; Laurier C.; Sordo M.","Bischoff, Kerstin (23391844100); Firan, Claudiu S. (18233545800); Paiu, Raluca (8876187900); Nejdl, Wolfgang (57204338128); Laurier, Cyril (26031025000); Sordo, Mohamed (43462170300)","23391844100; 18233545800; 8876187900; 57204338128; 26031025000; 43462170300","Music mood and theme classification - A hybrid approach","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","60","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862927717&partnerID=40&md5=20eb93dbd217ac921da106e24d0ad4d1","L3S Research Center, Hannover, Appelstr. 4, Germany; Music Technology Group, Universitat Pompeu Fabra, Spain","Bischoff K., L3S Research Center, Hannover, Appelstr. 4, Germany; Firan C.S., L3S Research Center, Hannover, Appelstr. 4, Germany; Paiu R., L3S Research Center, Hannover, Appelstr. 4, Germany; Nejdl W., L3S Research Center, Hannover, Appelstr. 4, Germany; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Spain","Music perception is highly intertwined with both emotions and context. Not surprisingly, many of the users' information seeking actions aim at retrieving music songs based on these perceptual dimensions - moods and themes, expressing how people feel about music or which situations they associate it with. In order to successfully support music retrieval along these dimensions, powerful methods are needed. Still, most existing approaches aiming at inferring some of the songs' latent characteristics focus on identifying musical genres. In this paper we aim at bridging this gap between users' information needs and indexed music features by developing algorithms for classifying music songs by moods and themes. We extend existing approaches by also considering the songs' thematic dimensions and by using social data from the Last.fm music portal, as support for the classification tasks. Our methods exploit both audio features and collaborative user annotations, fusing them to improve overall performance. Evaluation performed against the AllMusic.com ground truth shows that both kinds of information are complementary and should be merged for enhanced classification accuracy. © 2009 International Society for Music Information Retrieval.","","Behavioral research; Information retrieval; Audio features; Classification accuracy; Classification tasks; Collaborative users; Ground truth; Hybrid approach; Information need; Information seeking; Last.fm; Music perception; Music retrieval; Musical genre; Perceptual dimensions; Social datum; Classification (of information)","K. Bischoff; L3S Research Center, Hannover, Appelstr. 4, Germany; email: bischoff@L3S.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hankinson A.; Pugin L.; Fujinaga I.","Hankinson, Andrew (54970482500); Pugin, Laurent (23009752900); Fujinaga, Ichiro (9038140900)","54970482500; 23009752900; 9038140900","Interfaces for document representation in digital music libraries","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856807509&partnerID=40&md5=b2506d09669413f65c7a2066816d2663","Schulich School of Music, McGill University, Montréal, QC, Canada; RISM, Bern, Switzerland","Hankinson A., Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., RISM, Bern, Switzerland; Fujinaga I., Schulich School of Music, McGill University, Montréal, QC, Canada","Musical documents, that is, documents whose primary content is printed music, introduce interesting design challenges for presentation in an online environment. Considerations for the unique properties of printed msic, as well as users' expected levels of comfort with these materials, present opportunities for developing a viewer specifically tailored to displaying musical documents. This paper outlines five design considerations for a music document viewer, drawing examples from existing digital music libraries. We then present our work towards incorporating these considerations in a new digital music library system currently under development. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Libraries; Design challenges; Design considerations; Digital music libraries; Document Representation; Online environments; Digital libraries","A. Hankinson; Schulich School of Music, McGill University, Montréal, QC, Canada; email: ahankinson@music.mcgill.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Reed J.T.; Ueda Y.; Siniscalchi S.; Uchiyama Y.; Sagayama S.; Lee C.-H.","Reed, J.T. (34969598900); Ueda, Yushi (36609438200); Siniscalchi, S. (14007936900); Uchiyama, Yuki (35754147000); Sagayama, Shigeki (7004859104); Lee, C.-H. (7410147008)","34969598900; 36609438200; 14007936900; 35754147000; 7004859104; 7410147008","Minimum classification error training to improve isolated chord recognition","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445119&partnerID=40&md5=c1083245d0f2b017fb683f8a4acea026","Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States; Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Department of Electronics and Telecommunications, Norwegian University of Science and Technology, Trondheim, Norway","Reed J.T., Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States; Ueda Y., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Siniscalchi S., Department of Electronics and Telecommunications, Norwegian University of Science and Technology, Trondheim, Norway; Uchiyama Y., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Hongo, Bunkyo-ku, Tokyo 113-8656, Japan; Lee C.-H., Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States","Audio chord detection is the combination of two separate tasks: recognizing what chords are played and determining when chords are played. Most current audio chord detection algorithms use hidden Markov model (HMM) classifiers because of the task similarity with automatic speech recognition. For most speech recognition algorithms, the performance is measured by word error rate; i.e., only the identity of recognized segments is considered because word boundaries in continuous speech are often ambiguous. In contrast, audio chord detection performance is typically measured in terms of frame error rate, which considers both timing and classification. This paper treats these two tasks separately and focuses on the first problem; i.e., classifying the correct chords given boundary information. The best performing chroma/HMM chord detection algorithm, as measured in the 2008 MIREX Audio Chord Detection Contest, is used as the baseline in this paper. Further improvements are made to reduce feature correlation, account for differences in tuning, and incorporate minimum classification error (MCE) training in obtaining chord HMMs. Experiments demonstrate that classification rates can be improved with tuning compensation and MCE discriminative training. © 2009 International Society for Music Information Retrieval.","","Hidden Markov models; Information retrieval; Signal detection; Speech processing; Automatic speech recognition; Boundary information; Chord recognition; Classification rates; Continuous speech; Detection algorithm; Detection performance; Discriminative training; Feature correlation; Frame error rate; Hidden Markov model(HMM); Minimum classification error; Minimum classification error training; Recognition algorithm; Word error rate; Speech recognition","J.T. Reed; Georgia Institute of Technology, School of Electrical and Computer Engineering, Atlanta, GA 30332, United States; email: jreed@ece.gatech.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Khadkevich M.; Omologo M.","Khadkevich, Maksim (49061151500); Omologo, Maurizio (6701724204)","49061151500; 6701724204","Use of hidden markov models and factored language models for automatic chord recognition","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873631003&partnerID=40&md5=119b0d25c20b74c5c188fdd4860aaa4a","FBK-irst, Universitá degli studi di Trento, Povo - 38050, Trento, Via Sommarive, 14, Italy; Fondazione Bruno Kessler-irst, Povo - 38050 Trento, Via Sommarive, 18, Italy","Khadkevich M., FBK-irst, Universitá degli studi di Trento, Povo - 38050, Trento, Via Sommarive, 14, Italy; Omologo M., Fondazione Bruno Kessler-irst, Povo - 38050 Trento, Via Sommarive, 18, Italy","This paper focuses on automatic extraction of acoustic chord sequences from a musical piece. Standard and factored language models are analyzed in terms of applicability to the chord recognition task. Pitch class profile vectors that represent harmonic information are extracted from the given audio signal. The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring, applying the language model weight. We performed several experiments using the proposed technique. Results obtained on 175 manually-labeled songs provided an increase in accuracy of about 2%. © 2009 International Society for Music Information Retrieval.","","Computer music; Hidden Markov models; Information retrieval; Viterbi algorithm; Audio signal; Automatic extraction; Chord recognition; Chord sequence; Language model; Language model weight; Lattice rescoring; Musical pieces; Viterbi decoder; Computational linguistics","M. Khadkevich; FBK-irst, Universitá degli studi di Trento, Povo - 38050, Trento, Via Sommarive, 14, Italy; email: khadkevich@fbk.eu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Baur D.; Langer T.; Butz A.","Baur, Dominikus (24477437400); Langer, Tim (26534443500); Butz, Andreas (55150450600)","24477437400; 26534443500; 55150450600","Shades of music: Letting users discover sub-song similarities","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866716141&partnerID=40&md5=395c1682ef265506ce608d82d06ff35e","Media Informatics Group, University of Munich (LMU), Munich, Germany","Baur D., Media Informatics Group, University of Munich (LMU), Munich, Germany; Langer T., Media Informatics Group, University of Munich (LMU), Munich, Germany; Butz A., Media Informatics Group, University of Munich (LMU), Munich, Germany","Many interesting pieces of music violate established structures or rules of their genre on purpose. These songs can be very atypical in their interior structure and their different parts might actually allude to entirely different other songs or genres. We present a query-by-example-based user interface that shows songs related to the one currently playing. This relation is not based on overall similarity, but on the similarity between the part currently playing and parts of other songs in the collection along different dimensions (pitch, timbre, bars, beats, loudness). The similarity is initially computed automatically, but can be corrected by the user. Once a sufficient number of corrections has been made, we expect the similarity measure to reach an even higher precision. Our system thereby allows users to discover hidden similarities on the level of song sections instead of whole songs. © 2009 International Society for Music Information Retrieval.","","User interfaces; Interior structure; Similarity measure; Information retrieval","D. Baur; Media Informatics Group, University of Munich (LMU), Munich, Germany; email: dominikus.baur@ifi.lmu.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hu X.; Stephen Downie J.; Ehmann A.F.","Hu, Xiao (55496358400); Stephen Downie, J. (7102932568); Ehmann, Andreas F. (8988651500)","55496358400; 7102932568; 8988651500","Lyric text mining in music mood classification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","84","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668220&partnerID=40&md5=db47a6f039518a8e7be1e2960c85286d","International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Hu X., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Stephen Downie J., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","This research examines the role lyric text can play in improving audio music mood classification. A new method is proposed to build a large ground truth set of 5,585 songs and 18 mood categories based on social tags so as to reflect a realistic, user-centered perspective. A relatively complete set of lyric features and representation models were investigated. The best performing lyric feature set was also compared to a leading audio-based system. In combining lyric and audio sources, hybrid feature sets built with three different feature selection methods were also examined. The results show patterns at odds with findings in previous studies: audio features do not always outperform lyrics features, and combining lyrics and audio features can improve performance in many mood categories, but not all of them. © 2009 International Society for Music Information Retrieval.","","Data mining; Information retrieval; Audio features; Audio music; Audio sources; Audio-based; Feature selection methods; Feature sets; Ground truth; Hybrid features; Improve performance; Representation model; Social Tags; Text mining; User-centered; Text processing","X. Hu; International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; email: xiaohu@illinois.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"McFee B.; Lanckriet G.","McFee, Brian (34875379700); Lanckriet, Gert (7801431767)","34875379700; 7801431767","Heterogeneous embedding for subjective artist similarity","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051504689&partnerID=40&md5=21c96b3001a83b00c213cf753bde891d","Computer Science and Engineering, University of California, San Diego, United States; Electrical and Computer Engineering, University of California, San Diego, United States","McFee B., Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","We describe an artist recommendation system which integrates several heterogeneous data sources to form a holistic similarity space. Using social, semantic, and acoustic features, we learn a low-dimensional feature transformation which is optimized to reproduce human-derived measurements of subjective similarity between artists. By producing low-dimensional representations of artists, our system is suitable for visualization and recommendation tasks. © 2009 International Society for Music Information Retrieval.","","Semantics; Acoustic features; Artist similarities; Feature transformations; Heterogeneous data sources; Low-dimensional representation; Measurements of; Similarity spaces; Information retrieval","B. McFee; Computer Science and Engineering, University of California, San Diego, United States; email: bmcfee@cs.ucsd.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lemström K.; Wiggins G.A.","Lemström, Kjell (7006564183); Wiggins, Geraint A. (14032393700)","7006564183; 14032393700","Formalizing invariances for content-based music retrieval","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873620003&partnerID=40&md5=f4e73de3c11852ea9c7301ca0ea5c8e2","Department of Computer Science, University of Helsinki, Finland; Department of Computing Goldsmiths, University of London, United Kingdom","Lemström K., Department of Computer Science, University of Helsinki, Finland; Wiggins G.A., Department of Computing Goldsmiths, University of London, United Kingdom","Invariances are central concepts in content-based music retrieval. Musical representations and similarity measures are designed to capture musically relevant invariances, such as transposition invariance. Though regularly used, their explicit definition is usually omitted because of the heavy formalism required. The lack of explicit definition, however, can result in misuse or misunderstanding of the terms. We discuss the musical relevance of various musical invariances and develop a set-theoretic formalism, for defining and classifying them. Using it, we define the most common invariances, and give a taxonomy which they inhabit. The taxonomy serves as a useful tool for idetinfying where work is needed to address real world problems in content-based music retrieval. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Content-based music retrieval; Musical representations; Real-world problem; Similarity measure; Transposition invariance; Taxonomies","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Ferguson S.; Cabrera D.","Ferguson, Sam (8367076100); Cabrera, Densil (18036534200)","8367076100; 18036534200","Auditory spectral summarisation for audio signals with musical applications","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873622300&partnerID=40&md5=84f1ff9926197332a3f16ee15f09178b","Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia","Ferguson S., Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia; Cabrera D., Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia","Methods for spectral analysis of audio signals and their graphical display are widespread. However, assessing music and audio in the visual domain involves a number of challenges in the translation between auditory images into mental or symbolically represented concepts. This paper presents a spectral analysis method that exists entirely in the auditory domain, and results in an auditory presentation of a spectrum. It aims to strip a segment of audio signal of its temporal content, resulting in a quasi-stationary signal that possesses a similar spectrum to the original signal. The method is extended and applied for the purpose of music summarisation. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Spectrum analysis; Audio signal; Graphical displays; Original signal; Quasi-stationary signals; Spectral analysis method; Audio acoustics","S. Ferguson; Faculty of Design, Architecture and Building, University of Technology, Sydney, Australia; email: samuel.ferguson@uts.edu.au","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Jacobson K.; Raimond Y.; Sandler M.","Jacobson, Kurt (37107739300); Raimond, Yves (23135967900); Sandler, Mark (7202740804)","37107739300; 23135967900; 7202740804","An ecosystem for transparent music similarity in an openworld","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955376846&partnerID=40&md5=ce4faca724f8d41251c0d7df61fd5708","Centre for Digital Music, Queen Mary University of London, United Kingdom; BBC, London, United Kingdom","Jacobson K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Raimond Y., BBC, London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, United Kingdom","There exist many methods for deriving music similarity associations and additional variations are likely to be seen in the future. In this work we introduce the Similarity Ontology for describing associations between items. Using a combination of RDF/OWL and N3, our ontology allows for transparency and provenance tracking in a distributed and open system. We describe a similarity ecosystem where agents assert and aggregate similarity statements on the Web of Data allowing a client application to make queries for recommendation, playlisting, or other tasks. In this ecosystem any number of similarity derivation methods can exist side-by-side, specifying similarity relationships as well as the processes used to derive these statements. The data consumer can then select which similarity statements to trust based on knowledge of the similarity derivation processes or a list of trusted assertion agents. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Client applications; Music similarity; Side by sides; Web of datum; Ecosystems","K. Jacobson; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: kurtjx@gmail.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Panagakis Y.; Kotropoulos C.; Arce G.R.","Panagakis, Yannis (35503932300); Kotropoulos, Constantine (35563688200); Arce, Gonzalo R. (7006653894)","35503932300; 35563688200; 7006653894","Music genre classification using locality preserving non-negative tensor factorization and sparse representations","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","53","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873668627&partnerID=40&md5=753c869741b8966271b2eb7e27cc907d","Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece; Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE, United States","Panagakis Y., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece; Kotropoulos C., Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece, Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE, United States; Arce G.R., Dept. of Electrical and Computer Engineering, University of Delaware, Newark DE, United States","A robust music genre classification framework is proposed that combines the rich, psycho-physiologically grounded properties of auditory cortical representations of music recordings and the power of sparse representation-based classifiers. A novel multilinear subspace analysis method that incorporates the underlying geometrical structure of the cortical representations space into non-negative tensor factorization is proposed for dimensionality reduction compatible to the working principle of sparse representationbased classification. The proposed method is referred to as Locality Preserving Non-Negative Tensor Factorization (LPNTF). Dimensionality reduction is shown to play a crucial role within the classification framework under study. Music genre classification accuracy of 92.4% and 94.38% on the GTZAN and the ISMIR2004 Genre datasets is reported, respectively. Both accuracies outperform any accuracy ever reported for state of the art music genre classification algorithms applied to the aforementioned datasets. © 2009 International Society for Music Information Retrieval.","","Classification (of information); Factorization; Information retrieval; Metal drawing; Tensors; Classification framework; Dimensionality reduction; Geometrical structure; Locality-preserving; Music genre classification; Music recording; Non negatives; Sparse representation; State of the art; Subspace analysis; Tensor factorization; Working principles; Computer music","Y. Panagakis; Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki GR-54124, Box 451, Greece; email: panagakis@aiia.csd.auth.gr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Yang Y.-H.; Lin Y.-C.; Lee A.; Chen H.","Yang, Yi-Hsuan (55218558400); Lin, Yu-Ching (57203772039); Lee, Ann (55477330500); Chen, Homer (8236841800)","55218558400; 57203772039; 55477330500; 8236841800","Improving musical concept detection by ordinal regression and context fusion","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873672285&partnerID=40&md5=6cab6dddd1607d25af789559be343e1c","National Taiwan University, Taiwan","Yang Y.-H., National Taiwan University, Taiwan; Lin Y.-C., National Taiwan University, Taiwan; Lee A., National Taiwan University, Taiwan; Chen H., National Taiwan University, Taiwan","To facilitate information retrieval of large-scale music databases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept detection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label classification methods. To further improve the detection accuracy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selection to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924. © 2009 International Society for Music Information Retrieval.","","Co-occurrence pattern; Concept correlation; Concept detection; Concept selection; Data imbalance; Detection accuracy; Multi-label classifications; Music database; Musical concepts; Ordinal regression; Relationship between concepts; Research topics; Information retrieval","Y.-H. Yang; National Taiwan University, Taiwan; email: affige@gmail.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lübbers D.; Jarke M.","Lübbers, Dominik (55584877900); Jarke, Matthias (7006359305)","55584877900; 7006359305","Adaptive multimodal exploration of music collections","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053125498&partnerID=40&md5=9c770d751e5918fb8623e54b496f646f","Informatik 5, RWTH Aachen University, Aachen, Germany; Dept. Applied Information Technology, German University of Technology, Muscat, Oman","Lübbers D., Informatik 5, RWTH Aachen University, Aachen, Germany, Dept. Applied Information Technology, German University of Technology, Muscat, Oman; Jarke M., Informatik 5, RWTH Aachen University, Aachen, Germany","Discovering music that we like rarely happens as a result of a directed search. Except for the case where we have exact meta data at hand it is hard to articulate what song is attractive to us. Therefore it is essential to develop and evaluate systems that support guided exploratory browsing of the music space. While a number of algorithms for organizing music collections according to a given similarity measure have been applied successfully, the generated structure is usually only presented visually and listening requires cumbersome skipping through the individual pieces. To close this media gap we describe an immersive multimodal exploration environment which extends the presentation of a song collection in a video-game-like virtual 3-D landscape by carefully adjusted spatialized plackback of songs. The user can freely navigate through the virtual world guided by the acoustic clues surrounding him. Observing his interaction with the environment the system furthermore learns the user's way of structuring his collection by adapting a weighted combination of a wide range of integrated content-based, meta-data-based and collaborative similarity measures. Our evaluation proves the importance of auditory feedback for music exploration and shows that our system is capable of adjusting to different notions of similarity. © 2009 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Virtual reality; Auditory feedback; Content-based; Directed searches; Immersive; Multi-modal; Music collection; Similarity measure; Virtual worlds; Three dimensional computer graphics","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Anglade A.; Ramirez R.; Dixon S.","Anglade, Amélie (24070116200); Ramirez, Rafael (35280935600); Dixon, Simon (7201479437)","24070116200; 35280935600; 7201479437","Genre classification using harmony rules induced from automatic chord transcriptions","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955414033&partnerID=40&md5=f09702a371892bf6b4f11eef4a66e41c","Centre for Digital Music, Queen Mary University of London, United Kingdom; Music Technology Group, Universitat Pompeu Fabra, Spain","Anglade A., Centre for Digital Music, Queen Mary University of London, United Kingdom; Ramirez R., Music Technology Group, Universitat Pompeu Fabra, Spain; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","We present an automatic genre classification technique making use of frequent chord sequences that can be applied on symbolic as well as audio data. We adopt a first-order logic representation of harmony and musical genres: pieces of music are represented as lists of chords and musical genres are seen as context-free definite clause grammars using subsequences of these chord lists. To induce the contextfree definite clause grammars characterising the genres we use a first-order logic decision tree induction algorithm. We report on the adaptation of this classification framework to audio data using an automatic chord transcription algorithm. We also introduce a high-level harmony representation scheme which describes the chords in term of both their degrees and chord categories. When compared to another high-level harmony representation scheme used in a previous study, it obtains better classification accuracies and shorter run times. We test this framework on 856 audio files synthesized from Band in a Box files and covering 3 main genres, and 9 subgenres. We perform 3-way and 2-way classification tasks on these audio files and obtain good classification results: between 67% and 79% accuracy for the 2-way classification tasks and between 58% and 72% accuracy for the 3-way classification tasks. © 2009 International Society for Music Information Retrieval.","","Algorithms; Context sensitive grammars; Decision trees; Formal logic; Information retrieval; Audio data; Audio files; Automatic genre classification; Chord sequence; Classification accuracy; Classification framework; Classification results; Classification tasks; Context-free; Definite clause grammars; First order logic; Genre classification; Musical genre; Representation schemes; Transcription","A. Anglade; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: amelie.anglade@elec.qmul.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Pohle T.; Schnitzer D.; Schedl M.; Knees P.; Widmer G.","Pohle, Tim (14036302300); Schnitzer, Dominik (23996271700); Schedl, Markus (8684865900); Knees, Peter (8219023200); Widmer, Gerhard (7004342843)","14036302300; 23996271700; 8684865900; 8219023200; 7004342843","On rhythm and general music similarity","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","59","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869800834&partnerID=40&md5=1a0ddd86005f5098010899ec6f38ca48","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, OFAI, Wien, Austria","Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Wien, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, OFAI, Wien, Austria","The contribution of this paper is threefold: First, we propose modifications to Fluctuation Patterns [14]. The resulting descriptors are evaluated in the task of rhythm similarity computation on the ""Ballroom Dancers"" collection. Second, we show that by combining these rhythmic descriptors with a timbral component, results for rhythm similarity computation are improved beyond the level obtained when using the rhythm descriptor component alone. Third, we present one ""unified"" algorithm with fixed parameter set. This algorithm is evaluated on three different music collections. We conclude from these evaluations that the computed similarities reflect relevant aspects both of rhythm similarity and of general music similarity. The performance can be improved by tuning parameters of the ""unified"" algorithm to the specific task (rhythm similarity / general music similarity) and the specific collection, respectively. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Descriptors; Music collection; Music similarity; Parameter set; Similarity computation; Specific tasks; Tuning parameter; Algorithms","T. Pohle; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: music@jku.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hillewaere R.; Manderick B.; Conklin D.","Hillewaere, Ruben (36720698100); Manderick, Bernard (6602920805); Conklin, Darrell (57220096325)","36720698100; 6602920805; 57220096325","Global feature versus event models for folk song classification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873630448&partnerID=40&md5=44637cb229b9a1c81c53e07505a5e420","Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Music Informatics Research Group, Department of Computing, City University London, United Kingdom","Hillewaere R., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Manderick B., Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; Conklin D., Music Informatics Research Group, Department of Computing, City University London, United Kingdom","Music classification has been widely investigated in the past few years using a variety ofmachine learning approaches. In this study, a corpus of 3367 folk songs, divided into six geographic regions, has been created and is used to evaluate two popular yet contrasting methods for symbolic melody classification. For the task of folk song classification, a global feature approach, which summarizes a melody as a feature vector, is outperformed by an event model of abstract event features. The best accuracy obtained on the folk song corpus was achieved with an ensemble of event models. These results indicate that the event model should be the default model of choice for folk song classification. © 2009 International Society for Music Information Retrieval.","","Default model; Event model; Feature vectors; Folk songs; Geographic regions; Global feature; Learning approach; Music classification; Information retrieval","R. Hillewaere; Department of Computing, Computational Modeling Lab, Vrije Universiteit Brussel, Brussels, Belgium; email: rhillewa@vub.ac.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"MacRitchie J.; Buck B.; Bailey N.J.","MacRitchie, J. (55587318600); Buck, B. (55587482200); Bailey, N.J. (7103189467)","55587318600; 55587482200; 7103189467","Visualising musical structure through performance gesture","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84555199062&partnerID=40&md5=e2290e95e4f89edf366c456e423f0fb6","Centre for Music Technology, University of Glasgow, United Kingdom","MacRitchie J., Centre for Music Technology, University of Glasgow, United Kingdom; Buck B., Centre for Music Technology, University of Glasgow, United Kingdom; Bailey N.J., Centre for Music Technology, University of Glasgow, United Kingdom","A musical performance is seen as the performer's interpretation of a musical score, illuminating the interaction between the musical structure and implied emotive character [1]. It has been demonstrated that performers' physical gestures correlate with structural and emotional aspects of the piece they are performing and that this information can be decoded by an audience when presented with a visualonly performance [2]. This paper investigates the relationship between direction of physical movement and underlying musical structures. The Vicon motion capture system is used to record 3D movements made by nine university-level pianists performing Chopin preludes op.28 Nos 6 and 7. The examination of several pianists provides insight into the similarity and differences in gestures between performers and how these relate to structure. Principal Component Analysis (PCA) of these performances and consequent analysis of variance reveals a relationship between extrema of the first six significant components and timing of phrasing structure in Prelude 7 where motion troughs consistently lag behind the occurence of phrase boundaries in the audio. This relationship is then examined for Prelude 6 which encompasses longer, expanded phrases and changes in rhythm. These expanded phrases are associated with elongated or split gestures, and variations of the motif with changes in movement. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Principal component analysis; Emotional aspect; Motion capture system; Musical performance; Musical score; Musical structures; Phrase boundary; Physical movements; Musical instruments","J. MacRitchie; Centre for Music Technology, University of Glasgow, United Kingdom; email: j.macritchie@elec.gla.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Law E.; West K.; Mandel M.; Bay M.; Stephen Downie J.","Law, Edith (35173013200); West, Kris (15766636800); Mandel, Michael (14060460000); Bay, Mert (56259607500); Stephen Downie, J. (7102932568)","35173013200; 15766636800; 14060460000; 56259607500; 7102932568","Evaluation of algorithms using games: The case of music tagging","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","125","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873647698&partnerID=40&md5=dc297e6645a24a4c9e092eddfa3277f2","CMU, United States; IMIRSEL/UIUC, United States; Columbia University, United States","Law E., CMU, United States; West K., IMIRSEL/UIUC, United States; Mandel M., Columbia University, United States; Bay M., IMIRSEL/UIUC, United States; Stephen Downie J., IMIRSEL/UIUC, United States","Search by keyword is an extremely popular method for retrieving music. To support this, novel algorithms that automatically tag music are being developed. The conventional way to evaluate audio tagging algorithms is to compute measures of agreement between the output and the ground truth set. In this work, we introduce a new method for evaluating audio tagging algorithms on a large scale by collecting set-level judgments from players of a human computation game called TagATune. We present the design and preliminary results of an experiment comparing five algorithms using this new evaluation metric, and contrast the results with those obtained by applying several conventional agreement-based evaluation metrics. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Evaluation metrics; Ground truth; Human computation games; Novel algorithm; Algorithms","E. Law; CMU, United States; email: edith@cmu.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Park T.H.; Li Z.; Wu W.","Park, Tae Hong (56328644900); Li, Zhiye (55587991500); Wu, Wen (55503006800)","56328644900; 55587991500; 55503006800","Easy does it: The electro-acoustic music analysis toolbox","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865489147&partnerID=40&md5=ed1db073f50a0321a7b5125e58ac816d","Tulane University, United States","Park T.H., Tulane University, United States; Li Z., Tulane University, United States; Wu W., Tulane University, United States","In this paper we present the EASY (Electro-Acoustic muSic analYsis) Toolbox software system for assisting electro-acoustic music analysis. The primary aims of the system are to present perceptually relevant features and audio descriptors via visual designs to gain more insight into electro-acoustic music works and provide easy-touse ""click-and-go"" software interface paradigms for practical use of the system by non-experts and experts alike. The development of the EASY system exploits MIR techniques with particular emphasis on the electroacoustic music repertoire - musical pieces that concentrate on timbral dimensions rather than traditional elements such as pitch, melody, harmony, and rhythm. The project was mainly inspired by the lack of software tools available for aiding electro-acoustic music analysis. The system's frameworks, feature analysis algorithms, along with the initial analyses of pieces are presented here. © 2009 International Society for Music Information Retrieval.","","Computer software; Information retrieval; Descriptors; Electro-acoustic; Electroacoustic music; Feature analysis; Music analysis; Musical pieces; Practical use; Relevant features; Software interfaces; Software systems; Traditional elements; Visual design; Computer music","T.H. Park; Tulane University, United States; email: park@tulane.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Hu D.J.; Saul L.K.","Hu, Diane J. (24777966100); Saul, Lawrence K. (7005701303)","24777966100; 7005701303","A probabilistic topic model for unsupervised learning of musical key-profiles","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864117534&partnerID=40&md5=004bce46bbd2e2a6e6c7446ed2e5cbf6","Department of Computer Science and Engineering, University of California, San Diego, United States","Hu D.J., Department of Computer Science and Engineering, University of California, San Diego, United States; Saul L.K., Department of Computer Science and Engineering, University of California, San Diego, United States","We describe a probabilisticmodel for learning musical keyprofiles from symbolic files of polyphonic, classical music. Our model is based on Latent Dirichlet Allocation (LDA), a statistical approach for discovering hidden topics in large corpora of text. In our adaptation of LDA, symbolic music files play the role of text documents, groups of musical notes play the role of words, and musical keyprofiles play the role of topics. The topics are discovered as significant, recurring distributions over twelve neutral pitch-classes. Though discovered automatically, these distributions closely resemble the traditional key-profiles used to indicate the stability and importance of neutral pitchclasses in the major and minor keys of western music. Unlike earlier approaches based on human judgement, our model learns key-profiles in an unsupervised manner, inferring them automatically from a large musical corpus that contains no key annotations. We show how these learned key-profiles can be used to determine the key of a musical piece and track its harmonic modulations. We also show how the model's inferences can be used to compare musical pieces based on their harmonic structure. © 2009 International Society for Music Information Retrieval.","","Statistics; Classical musics; Harmonic modulations; Harmonic structures; Large corpora; Latent dirichlet allocations; Music files; Musical notes; Musical pieces; Probabilistic topic models; Statistical approach; Text document; Information retrieval","D.J. Hu; Department of Computer Science and Engineering, University of California, San Diego, United States; email: dhu@cs.ucsd.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Stephen Downie J.; Byrd D.; Crawford T.","Stephen Downie, J. (7102932568); Byrd, Donald (55041494100); Crawford, Tim (15054056900)","7102932568; 55041494100; 15054056900","Ten years of ismir: Reflections on challenges and opportunities","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","43","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873625272&partnerID=40&md5=2a7bc2124c71b3d5ddf34325522acf76","University of Illinois, Urbana-Champaign, United States; Indiana University, Bloomington, United States; Goldsmiths College University, London, United Kingdom","Stephen Downie J., University of Illinois, Urbana-Champaign, United States; Byrd D., Indiana University, Bloomington, United States; Crawford T., Goldsmiths College University, London, United Kingdom","The International Symposium on Music Information Retrieval (ISMIR) was born on 13 August 1999. This paper expresses the opinions of three of ISMIR's founders as they reflect upon what has happened during its first decade. The paper provides the background context for the events that led to the establishment of ISMIR. We highlight the first ISMIR, held in Plymouth, MA in October of 2000, and use it to elucidate key trends that have influenced subsequent ISMIRs. Indicators of growth and success drawn from ISMIR publication data are presented. The role that the Music Information Retrieval Evaluation eXchange (MIREX) has played at ISMIR is examined. The factors contributing to ISMIR's growth and success are also enumerated. The paper concludes with a set of challenges and opportunities that the newly formed International Society for Music Information Retrieval should embrace to ensure the future vitality of the conference series and the ISMIR community. © 2009 International Society for Music Information Retrieval.","","International society; Key trends; Music information retrieval; Plymouth; Information retrieval","J. Stephen Downie; University of Illinois, Urbana-Champaign, United States; email: jdownie@illinois.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Yoshii K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","7103400120; 7403505330","Continuous PLSI and smoothing techniques for hybrid music recommendation","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867157558&partnerID=40&md5=d272d58716ac709902a554d824546ecd","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents an extended probabilistic latent semantic indexing (pLSI) for hybrid music recommendation that deals with rating data provided by users and with contentbased data extracted from audio signals. The original pLSI can be applied to collaborative filtering by treating users and items as discrete random variables that follow multinomial distributions. In hybrid recommendation, it is necessary to deal with musical contents that are usually represented as continuous vectorial values. To do this, we propose a continuous pLSI that incorporates Gaussian mixture models. This extension, however, causes a severe local optima problem because it increases the number of parameters drastically. This is considered to be a major factor generating ""hubs,"" which are items that are inappropriately recommended to almost all users. To solve this problem, we tested three smoothing techniques: multinomial smoothing,Gaussian parameter tying, and artist-based item clustering. The experimental results revealed that although the first method improved nothing, the others significantly improved the recommendation accuracy and reduced the hubness. This indicates that it is important to appropriately limit the model complexity to use the pLSI in practical. © 2009 International Society for Music Information Retrieval.","","Audio signal; Content-based; Discrete random variables; Gaussian Mixture Model; Gaussian parameters; Hubness; Hybrid recommendation; Local optima; Major factors; Model complexity; Multinomial distributions; Multinomials; Music recommendation; Probabilistic latent semantic indexing; Recommendation accuracy; Smoothing techniques; Information retrieval","K. Yoshii; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: k.yoshii@aist.go.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Maillet F.; Eck D.; Desjardins G.; Lamere P.","Maillet, François (25723551800); Eck, Douglas (12141444300); Desjardins, Guillaume (58285020500); Lamere, Paul (15765776900)","25723551800; 12141444300; 58285020500; 15765776900","Steerable playlist generation by learning song similarity from radio station playlists","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","53","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867145161&partnerID=40&md5=503f67e180b68991ffb89e16f336f020","DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Echo Nest, Somerville, United States","Maillet F., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Eck D., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Desjardins G., DIRO, CIRMMT, Université de Montréal, Montreal, Canada; Lamere P., Echo Nest, Somerville, United States","This paper presents an approach to generating steerable playlists. We first demonstrate a method for learning song transition probabilities from audio features extracted from songs played in professional radio station playlists. We then show that by using this learnt similarity function as a prior, we are able to generate steerable playlists by choosing the next song to play not simply based on that prior, but on a tag cloud that the user is able to manipulate to express the high-level characteristics of the music he wishes to listen to. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Audio features; Similarity functions; Tag clouds; Transition probabilities; Radio stations","F. Maillet; DIRO, CIRMMT, Université de Montréal, Montreal, Canada; email: mailletf@iro.umontreal.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Grachten M.; Schedl M.; Pohle T.; Widmer G.","Grachten, Maarten (8974600000); Schedl, Markus (8684865900); Pohle, Tim (14036302300); Widmer, Gerhard (7004342843)","8974600000; 8684865900; 14036302300; 7004342843","The ISMIR cloud: A decade of ISMIR conferences at your fingertips","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572127&partnerID=40&md5=f9f8f11e0d0d2be11161aa136c560875","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Grachten M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper, we analyze the proceedings of the past International Symposia on Music Information Retrieval (ISMIR). We extract meaningful term sets from the accepted submissions and apply term weighting and Web-based filtering techniques to distill information about the topics covered by the papers. This enables us to visualize and interpret the change of hot ISMIR topics in the course of time. Furthermore, the performed analysis allows for assessing the cumulative ISMIR proceedings by semantic content (rather than by literal text search). To illustrate this, we introduce two prototype applications that are publicly accessible online 1 . The first allows the user to search for ISMIR publications by selecting subsets of ISMIR topics. The second provides interactive visual access to the joint content of ISMIR publications in the form of a tag cloud - the ISMIR Cloud. © 2009 International Society for Music Information Retrieval.","","Semantics; Filtering technique; Music information retrieval; Publicly accessible; Semantic content; Tag clouds; Term weighting; Text search; Visual access; Information retrieval","M. Grachten; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: music@jku.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Salamon J.; Rohrmeier M.","Salamon, Justin (55184866100); Rohrmeier, Martin (6507901506)","55184866100; 6507901506","A quantitative evaluation of a two stage retrieval approach for a melodic query by example system","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956293795&partnerID=40&md5=286d7d277027cd0c323535c321b75041","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Centre for Music and Science, Faculty of Music, University of Cambridge, United Kingdom","Salamon J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Rohrmeier M., Centre for Music and Science, Faculty of Music, University of Cambridge, United Kingdom","We present a two-stage approach for retrieval in a melodic Query by Example system inspired by the BLAST algorithm used in bioinformatics for DNA matching. The first stage involves an indexing method using n-grams and reduces the number of targets to consider in the second stage. In the second stage we use a matching algorithm based on local alignment with modified cost functions which take into account musical considerations. We evaluate our system using queries made by real users utilising both short-term and long-term memory, and present a detailed study of the system's parameters and how they affect retrieval performance and efficiency. We show that whilst similar approaches were shown to be unsuccessful for Query by Humming (where singing and transcription errors result in queries with higher error rates), in the case of our system the approach is successful in reducing the database size without decreasing retrieval performance. © 2009 International Society for Music Information Retrieval.","","Algorithms; Bioinformatics; Information retrieval; Query languages; Database size; Error rate; Indexing methods; Local alignment; Long term memory; Matching algorithm; N-grams; Quantitative evaluation; Query by humming; Query-by example; Retrieval performance; Two-stage approaches; Query processing","J. Salamon; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: justin.salamon@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Li B.; Smith J.B.L.; Fujinaga I.","Li, Beinan (22980588500); Smith, Jordan B.L. (55582613300); Fujinaga, Ichiro (9038140900)","22980588500; 55582613300; 9038140900","Optical audio reconstruction for stereo phonograph records using white light interferometry","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866309161&partnerID=40&md5=3397902173dc999ebea1d40577292b48","Schulich School of Music, Music Technology Area, McGill University, Canada","Li B., Schulich School of Music, Music Technology Area, McGill University, Canada; Smith J.B.L., Schulich School of Music, Music Technology Area, McGill University, Canada; Fujinaga I., Schulich School of Music, Music Technology Area, McGill University, Canada","Our work focuses on optically reconstructing the stereo audio signal of a 33 rpm long-playing (LP) record using a white-light interferometry-based approach. Previously, a theoretical framework was presented, alongside the primitive reconstruction result from a few cycles of a stereo sinusoidal test signal. To reconstruct an audible duration of a longer stereo signal requires tackling new problems, such as disc warping, image alignment, and eliminating the effects of noise and broken grooves. This paper proposes solutions to these problems, and presents the complete workflow of our Optical Audio Reconstruction (OAR) system. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Few-cycle; Image alignment; Stereo audio signal; Test signal; Theoretical framework; White light; White-light interferometry; Interferometry","B. Li; Schulich School of Music, Music Technology Area, McGill University, Canada; email: beinan.li@mail.mcgill.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Karydis I.; Nanopoulos A.; Gabriel H.-H.; Spiliopoulou M.","Karydis, Ioannis (36950839300); Nanopoulos, Alexandros (6603555418); Gabriel, Hans-Henning (23389249100); Spiliopoulou, Myra (56248430300)","36950839300; 6603555418; 23389249100; 56248430300","Tag-aware spectral clustering of music items","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866696691&partnerID=40&md5=192164a76d06e27c1b8cb8907726bc43","Department of Informatics, Ionian University, Greece; Institute of Informatics, Hildesheim University, Germany; Faculty of Computer Science, Otto-von-Guericke-University, Magdeburg, Germany","Karydis I., Department of Informatics, Ionian University, Greece; Nanopoulos A., Institute of Informatics, Hildesheim University, Germany; Gabriel H.-H., Faculty of Computer Science, Otto-von-Guericke-University, Magdeburg, Germany; Spiliopoulou M., Faculty of Computer Science, Otto-von-Guericke-University, Magdeburg, Germany","Social tagging is an increasingly popular phenomenon with substantial impact on Music Information Retrieval (MIR). Tags express the personal perspectives of the user on the music items (such as songs, artists, or albums) they tagged. These personal perspectives should be taken into account in MIR tasks that assess the similarity between music items. In this paper, we propose an novel approach for clustering music items represented in social tagging systems. Its characteristic is that it determines similarity between items by preserving the 3-way relationships among the inherent dimensions of the data, i.e., users, items, and tags. Conversely to existing approaches that use reductions to 2- way relationships (between items-users or items-tags), this characteristic allows the proposed algorithm to consider the personal perspectives of tags and to improve the clustering quality. Due to the complexity of social tagging data, we focus on spectral clustering that has been proven effective in addressing complex data. However, existing spectral clustering algorithms work with 2-way relationships. To overcome this problem, we develop a novel data-modeling scheme and a tag-aware spectral clustering procedure that uses tensors (high-dimensional arrays) to store the multigraph structures that capture the personalised aspects of similarity. Experimental results with data from Last.fm indicate the superiority of the proposed method in terms of clustering quality over conventional spectral clustering approaches that consider only 2-way relationships. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Social networking (online); Clustering quality; Complex data; High-dimensional; Last.fm; Multigraphs; Music information retrieval; Personal perspective; Social tagging; Social tagging systems; Spectral clustering; Spectral clustering algorithms; Clustering algorithms","I. Karydis; Department of Informatics, Ionian University, Greece; email: karydis@ionio.gr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Raphael C.","Raphael, Christopher (7004214964)","7004214964","Symbolic and structrual representation of melodic expression","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873630337&partnerID=40&md5=f1a50d59c1df5952856d6b6f92fa22aa","School of Informatics and Computing, Indiana Univ., Bloomington, United States","Raphael C., School of Informatics and Computing, Indiana Univ., Bloomington, United States","A method for expressive melody synthesis is presented seeking to capture the structural and prosodic (stress, direction, and grouping) elements of musical interpretation. The interpretation of melody is represented through a hierarchical structural decomposition and a note-level prosodic annotation. An audio performance of the melody is constructed using the time-evolving frequency and intensity functions. A method is presented that transforms the expressive annotation into the frequency and intensity functions, thus giving the audio performance. In this framework, the problem of expressive rendering is cast as estimation of structural decomposition and the prosodic annotation. Examples are presented on a dataset of around 50 folk-like melodies, realized both from hand-marked and estimated annotations. © 2009 International Society for Music Information Retrieval.","","Expressive rendering; Intensity functions; Structural decomposition; Information retrieval","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Proutskova P.; Casey M.","Proutskova, Polina (55586627700); Casey, Michael (15080769900)","55586627700; 15080769900","You call that singing? Ensemble classification for multi-cultural collections of music recordings","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955425305&partnerID=40&md5=b30c475c6dd89e01dfb0eed5b153bfd7","Department of Computing, Goldsmiths, London, United Kingdom; Bregman Music Research Laboratory, Dartmouth College, United States","Proutskova P., Department of Computing, Goldsmiths, London, United Kingdom; Casey M., Bregman Music Research Laboratory, Dartmouth College, United States","The wide range of vocal styles, musical textures and recording techniques found in ethnomusicological field recordings leads us to consider the problem of automatically labeling the content to know whether a recording is a song or instrumental work. Furthermore, if it is a song, we are interested in labeling aspects of the vocal texture: e.g. solo, choral, acapella or singing with instruments. We present evidence to suggest that automatic annotation is feasible for recorded collections exhibiting a wide range of recording techniques and representing musical cultures from around the world. Our experiments used the Alan Lomax Cantometrics training tapes data set, to encourage future comparative evaluations. Experiments were conducted with a labeled subset consisting of several hundred tracks, annotated at the track and frame levels, as acapella singing, singing plus instruments or instruments only. We trained frame-by-frame SVM classifiers using MFCC features on positive and negative exemplars for two tasks: per-frame labeling of singing and acapella singing. In a further experiment, the frame-by-frame classifier outputs were integrated to estimate the predominant content of whole tracks. Our results show that frame-byframe classifiers achieved 71% frame accuracy and whole track classifier integration achieved 88% accuracy. We conclude with an analysis of classifier errors suggesting avenues for developing more robust features and classifier strategies for large ethnographically diverse collections. © 2009 International Society for Music Information Retrieval.","","Experiments; Information retrieval; Instruments; Textures; Automatic annotation; Classifier integrations; Comparative evaluations; Data set; Ensemble classification; Field recording; Music recording; SVM classifiers; Audio recordings","P. Proutskova; Department of Computing, Goldsmiths, London, United Kingdom; email: p.proutskova@gold.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Ferraro P.; Hanna P.; Imbert L.; Izard T.","Ferraro, Pascal (23134820600); Hanna, Pierre (23134483000); Imbert, Laurent (6602730126); Izard, Thomas (35117824300)","23134820600; 23134483000; 6602730126; 35117824300","Accelerating query-by-humming on GPU","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053117737&partnerID=40&md5=56488dd608e55c7f173719b2aa6807f7","LaBRI - U. Bordeaux 1, France; PIMS/CNRS - U. Calgary, Canada; Lirmm - CNRS, France; Lirmm - U. Montpellier 2, France","Ferraro P., LaBRI - U. Bordeaux 1, France, PIMS/CNRS - U. Calgary, Canada; Hanna P., LaBRI - U. Bordeaux 1, France; Imbert L., PIMS/CNRS - U. Calgary, Canada, Lirmm - CNRS, France; Izard T., Lirmm - U. Montpellier 2, France","Searching for similarities in large musical databases has become a common procedure. Local alignment methods, based on dynamic programming, explore all the possible matchings between two musical pieces; and as a result return the optimal local alignment. Unfortunately these very powerful methods have a very high computational cost. The exponential growth of musical databases makes exact alignment algorithm unrealistic for searching similarities. Alternatives have been proposed in bioinformatics either by using heuristics or by developing faster implementation of exact algorithm. The main motivation of this work is to exploit the huge computational power of commonly available graphic cards to develop high performance solutions for Query-by-Humming applications. In this paper, we present a fast implementation of a local alignment method, which allows to retrieve a hummed query in a database of MIDI files, with good accuracy, in a time up to 160 times faster than other comparable systems. © 2009 International Society for Music Information Retrieval.","","Algorithms; Alignment; Bioinformatics; Query processing; Smart cards; Alignment algorithms; Computational costs; Computational power; Exact algorithms; Exponential growth; Fast implementation; Faster implementation; Graphic cards; Local alignment; Local alignment method; Matchings; MIDI files; Musical database; Musical pieces; Query-by-humming; Information retrieval","P. Ferraro; LaBRI - U. Bordeaux 1, France; email: ferraro@cpsc.ucalgary.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Ganseman J.; Scheunders P.; D'Haes W.","Ganseman, Joachim (42061332400); Scheunders, Paul (7003845862); D'Haes, Wim (6507641985)","42061332400; 7003845862; 6507641985","Using XML-formatted scores in real-time applications","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873664005&partnerID=40&md5=43f16451858e4e233b93cecb8c43dee8","Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","Ganseman J., Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; Scheunders P., Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; D'Haes W., Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","In this paper we present fast and scalable methods to access relevant data from music scores stored in an XML based notation format, with the explicit goal of using scores in real-time audio processing frameworks. Quick and easy access is important when accessing or traversing a score, for instance for real-time playback. Any time complexity improvement in these contexts is valuable, while memory constraints are usually less important. We show that with some well chosen design choices and precomputation of the necessary data, runtime time complexity of several key score manipulation operations can be reduced to a level that allows use in a real-time context. © 2009 International Society for Music Information Retrieval.","","Audio acoustics; Information retrieval; Scalability; Memory constraints; Music scores; Pre-computation; Real-time application; Real-time audio; Runtimes; Scalable methods; Time complexity; XML","J. Ganseman; Dept. of Physics, IBBT - Visionlab, University of Antwerp, B-2610 Wilrijk (Antwerp), building N, Belgium; email: joachim.ganseman@ua.ac.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Fuhrmann F.; Haro M.; Herrera P.","Fuhrmann, Ferdinand (36642587400); Haro, Martín (36642288600); Herrera, Perfecto (24824250300)","36642587400; 36642288600; 24824250300","Scalability, generality and temporal aspects in automatic recognition of predominant musical instruments in polyphonic music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605129&partnerID=40&md5=3c7a876a11383cc45c13a6e7d5a8db71","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Fuhrmann F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we present an approach towards the classification of pitched and unpitched instruments in polyphonic audio. In particular, the presented study accounts for three aspects currently lacking in literature: model scalability to polyphonic data, model generalisation in respect to the number of instruments, and incorporation of perceptual information. Therefore, our goal is a unifying recognition framework which enables the extraction of the main instruments' information. The applied methodology consists of training classifiers with audio descriptors, using extensive datasets to model the instruments sufficiently. All data consist of real world music, including categories of 11 pitched and 3 percussive instruments. We designed our descriptors by temporal integration of the raw feature values, which are directly extracted from the polyphonic data. Moreover, to evaluate the applicability of modelling temporal aspects in polyphonic audio, we studied the performance of different encodings of the temporal information. Along with accuracies of 63% and 78% for the pitched and percussive classification task, results show both the importance of temporal encoding as well as strong limitations of modelling it accurately. © 2009 International Society for Music Information Retrieval.","","Encoding (symbols); Information retrieval; Scalability; Automatic recognition; Classification tasks; Descriptors; Encodings; Feature values; Model generalisation; Perceptual information; Polyphonic music; Temporal aspects; Temporal information; Temporal integration; Data integration","F. Fuhrmann; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: ferdinand.fuhrmann@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Kim J.H.; Tomasik B.; Turnbull D.","Kim, Joon Hee (55587644200); Tomasik, Brian (35182051900); Turnbull, Douglas (8380095700)","55587644200; 35182051900; 8380095700","Using artist similarity to propagate semantic information","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873643407&partnerID=40&md5=9c0900a57f751f6d73f5d20c86d1389b","Department of Computer Science, Swarthmore College, United States","Kim J.H., Department of Computer Science, Swarthmore College, United States; Tomasik B., Department of Computer Science, Swarthmore College, United States; Turnbull D., Department of Computer Science, Swarthmore College, United States","Tags are useful text-based labels that encode semantic information about music (instrumentation, genres, emotions, geographic origins). While there are a number of ways to collect and generate tags, there is generally a data sparsity problem in which very few songs and artists have been accurately annotated with a sufficiently large set of relevant tags. We explore the idea of tag propagation to help alleviate the data sparsity problem. Tag propagation, originally proposed by Sordo et al., involves annotating a novel artist with tags that have been frequently associated with other similar artists. In this paper, we explore four approaches for computing artists similarity based on different sources of music information (user preference data, social tags, web documents, and audio content). We compare these approaches in terms of their ability to accurately propagate three different types of tags (genres, acoustic descriptors, social tags). We find that the approach based on collaborative filtering performs best. This is somewhat surprising considering that it is the only approach that is not explicitly based on notions of semantic similarity. We also find that tag propagation based on content-based music analysis results in relatively poor performance. © 2009 International Society for Music Information Retrieval.","","Architectural acoustics; Information retrieval; Artist similarities; Audio content; Content-based; Data sparsity problems; Descriptors; Geographic origins; Music analysis; Music information; Poor performance; Preference data; Semantic information; Semantic similarity; Social Tags; Tag propagation; Web document; Semantics","J.H. Kim; Department of Computer Science, Swarthmore College, United States; email: joonhee.kim@alum.swarthmore.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bretherton D.; Smith D.A.; Schraefel M.; Polfreman R.; Everist M.; Brooks J.; Lambert J.","Bretherton, David (26036396900); Smith, Daniel Alexander (57198519758); Schraefel, Mc (57193559107); Polfreman, Richard (36608775200); Everist, Mark (26033865700); Brooks, Jeanice (26032819000); Lambert, Joe (36651283600)","26036396900; 57198519758; 57193559107; 36608775200; 26033865700; 26032819000; 36651283600","Integrating musicology's heterogeneous data sources for better exploration","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873620826&partnerID=40&md5=c5f1464aa8bad26f825fa1e2905e7d0b","University of Southampton, Southampton, SO17 1BJ, United Kingdom","Bretherton D., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Smith D.A., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Schraefel M., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Polfreman R., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Everist M., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Brooks J., University of Southampton, Southampton, SO17 1BJ, United Kingdom; Lambert J., University of Southampton, Southampton, SO17 1BJ, United Kingdom","Musicologists have to consult an extraordinarily heterogeneous body of primary and secondary sources during all stages of their research. Many of these sources are now available online, but the historical dispersal of material across libraries and archives has now been replaced by segregation of data and metadata into a plethora of online repositories. This segregation hinders the intelligent manipulation of metadata, and means that extracting large tranches of basic factual information or running multi-part search queries is still enormously and needlessly time consuming. To counter this barrier to research, the ""musicSpace"" project is experimenting with integrating access to many of musicology's leading data sources via a modern faceted browsing interface that utilises Semantic Web and Web2.0 technologies such as RDF and AJAX. This will make previously intractable search queries tractable, enable musicologists to use their time more efficiently, and aid the discovery of potentially significant information that users did not think to look for. This paper outlines our work to date. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Data and metadata; Data-sources; Faceted browsing; Factual information; Heterogeneous data sources; Intelligent manipulation; Online repositories; Search queries; Secondary sources; Metadata","D. Bretherton; University of Southampton, Southampton, SO17 1BJ, United Kingdom; email: D.Bretherton@soton.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Gó mez E.; Haro M.; Herrera P.","Gó mez, Emilia (55587304300); Haro, Martín (36642288600); Herrera, Perfecto (24824250300)","55587304300; 36642288600; 24824250300","Music and geography: Content description of musical audio from different parts of theworld","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873621358&partnerID=40&md5=4a8edbfed4ec04ae53c0da9abdf19f26","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Gó mez E., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper analyses how audio features related to different musical facets can be useful for the comparative analysis and classification of music from diverse parts of the world. The music collection under study gathers around 6,000 pieces, including traditional music from different geographical zones and countries, as well as a varied set of Western musical styles. We achieve promising results when trying to automatically distinguish music fromWestern and non-Western traditions. A 86.68% of accuracy is obtained using only 23 audio features, which are representative of distinct musical facets (timbre, tonality, rhythm), indicating their complementarity for music description. We also analyze the relative performance of the different facets and the capability of various descriptors to identify certain types of music. We finally present some results on the relationship between geographical location and musical features in terms of extracted descriptors. All the reported outcomes demonstrate that automatic description of audio signals together with data mining techniques provide means to characterize huge music collections from different traditions, complementing ethnomusicological manual analysis and providing a link between music and geography. © 2009 International Society for Music Information Retrieval.","","Audio recordings; Audio signal processing; Information retrieval; Audio features; Audio signal; Comparative analysis; Content description; Data mining techniques; Descriptors; Geographical locations; Geographical zones; Manual analysis; Music collection; Musical audio; Musical features; Paper analysis; Relative performance; Audio acoustics","E. Gó mez; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: emilia.gomez@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Tidhar D.; Fazekas G.; Kolozali S.; Sandler M.","Tidhar, Dan (36629391500); Fazekas, György (37107520200); Kolozali, Sefki (49961785700); Sandler, Mark (7202740804)","36629391500; 37107520200; 49961785700; 7202740804","Publishing music similarity features on the semantic web","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955430512&partnerID=40&md5=bdd8a74ed81af4e283f302ec04496c47","Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","Tidhar D., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Fazekas G., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Kolozali S., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom","We describe the process of collecting, organising and publishing a large set of music similarity features produced by the SoundBite [10] playlist generator tool. These data can be a valuable asset in the development and evaluation of new Music Information Retrieval algorithms. They can also be used in Web-based music search and retrieval applications. For this reason, we make a database of features available on the Semantic Web via a SPARQL end-point, which can be used in Linked Data services. We provide examples of using the data in a research tool, as well as in a simple web application which responds to audio queries and finds a set of similar tracks in our database. © 2009 International Society for Music Information Retrieval.","","Information retrieval; End points; Generator tool; Linked datum; Music information retrieval; Music similarity; Research tools; Search and retrieval; WEB application; Query languages","D. Tidhar; Centre for Digital Music, Queen Mary University of London, London E1, Mile End Road, United Kingdom; email: dan.tidhar@elec.qmul.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Maxwell J.B.; Pasquier P.; Eigenfeldt A.","Maxwell, James B. (55582782500); Pasquier, Philippe (8850202000); Eigenfeldt, Arne (24724145100)","55582782500; 8850202000; 24724145100","Hierarchical sequential memory for music: A cognitive model","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857236743&partnerID=40&md5=52608dd4ee319b018dfb25fb0cca1c1a","Simon Fraser University, Canada","Maxwell J.B., Simon Fraser University, Canada; Pasquier P., Simon Fraser University, Canada; Eigenfeldt A., Simon Fraser University, Canada","We propose a new machine-learning framework called the Hierarchical Sequential Memory for Music, or HSMM. The HSMM is an adaptation of the Hierarchical Temporal Memory (HTM) framework, designed to make it better suited to musical applications. The HSMM is an online learner, capable of recognition, generation, continuation, and completion of musical structures. © 2009 International Society for Music Information Retrieval.","","Cognitive model; Hierarchical temporal memory (htm); Machine-learning; Musical structures; Information retrieval","J.B. Maxwell; Simon Fraser University, Canada; email: jbmaxwel@sfu.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Laurier C.; Sordo M.; Serr̀a J.; Herrera P.","Laurier, Cyril (26031025000); Sordo, Mohamed (43462170300); Serr̀a, Joan (35749172500); Herrera, Perfecto (24824250300)","26031025000; 43462170300; 35749172500; 24824250300","Music mood representations from social tags","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","92","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632528&partnerID=40&md5=3178d5d675e12fcdaffd93ae259397a8","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Laurier C., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Sordo M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Serr̀a J., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper presents findings about mood representations. We aim to analyze how do people tag music by mood, to create representations based on this data and to study the agreement between experts and a large community. For this purpose, we create a semantic mood space from last.fm tags using Latent Semantic Analysis. With an unsupervised clustering approach, we derive from this space an ideal categorical representation. We compare our community based semantic space with expert representations from Hevner and the clusters from the MIREX Audio Mood Classification task. Using dimensional reduction with a Self-Organizing Map, we obtain a 2D representation that we compare with the dimensional model from Russell. We present as well a tree diagram of the mood tags obtained with a hierarchical clustering approach. All these results show a consistency between the community and the experts as well as some limitations of current expert models. This study demonstrates a particular relevancy of the basic emotions model with four mood clusters that can be summarized as: happy, sad, angry and tender. This outcome can help to create better ground truth and to provide more realistic mood classification algorithms. Furthermore, this method can be applied to other types of representations to build better computational models. © 2009 International Society for Music Information Retrieval.","","Conformal mapping; Information retrieval; Basic emotions; Classification algorithm; Classification tasks; Community-based; Computational model; Dimensional model; Dimensional reduction; Expert model; Ground truth; Hierarchical clustering approach; Last.fm; Latent Semantic Analysis; Semantic Space; Social Tags; Tree diagram; Unsupervised clustering; Semantics","C. Laurier; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: cyril.laurier@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Klapuri A.","Klapuri, Anssi (6602945099)","6602945099","A method for visualizing the pitch content of polyphonic music signals","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873652876&partnerID=40&md5=92f9a7dc466914ac93cae0a10ab9c66d","Department of Signal Processing, Tampere University of Technology, Finland","Klapuri A., Department of Signal Processing, Tampere University of Technology, Finland","This paper proposes a method for visualizing the pitch content of polyphonicmusic signals. More specifically, amodel is proposed for calculating the salience of pitch candidates within a given pitch range, and an optimization technique is proposed to find the parameters of the model. The aim is to produce a continuous function which shows peaks at the positions of true pitches and where spurious peaks at multiples and submultiples of the true pitches are suppressed. The proposed method was evaluated using synthesized MIDI signals, for which it outperformed a baseline method in terms of precision and recall. A straightforward visualization technique is proposed to render the pitch salience function on the traditional staves when the musical key and barline information is available. © 2009 International Society for Music Information Retrieval.","","Baseline methods; Continuous functions; Optimization techniques; Polyphonic music; Precision and recall; Visualization technique; Information retrieval","A. Klapuri; Department of Signal Processing, Tampere University of Technology, Finland; email: anssi.klapuri@tut.fi","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lee J.H.; Cameron Jones M.; Stephen Downie J.","Lee, Jin Ha (57190797465); Cameron Jones, M. (23008559000); Stephen Downie, J. (7102932568)","57190797465; 23008559000; 7102932568","An analysis of ismir proceedings: Patterns of authorship, topic, and citation","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955445724&partnerID=40&md5=a464fd0043db394f1ea13785cac25f7a","Information School, University of Washington, United States; University of Illinois, Urbana-Champaign, United States","Lee J.H., Information School, University of Washington, United States; Cameron Jones M., University of Illinois, Urbana-Champaign, United States; Stephen Downie J., University of Illinois, Urbana-Champaign, United States","This paper presents analyses of peer-reviewed papers and posters published in the past nine years of ISMIR proceedings: examining publication and authorship practices, topics and titles of research, as well as the citation patterns among the ISMIR proceedings. The main objective is to provide an overview of the progress made over the past nine years in the ISMIR community and to obtain some insights into where the community should be heading in the coming years. Overall, the ISMIR community has grown considerably over the past nine years, both in the number of papers and posters published each year, as well as the number of authors contributing. Furthermore, the amount of collaboration among authors, as reflected in co-authorship, has increased. Main areas of research are revealed by an analysis of most commonly used title terms. Also, major authors and research groups are identified by analyzing the co-authorship and citation patterns in ISMIR proceedings. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Research; Coauthorship; Research groups; Title terms; Publishing","J.H. Lee; Information School, University of Washington, United States; email: jinhalee@uw.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Haro M.; Herrera P.","Haro, Martín (36642288600); Herrera, Perfecto (24824250300)","36642288600; 24824250300","From low-level to song-level percussion descriptors of polyphonic music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954441720&partnerID=40&md5=b3d06b26d114ee1be0c634cb4e6a2862","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Haro M., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","We address here the automatic description of percussive events in real-world polyphonic music. By taking a pattern recognition approach we evaluate more than 2,450 objectlevel features. Three binary instrument-wise support vector machines (SVM) are built from a training set of more that 100 songs and 10 genres. Then, we use these binary models to build a drum transcription system achieving comparable results with state of the art algorithms. Finally, we present 17 song-level percussion descriptors computed from the imperfect output of the transcription algorithm. We evaluate the usefulness of the proposed descriptors in music information retrieval (MIR) tasks like genre classification, danceability estimation and Western vs. non- Western music discrimination. We conclude that the presented song-level percussion descriptors provide complementary information to ""classic"" descriptors, that can help in the previously mentioned MIR tasks. © 2009 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Musical instruments; Pattern recognition; Support vector machines; Binary models; Descriptors; Genre classification; Music information retrieval; Polyphonic music; Real-world; State-of-the-art algorithms; Training sets; Computer music","M. Haro; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: martin.haro@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lukashevich H.; Abeßer J.; Dittmar C.; Grossmann H.","Lukashevich, Hanna (6508121862); Abeßer, Jakob (36607532300); Dittmar, Christian (15051598000); Grossmann, Holger (36642315700)","6508121862; 36607532300; 15051598000; 36642315700","From multi-labeling to multi-domain-labeling: A novel two-dimensional approach to music genre classification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052114328&partnerID=40&md5=0cfa399f84fffbc91508c72f4bbf174f","Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany","Lukashevich H., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Abeßer J., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Dittmar C., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; Grossmann H., Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany","In this publication we describe a novel two-dimensional approach for automatic music genre classification. Although the subject poses a well studied task in Music Information Retrieval, some fundamental issues of genre classification have not been covered so far. Especiallymany modern genres are influenced by manifold musical styles. Most of all, this holds true for the broad category ""World Music"", which comprises many different regional styles and a mutual mix up thereof. A common approach to tackle this issue in manual categorization is to assign multiple genre labels to a single recording. However, for commonly used automatic classification algorithms, multilabeling poses a problem due to its ambiguities. Thus, we propose to break down multi-label genre annotations into single-label annotations within given time segments and musical domains. A corresponding multi-stage evaluation based on a representative set of items from a global music taxonomy is performed and discussed accordingly. Therefore, we conduct 3 different experiments that cover multi-labeling, multi-labeling with time segmentation and the proposed multi-domain labeling. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automatic classification; Break down; Genre classification; Multi domains; Multi-label; Multi-stage; Multilabeling; Music genre classification; Music information retrieval; Musical domains; Time segmentation; Time segments; Two dimensional","H. Lukashevich; Fraunhofer Institute for Digital Media Technologies, Ilmenau, Germany; email: lkh@idmt.fraunhofer.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Tomasik B.; Kim J.H.; Ladlow M.; Augat M.; Tingle D.; Wicentowski R.; Turnbull D.","Tomasik, Brian (35182051900); Kim, Joon Hee (55587644200); Ladlow, Margaret (55588211900); Augat, Malcolm (35799381600); Tingle, Derek (36054272900); Wicentowski, Richard (8445496600); Turnbull, Douglas (8380095700)","35182051900; 55587644200; 55588211900; 35799381600; 36054272900; 8445496600; 8380095700","Using regression to combine data sources for semantic music discovery","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863731098&partnerID=40&md5=e0be98655a2ef2b4de6845eb38e78250","Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States","Tomasik B., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Kim J.H., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Ladlow M., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Augat M., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Tingle D., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Wicentowski R., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; Turnbull D., Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States","In the process of automatically annotating songs with descriptive labels, multiple types of input information can be used. These include keyword appearances in web documents, acoustic features of the song's audio content, and similarity with other tagged songs. Given these individual data sources, we explore the question of how to aggregate them. We find that fixed-combination approaches like sum and max perform well but that trained linear regression models work better. Retrieval performance improves with more data sources. On the other hand, for large numbers of training songs, Bayesian hierarchical models that aim to share information across individual tag regressions offer no advantage. © 2009 International Society for Music Information Retrieval.","","Hierarchical systems; Information retrieval; Labels; Regression analysis; Acoustic features; Audio content; Bayesian hierarchical model; Data-sources; Linear regression models; Retrieval performance; Web document; Semantics","B. Tomasik; Department of Computer Science, Swarthmore College, Swarthmore PA 19081, United States; email: btomasi1@alum.swarthmore.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Thomas V.; Fremerey C.; Damm D.; Clausen M.","Thomas, Verena (56519204500); Fremerey, Christian (23396821400); Damm, David (57197094801); Clausen, Michael (56225233200)","56519204500; 23396821400; 57197094801; 56225233200","Slave: A score-lyrics-audio-video-explorer","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873659612&partnerID=40&md5=34a969da8bb1181dadbeefc2ef8cd933","Department of Computer Science III, University of Bonn, Germany","Thomas V., Department of Computer Science III, University of Bonn, Germany; Fremerey C., Department of Computer Science III, University of Bonn, Germany; Damm D., Department of Computer Science III, University of Bonn, Germany; Clausen M., Department of Computer Science III, University of Bonn, Germany","We introduce the music exploration system SLAVE, which is based upon previous developments of our group. SLAVE manages multimedia music collections and allows for multimodal navigation, playback, and visualization in an efficient and user-friendly manner. 1 While previously the focus of our system development has been the simultaneous exploration of digitized sheet music and audio, with SLAVE we enhance the functionalities by video and lyrics to achieve a more comprehensive music interaction. In this paper, we concentrate on two aspects. Firstly, we integrate video documents into our framework. Secondly, we introduce a graphical user interface for semi-automatic feature extraction, indexing, and synchronization of heterogeneous music collections. The output of this GUI is used by SLAVE to offer both high quality audio and video playback with time-synchronous display of digitized sheet music and content-based search. © 2009 International Society for Music Information Retrieval.","","Audio recordings; Computer graphics; Graphical user interfaces; Information retrieval; Content-based search; High-quality audio; Multi-modal; Music collection; Music exploration systems; Music interaction; Semi-automatics; System development; Video documents; Audio acoustics","V. Thomas; Department of Computer Science III, University of Bonn, Germany; email: thomas@iai.uni-bonn.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Langlois T.; Marques G.","Langlois, Thibault (6602432323); Marques, Gonçalo (23995645000)","6602432323; 23995645000","A music classification method based on timbral features","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873657155&partnerID=40&md5=88384ee6c4b6c17c129134e48bae63bd","Faculdade de Ciências, Universidade de Lisboa, Portugal; Instituto Superior de Engenharia de Lisboa, Portugal","Langlois T., Faculdade de Ciências, Universidade de Lisboa, Portugal; Marques G., Instituto Superior de Engenharia de Lisboa, Portugal","This paper describes amethod formusic classification based solely on the audio contents of the music signal. More specifically, the audio signal is converted into a compact symbolic representation that retains timbral characteristics and accounts for the temporal structure of a music piece. Models that capture the temporal dependencies observed in the symbolic sequences of a set of music pieces are built using a statistical language modeling approach. The proposed method is evaluated on two classification tasks (Music Genre classification and Artist Identification) using publicly available datasets. Finally, a distance measure between music pieces is derived from the method and examples of playlists generated using this distance are given. The proposed method is compared with two alternative approaches which include the use of Hidden Markov Models and a classification scheme that ignores the temporal structure of the sequences of symbols. In both cases the proposed approach outperforms the alternatives. © 2009 International Society for Music Information Retrieval.","","Audio signal processing; Classification (of information); Computational linguistics; Hidden Markov models; Information retrieval; Natural language processing systems; Alternative approach; Audio content; Audio signal; Classification scheme; Distance measure; Music classification; Music genre classification; Music signals; Statistical language modeling; Symbolic representation; Symbolic sequence; Temporal structures; Two classification; Audio acoustics","T. Langlois; Faculdade de Ciências, Universidade de Lisboa, Portugal; email: tl@di.fc.ul.pt","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Juhász Z.","Juhász, Zoltán (56935971200)","56935971200","Motive identification in 22 folksong corpora using dynamic time warping and self organizing maps","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861482921&partnerID=40&md5=cc82dd5036f740db231e1c6c2979bb27","Research Institute for Technical Physics and Materials Science, Budapest H-1525, P.O.B 49, Hungary","Juhász Z., Research Institute for Technical Physics and Materials Science, Budapest H-1525, P.O.B 49, Hungary","A system for automatic motive identification of large folksong corpora is described in this article. The method is based on a dynamic time warping algorithm determining inherent repeating elements of the melodies and a self-organizing map that learns the most typical motive contours. Using this system, the typical motive collections of 22 cultures in Eurasia have been determined, and another great common self organising map has been trained by the unified collection of the national/areal motive collections. The analysis of the overlaps of the national-areal excitations on the common map allowed us to draw a graph of connections, which shows two main distinct groups, according to the geographical distribution. © 2009 International Society for Music Information Retrieval.","","Conformal mapping; Self organizing maps; Common map; Dynamic time warping; Dynamic time warping algorithms; Eurasia; Information retrieval","Z. Juhász; Research Institute for Technical Physics and Materials Science, Budapest H-1525, P.O.B 49, Hungary; email: juhasz@mfa.kfki","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Govaerts S.; Duval E.","Govaerts, Sten (7801629780); Duval, Erik (7006487422)","7801629780; 7006487422","A web-based approach to determine the origin of an artist","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051518421&partnerID=40&md5=8d131e140be1e667484072466fa5eade","Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","Govaerts S., Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Duval E., Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","One can define the origin of an artist as the geographical location where he started his career. The origin is an important metadata element, because it can help to specify subgenres, be an indicator of regional popularity and improve recommendations. In this paper, we present six methods to determine the origin, based on Web data sources: one extracts data from Last.fm, two query Freebase and three analyze biographies. We evaluate the different methods with 11275 artists. Circa 55% of the artists can be classified using biographies. The best Freebase method can classify 26% and the Last.fm based method 7%. When comparing on accuracy, the Last.fm and Freebase methods perform similarly with around 90% accuracy. For the biography-based methods we achieve 71%. To improve coverage, a final, hybrid method achieves 77% accuracy and 60% coverage. The accuracy of the continent classification is 87%. As a showcase for our classifier, we developed a mashup application that displays, among others, information about the origin of artists from radio station playlists on a map. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Metadata; Radio stations; Geographical locations; Hybrid method; Last.fm; Mashup applications; Web data sources; Web-based approach; Biographies","S. Govaerts; Department of Computer Science, K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; email: sten.govaerts@cs.kuleuven.be","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Kato M.P.","Kato, Makoto P. (35317662700)","35317662700","Rhythmixearch: Searching for unknown music by mixing known music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873616645&partnerID=40&md5=467b99f2a2c667299859448653bfa47c","Department of Social Informatics, Graduate School of Informatics, Kyoto University, Kyoto, Japan","Kato M.P., Department of Social Informatics, Graduate School of Informatics, Kyoto University, Kyoto, Japan","We present a novel method for searching for unknown music. RhythMiXearch is a music search system we developed that can accept two music inputs and mix those inputs to search for music that could reasonably be a result of the mixture. This approach expands the ability of Query-by-Example and allows greater flexibility for users in finding unknown music. Each music piece stored by our system is characterized by text data written by users, i.e., review data. We used Latent Dirichlet Allocation (LDA) to capture semantics from the reviews that were then used to characterize the music by Hevner's eight impression categories. RhythMiXearch mixes two music inputs in accordance with a probabilistic mixture model and finds music that is the most likely product of the mixture. Our experimental results indicate that the proposed method is comparable to human in searching for music by multiple examples. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Semantics; Statistics; Latent dirichlet allocations; Most likely; Probabilistic mixture models; Query-by-example; Review datum; Search system; Text data; Mixtures","M.P. Kato; Department of Social Informatics, Graduate School of Informatics, Kyoto University, Kyoto, Japan; email: kato@dl.kuis.kyoto-u.ac.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Bello J.P.","Bello, Juan Pablo (7102889110)","7102889110","Grouping recorded music by structural similarity","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873629090&partnerID=40&md5=891335b58820a65bb95fedea31b72e5e","Music and Audio Research Lab (MARL), New York University, United States","Bello J.P., Music and Audio Research Lab (MARL), New York University, United States","This paper introduces a method for the organization of recorded music according to structural similarity. It uses the Normalized Compression Distance (NCD) to measure the pairwise similarity between songs, represented using beat-synchronous self-similarity matrices. The approach is evaluated on its ability to cluster a collection into groups of performances of the same musical work. Tests are aimed at finding the combination of system parameters that improve clustering, and at highlighting the benefits and shortcomings of the proposed method. Results show that structural similarities can be well characterized by this approach, given consistency in beat tracking and overall song structure. © 2009 International Society for Music Information Retrieval.","","Audio recordings; Information retrieval; Beat tracking; Normalized compression distance; Self-similarities; Song structure; Structural similarity; Encoding (symbols)","J.P. Bello; Music and Audio Research Lab (MARL), New York University, United States; email: jpbello@nyu.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Eerola T.; Lartillot O.; Toiviainen P.","Eerola, Tuomas (6602209042); Lartillot, Olivier (6507137446); Toiviainen, Petri (6602829513)","6602209042; 6507137446; 6602829513","Prediction of multidimensional emotional ratings in music from audio using multivariate regression models","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","124","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873646150&partnerID=40&md5=14ed0004388ee1ab623b2ec10743b3a2","Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Toiviainen P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Content-based prediction of musical emotions and moods has a large number of exciting applications in Music Information Retrieval. However, what should be predicted, and precisely how, remain a challenge in the field. We provide an empirical comparison of two common paradigms of emotion representation in music, opposing a multidimensional space to a set of basic emotions. New groundtruth data consisting of film soundtracks was used to assess the compatibility of these models. The findings suggest that the two are highly compatible and a quantitative mapping between the two is provided. Next we propose a model predicting perceived emotions based on a set of features extracted from the audio. The feature selection and transformation is given special emphasis and three separate data reduction techniques are compared (stepwise regression, principal component analysis, and partial least squares regression). Best linear models consisting of 2- 5 predictors from the data reduction process were able to account for between 58 and 85% of the variance. In general, partial least squares models performed the best and the data transformation has a significant role in building linear models. © 2009 International Society for Music Information Retrieval.","","Behavioral research; Data reduction; Forecasting; Information retrieval; Principal component analysis; Basic emotions; Content-based; Data transformation; Emotion representation; Empirical - comparisons; In-buildings; Multi-dimensional space; Multivariate regression models; Music information retrieval; Musical emotion; Partial least squares models; Partial least squares regression; Quantitative mapping; Reduction process; Reduction techniques; Stepwise regression; Audio acoustics","T. Eerola; Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; email: Tuomas.Eerola@jyu.fi","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Julià C.F.; Jordà S.","Julià, Carles F. (25927215100); Jordà, Sergi (8228030900)","25927215100; 8228030900","Songexplorer: A tabletop application for exploring large collections of songs","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053102744&partnerID=40&md5=5f725e5917a3d8ca5395a107810fc143","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","Julià C.F., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; Jordà S., Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","This paper presents SongExplorer, a system for the exploration of large music collections on tabletop interfaces. SongExplorer addresses the problem of finding new interesting songs on large music databases, from an interaction design perspective. Using high level descriptors of musical songs, SongExplorer creates a coherent 2D map based on similarity, in which neighboring songs tend to be more similar. All songs are represented as throbbing circles that highlight their more relevant high-level properties, and the resulting music map is browseable and zoomable by the users who can use their fingers as well as specially designed tangible pucks, for helping them to find interesting music, independently of their previous knowledge of the collection. SongExplorer also offers basic player capabilities, allowing the users to organize the songs they have just discovered into playlists which can be manipulated as well as played and displayed. In this paper, the system hardware, software and interaction design are explained, and the usability tests carried are presented. Finally, conclusions and future work are discussed. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Descriptors; Interaction design; Large music collections; Music database; System hardware; Tabletop applications; Tabletop interfaces; Usability tests; Software testing","C.F. Julià; Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain; email: carles.fernandez@upf.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Su M.-Y.; Yang Y.-H.; Lin Y.-C.; Chen H.","Su, Min-Yian (55588412500); Yang, Yi-Hsuan (55218558400); Lin, Yu-Ching (57203772039); Chen, Homer (8236841800)","55588412500; 55218558400; 57203772039; 8236841800","An integrated approach to music boundary detection","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856929813&partnerID=40&md5=2a902758a658f39d950fe0a5c1d4187c","National Taiwan University, Taiwan","Su M.-Y., National Taiwan University, Taiwan; Yang Y.-H., National Taiwan University, Taiwan; Lin Y.-C., National Taiwan University, Taiwan; Chen H., National Taiwan University, Taiwan","Music boundary detection is a fundamental step of music analysis and summarization. Existing works use either unsupervised or supervised methodologies to detect boundary. In this paper, we propose an integrated approach that takes advantage of both methodologies. In particular, a graph-theoretic approach is proposed to fuse the results of an unsupervised model and a supervised one by the knowledge of the typical length of a music section. To further improve accuracy, a number of novel mid-level features are developed and incorporated to the boundary detection framework. Evaluation result on the RWC dataset shows the effectiveness of the proposed approach. © 2009 International Society for Music Information Retrieval.","","Graph theory; Information retrieval; Stress intensity factors; Boundary detection; Evaluation results; Graph theoretic approach; Integrated approach; Mid-level features; Music analysis; Integrated control","M.-Y. Su; National Taiwan University, Taiwan; email: sui751004@gmail.com","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Marolt M.","Marolt, Matija (6603601816)","6603601816","Probabilistic segmentation and labeling of ethnomusicological field recordings","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426490&partnerID=40&md5=4f07d3cf11abe8ecbbc60f7a32824e33","Faculty of Computer and Information Science, University of Ljubljana, Slovenia","Marolt M., Faculty of Computer and Information Science, University of Ljubljana, Slovenia","The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ehtnomuse archive is presented. © 2009 International Society for Music Information Retrieval.","","Algorithms; Information retrieval; Bell chiming; Environmental noise; Field recording; Manual segmentation; Music performance; Posterior probability; Prior knowledge; Probabilistic models; Robust algorithm; Audio recordings","M. Marolt; Faculty of Computer and Information Science, University of Ljubljana, Slovenia; email: matija.marolt@fri.uni-lj.si","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Weil J.; Sikora T.; Durrieu J.-L.; Richard G.","Weil, Jan (55311448900); Sikora, Thomas (36777640600); Durrieu, J.-L. (26424171600); Richard, Gaël (57195915952)","55311448900; 36777640600; 26424171600; 57195915952","Automatic generation of lead sheets from polyphonic music signals","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052979880&partnerID=40&md5=32787e30b7a614d796532e65180b16a0","Communication Systems Group, Technische Universität Berlin, Germany; Institut Telecom, Telecom ParisTech, CNRS LTCI, France","Weil J., Communication Systems Group, Technische Universität Berlin, Germany; Sikora T., Communication Systems Group, Technische Universität Berlin, Germany; Durrieu J.-L., Institut Telecom, Telecom ParisTech, CNRS LTCI, France; Richard G., Institut Telecom, Telecom ParisTech, CNRS LTCI, France","A lead sheet is a type of music notation which summarizes the content of a song. The usual elements that are reproduced are the melody, chords, tempo, time signature, style and the lyrics, if any. In this paper we propose a system that aims at transcribing both the melody and the associated chords in a beat-synchronous framework. A beat tracker identifies the pulse positions and thus defines a beat grid on which the chord sequence and the melody notes are mapped. The harmonic changes are used to estimate the time signature and the down beats as well as the key of the piece. The different modules perform very well on each of the different tasks, and the lead sheets that were rendered show the potential of the approaches adopted in this paper. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automatic Generation; Chord sequence; Music notation; Polyphonic music; Pulse positions; Audio signal processing","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Godøy R.I.; Jensenius A.R.","Godøy, Rolf Inge (7005818369); Jensenius, Alexander Refsum (14026820200)","7005818369; 14026820200","Body movement in music information retrieval","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871795195&partnerID=40&md5=de36be4659a31179aefb43773e3d02fb","Department of Musicology, FourMs, University of Oslo, Norway","Godøy R.I., Department of Musicology, FourMs, University of Oslo, Norway; Jensenius A.R., Department of Musicology, FourMs, University of Oslo, Norway","We can see many and strong links between music and human body movement in musical performance, in dance, and in the variety of movements that people make in listening situations. There is evidence that sensations of human body movement are integral to music as such, and that sensations of movement are efficient carriers of information about style, genre, expression, and emotions. The challenge now in MIR is to develop means for the extraction and representation of movement-inducing cues from musical sound, as well as to develop possibilities for using body movement as input to search and navigation interfaces in MIR. © 2009 International Society for Music Information Retrieval.","","Body movements; Human body movement; Music information retrieval; Musical performance; Musical sounds; Navigation interface; Strong link; Information retrieval","R.I. Godøy; Department of Musicology, FourMs, University of Oslo, Norway; email: r.i.godoy@imv.uio.no","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Mauch M.; Noland K.; Dixon S.","Mauch, Matthias (36461512900); Noland, Katy (18042247700); Dixon, Simon (7201479437)","36461512900; 18042247700; 7201479437","Using musical structure to enhance automatic chord transcription","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","65","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873653194&partnerID=40&md5=4bbde6022f17cd018ba4f7c30bd43f07","Centre for Digital Music, Queen Mary University of London, United Kingdom","Mauch M., Centre for Digital Music, Queen Mary University of London, United Kingdom; Noland K., Centre for Digital Music, Queen Mary University of London, United Kingdom; Dixon S., Centre for Digital Music, Queen Mary University of London, United Kingdom","Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy. © 2009 International Society for Music Information Retrieval.","","Extraction; Information retrieval; Baseline methods; Computing-task; Context models; Musical structures; Repetitive structure; Segmentation informations; Smoothing techniques; Audio acoustics","M. Mauch; Centre for Digital Music, Queen Mary University of London, United Kingdom; email: matthias.mauch@elec.qmul.ac.uk","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Niedermayer B.","Niedermayer, Bernhard (37113938000)","37113938000","Improving accuracy of polyphonic music-to-score alignment","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472299&partnerID=40&md5=c1a6e45b4522f88f57e5162e260b0bc9","Department for Computational Perception, Johannes Kepler University, Linz, Austria","Niedermayer B., Department for Computational Perception, Johannes Kepler University, Linz, Austria","This paper presents a new method to refine music-to-score alignments. The proposed system works offline in two passes, where in the first step a state-of-the art alignment based on chroma vectors and dynamic time warping is performed. In the second step a non-negative matrix factorization is calculated within a small search window around each predicted note onset, using pretrained tone models of only those pitches which are expected to be played within that window. Note onsets are then reset according to the pitch activation patterns yielded by the matrix factorization. In doing so, we are able to resolve individual notes within a chord. We show that this method is feasible of increasing the accuracy of aligned note's onsets which are already aligned relatively near to the real note attack. However it is so far not suitable for the detection and correction of outliers which are displaced by a large timespan. We also compared our system to a reference method showing that it outperforms bandpass filtering based onset detection in the refinement step. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Activation patterns; Band pass filtering; Dynamic time warping; Matrix factorizations; Nonnegative matrix factorization; Offline; Onset detection; Reference method; Refinement step; Search windows; State of the art; Alignment","B. Niedermayer; Department for Computational Perception, Johannes Kepler University, Linz, Austria; email: bernhard.niedermayer@jku.at","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Ishizaki H.; Hoashi K.; Takishima Y.","Ishizaki, Hiromi (16068647300); Hoashi, Keiichiro (6603899930); Takishima, Yasuhiro (6602909921)","16068647300; 6603899930; 6602909921","Full-automatic DJ mixing system with optimal tempo adjustment based on measurement function of user discomfort","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873632142&partnerID=40&md5=c213acb800c3bb6d77f96d3cd698bc74","KDDI R and D Laboratories Inc., Japan","Ishizaki H., KDDI R and D Laboratories Inc., Japan; Hoashi K., KDDI R and D Laboratories Inc., Japan; Takishima Y., KDDI R and D Laboratories Inc., Japan","This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing. In this paper, we assume that the difference between the original and adjusted songs is the main cause of user discomfort in the mixed song. In order to preserve user comfort, we define the measurement function of user discomfort based on the results of a subjective experiment. Furthermore, this paper proposes a unique tempo adjustment technique called ""optimal tempo adjustment"", which is robust for any combination of tempi of songs to be mixed. In the subjective experiment, the proposed method obtained higher averages of user ratings on three evaluation items compared to the conventional method. These results indicate that our system is able to preserve user comfort. © 2009 International Society for Music Information Retrieval.","","Experiments; Information retrieval; Optimization; Conventional methods; Evaluation items; Measurement function; Mixing method; Mixing system; Subjective experiments; User comforts; User rating; Mixing","H. Ishizaki; KDDI R and D Laboratories Inc., Japan; email: ishizaki@kddilabs.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lin H.-Y.; Lin Y.-T.; Tien M.-C.; Wu J.-L.","Lin, Heng-Yi (55587671600); Lin, Yin-Tzu (57209831317); Tien, Ming-Chun (57204099478); Wu, Ja-Ling (7409250086)","55587671600; 57209831317; 57204099478; 7409250086","Music paste: Concatenating music clips based on chroma and rhythm features","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661485&partnerID=40&md5=025d4d5ff848f3b1366303e050902c70","Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","Lin H.-Y., Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; Lin Y.-T., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Tien M.-C., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Wu J.-L., Department of Computer Science and Information Engineering, National Taiwan University, Taiwan, Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","In this paper, we provide a tool for automatically choosing appropriate music clips from a given audio collection and properly combining the chosen clips. To seamlessly concatenate two different music clips without causing any audible defect is really a hard nut to crack. Borrowing the idea from the musical dice game and the DJ's strategy and considering psychoacoustics, we employ the currently available audio analysis and editing techniques to paste music sounded as pleasant as possible. Besides, we conduct subjective evaluations on the correlation between pasting methods and the auditory quality of combined clips. The experimental results show that the automatically generated music pastes are acceptable to most of the evaluators. The proposed system can be used to generate lengthened or shortened background music and dancing suite, which is useful for some audio-assisted multimedia applications. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Quality control; Audio analysis; Auditory quality; Automatically generated; Background musics; Dice games; Multimedia applications; Music clips; Rhythm features; Subjective evaluations; Audio acoustics","H.-Y. Lin; Department of Computer Science and Information Engineering, National Taiwan University, Taiwan; email: waquey@cmlab.csie.ntu.edu.tw","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Kuuskankare M.; Laurson M.","Kuuskankare, Mika (56301483200); Laurson, Mikael (15519696600)","56301483200; 15519696600","MIR in ENP - Rule-based music information retrieval from symbolic music notation","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873661087&partnerID=40&md5=f5c4977c67becd0beb8e271f0abf4e0e","Centre for Music and Technology, Sibelius Academy, Finland","Kuuskankare M., Centre for Music and Technology, Sibelius Academy, Finland; Laurson M., Centre for Music and Technology, Sibelius Academy, Finland","Symbolic music information retrieval is one of the most underrepresented areas in the field of MIR. Here, symbolic music means common practice music notation-the musician readable format. In this paper we introduce a novel rule-based symbolic music retrieval mechanism. The Scripting system-ENP-Script-is augmentedwithMIR functionality. It allows us to performsophisticated retrieval operations on symbolic musical scores prepared with the help of the music notation system ENP. We will also give a special attention to visualization of the query results. All the statistical queries, such as histograms, are visualized with the help of common music notation where appropriate. N-grams and more complex queries-the ones dealing with voice leading, for example- are visualized directly in the score. Our aim is to demonstrate the power and expressivity of the combination of common music notation and a rulebased scripting language through several challenging examples. © 2009 International Society for Music Information Retrieval.","","Music information retrieval; Music notation; Music retrieval; Musical score; N-grams; Query results; Rule based; Scripting languages; Statistical queries; Symbolic music information; Information retrieval","M. Kuuskankare; Centre for Music and Technology, Sibelius Academy, Finland; email: mkuuskan@siba.fi","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Müller M.; Grosche P.; Wiering F.","Müller, Meinard (7404689873); Grosche, Peter (55413290700); Wiering, Frans (8976178100)","7404689873; 55413290700; 8976178100","Robust segmentation and annotation of folk song recordings","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864694464&partnerID=40&md5=3d421ff5b639440dddfcb7ede3375f59","MPI Informatik, Saarland University, Saarbrücken, Germany; Department of Information and Computing Sciences, Utrecht University, Utrecht, Netherlands","Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Grosche P., MPI Informatik, Saarland University, Saarbrücken, Germany; Wiering F., Department of Information and Computing Sciences, Utrecht University, Utrecht, Netherlands","Even though folk songs have been passed down mainly by oral tradition, most musicologists study the relation between folk songs on the basis of score-based transcriptions. Due to the complexity of audio recordings, once having the transcriptions, the original recorded tunes are often no longer studied in the actual folk song research though they still may contain valuable information. In this paper, we introduce an automated approach for segmenting folk song recordings into its constituent stanzas, which can then be made accessible to folk song researchers by means of suitable visualization, searching, and navigation interfaces. Performed by elderly non-professional singers, the main challenge with the recordings is that most singers have serious problems with the intonation, fluctuating with their voices even over several semitones throughout a song. Using a combination of robust audio features along with various cleaning and audio matching strategies, our approach yields accurate segmentations even in the presence of strong deviations. © 2009 International Society for Music Information Retrieval.","","Image segmentation; Information retrieval; Transcription; Audio features; Audio matching; Automated approach; Folk songs; Navigation interface; Oral tradition; Robust segmentation; Audio recordings","M. Müller; MPI Informatik, Saarland University, Saarbrücken, Germany; email: meinard@mpi-inf.mpg.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Nichols E.; Morris D.; Basu S.; Raphael C.","Nichols, Eric (57196921182); Morris, Dan (55547128588); Basu, Sumit (14064542600); Raphael, Christopher (7004214964)","57196921182; 55547128588; 14064542600; 7004214964","Relationships between lyrics and melody in popular music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873670905&partnerID=40&md5=65246405fb3883543a8d2b12b3351fcc","Indiana University, Bloomington, IN, United States; Microsoft Research, Redmond, WA, United States","Nichols E., Indiana University, Bloomington, IN, United States; Morris D., Microsoft Research, Redmond, WA, United States; Basu S., Microsoft Research, Redmond, WA, United States; Raphael C., Indiana University, Bloomington, IN, United States","Composers of popular music weave lyrics, melody, and instrumentation together to create a consistent and compelling emotional scene. The relationships among these elements are critical to musical communication, and understanding the statistics behind these relationships can contribute to numerous problems in music information retrieval and creativity support. In this paper, we present the results of an observational study on a large symbolic database of popular music; our results identify several patterns in the relationship between lyrics and melody. © 2009 International Society for Music Information Retrieval.","","Creativity support; Music information retrieval; Musical communication; Observational study; Popular music; Symbolic database; Information retrieval","E. Nichols; Indiana University, Bloomington, IN, United States; email: epnichol@indiana.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Tsuchihashi Y.; Kitahara T.; Katayose H.","Tsuchihashi, Yusuke (26422542600); Kitahara, Tetsuro (7201371361); Katayose, Haruhiro (8341539100)","26422542600; 7201371361; 8341539100","Using bass-line features for content-based mir","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76949085512&partnerID=40&md5=cee0d97f42b74f7f9ae8c43af5e5defc","Kwansei Gakuin University, Japan; CrestMuse Project, CREST, JST, Japan","Tsuchihashi Y., Kwansei Gakuin University, Japan; Kitahara T., Kwansei Gakuin University, Japan, CrestMuse Project, CREST, JST, Japan; Katayose H., Kwansei Gakuin University, Japan, CrestMuse Project, CREST, JST, Japan","We propose new audio features that can be extracted from bass lines. Most previous studies on content-based music information retrieval (MIR) used low-level features such as the mel-frequency cepstral coefficients and spectral centroid. Musical similarity based on these features works well to some extent but has a limit to capture fine musical characteristics. Because bass lines play important roles in both harmonic and rhythmic aspects and have a different style for each music genre, our bass-line features are expected to improve the similarity measure and classification accuracy. Furthermore, it is possible to achieve a similarity measure that enhances the bass-line characteristics by weighting the bass-line and other features. Results for applying our features to automatic genre classification and music collection visualization showed that our features improved genre classification accuracy and did achieve a similarity measure that enhances bass-line characteristics.","","Information retrieval; Information science; Audio features; Automatic genre classification; Classification accuracy; Content-based; Genre classification; Low-level features; Mel-frequency cepstral coefficients; Music collection; Music genre; Music information retrieval; Musical similarity; Similarity measure; Classification (of information)","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Ganseman J.; Scheunders P.; D'haes W.","Ganseman, Joachim (42061332400); Scheunders, Paul (7003845862); D'haes, Wim (6507641985)","42061332400; 7003845862; 6507641985","Using xquery on musicxml databases for musicological analysis","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472565&partnerID=40&md5=a41c2f41b5aaeb0f2a04f4b8197828ec","IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","Ganseman J., IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; Scheunders P., IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; D'haes W., Mu Technologies NV, B-3500 Hasselt, Singelbeekstraat 121, Belgium","MusicXML is a fairly recent XML-based file format for music scores, now supported by many score and audio editing software applications. Several online score library projects exist or are emerging, some of them using MusicXML as main format. When storing a large set of XML-encoded scores in an XML database, XQuery can be used to retrieve information from this database. We present some small practical examples of such large scale analysis, using the Wikifonia lead sheet database and the eXist XQuery engine. This shows the feasibility of automated musicological analysis on digital score libraries using the latest software tools. Bottom line: it's easy.","","Audio acoustics; Information retrieval; XML; Bottom lines; Digital score; Large-scale analysis; Software applications; XML database; XML-based files; Database systems","J. Ganseman; IBBT - Visionlab, Dept. of Physics, University of Antwerp, B-2610 Wilrijk (Antwerp), Universiteitsplein 1, Belgium; email: joachim.ganseman@ua.ac.be","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Wang F.; Wang X.; Shao B.; Li T.; Ogihara M.","Wang, Fei (56177292700); Wang, Xin (55736990800); Shao, Bo (25825665100); Li, Tao (55553727585); Ogihara, Mitsunori (54420747900)","56177292700; 55736990800; 25825665100; 55553727585; 54420747900","Tag integrated multi-label music style classification with hypergraph","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954634120&partnerID=40&md5=596d8e5c1c730b3b38f1626435b9dc01","Florida International University, United States; University of Miami, United States","Wang F., Florida International University, United States; Wang X., Florida International University, United States; Shao B., Florida International University, United States; Li T., Florida International University, United States; Ogihara M., University of Miami, United States","Automatic music style classification is an important, but challenging problem in music information retrieval. It has a number of applications, such as indexing of and searching in musical databases. Traditional music style classification approaches usually assume that each piece of music has a unique style and they make use of the music contents to construct a classifier for classifying each piece into its unique style. However, in reality, a piece may match more than one, even several different styles. Also, in this modern Web 2.0 era, it is easy to get a hold of additional, indirect information (e.g., music tags) about music. This paper proposes a multi-label music style classification approach, called Hypergraph integrated Support Vector Machine (HiSVM), which can integrate both music contents and music tags for automatic music style classification. Experimental results based on a real world data set are presented to demonstrate the effectiveness of the method. © 2009 International Society for Music Information Retrieval.","","Classification approach; Hypergraph; Integrated supports; Multi-label; Music contents; Music information retrieval; Musical database; Real world data; Web 2.0; Information retrieval","F. Wang; Florida International University, United States; email: feiwang@cs.fiu.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Thompson J.; McKay C.; Burgoyne J.A.; Fujinaga I.","Thompson, Jessica (55582942400); McKay, Cory (14033215600); Burgoyne, John Ashley (23007865600); Fujinaga, Ichiro (9038140900)","55582942400; 14033215600; 23007865600; 9038140900","Additions and improvements to the ace 2.0 music classifier","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952330591&partnerID=40&md5=f2986d9d94a2f1c2d2c2959f1c6fa5cb","Music Technology, McGill University, Canada; CIRMMT, McGill University, Canada","Thompson J., Music Technology, McGill University, Canada; McKay C., CIRMMT, McGill University, Canada; Burgoyne J.A., CIRMMT, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper presents additions and improvements to the Autonomous Classification Engine (ACE), a framework for using and optimizing classifiers. Given a set of feature values, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality- reduction techniques in order to arrive at a configuration that is well-suited to a given problem. Changes and additions have been made to ACE in order to increase its functionality as well as to make it easier to use and incorporate into other software frameworks. Details are provided on ACE's remodeled class structure and associated API, the improved command line and graphical user interfaces, a new ACE XML 2.0 ZIP file format and expanded statistical reporting associated with cross validation. The resulting improved processing and methods of operation are also discussed. © 2009 International Society for Music Information Retrieval.","","Graphical user interfaces; Class structures; Classifier ensembles; Command line; Cross validation; Feature values; File formats; Reduction techniques; Software frameworks; Statistical reporting; Information retrieval","J. Thompson; Music Technology, McGill University, Canada; email: jessica.thompson@mail.mcgill.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Miotto R.; Orio N.","Miotto, Riccardo (57202034402); Orio, Nicola (6507928255)","57202034402; 6507928255","A music identification system based on chroma indexing and statistical modeling","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442069&partnerID=40&md5=6c1427f52b888a9485526eaf4afefe39","Department of Information Engineering, University of Padova, Italy","Miotto R., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","A methodology is described for the automatic identification of classical music works. It can be considered an extension of fingerprinting techniques because the identification is carried out also when the query is a different performance of the work stored in the database, possibly played by different instruments and with background noise. The proposed methodology integrates an already existing approach based on hidden Markov models with an additional component that aims at improving scalability. The general idea is to carry out a clustering of the collection to highlight a limited number of candidates to be used for the HMM-based identification. Clustering is computed using the chroma features of the music works, hashed in a single value and retrieved using a bag of terms approach. Evaluation results are provided to show the validity of the combined approaches.","","Automation; Hidden Markov models; Query processing; Automatic identification; Background noise; Chroma features; Evaluation results; Fingerprinting techniques; Music identification; Single-value; Statistical modeling; Information retrieval","R. Miotto; Department of Information Engineering, University of Padova, Italy; email: miottori@dei.unipd.it","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Winget M.A.","Winget, Megan A. (23395559800)","23395559800","The liner notes digitization project: Providing users with cultural, historical, and critical music information","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467889&partnerID=40&md5=e3ebc0b5c0081aa27671888254b333cf","School of Information, University of Texas at Austin, Austin, TX 78712-0390, 1 University Station, D7000, United States","Winget M.A., School of Information, University of Texas at Austin, Austin, TX 78712-0390, 1 University Station, D7000, United States","Digitizing cultural information is a complex endeavor. Not only do users expect to have access to primary information like digital music files; it is also becoming more important for digital systems to provide contextual information for the primary artifacts contained within. The Liner Notes Markup Language (LNML) was developed to provide an XML vocabulary for encoding complex contextual documents that include an album's packaging, informational notes and inserts, liners, and album labels. This paper describes the development of the LNML framework, its major structural elements and functions, and some of the more pressing problems related to usability and purpose. The current LNML model is based on the examination and encoding of fifty albums from the 80s Rock genre. We are currently encoding fifty additional Jazz albums, which will provide data to augment and strengthen the model. Development of the LNML is ongoing, with plans to examine Classical and World Music examples to further augment the model.","","Encoding (symbols); Hypertext systems; Information retrieval; Contextual information; Digital music files; Digital system; Music information; Structural elements; Electronic document exchange","M.A. Winget; School of Information, University of Texas at Austin, Austin, TX 78712-0390, 1 University Station, D7000, United States; email: megan@ischool.utexas.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"JoséBurred J.; Cella C.E.; Peeters G.; Röbel A.; Schwarz D.","JoséBurred, Juan (55582607400); Cella, Carmine Emanuele (36614069100); Peeters, Geoffroy (22433836000); Röbel, Axel (7801333053); Schwarz, Diemo (7102731246)","55582607400; 36614069100; 22433836000; 7801333053; 7102731246","Using the SDIF sound description interchange format for audio features","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429140&partnerID=40&md5=02215fc1f101ce1bc0169110efa9a286","IRCAM, CNRS STMS, France","JoséBurred J., IRCAM, CNRS STMS, France; Cella C.E., IRCAM, CNRS STMS, France; Peeters G., IRCAM, CNRS STMS, France; Röbel A., IRCAM, CNRS STMS, France; Schwarz D., IRCAM, CNRS STMS, France","We present a set of extensions to the Sound Description Interchange Format (SDIF) for the purpose of storage and/or transmission of general audio descriptors. The aim is to allow portability and interoperability between the feature extraction module of an audio information retrieval application and the remaining modules, such as training, classification or clustering. A set of techniques addressing the needs of short-time features and temporal modeling over longer windows are proposed, together with the mechanisms that allow further extensions or adaptations by the user. The paper is completed by an overview of the general aspects of SDIF and its practical use by means of a set of existing programming interfaces for, among others, C, C++ and Matlab.","","Feature extraction; Information retrieval; Audio features; Descriptors; General aspects; Interchange formats; Programming interface; Temporal modeling; Audio acoustics","J. JoséBurred; IRCAM, CNRS STMS, France; email: burred@music.mcgill.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Yoshii K.; Goto M.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330)","7103400120; 7403505330","Music thumbnailer: Visualizing musical pieces in thumbnail images based on acoustic features","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448962&partnerID=40&md5=407800fc32be644ccc79a573cffcfa88","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan","This paper presents a principled method called MusicThumbnailer to transform musical pieces into visual thumbnail images based on acoustic features extracted from their audio signals. These thumbnails can help users immediately guess the musical contents of audio signals without trial listening. This method is consistent in ways that optimize thumbnails according to the characteristics of a target music collection. This means the appropriateness of transformation should be defined to eliminate ad hoc transformation rules. In this paper, we introduce three top-down criteria to improve memorability of thumbnails (generate gradations), deliver information more completely, and distinguish thumbnails more clearly. These criteria are mathematically implemented as minimization of brightness differences of adjacent pixels and maximization of brightness variances within and between thumbnails. The optimized parameters of a modified linear mapping model we assumed are obtained by minimizing a unified cost function based on the three criteria with a steepest descent method. Experimental results indicate that generated thumbnails can provide users with useful hints as to the musical contents of musical pieces.","","Information retrieval; Luminance; Acoustic features; Adjacent pixels; Audio signal; Brightness difference; Linear mapping; Music collection; Musical pieces; Optimized parameter; Topdown; Transformation rules; Optimization","K. Yoshii; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: k.yoshii@aist.go.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Hamanaka M.; Hirata K.; Tojo S.","Hamanaka, Masatoshi (35253968400); Hirata, Keiji (55730620800); Tojo, Satoshi (7103319884)","35253968400; 55730620800; 7103319884","Melody expectation method based on GTTM and TPS","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482483&partnerID=40&md5=c89b1259e11c883d8d461620a059d02e","University of Tsukuba, Tsukuba, Ibaraki, 1-1-1, Tenodai, Japan; NTT Communication Science Laboratories, Kyoto, 2-4, Hikaridai, Seikacho, Kei-hanna Science City, Japan; Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 1-1, Asahidai, Japan","Hamanaka M., University of Tsukuba, Tsukuba, Ibaraki, 1-1-1, Tenodai, Japan; Hirata K., NTT Communication Science Laboratories, Kyoto, 2-4, Hikaridai, Seikacho, Kei-hanna Science City, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 1-1, Asahidai, Japan","A method that predicts the next notes is described for assisting musical novices to play improvisations. Melody prediction is one of the most difficult problems in musical information retrieval because composers and players may or may not create melodies that conform to our expectation. The development of a melody expectation method is thus important for building a system that supports musical novices because melody expectation is one of the most basic skills for a musician. Unlike most previous prediction methods, which use statistical learning, our method evaluates the appropriateness of each candidate note from the view point of musical theory. In particular, it uses the concept of melody stability based on the generative theory of tonal music (GTTM) and the tonal pitch space (TPS) to evaluate the appropriateness of the melody. It can thus predict the candidate next notes not only from the surface structure of the melody but also from the deeper structure of the melody acquired by GTTM and TPS analysis. Experimental results showed that the method can evaluate the appropriateness of the melody sufficiently well.","","Information retrieval; Musical information retrieval; Prediction methods; Statistical learning; Tonal music; Forecasting","M. Hamanaka; University of Tsukuba, Tsukuba, Ibaraki, 1-1-1, Tenodai, Japan; email: hamanaka@iit.tsukuba.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Fields B.; Rhodes C.; Casey M.; Jacobson K.","Fields, Ben (35733701400); Rhodes, Christophe (57196565939); Casey, Michael (15080769900); Jacobson, Kurt (37107739300)","35733701400; 57196565939; 15080769900; 37107739300","Social playlists and bottleneck measurements: Exploiting musician social graphs using content-based dissimilarity and pairwise maximum flow values","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415927&partnerID=40&md5=481195a8158891b1f816601ceabc4cbd","Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Centre for Digital Music, Queen Mary University of London, United Kingdom","Fields B., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Rhodes C., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Casey M., Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; Jacobson K., Centre for Digital Music, Queen Mary University of London, United Kingdom","We have sampled the artist social network of Myspace and to it applied the pairwise relational connectivity measure Minimum cut/Maximum flow. These values are then compared to a pairwise acoustic Earth Mover's Distance measure and the relationship is discussed. Further, a means of constructing playlists using the maximum flow value to exploit both the social and acoustic distances is realized.","","Information retrieval; Social networking (online); Acoustic distance; Content-based; Earth Mover's distance; Maximum flows; Minimum cut; Social graphs; Social Networks; Flow graphs","B. Fields; Goldsmiths Digital Studios, Goldsmiths University of London, United Kingdom; email: b.fields@gold.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Raimond Y.; Sandler M.","Raimond, Yves (23135967900); Sandler, Mark (7202740804)","23135967900; 7202740804","A Web of musical information","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052082822&partnerID=40&md5=3c91b1662ba1f91c2f069dd669cd7276","Centre for Digital Music, Queen Mary, University of London, United Kingdom","Raimond Y., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, United Kingdom","We describe our recent achievements in interlinking several music-related data sources on the Semantic Web. In particular, we describe interlinked datasets dealing with Creative Commons content, editorial, encyclopedic, geographic and statistical data, along with queries they can answer and tools using their data. We describe our web services, providing an on-demand access to content-based features linked with such data sources and information pertaining to their creation (including processing steps, applied algorithms, inputs, parameters or associated developers). We also provide a tool allowing such music analysis services to be set up and scripted in a simple way.","","Query processing; Web services; Content-based features; Creative Commons; Data sets; Data-sources; Music analysis; Musical information; Processing steps; Statistical datas; Information retrieval","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kleedorfer F.; Knees P.; Pohle T.","Kleedorfer, Florian (23019351000); Knees, Peter (8219023200); Pohle, Tim (14036302300)","23019351000; 8219023200; 14036302300","OH OH OH WHOAH! Towards automatic topic detection in song lyrics","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426396&partnerID=40&md5=804b7f846333daec32e343ec47054bae","Studio Smart Agent Technologies, Research Studios, Austria; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Kleedorfer F., Studio Smart Agent Technologies, Research Studios, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","We present an algorithm that allows for indexing music by topic. The application scenario is an information retrieval system into which any song with known lyrics can be inserted and indexed so as to make a music collection browse-able by topic. We use text mining techniques for creating a vector space model of our lyrics collection and non-negative matrix factorization (NMF) to identify topic clusters which are then labeled manually. We include a discussion of the decisions regarding the parametrization of the applied methods. The suitability of our approach is assessed by measuring the agreement of test subjects who provide the labels for the topic clusters.","","Data mining; Factorization; Application scenario; Music collection; Nonnegative matrix factorization; Parametrizations; Text mining techniques; Topic detection; Vector space models; Information retrieval systems","F. Kleedorfer; Studio Smart Agent Technologies, Research Studios, Austria; email: florian.kleedorfer@researchstudio.at","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Baccigalupo C.; Plaza E.; Donaldson J.","Baccigalupo, Claudio (15062243300); Plaza, Enric (7004015031); Donaldson, Justin (24070691000)","15062243300; 7004015031; 24070691000","Uncovering affinity of artists to multiple genres from social behaviour data","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427741&partnerID=40&md5=c5bcdbbd5a94bed5ebd9fad9f486d52b","IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; Indiana University, School of Informatics, United States","Baccigalupo C., IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; Plaza E., IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; Donaldson J., Indiana University, School of Informatics, United States","In organisation schemes, musical artists are commonly identified with a unique 'genre' label attached, even when they have affinity to multiple genres. To uncover this hidden cultural awareness about multi-genre affinity, we present a new model based on the analysis of the way in which a community of users organise artists and genres in playlists. Our work is based on a novel dataset that we have elaborated identifying the co-occurrences of artists in the playlists shared by the members of a popular Web-based community, and that is made publicly available. The analysis defines an automatic social-based method to uncover relationships between artists and genres, and introduces a series of novel concepts that characterises artists and genres in a richer way than a unique 'genre' label would do.","","Cultural awareness; Data sets; Model-based OPC; Musical artists; Novel concept; Social behaviour; Social-based; Web-based communities; Information retrieval","C. Baccigalupo; IIIA, Artificial Intelligence Research Institute, CSIC, Spanish Council for Scientific Research, Spain; email: claudio@iiia.csic.es","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Ravelli E.; Richard G.; Daudet L.","Ravelli, Emmanuel (16069574800); Richard, Gael (57195915952); Daudet, Laurent (6602438418)","16069574800; 57195915952; 6602438418","Fast MIR in a sparse transform domain","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873435245&partnerID=40&md5=65a76c523c7a4dad9b4c81380610818f","Université Paris 6, France; TELECOM ParisTech., France","Ravelli E., Université Paris 6, France; Richard G., TELECOM ParisTech., France; Daudet L., Université Paris 6, France","We consider in this paper sparse audio coding as an alternative to transform audio coding for efficient MIR in the transform domain. We use an existing audio coder based on a sparse representation in a union of MDCT bases, and propose a fast algorithm to compute mid-level representations for beat tracking and chord recognition, respectively an onset detection function and a chromagram. The resulting transform domain system is significantly faster than a comparable state-of-the-art system while obtaining close performance above 8 kbps.","","Information retrieval; Multidetector computed tomography; Audio coders; Audio Coding; Beat tracking; Fast algorithms; Mid-level representation; Onset detection; Sparse representation; Sparse transform; State-of-the-art system; Transform domain; Audio signal processing","E. Ravelli; Université Paris 6, France; email: ravelli@lam.jussieu.fr","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Toh C.C.; Zhang B.; Wang Y.","Toh, Chee Chuan (55582426900); Zhang, Bingjun (56171073300); Wang, Ye (36103845200)","55582426900; 56171073300; 36103845200","Multiple-feature fusion based onset detection for solo singing voice","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472314&partnerID=40&md5=47bdca71e522d802fb65056dba97c2cd","School of Computing, National University of Singapore, Singapore, Singapore","Toh C.C., School of Computing, National University of Singapore, Singapore, Singapore; Zhang B., School of Computing, National University of Singapore, Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore","Onset detection is a challenging problem in automatic singing transcription. In this paper, we address singing onset detection with three main contributions. First, we outline the nature of a singing voice and present a new singing onset detection approach based on supervised machine learning. In this approach, two Gaussian Mixture Models (GMMs) are used to classify audio features of onset frames and non-onset frames. Second, existing audio features are thoroughly evaluated for this approach to singing onset detection. Third, feature-level and decision-level fusion are employed to fuse different features for a higher level of performance. Evaluated on a recorded singing database, the proposed approach outperforms state-of-the-art onset detection algorithms significantly.","","Audio features; Decision level fusion; Feature level; Gaussian mixture models; Onset detection; Supervised machine learning; Information retrieval","C.C. Toh; School of Computing, National University of Singapore, Singapore, Singapore; email: u0403701@nus.edu.sg","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Dopler M.; Schedl M.; Pohle T.; Knees P.","Dopler, Markus (55581991700); Schedl, Markus (8684865900); Pohle, Tim (14036302300); Knees, Peter (8219023200)","55581991700; 8684865900; 14036302300; 8219023200","Accessing music collections via representative cluster prototypes in a hierarchical organization scheme","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419184&partnerID=40&md5=3693e64dac787bf03cb735a5de284143","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Dopler M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","This paper addresses the issue of automatically organizing a possibly large music collection for intuitive access. We present an approach to cluster tracks in a hierarchical manner and to automatically find representative pieces of music for each cluster on each hierarchy level. To this end, audio signal-based features are complemented with features derived via Web content mining in a novel way. Automatic hierarchical clustering is performed using a variant of the Self-Organizing Map, which we further modified in order to create playlists containing similar tracks. The proposed approaches for playlist generation on a hierarchically structured music collection and finding prototypical tracks for each cluster are then integrated into the Traveller's Sound Player, a mobile audio player application that organizes music in a playlist such that the distances between consecutive tracks are minimal. We extended this player to deal with the hierarchical nature of the playlists generated by the proposed structuring approach. As for evaluation, we first assess the quality of the clustering method using the measure of entropy on a genre-annotated test set. Second, the goodness of the method to find prototypical tracks for each cluster is investigated in a user study.","","Audio acoustics; Conformal mapping; Information retrieval; Cluster prototype; Clustering methods; Hier-archical clustering; Hierarchical organizations; Large music collections; Mobile audio players; Music collection; Sound players; Test sets; User study; Web content mining; Audio recordings","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Tsunoo E.; Ono N.; Sagayama S.","Tsunoo, Emiru (35068648900); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","35068648900; 7202472899; 7004859104","Musical bass-line pattern clustering and its application to audio genre classification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953292148&partnerID=40&md5=501772da2c1a131fcfb93adac749747a","Graduate School of Information Science and Technology, University of Tokyo, Japan","Tsunoo E., Graduate School of Information Science and Technology, University of Tokyo, Japan; Ono N., Graduate School of Information Science and Technology, University of Tokyo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Japan","This paper discusses a new approach for clustering musical bass-line patterns representing particular genres and its application to audio genre classification. Many musical genres are characterized not only by timbral information but also by distinct representative bass-line patterns. So far this kind of temporal features have not so effectively been utilized. In particular, modern music songs mostly have certain fixed bar-long bass-line patterns per genre. For instance, while frequently bass-lines in rock music have constant pitch and a uniform rhythm, in jazz music there are many characteristic movements such as walking bass. We propose a representative bass-line pattern template extraction method based on k-means clustering handling a pitchshift problem. After extracting the fundamental bass-line pattern templates for each genre, distances from each template are calculated and used as a feature vector for supervised learning. Experimental result shows that the automatically calculated bass-line pattern information can be used for genre classification effectively and improve upon current approaches based on timbral features. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Feature vectors; Genre classification; ITS applications; K-means clustering; Musical genre; New approaches; Pattern clustering; Pattern information; Template extraction; Temporal features; Audio acoustics","E. Tsunoo; Graduate School of Information Science and Technology, University of Tokyo, Japan; email: tsunoo@hil.t.u-tokyo.ac.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Tsai W.-H.; Liao S.-J.; Lai C.","Tsai, Wei-Ho (13104438100); Liao, Shih-Jie (36608587100); Lai, Catherine (57666621000)","13104438100; 36608587100; 57666621000","Automatic identification of simultaneous singers in duet recordings","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866640041&partnerID=40&md5=6e4988fa663def9f461fbd2da24368ee","Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; Open Text Corporation, Ottawa, ON, Canada","Tsai W.-H., Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; Liao S.-J., Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; Lai C., Open Text Corporation, Ottawa, ON, Canada","The problem of identifying singers in music recordings has received considerable attention with the explosive growth of the Internet and digital media. Although a number of studies on automatic singer identification from acoustic features have been reported, most systems to date, however, reliably establish the identity of singers in solo recordings only. The research presented in this paper attempts to automatically identify singers in music recordings that contain overlapping singing voices. Two approaches to overlapping singer identification are proposed and evaluated. Results obtained demonstrate the feasibility of the systems.","","Automation; Digital storage; Information retrieval; Acoustic features; Automatic identification; Explosive growth; Music recording; Audio recordings","W.-H. Tsai; Graduate Institute of Computer and Communication Engineering, National Taipei University of Technology, Taipei, Taiwan; email: whtsai@ntut.edu.tw","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Moh Y.; Buhmann J.M.","Moh, Yvonne (24778468600); Buhmann, Joachim M. (7004241909)","24778468600; 7004241909","Kernel expansion for online preference tracking","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951967520&partnerID=40&md5=cee49eb4c1ec724b000ec67b0eb37476","Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland","Moh Y., Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland; Buhmann J.M., Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland","User preferences of music genres can significantly changes over time depending on fashions and the personal situation of music consumers. We propose a model to learn user preferences and their changes in an adaptive way. Our approach refines a model for user preferences by explicitly considering two plausible constraints of computational costs and limited storage space. The model is required to adapt itself to changing data distributions, and yet be able to compress ""historical"" data. We exploit the success of kernel SVM, and we consider an online expansion of the induced space as a preprocessing step to a simple linear online learner that updates with maximal agreement to previously seen data.","","Information retrieval; Computational costs; Data distribution; Induced spaces; Kernel expansion; Limited storage; Music genre; Online expansion; Pre-processing step; Digital storage","Y. Moh; Institute of Computational Science, Swiss Federal Institute of Technology (ETH) Zurich, Switzerland; email: tmoh@inf.ethz.ch","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Van Kranenburg P.; Volk A.; Wiering F.; Veltkamp R.C.","Van Kranenburg, Peter (35108158000); Volk, Anja (30567849900); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","35108158000; 30567849900; 8976178100; 7003421646","Musical models for folk-song melody alignment","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952347597&partnerID=40&md5=20d969a00bef9f148eda6ba56b8c3ec4","Department of Information and Computing Sciences, Utrecht University, Netherlands","Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands","In this paper we show that the modeling of musical knowledge within alignment algorithms results in a successful similarity approach to melodies. The score of the alignment of two melodies is taken as a measure of similarity. We introduce a number of scoring functions that model the influence of different musical parameters. The evaluation of their retrieval performance on a well-annotated set of 360 folk-song melodies with various kinds of melodic variation, shows that a combination of pitch, rhythm and segmentation-based scoring functions performs best, with a mean average precision of 0.83. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Alignment algorithms; Measure of similarities; Musical parameters; Retrieval performance; Scoring functions; Alignment","P. Van Kranenburg; Department of Information and Computing Sciences, Utrecht University, Netherlands; email: petervk@cs.uu.nl","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Riley J.","Riley, Jenn (57197504438)","57197504438","Application of the Functional Requirements for Bibliographic Records (FRBR) to music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052540367&partnerID=40&md5=696dad3f6f7019b09b88abff02b32dc6","Indiana University, Digital Library Program, United States","Riley J., Indiana University, Digital Library Program, United States","This paper describes work applying the Functional Requirements for Bibliographic Records (FRBR) model to music, as the basis for implementing a fully FRBR-compliant music digital library system. A detailed analysis of the FRBR and Functional Requirements for Authority Data (FRAD) entities and attributes is presented. The paper closes with a discussion of the ways in which FRBR is gaining adoption outside of the library environment in which it was born. This work benefits the MIR community by demonstrating a model that can be used in MIR systems for the storage of descriptive information in support of metadata-based searching, and by positioning the Variations system to be a source of robust descriptive information for use by third-party MIR systems.","","Bibliographic retrieval systems; Digital libraries; Digital storage; Information retrieval; Metadata; Bibliographic records; Descriptive information; Functional requirement; Music digital libraries; Search engines","J. Riley; Indiana University, Digital Library Program, United States; email: jenlrile@indiana.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Typke R.; Walczak-Typke A.C.","Typke, R. (6507237569); Walczak-Typke, A.C. (12240343500)","6507237569; 12240343500","A tunneling-vantage indexing method for non-metrics","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949634823&partnerID=40&md5=683e21f91f3c86033493dd738c1aebd4","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Kurt Gödel Research Center for Mathematical Logic, University of Vienna, Austria","Typke R., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Walczak-Typke A.C., Kurt Gödel Research Center for Mathematical Logic, University of Vienna, Austria","We consider an instance of the Earth Mover's Distance (EMD) useful for comparing rhythmical patterns. To make searches for r-near neighbours efficient, we decompose our search space into disjoint metric subspaces, in each of which the EMD reduces to the l1 norm. We then use a combined approach of two methods, one for searching within the subspaces, the other for searching between them. For the former, we show how one can use vantage indexing without false positives nor false negatives for solving the exact r-near neighbour problem, and find an optimum number and placement of vantage objects for this result. For searching between subspaces, where the EMD is not a metric, we show how one can guarantee that still no false negatives occur, and the percentage of false positives is reduced as the search radius is increased.","","Earth Mover's distance; False negatives; False positive; Indexing methods; Optimum number; Search spaces; Information retrieval","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"McKay C.; Fujinaga I.","McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","14033215600; 9038140900","Combining features extracted from audio, symbolic and cultural sources","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462486&partnerID=40&md5=4e8a1212a09e70f30c0fe73c5ff52015","Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","McKay C., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","This paper experimentally investigates the classification utility of combining features extracted from separate audio, symbolic and cultural sources of musical information. This was done via a series of genre classification experiments performed using all seven possible combinations and subsets of the three corresponding types of features. These experiments were performed using jMIR, a software suite designed for use both as a toolset for performing MIR research and as a platform for developing and sharing new algorithms. The experimental results indicate that combining feature types can indeed substantively improve classification accuracy. Accuracies of 96.8% and 78.8% were attained respectively on 5 and 10-class genre taxonomies when all three feature types were combined, compared to average respective accuracies of 85.5% and 65.1% when features extracted from only one of the three sources of data were used. It was also found that combining feature types decreased the seriousness of those misclassifications that were made, on average, particularly when cultural features were included.","","Experiments; Information retrieval; Classification accuracy; Feature types; Genre classification; Misclassifications; Musical information; Software suite; Toolsets; Audio acoustics","C. McKay; Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; email: cory.mckay@mail.mcgill.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Daniel A.; Emiya V.; David B.","Daniel, Adrien (56649623100); Emiya, Valentin (18041964000); David, Bertrand (7103015940)","56649623100; 18041964000; 7103015940","Perceptually-based evaluation of the errors usually made when automatically transcribing music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873454679&partnerID=40&md5=4f061999eb77650db8ce132aa88db26f","TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France","Daniel A., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France; Emiya V., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France; David B., TELECOM ParisTech (ENST), CNRS LTCI, 75634 Paris Cedex 13, 46, rue Barrault, France","This paper investigates the perceptual importance of typical errors occurring when transcribing polyphonic music excerpts into a symbolic form. The case of the automatic transcription of piano music is taken as the target application and two subjective tests are designed. The main test aims at understanding how human subjects rank typical transcription errors such as note insertion, deletion or replacement, note doubling, incorrect note onset or duration, and so forth. The Bradley-Terry-Luce (BTL) analysis framework is used and the results show that pitch errors are more clearly perceived than incorrect loudness estimations or temporal deviations from the original recording. A second test presents a first attempt to include this information in more perceptually motivated measures for evaluating transcription systems.","","Audio recordings; Errors; Information retrieval; Automatic transcription; Human subjects; Piano music; Pitch errors; Polyphonic music; Subjective tests; Target application; Transcription","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Anglade A.; Dixon S.","Anglade, Amélie (24070116200); Dixon, Simon (7201479437)","24070116200; 7201479437","Characterisation of harmony with inductive logic programming","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053098601&partnerID=40&md5=1c76ca682bf73267e0f3a5662541437f","Queen Mary University of London, Centre for Digital Music, United Kingdom","Anglade A., Queen Mary University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary University of London, Centre for Digital Music, United Kingdom","We present an approach for the automatic characterisation of the harmony of song sets making use of relational induction of logical rules. We analyse manually annotated chord data available in RDF and interlinked with web identifiers for chords which themselves give access to the root, bass, component intervals of the chords. We pre-process these data to obtain high-level information such as chord category, degree and intervals between chords before passing them to an Inductive Logic Programming software which extracts the harmony rules underlying them. This framework is tested over the Beatles songs and the Real Book songs. It generates a total over several experiments of 12,450 harmony rules characterising and differentiating the Real Book (jazz) songs and the Beatles' (pop) music. Encouragingly, a preliminary analysis of the most common rules reveals a list of well-known pop and jazz patterns that could be completed by a more in depth analysis of the other rules.","","Information retrieval; High-level information; In-depth analysis; Logical rules; Preliminary analysis; Inductive logic programming (ILP)","A. Anglade; Queen Mary University of London, Centre for Digital Music, United Kingdom; email: amelie.anglade@elec.qmul.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Lartillot O.; Eerola T.; Toiviainen P.; Fornari J.","Lartillot, Olivier (6507137446); Eerola, Tuomas (6602209042); Toiviainen, Petri (6602829513); Fornari, Jose (25639718900)","6507137446; 6602209042; 6602829513; 25639718900","Multi-feature modeling of pulse clarity: Design, validation and optimization","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","97","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473239&partnerID=40&md5=53d438a5b0b17a7fb76b95a37e6646af","Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Lartillot O., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Eerola T., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Toiviainen P., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; Fornari J., Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland","Pulse clarity is considered as a high-level musical dimension that conveys how easily in a given musical piece, or a particular moment during that piece, listeners can perceive the underlying rhythmic or metrical pulsation. The objective of this study is to establish a composite model explaining pulse clarity judgments from the analysis of audio recordings. A dozen of descriptors have been designed, some of them dedicated to low-level characterizations of the onset detection curve, whereas the major part concentrates on descriptions of the periodicities developed throughout the temporal evolution of music. A high number of variants have been derived from the systematic exploration of alternative methods proposed in the literature on onset detection curve estimation. To evaluate the pulse clarity model and select the best predictors, 25 participants have rated the pulse clarity of one hundred excerpts from movie soundtracks. The mapping between the model predictions and the ratings was carried out via regressions. Nearly a half of listeners' rating variance can be explained via a combination of periodicity-based factors.","","Alternative methods; Composite models; Curve estimation; Descriptors; Model prediction; Musical pieces; Onset detection; Systematic exploration; Temporal evolution; Information retrieval","O. Lartillot; Finnish Centre of Excellence in Interdisciplinary Music Research, University of Jyväskylä, Finland; email: olivier.lartillot@campus.jyu.fi","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Flanagan P.","Flanagan, Patrick (55583483900)","55583483900","Quantifying metrical ambiguity","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870940787&partnerID=40&md5=b55b2e0a31602661226887ef9215a3d2","","","This paper explores how data generated by meter induction models may be recycled to quantify metrical ambiguity, which is calculated by measuring the dispersion of metrical induction strengths across a population of possible meters. A measure of dispersion commonly used in economics to measure income inequality, the Gini coefficient, is introduced for this purpose. The value of this metric as a rhythmic descriptor is explored by quantifying the ambiguity of several common clave patterns and comparing the results to other metrics of rhythmic complexity and syncopation.","","Information retrieval; Population statistics; Descriptors; Gini coefficients; Income inequality; Dispersions","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Müller M.; Konz V.; Scharfstein A.; Ewert S.; Clausen M.","Müller, Meinard (7404689873); Konz, Verena (36459884900); Scharfstein, Andi (55588346900); Ewert, Sebastian (32667575400); Clausen, Michael (56225233200)","7404689873; 36459884900; 55588346900; 32667575400; 56225233200","Towards automated extraction of tempo parameters from expressive music recordings","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952339105&partnerID=40&md5=b28560ba4f0ef58c6f84449d908e85bf","MPI Informatik, Saarland University, Saarbrücken, Germany; Computer Science, Bonn University, Bonn, Germany","Müller M., MPI Informatik, Saarland University, Saarbrücken, Germany; Konz V., MPI Informatik, Saarland University, Saarbrücken, Germany; Scharfstein A., MPI Informatik, Saarland University, Saarbrücken, Germany; Ewert S., Computer Science, Bonn University, Bonn, Germany; Clausen M., Computer Science, Bonn University, Bonn, Germany","A performance of a piece of music heavily depends on the musician's or conductor's individual vision and personal interpretation of the given musical score. As basis for the analysis of artistic idiosyncrasies, one requires accurate annotations that reveal the exact timing and intensity of the various note events occurring in the performances. In the case of audio recordings, this annotation is often done manually, which is prohibitive in view of large music collections. In this paper, we present a fully automatic approach for extracting temporal information from a music recording using score-audio synchronization techniques. This information is given in the form of a tempo curve that reveals the relative tempo difference between an actual performance and some reference representation of the underlying musical piece. As shown by our experiments on harmony-based Western music, our approach allows for capturing the overall tempo flow and for certain classes of music even finer expressive tempo nuances. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automated extraction; Automatic approaches; Large music collections; Music recording; Musical pieces; Musical score; Synchronization technique; Temporal information; Audio recordings","M. Müller; MPI Informatik, Saarland University, Saarbrücken, Germany; email: meinard@mpi-inf.mpg.de","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Downie J.S.; Bay M.; Ehmann A.F.; Jones M.C.","Downie, J. Stephen (7102932568); Bay, Mert (56259607500); Ehmann, Andreas F. (8988651500); Jones, M. Cameron (57199028615)","7102932568; 56259607500; 8988651500; 57199028615","Audio cover song identification: Mirex 2006-2007 results and analyses","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475412&partnerID=40&md5=fa899ccf953819b734dad31b7e9c5cd3","International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Bay M., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","This paper presents analyses of the 2006 and 2007 results of the Music Information Retrieval Evaluation eXchange (MIREX) Audio Cover Song Identification (ACS) tasks. The Music Information Retrieval Evaluation eXchange (MIREX) is a community-based endeavor to scientifically evaluate music information retrieval (MIR) algorithms and techniques. The ACS task was created to motivate MIR researchers to expand their notions of similarity beyond acoustic similarity to include the important idea that musical works retain their identity notwithstanding variations in style, genre, orchestration, rhythm or melodic ornamentation, etc. A series of statistical analyses were performed that indicate significant improvements in this domain have been made over the course of 2006-2007. Post-hoc analyses reveal distinct differences between individual systems and the effects of certain classes of queries on performance. This paper discusses some of the techniques that show promise in this research domain.","","Insecticides; Individual systems; Music information retrieval; Research domains; Information retrieval","J.S. Downie; International Music Information Retrieval Systems Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; email: jdownie@uiuc.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Masahiro N.; Takaesu H.; Demachi H.; Oono M.; Saito H.","Masahiro, Niitsuma (36638174100); Takaesu, Hiroshi (55583643900); Demachi, Hazuki (55582329100); Oono, Masaki (6602818215); Saito, Hiroaki (55619315125)","36638174100; 55583643900; 55582329100; 6602818215; 55619315125","Development of an automatic music selection system based on runner's step frequency","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415395&partnerID=40&md5=6ebadee91dddfa17d3f5e14b8bdbdc56","Department of Computer Science, Keio University, Yokohama, Japan","Masahiro N., Department of Computer Science, Keio University, Yokohama, Japan; Takaesu H., Department of Computer Science, Keio University, Yokohama, Japan; Demachi H., Department of Computer Science, Keio University, Yokohama, Japan; Oono M., Department of Computer Science, Keio University, Yokohama, Japan; Saito H., Department of Computer Science, Keio University, Yokohama, Japan","This paper presents an automatic music selection system based on runner's step frequency. Recent development of portable music players like iPod has increased the number of those who listen to music while exercising. However, few systems which connect exercises with music selection have been developed. We propose a system that automatically selects music suitable for user's running exercises. Although many parameters can be taken into account, as a first step we focus on runner's step frequency. This system selects music with tempo suitable for runner's step frequency and when runner's step frequency changes, it executes another music selection. The system consists of three modules: step frequency estimation, music selection, and music playing. In the first module, runner's step frequency is estimated from data derived from an acceleration sensor. In the second module, appropriate music is selected based on the estimated step frequency. In the third module, the selected music is played until runner's step frequency changes. In the experiment, subjects ran on a running machine at different paces listening to the music selected by the proposed system. Experimental results show that the system can estimate runner's SPM accurately and on the basis of the estimated SPM it can select music appropriate for users' exercises with more than 85.0% accuracy, and makes running exercises more pleasing.","","Acceleration sensors; Portable music player; Selection systems; Step frequency; Information retrieval","N. Masahiro; Department of Computer Science, Keio University, Yokohama, Japan; email: niizuma@nak.ics.keio.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Hoffman M.; Blei D.; Cook P.","Hoffman, Matthew (17434675700); Blei, David (55914504500); Cook, Perry (57203105418)","17434675700; 55914504500; 57203105418","Content-based musical similarity computation using the hierarchical dirichlet process","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420337&partnerID=40&md5=221726171f59818ff41bb91bcb69114e","Princeton University, Dept. of Computer Science, United States; Dept. of Music, Princeton University, United States","Hoffman M., Princeton University, Dept. of Computer Science, United States; Blei D., Princeton University, Dept. of Computer Science, United States; Cook P., Princeton University, Dept. of Computer Science, United States, Dept. of Music, Princeton University, United States","We develop a method for discovering the latent structure in MFCC feature data using the Hierarchical Dirichlet Process (HDP). Based on this structure, we compute timbral similarity between recorded songs. The HDP is a nonparametric Bayesian model. Like the Gaussian Mixture Model (GMM), it represents each song as a mixture of some number of multivariate Gaussian distributions However, the number of mixture components is not fixed in the HDP, but is determined as part of the posterior inference process. Moreover, in the HDP the same set of Gaussians is used to model all songs, with only the mixture weights varying from song to song. We compute the similarity of songs based on these weights, which is faster than previous approaches that compare single Gaussian distributions directly. Experimental results on a genre-based retrieval task illustrate that our HDP-based method is both faster and produces better retrieval quality than such previous approaches.","","Bayesian networks; Image segmentation; Content-based; Feature data; Gaussian Mixture Model; Gaussians; Hierarchical Dirichlet process; Inference process; Latent structures; Mixture components; Multivariate Gaussian Distributions; Musical similarity; Non-parametric Bayesian; Retrieval quality; Information retrieval","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kirlin P.B.; Utgoff P.E.","Kirlin, Phillip B. (55582044600); Utgoff, Paul E. (6602417932)","55582044600; 6602417932","A framework for automated Schenkerian analysis","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249244118&partnerID=40&md5=cf3632e4fc71c8eb5179839216ec1330","Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","Kirlin P.B., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; Utgoff P.E., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","In Schenkerian analysis, one seeks to find structural dependences among the notes of a composition and organize these dependences into a coherent hierarchy that illustrates the function of every note. This type of analysis reveals multiple levels of structure in a composition by constructing a series of simplifications of a piece showing various elaborations and prolongations. We present a framework for solving this problem, called IVI, that uses a state-space search formalism. IVI includes multiple interacting components, including modules for various preliminary analyses (harmonic, melodic, rhythmic, and cadential), identifying and performing reductions, and locating pieces of the Ursatz. We describe a number of the algorithms by which IVI forms, stores, and updates its hierarchy of notes, along with details of the Ursatz-finding algorithm. We illustrate IVI's functionality on an excerpt from a Schubert piano composition, and also discuss the issues of subproblem interactions and the multiple parsings problem.","","Multiple levels; Preliminary analysis; State-space; Structural dependence; Information retrieval","P.B. Kirlin; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; email: pkirlin@cs.umass.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Murata K.; Nakadai K.; Yoshii K.; Takeda R.; Torii T.; Okuno H.G.; Hasegawa Y.; Tsujino H.","Murata, Kazumasa (35409926600); Nakadai, Kazuhiro (6603685669); Yoshii, Kazuyoshi (7103400120); Takeda, Ryu (14046031200); Torii, Toyotaka (24479947700); Okuno, Hiroshi G. (7102397930); Hasegawa, Yuji (55726742400); Tsujino, Hiroshi (7004250223)","35409926600; 6603685669; 7103400120; 14046031200; 24479947700; 7102397930; 55726742400; 7004250223","A robot singer with music recognition based on real-time beat tracking","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455176629&partnerID=40&md5=69ec73c922ebf93f21e7e546dc023b27","Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan; Honda Research Institute Japan Co., Ltd., Japan; Graduate School of Informatics, Kyoto University, Japan","Murata K., Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan; Nakadai K., Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan, Honda Research Institute Japan Co., Ltd., Japan; Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Takeda R., Graduate School of Informatics, Kyoto University, Japan; Torii T., Honda Research Institute Japan Co., Ltd., Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan; Hasegawa Y., Honda Research Institute Japan Co., Ltd., Japan; Tsujino H., Honda Research Institute Japan Co., Ltd., Japan","A robot that can provide an active and enjoyable user interface is one of the most challenging applications for music information processing, because the robot should cope with high-power noises including self voices and motor noises. This paper proposes noise-robust musical beat tracking by using a robot-embedded microphone, and describes its application to a robot singer with music recognition. The proposed beat tracking introduces two key techniques, that is, spectro-temporal pattern matching and echo cancellation. The former realizes robust tempo estimation with a shorter window length, thus, it can quickly adapt to tempo changes. The latter is effective to cancel self periodic noises such as stepping, scatting, and singing. We constructed a robot singer based on the proposed beat tracking for Honda ASIMO. The robot detects a musical beat with its own microphone in a noisy environment. It tries to recognize music based on the detected musical beat. When it successfully recognizes music, it sings while stepping according to the beat. Otherwise, it performs scatting instead of singing because the lyrics are unavailable. Experimental results showed fast adaptation to tempo changes and high robustness in beat tracking even when stepping, scatting and singing.","","Information retrieval; Microphones; Pattern matching; User interfaces; Beat tracking; Fast adaptations; High robustness; Key techniques; Motor noise; Music information processing; Music recognition; Noisy environment; Periodic noise; Robots","K. Murata; Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Japan; email: murata@cyb.mei.titech.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Pearce M.T.; Müllensiefen D.; Wiggins G.A.","Pearce, M.T. (8674558800); Müllensiefen, D. (23019169400); Wiggins, G.A. (14032393700)","8674558800; 23019169400; 14032393700","A comparison of statistical and rule-based models of melodic segmentation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053111706&partnerID=40&md5=654ad136fd432762dbd152c5919173b0","Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom","Pearce M.T., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom; Müllensiefen D., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom; Wiggins G.A., Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom","We introduce a new model for melodic segmentation based on information-dynamic analysis of melodic structure. The performance of the model is compared to several existing algorithms in predicting the annotated phrase boundaries in a large corpus of folk music.","","Melodic structure; Phrase boundary; Rule-based models; Information retrieval","M.T. Pearce; Centre for Computation, Cognition and Culture, Goldsmiths, University of London, United Kingdom; email: m.pearce@gold.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Mauch M.; Dixon S.","Mauch, Matthias (36461512900); Dixon, Simon (7201479437)","36461512900; 7201479437","A discrete mixture model for chord labelling","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955840442&partnerID=40&md5=5d1df9525e3999d2acaa889a9ab1c1d2","Queen Mary, University of London, Centre for Digital Music, United Kingdom","Mauch M., Queen Mary, University of London, Centre for Digital Music, United Kingdom; Dixon S., Queen Mary, University of London, Centre for Digital Music, United Kingdom","Chord labels for recorded audio are in high demand both as an end product used by musicologists and hobby musicians and as an input feature for music similarity applications. Many past algorithms for chord labelling are based on chromagrams, but distribution of energy in chroma frames is not well understood. Furthermore, non-chord notes complicate chord estimation. We present a new approach which uses as a basis a relatively simple chroma model to represent short-time sonorities derived from melody range and bass range chromagrams. A chord is then modelled as a mixture of these sonorities, or subchords. We prove the practicability of the model by implementing a hidden Markov model (HMM) for chord labelling, in which we use the discrete subchord features as observations. We model gamma-distributed chord durations by duplicate states in the HMM, a technique that had not been applied to chord labelling. We test the algorithm by five-fold cross-validation on a set of 175 hand-labelled songs performed by the Beatles. Accuracy figures compare very well with other state of the art approaches. We include accuracy specified by chord type as well as a measure of temporal coherence.","","Algorithms; Hidden Markov models; Information retrieval; Cross validation; Discrete mixture; End-products; Gamma-distributed; High demand; Input features; Music similarity; State-of-the-art approach; Temporal coherence; Audio acoustics","M. Mauch; Queen Mary, University of London, Centre for Digital Music, United Kingdom; email: matthias.mauch@elec.qmul.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Sumi K.; Itoyama K.; Yoshii K.; Komatani K.; Ogata T.; Okuno H.G.","Sumi, Kouhei (36141454400); Itoyama, Katsutoshi (18042499100); Yoshii, Kazuyoshi (7103400120); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","36141454400; 18042499100; 7103400120; 35577813100; 7402000772; 7102397930","Automatic chord recognition based on probabilistic integration of chord transition and bass pitch estimation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052068532&partnerID=40&md5=65c7c1cc5bd6fa5b9c81122add5c2c20","Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","Sumi K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Itoyama K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Yoshii K., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper presents a method that identifies musical chords in polyphonic musical signals. As musical chords mainly represent the harmony of music and are related to other musical elements such as melody and rhythm, the performance of chord recognition should improve if this interrelationship is taken into consideration. Nevertheless, this interrelationship has not been utilized in the literature as far as the authors are aware. In this paper, bass lines are utilized as clues for improving chord recognition because they can be regarded as an element of the melody. A probabilistic framework is devised to uniformly integrate bass lines extracted by using bass pitch estimation into a hypothesis-search-based chord recognition. To prune the hypothesis space of the search, the hypothesis reliability is defined as the weighted sum of three reliabilities: the likelihood of Gaussian Mixture Models for the observed features, the joint probability of chord and bass pitch, and the chord transition N-gram probability. Experimental results show that our method recognized the chord sequences of 150 songs in twelve Beatles albums; the average frame-rate accuracy of the results was 73.4%.","Bass line; Chord recognition; Hypothesis search; Probabilistic integration","Computer music; Information retrieval; Bass line; Chord recognition; Chord sequence; Frame-rate; Gaussian Mixture Model; Hypothesis search; Hypothesis space; Joint probability; Pitch estimation; Polyphonic musical signals; Probabilistic framework; Probabilistic integration; Weighted Sum; Acoustic variables measurement","K. Sumi; Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: ksumi@kuis.kyoto-u.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Trohidis K.; Tsoumakas G.; Kalliris G.; Vlahavas I.","Trohidis, Konstantinos (54941217600); Tsoumakas, Grigorios (8088070600); Kalliris, George (6504499954); Vlahavas, Ioannis (57191439209)","54941217600; 8088070600; 6504499954; 57191439209","Multi-label classification of music into emotions","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","649","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873447495&partnerID=40&md5=a28dd212d81d2cebb943bdc60eab3a5c","Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; Dept. of Informatics, Aristotle University of Thessaloniki, Greece","Trohidis K., Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; Tsoumakas G., Dept. of Informatics, Aristotle University of Thessaloniki, Greece; Kalliris G., Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; Vlahavas I., Dept. of Informatics, Aristotle University of Thessaloniki, Greece","In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.","","Information retrieval; Audio features; Automated detection; Classification tasks; Feature selection methods; Multi-label; Predictive power; Algorithms","K. Trohidis; Dept. of Journalism and Mass Communication, Aristotle University of Thessaloniki, Greece; email: trohidis2000@yahoo.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Niedermayer B.","Niedermayer, Bernhard (37113938000)","37113938000","Non-negative matrix division for the automatic transcription of polyphonic music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856171852&partnerID=40&md5=88db8781758c6e910581924f34497b18","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Niedermayer B., Department of Computational Perception, Johannes Kepler University, Linz, Austria","In this paper we present a new method in the style of non-negative matrix factorization for automatic transcription of polyphonic music played by a single instrument (e.g., a piano). We suggest using a fixed repository of base vectors corresponding to tone models of single pitches played on a certain instrument. This assumption turns the blind factorization into a kind of non-negative matrix division for which an algorithm is presented. The same algorithm can be applied for learning the model dictionary from sample tones as well. This method is biased towards the instrument used during the training phase. But this is admissible in applications like performance analysis of solo music. The proposed approach is tested on a Mozart sonata where a symbolic representation is available as well as the recording on a computer controlled grand piano.","","Algorithms; Factorization; Information retrieval; Instruments; Automatic transcription; Base vectors; Grand piano; Non-negative matrix; Nonnegative matrix factorization; Performance analysis; Polyphonic music; Symbolic representation; Training phase; Transcription","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Xiao L.; Tian A.; Li W.; Zhou J.","Xiao, Linxing (25423235100); Tian, Aibo (48761984600); Li, Wen (57196305966); Zhou, Jie (56939394300)","25423235100; 48761984600; 57196305966; 56939394300","Using a statistic model tocapture the association between timbre and perceived tempo","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461657&partnerID=40&md5=06ff45bdf07073a50efc0bfa9a6a0f9f","Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China","Xiao L., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Tian A., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Li W., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China; Zhou J., Tsinghua National Laboratory for Information Science and Technology, Department of Automation, Tsinghua University, Beijing 100084, China","The estimation of the perceived tempo is required in many MIR applications. However, automatic tempo estimation itself is still an open problem due to the insufficient understanding of the inherent mechanisms of the tempo perception. Published methods only use the information of rhythm pattern, so they may meet the half/double tempo error problem. To solve this problem, We propose to use statistic model to investigate the association between timbre and tempo and use timbre information to improve the performance of tempo estimation. Experiment results show that this approach performs at least comparably to existing tempo extraction algorithms.","","Estimation; Information retrieval; Information use; Error problems; Extraction algorithms; Statistic model; Use statistics; Problem solving","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Riley M.; Heinen E.; Ghosh J.","Riley, Matthew (57197270459); Heinen, Eric (55583494900); Ghosh, Joydeep (35556611300)","57197270459; 55583494900; 35556611300","A text retrieval approach to content-based audio retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949578539&partnerID=40&md5=ade085468c8aa9e36f468007fa83313c","University of Texas, Austin, United States","Riley M., University of Texas, Austin, United States; Heinen E., University of Texas, Austin, United States; Ghosh J., University of Texas, Austin, United States","This paper presents a novel approach to robust, content-based retrieval of digital music. We formulate the hashing and retrieval problems analogously to that of text retrieval and leverage established results for this unique application. Accordingly, songs are represented as a ""Bag-of-Audio- Words"" and similarity calculations follow directly from the well-known Vector Space model [12]. We evaluate our system on a 4000 song data set to demonstrate its practical applicability, and evaluation shows our technique to be robust to a variety of signal distortions. Most interestingly, the system is capable of matching studio recordings to live recordings of the same song with high accuracy.","","Audio recordings; Content based retrieval; Audio retrieval; Content-based; Data sets; Digital music; Similarity calculation; Studio recordings; Text retrieval; Vector space models; Information retrieval","M. Riley; University of Texas, Austin, United States; email: mriley@gmail.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"De Lima E.T.; Ramalho G.","De Lima, Ernesto Trajano (57189900528); Ramalho, Geber (6602152013)","57189900528; 6602152013","On rhythmic pattern extraction in bossa nova music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428739&partnerID=40&md5=0a582253ff9686ea62fa6731e4783305","Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil","De Lima E.T., Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil; Ramalho G., Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil","The analysis of expressive performance, an important research topic in Computer Music, is almost exclusively devoted to the study of Western Classical piano music. Instruments like the acoustic guitar and styles like Bossa Nova and Samba have been little studied, despite their harmonic and rhythmic richness. This paper describes some experimental results obtained with the extraction of rhythmic patterns from the guitar accompaniment of Bossa Nova songs. The songs, played by two different performers and recorded with the help of a MIDI guitar, were represented as strings and processed by FlExPat, a string matching algorithm. The results obtained were then compared to a previously acquired catalogue of ""good"" patterns.","","Computer music; Information retrieval; Scheduling; Acoustic guitar; Piano music; Research topics; Rhythmic patterns; String matching algorithm; Musical instruments","E.T. De Lima; Centro de Informätica (CIn), Univ. Federal de Pernambuco, 50732-970 - Recife - PE, Caixa Postal 7851, Brazil; email: etl@cin.ufpe.br","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Jacobson K.; Sandler M.; Fields B.","Jacobson, Kurt (37107739300); Sandler, Mark (7202740804); Fields, Ben (35733701400)","37107739300; 7202740804; 35733701400","Using audio analysis and network structure to identify communities in on-line social networks of artists","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427408&partnerID=40&md5=91fc7a4ea8a5f93f3d66c827de23436a","Centre for Digital Music, Queen Mary, University of London, United Kingdom; Goldsmiths Digital Studios, Goldsmiths, University of London, United Kingdom","Jacobson K., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, United Kingdom; Fields B., Goldsmiths Digital Studios, Goldsmiths, University of London, United Kingdom","Community detection methods from complex network theory are applied to a subset of the Myspace artist network to identify groups of similar artists. Methods based on the greedy optimization of modularity and random walks are used. In a second iteration, inter-artist audio-based similarity scores are used as input to enhance these community detection methods. The resulting community structures are evaluated using a collection of artist-assigned genre tags. Evidence suggesting the Myspace artist network structure is closely related to musical genre is presented and a Semantic Web service for accessing this structure is described.","","Information retrieval; Population dynamics; Social networking (online); Audio analysis; Audio-based; Community detection; Community structures; Complex network theory; Greedy optimization; Musical genre; Network structures; Random Walk; Similarity scores; Social Networks; Iterative methods","K. Jacobson; Centre for Digital Music, Queen Mary, University of London, United Kingdom; email: kurt.jacobson@elec.qmul.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Dannenberg R.B.; Wasserman L.","Dannenberg, Roger B. (7003266250); Wasserman, Larry (57191231290)","7003266250; 57191231290","Estimating the error distribution of a tap sequence without ground truth","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751665767&partnerID=40&md5=baa2f2d4424e7853724be02b5bae8fba","School of Computer Science, Carnegie Mellon University, United States","Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Wasserman L., School of Computer Science, Carnegie Mellon University, United States","Detecting beats, estimating tempo, aligning scores to audio, and detecting onsets are all interesting problems in the field of music information retrieval. In much of this research, it is convenient to think of beats as occuring at precise time points. However, anyone who has attempted to label beats by hand soon realizes that precise annotation of music audio is not possible. A common method of beat annotation is simply to tap along with audio and record the tap times. This raises the question: How accurate are the taps? It may seem that an answer to this question would require knowledge of ""true"" beat times. However, tap times can be characterized as a random distribution around true beat times. Multiple independent taps can be used to estimate not only the location of the true beat time, but also the statistical distribution of measured tap times around the true beat time. Thus, without knowledge of true beat times, and without even requiring the existence of precise beat times, we can estimate the uncertainty of tap times. This characterization of tapping can be useful for estimating tempo variation and evaluating alternative annotation methods. © 2009 International Society for Music Information Retrieval.","","Estimation; Information retrieval; Annotation methods; Error distributions; Ground truth; Music information retrieval; Random distribution; Statistical distribution; Time points; Uncertainty analysis","","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Turnbull D.; Barrington L.; Lanckriet G.","Turnbull, Douglas (8380095700); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","8380095700; 14041197300; 7801431767","Five approaches to collecting tags for music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","77","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467755&partnerID=40&md5=c8425a97fbc9470b91f6d73efc08b5ad","UC, San Diego, United States","Turnbull D., UC, San Diego, United States; Barrington L., UC, San Diego, United States; Lanckriet G., UC, San Diego, United States","We compare five approaches to collecting tags for music: conducting a survey, harvesting social tags, deploying annotation games, mining web documents, and autotagging audio content. The comparison includes a discussion of both scalability (financial cost, human involvement, and computational resources) and quality (the cold start problem & popularity bias, strong vs. weak labeling, vocabulary structure & size, and annotation accuracy). We then describe one state-of-the-art system for each approach. The performance of each system is evaluated using a tag-based music information retrieval task. Using this task, we are able to quantify the effect of popularity bias on each approach by making use of a subset of more popular (short-head) songs and a set of less popular (long-tail) songs. Lastly, we propose a simple hybrid context-content system that combines our individual approaches and produces superior retrieval results.","","Audio content; Cold start problems; Computational resources; Financial costs; Music information retrieval; State-of-the-art system; Tag-based; Web document; Information retrieval","D. Turnbull; UC, San Diego, United States; email: dturnbul@cs.ucsd.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Camacho A.","Camacho, Arturo (7103095969)","7103095969","Detection of pitched/unpitched sound using pitch strength clustering","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955789679&partnerID=40&md5=a0f6ab2342c6a5b6e158370a7b3b0b33","Computer and Information Science and Engineering Department, University of Florida, Gainesville, FL 32611, United States","Camacho A., Computer and Information Science and Engineering Department, University of Florida, Gainesville, FL 32611, United States","A method for detecting pitched/unpitched sound is presented. The method tracks the pitch strength trace of the signal, determining clusters of pitch and unpitched sound. The criterion used to determine the clusters is the local maximization of the distance between the centroids. The method makes no assumption about the data except that the pitched and unpitched clusters have different centroids. This allows the method to dispense with free parameters. The method is shown to be more reliable than using fixed thresholds when the SNR is unknown.","","Fixed threshold; Free parameters; Information retrieval","A. Camacho; Computer and Information Science and Engineering Department, University of Florida, Gainesville, FL 32611, United States; email: acamacho@cise.ufl.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Barrington L.; Yazdani M.; Turnbull D.; Lanckriet G.","Barrington, Luke (14041197300); Yazdani, Mehrdad (35231951800); Turnbull, Douglas (8380095700); Lanckriet, Gert (7801431767)","14041197300; 35231951800; 8380095700; 7801431767","Combining feature kernels for semantic music retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421439&partnerID=40&md5=6b0e2ff377d9383f7f16a70fbac86448","Electrical and Computer Engineering, University of California, San Diego, United States; Computer Science and Engineering, University of California, San Diego, United States","Barrington L., Electrical and Computer Engineering, University of California, San Diego, United States; Yazdani M., Electrical and Computer Engineering, University of California, San Diego, United States; Turnbull D., Electrical and Computer Engineering, University of California, San Diego, United States, Computer Science and Engineering, University of California, San Diego, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, United States","We apply a new machine learning tool, kernel combination, to the task of semantic music retrieval. We use 4 different types of acoustic content and social context feature sets to describe a large music corpus and derive 4 individual kernel matrices from these feature sets. Each kernel is used to train a support vector machine (SVM) classifier for each semantic tag (e.g., 'aggressive', 'classic rock', 'distorted electric guitar') in a large tag vocabulary. We examine the individual performance of each feature kernel and then show how to learn an optimal linear combination of these kernels using convex optimization. We find that the retrieval performance of the SVMs trained using the combined kernel is superior to SVMs trained using the best individual kernel for a large number of tags. In addition, the weights placed on individual kernels in the linear combination reflect the relative importance of each feature set when predicting a tag.","","Convex optimization; Information retrieval; Semantics; Electric guitar; Feature sets; Individual performance; Kernel matrices; Linear combinations; Music retrieval; Retrieval performance; Social context; Support vector machines","L. Barrington; Electrical and Computer Engineering, University of California, San Diego, United States; email: lbarrington@ucsd.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Scaringella N.","Scaringella, Nicolas (12761231600)","12761231600","Timbre and rhythmic TRAP-TANDEM features for music information retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863762281&partnerID=40&md5=0458b163436ae1afd595bf8251b81da8","Idiap Research Institute, Martigny, Switzerland; Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland","Scaringella N., Idiap Research Institute, Martigny, Switzerland, Ecole Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland","The enormous growth of digital music databases has led to a comparable growth in the need for methods that help users organize and access such information. One area in particular that has seen much recent research activity is the use of automated techniques to describe audio content and to allow for its identification, browsing and retrieval. Conventional approaches to music content description rely on features characterizing the shape of the signal spectrum in relatively short-term frames. In the context of Automatic Speech Recognition (ASR), Hermansky [7] described an interesting alternative to short-term spectrum features, the TRAP-TANDEM approach which uses long-term band-limited features trained in a supervised fashion. We adapt this idea to the specific case of music signals and propose a generic system for the description of temporal patterns. The same system with different settings is able to extract features describing either timbre or rhythmic content. The quality of the generated features is demonstrated in a set of music retrieval experiments and compared to other state-of-the-art models.","","Audio signal processing; Spectrum analysis; Audio content; Automated techniques; Automatic speech recognition; Conventional approach; Digital music; Generic system; Music contents; Music information retrieval; Music retrieval; Music signals; Research activities; Signal spectrum; Spectrum features; Temporal pattern; Information retrieval","N. Scaringella; Idiap Research Institute, Martigny, Switzerland; email: nicolas.scaringella@epfl.ch","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Manaris B.; Krehbiel D.; Roos P.; Zalonis T.","Manaris, Bill (6602079213); Krehbiel, Dwight (57204989595); Roos, Patrick (36469180000); Zalonis, Thomas (55582631000)","6602079213; 57204989595; 36469180000; 55582631000","Armonique: Experiments in content-based similarity retrieval using power-law melodic and timbre metrics","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052432239&partnerID=40&md5=be9edabda185abe59dd437e481bcbb24","Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States; Psychology Department, Bethel College, North Newton, KS 67117, 300 E. 27th Street, United States","Manaris B., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States; Krehbiel D., Psychology Department, Bethel College, North Newton, KS 67117, 300 E. 27th Street, United States; Roos P., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States; Zalonis T., Computer Science Department, College of Charleston, Charleston, SC 29424, 66 George Street, United States","This paper presents results from an on-going MIR study utilizing hundreds of melodic and timbre features based on power laws for content-based similarity retrieval. These metrics are incorporated into a music search engine prototype, called Armonique. This prototype is used with a corpus of 9153 songs encoded in both MIDI and MP3 to identify pieces similar to and dissimilar from selected songs. The MIDI format is used to extract various power-law features measuring proportions of music-theoretic and other attributes, such as pitch, duration, melodic intervals, and chords. The MP3 format is used to extract power-law features measuring proportions within FFT power spectra related to timbre. Several assessment experiments have been conducted to evaluate the effectiveness of the similarity model. The results suggest that power-law metrics are very promising for content-based music querying and retrieval, as they seem to correlate with aspects of human emotion and aesthetics.","","Information retrieval; Search engines; Content-based; Human emotion; Melodic intervals; Power-law; Power-spectra; Similarity models; Similarity retrieval; Experiments","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Lee K.; Cremer M.","Lee, Kyogu (8597995500); Cremer, Markus (57198310932)","8597995500; 57198310932","Segmentation-based lyrics-audio alignment using dynamic programming","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423338&partnerID=40&md5=33e315cbfdf989df664cd0c166df2d07","Media Technology Lab., Gracenote, Emeryville, CA 94608, United States","Lee K., Media Technology Lab., Gracenote, Emeryville, CA 94608, United States; Cremer M., Media Technology Lab., Gracenote, Emeryville, CA 94608, United States","In this paper, we present a system for automatic alignment of textual lyrics with musical audio. Given an input audio signal, structural segmentation is first performed and similar segments are assigned a label by computing the distance between the segment pairs. Using the results of segmentation and hand-labeled paragraphs in lyrics as a pair of input strings, we apply a dynamic programming (DP) algorithm to find the best alignment path between the two strings, achieving segment-to-paragraph synchronization. We demonstrate that the proposed algorithm performs well for various kinds of musical audio.","","Algorithms; Alignment; Dynamic programming; Information retrieval; Audio signal; Automatic alignment; Dynamic programming algorithm; Input string; Musical audio; Audio acoustics","K. Lee; Media Technology Lab., Gracenote, Emeryville, CA 94608, United States; email: klee@gracenote.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Sapp C.S.","Sapp, Craig Stuart (14032002500)","14032002500","Hybrid numeric/rank similarity metrics for musical performance analysis","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","35","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861214286&partnerID=40&md5=3e5f39268eeda8b9da26df36c2bfd367","CHARM, Royal Holloway, University of London, United Kingdom","Sapp C.S., CHARM, Royal Holloway, University of London, United Kingdom","This paper describes a numerical method for examining similarities among tempo and loudness features extracted from recordings of the same musical work and evaluates its effectiveness compared to Pearson correlation. Starting with correlation at multiple timescales, other concepts such as a performance ""noise-floor"" are used to generate measurements which are more refined than correlation alone. The measurements are evaluated and compared to plain correlation in their ability to identify performances of the same Chopin mazurka played by the same pianist out of a collection of recordings by various pianists.","","Correlation methods; Information retrieval; Multiple timescales; Musical performance; Pearson correlation; Similarity metrics; Audio recordings","C.S. Sapp; CHARM, Royal Holloway, University of London, United Kingdom; email: craig.sapp@rhul.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Magno T.; Sable C.","Magno, Terence (16175709200); Sable, Carl (23499382500)","16175709200; 23499382500","A comparison of signal-based music recommendation to genre labels, collaborative filtering, musicological analysis, human recommendation, and random baseline","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958014369&partnerID=40&md5=906fb0d7d311b7cc5457274b2921f6ad","Cooper Union, United States","Magno T., Cooper Union, United States; Sable C., Cooper Union, United States","The emergence of the Internet as today's primary medium of music distribution has brought about demands for fast and reliable ways to organize, access, and discover music online. To date, many applications designed to perform such tasks have risen to popularity; each relies on a specific form of music metadata to help consumers discover songs and artists that appeal to their tastes. Very few of these applications, however, analyze the signal waveforms of songs directly. This low-level representation can provide dimensions of information that are inaccessible by metadata alone. To address this issue, we have implemented signalbased measures of musical similarity that have been optimized based on their correlations with human judgments. Furthermore, multiple recommendation engines relying on these measures have been implemented. These systems recommend songs to volunteers based on other songs they find appealing. Blind experiments have been conducted in which volunteers rate the systems' recommendations along with recommendations of leading online music discovery tools (Allmusic which uses genre labels, Pandora which uses musicological analysis, and Last.fm which uses collaborative filtering), random baseline recommendations, and personal recommendations by the first author. This paper shows that the signal-based engines perform about as well as popular, commercial, state-of-the-art systems.","","Information retrieval; Metadata; Collaborative filtering; Human judgments; Last.fm; Music distribution; Music recommendation; Musical similarity; Online music; Signal waveform; State-of-the-art system; Online systems","T. Magno; Cooper Union, United States; email: magno.nyc@gmail.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Inskip C.; Macfarlane A.; Rafferty P.","Inskip, Charlie (23570463400); Macfarlane, Andy (7102685938); Rafferty, Pauline (24923964800)","23570463400; 7102685938; 24923964800","Music, movies and meaning: Communication in film-makers' Search for pre-existing music, and the implications for music information retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951178915&partnerID=40&md5=aa26efc2ea9132c97fecc638c2d8e281","Dept. of Information Science, City University London, United Kingdom; Dept. of Information Studies, University of Aberystwyth, United Kingdom","Inskip C., Dept. of Information Science, City University London, United Kingdom; Macfarlane A., Dept. of Information Science, City University London, United Kingdom; Rafferty P., Dept. of Information Studies, University of Aberystwyth, United Kingdom","While the use of music to accompany moving images is widespread, the information behaviour, communicative practice and decision making by creative professionals within this area of the music industry is an under-researched area. This investigation discusses the use of music in films and advertising focusing on communication and meaning of the music and introduces a reflexive communication model. The model is discussed in relation to interviews with a sample of music professionals who search for and use music for their work. Key factors in this process include stakeholders, briefs, product knowledge and relevance. Searching by both content and context is important, although the final decision when matching music to picture is partly intuitive and determined by a range of stakeholders.","","Information retrieval; Information theory; Communication models; Creative professionals; Film-makers; Final decision; Moving image; Music industry; Music information retrieval; Product knowledge; Communication","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Knopke I.","Knopke, Ian (24381706300)","24381706300","The perlhumdrum and perllilypond toolkits for symbolic music information retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955182398&partnerID=40&md5=900aa7411dbeaad2759e765cf1f4570f","Goldsmiths Digital Studios, United Kingdom","Knopke I., Goldsmiths Digital Studios, United Kingdom","PerlHumdrum is an alternative toolkit for working with large numbers of Humdrum scores. While based on the original Humdrum toolkit, it is a completely new, self-contained implementation that can serve as a replacement, and may be a better choice for some computing systems. PerlHumdrum is fully object-oriented, is designed to easily facilitate analysis and processing of multiple humdrum files, and to answer common musicological questions across entire sets, collections of music, or even the entire output of single or multiple composers. Several extended capabilities that are not available in the original toolkit are also provided, such as translation of MIDI scores to Humdrum, provisions for constructing graphs, a graphical user interface for non-programmers, and the ability to generate complete scores or partial musical examples as standard musical notation using PerlLilypond. These tools are intended primarily for use by music theorists, computational musicologists, and Music Information Retrieval (MIR) researchers.","","Computer systems; Graphical user interfaces; Computing system; Music information retrieval; Musical notation; Object oriented; Symbolic music information; Information retrieval","I. Knopke; Goldsmiths Digital Studios, United Kingdom; email: ian.knopke@gmail.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Müller M.; Ewert S.","Müller, Meinard (7404689873); Ewert, Sebastian (32667575400)","7404689873; 32667575400","Joint structure analysis with applications to music annotation and synchronization","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873427712&partnerID=40&md5=06abadd6ca4443eee6a35211f93e7a0a","Saarland University, MPI Informatik, 66123 Saarbrücken, Campus E1 4, Germany; Bonn University, Computer Science III, 53117 Bonn, Römerstr. 164, Germany","Müller M., Saarland University, MPI Informatik, 66123 Saarbrücken, Campus E1 4, Germany; Ewert S., Bonn University, Computer Science III, 53117 Bonn, Römerstr. 164, Germany","The general goal of music synchronization is to automatically align different versions and interpretations related to a given musical work. In computing such alignments, recent approaches assume that the versions to be aligned correspond to each other with respect to their overall global structure. However, in real-world scenarios, this assumption is often violated. For example, for a popular song there often exist various structurally different album, radio, or extended versions. Or, in classical music, different recordings of the same piece may exhibit omissions of repetitions or significant differences in parts such as solo cadenzas. In this paper, we introduce a novel approach for automatically detecting structural similarities and differences between two given versions of the same piece. The key idea is to perform a single structural analysis for both versions simultaneously instead of performing two separate analyses for each of the two versions. Such a joint structure analysis reveals the repetitions within and across the two versions. As a further contribution, we show how this information can be used for deriving musically meaningful partial alignments and annotations in the presence of structural variations.","","Information retrieval; Extended versions; Global structure; Joint structure; Partial alignment; Popular song; Real-world scenario; Separate analysis; Structural similarity; Structural variations; Alignment","M. Müller; Saarland University, MPI Informatik, 66123 Saarbrücken, Campus E1 4, Germany; email: meinard@mpi-inf.mpg.de","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Maxwell J.B.; Eigenfeldt A.","Maxwell, James B. (55582782500); Eigenfeldt, Arne (24724145100)","55582782500; 24724145100","A music database and query system for recombinant composition","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855833240&partnerID=40&md5=5564d277ed5b20be5a0d701d0f425ee8","School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada","Maxwell J.B., School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada; Eigenfeldt A., School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada","We propose a design and implementation for a music information database and query system, the MusicDB, which can be used for Music Information Retrieval (MIR). The MusicDB is implemented as a Java package, and is loaded in MaxMSP using the mxj external. The MusicDB contains a music analysis module, capable of extracting musical information from standard MIDI files, and a search engine. The search engine accepts queries in the form of a simple six-part syntax, and can return a variety of different types of musical information, drawing on the encoded knowledge of musical form stored in the database.","","Information retrieval; Query languages; Query processing; Java packages; MIDI files; Music analysis; Music database; Music information; Music information retrieval; Musical information; Query systems; Search engines","J.B. Maxwell; School for the Contemporary Arts, Simon Fraser University, Burnaby, BC, Canada; email: jbmaxwel@sfu.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Itoyama K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.","Itoyama, Katsutoshi (18042499100); Goto, Masataka (7403505330); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","18042499100; 7403505330; 35577813100; 7402000772; 7102397930","Instrument equalizer for query-by-example retrieval: Improving sound source separation based on integrated harmonic and inharmonic models","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052051446&partnerID=40&md5=f8db8deaa01a05f21fd994656cc5bd5e","Graduate School of Infomatics, Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Itoyama K., Graduate School of Infomatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., Graduate School of Infomatics, Kyoto University, Japan; Ogata T., Graduate School of Infomatics, Kyoto University, Japan; Okuno H.G., Graduate School of Infomatics, Kyoto University, Japan","This paper describes a music remixing interface, called Instrument Equalizer, that allows users to control the volume of each instrument part within existing audio recordings in real time. Although query-by-example retrieval systems need a user to prepare favorite examples (songs) in general, our interface gives a user to generate examples from existing ones by cutting or boosting some instrument/vocal parts, resulting in a variety of retrieved results. To change the volume, all instrument parts are separated from the input sound mixture using the corresponding standard MIDI file. For the separation, we used an integrated tone (timbre) model consisting of harmonic and inharmonic models that are initialized with template sounds recorded from a MIDI sound generator. The remaining but critical problem here is to deal with various performance styles and instrument bodies that are not given in the template sounds. To solve this problem, we train probabilistic distributions of timbre features by using various sounds. By adding a new constraint of maximizing the likelihood of timbre features extracted from each tone model, we succeeded in estimating model parameters that better express actual timbre.","","Acoustic generators; Equalizers; Harmonic analysis; Information retrieval; Probability distributions; Separation; Critical problems; MIDI files; Model parameters; Not given; Probabilistic distribution; Query-by-example; Real time; Retrieval systems; Sound source separation; Instruments","K. Itoyama; Graduate School of Infomatics, Kyoto University, Japan; email: itoyama@kuis.kyoto-u.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"McKay C.; Burgoyne J.A.; Thompson J.; Fujinaga I.","McKay, Cory (14033215600); Burgoyne, John Ashley (23007865600); Thompson, Jessica (55582942400); Fujinaga, Ichiro (9038140900)","14033215600; 23007865600; 55582942400; 9038140900","Using ace XML 2.0 to store and share feature, instance and class data for musical classification","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952354248&partnerID=40&md5=55df1362eee45399f21a89bfd0c57bc1","CIRMMT, McGill University, Canada; Music Technology, McGill University, Canada","McKay C., CIRMMT, McGill University, Canada; Burgoyne J.A., CIRMMT, McGill University, Canada; Thompson J., Music Technology, McGill University, Canada; Fujinaga I., CIRMMT, McGill University, Canada","This paper introduces ACE XML 2.0, a set of file formats that are designed to meet the special representational needs of research in automatic music classification. Such standardized formats are needed to facilitate the sharing and long-term storage of valuable research data. ACE XML 2.0 is designed to represent a broad range of musical information clearly using a flexible, extensible, selfcontained and formally structured framework. An emphasis is placed on representing extracted feature values, feature descriptions, instance annotations, class ontologies and related metadata. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Metadata; XML; Feature description; Feature values; File formats; Long-term storage; Music classification; Musical information; Research data; Digital storage","C. McKay; CIRMMT, McGill University, Canada; email: cory.mckay@mail.mcgill.ca","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Lukashevich H.","Lukashevich, Hanna (6508121862)","6508121862","Towards quantitative measures of evaluating song segmentation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873479799&partnerID=40&md5=085ef7e1a4a4e8d4347de460901312b9","Fraunhofer IDMT, Ilmenau, Germany","Lukashevich H., Fraunhofer IDMT, Ilmenau, Germany","Automatic music structure analysis or song segmentation has immediate applications in the field of music information retrieval. Among these applications is active music navigation, automatic generation of audio summaries, automatic music analysis, etc. One of the important aspects of a song segmentation task is its evaluation. Commonly, that implies comparing the automatically estimated segmentation with a ground-truth, annotated by human experts. The automatic evaluation of segmentation algorithms provides the quantitative measure that reflects how well the estimated segmentation matches the annotated ground-truth. In this paper we present a novel evaluation measure based on information-theoretic conditional entropy. The principal advantage of the proposed approach lies in the applied normalization, which enables the comparison of the automatic evaluation results, obtained for songs with a different amount of states. We discuss and compare the evaluation scores commonly used for evaluating song segmentation at present. We provide several examples illustrating the behavior of different evaluation measures and weigh the benefits of the presented metric against the others.","","Information retrieval; Information theory; Automatic evaluation; Automatic Generation; Conditional entropy; Evaluation measures; Human expert; ITS evaluation; Music analysis; Music information retrieval; Music structure analysis; Quantitative measures; Audio acoustics","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Rabbat P.; Pachet F.","Rabbat, Patrick (55583481500); Pachet, François (6701441655)","55583481500; 6701441655","Direct and inverse inference in music databases: How to make a song funk?","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873468536&partnerID=40&md5=238a05edb1a276605c5fb864f2933470","Sophis, France; Sony CSL, France","Rabbat P., Sophis, France; Pachet F., Sony CSL, France","We propose an algorithm for exploiting statistical properties of large-scale metadata databases about music titles to answer musicological queries. We introduce two inference schemes called ""direct"" and ""inverse"" inference, based on an efficient implementation of a kernel regression approach. We describe an evaluation experiment conducted on a large-scale database of finegrained musical metadata. We use this database to train the direct inference algorithm, test it, and also to identify the optimal parameters of the algorithm. The inverse inference algorithm is based on the direct inference algorithm. We illustrate it with some examples.","","Algorithms; Information retrieval; Metadata; Query processing; Efficient implementation; Evaluation experiments; Inference algorithm; Kernel regression; Large-scale database; Music database; Optimal parameter; Statistical properties; Inference engines","P. Rabbat; Sophis, France; email: patrick.rabbat@sophis.net","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Rafailidis D.; Nanopoulos A.; Cambouropoulos E.; Manolopoulos Y.","Rafailidis, Dimitris (26323880300); Nanopoulos, Alexandros (6603555418); Cambouropoulos, Emilios (9632551100); Manolopoulos, Yannis (54397260800)","26323880300; 6603555418; 9632551100; 54397260800","Detection of stream segments in symbolic musical data","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415206&partnerID=40&md5=7ca6b676d972d1762b3b71d6c5a06b6c","Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; Dept. of Music Studies, Aristotle University of Thessaloniki, Greece","Rafailidis D., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; Nanopoulos A., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; Cambouropoulos E., Dept. of Music Studies, Aristotle University of Thessaloniki, Greece; Manolopoulos Y., Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece","A listener is thought to be able to organise musical notes into groups within musical streams/voices. A stream segment is a relatively short coherent sequence of tones that is separated horizontally from co-sounding streams and, vertically from neighbouring musical sequences. This paper presents a novel algorithm that discovers musical stream segments in symbolic musical data. The proposed algorithm makes use of a single set of fundamental auditory principles for the concurrent horizontal and vertical segregation of a given musical texture into stream segments. The algorithm is tested against a small manually-annotated dataset of musical excerpts, and results are analysed; it is shown that the technique is promising.","","Information retrieval; Coherent sequence; Data sets; Musical notes; Novel algorithm; Stream segments; Vertical segregation; Algorithms","D. Rafailidis; Dept. of Computer Science, Aristotle Univ. of Thessaloniki, Greece; email: draf@csd.auth.gr","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Izumitani T.; Kashino K.","Izumitani, Tomonori (24450599600); Kashino, Kunio (6701924954)","24450599600; 6701924954","A robust musical audio search method based on diagonal dynamic programming matching of self-similarity matrices","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960674681&partnerID=40&md5=6af5fc90342b72a89172d10894e276eb","NTT Communication Science Laboratories, Japan","Izumitani T., NTT Communication Science Laboratories, Japan; Kashino K., NTT Communication Science Laboratories, Japan","We propose a new musical audio search method based on audio signal matching that can cope with key and tempo variations. The method employs the self-similarity matrix of an audio signal to represent a key-invariant structure of musical audio. And, we use dynamic programming (DP) matching of self-similarity matrices to deal with time variations. However, conventional DP-based sequence matching methods cannot be directly applied for self-similarity matrices because they cannot treat gaps independently of other time frames. We resolve this problem by introducing ""matched element indices,"" which reflect the history of matching, to a DP-based sequence matching method. We performed experiments using musical audio signals. The results indicate that the proposed method improves the detection accuracy in comparison to that that obtained by two conventional methods, namely, DP matching with chroma-based vector rotations and a simple matching of self-similarity feature vectors.","","Dynamic programming; Information retrieval; Audio signal; Conventional methods; Detection accuracy; DP matching; Dynamic-programming matching; Feature vectors; Musical audio; Musical audio signal; Search method; Self-similarities; Self-similarity matrix; Sequence matching; Time frame; Time variations; Vector rotation; Audio acoustics","T. Izumitani; NTT Communication Science Laboratories, Japan; email: izumi@eye.brl.ntt.co.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Liu Y.; Wang Y.; Shenoy A.; Tsai W.-H.; Cai L.","Liu, Yuxiang (57857803800); Wang, Ye (36103845200); Shenoy, Arun (7102488484); Tsai, Wei-Ho (13104438100); Cai, Lianhong (7401563540)","57857803800; 36103845200; 7102488484; 13104438100; 7401563540","Clustering music recordings by their keys","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73249150711&partnerID=40&md5=24051f3c4d850955d99a220c834ac133","Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computing, National University of Singapore, Singapore, Singapore; Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan","Liu Y., Department of Computer Science and Technology, Tsinghua University, Beijing, China, School of Computing, National University of Singapore, Singapore, Singapore; Wang Y., School of Computing, National University of Singapore, Singapore, Singapore; Shenoy A.; Tsai W.-H., Department of Electronic Engineering, National Taipei University of Technology, Taipei, Taiwan; Cai L., Department of Computer Science and Technology, Tsinghua University, Beijing, China","Music key, a high level feature of musical audio, is an effective tool for structural analysis of musical works. This paper presents a novel unsupervised approach for clustering music recordings by their keys. Based on chroma-based features extracted from acoustic signals, an inter-recording distance metric which characterizes diversity of pitch distribution together with harmonic center of music pieces, is introduced to measure dissimilarities among musical features. Then, recordings are divided into categories via unsupervised clustering, where the best number of clusters can be determined automatically by minimizing estimated Rand Index. Any existing technique for key detection can then be employed to identify key assignment for each cluster. Empirical evaluation on a dataset of 91 pop songs illustrates an average cluster purity of 57.3% and a Rand Index of close to 50%, thus highlighting the possibility of integration with existing key identification techniques to improve accuracy, based on strong cross-correlation data available from this framework for input dataset.","","Audio acoustics; Information retrieval; Acoustic signals; Cross correlations; Data sets; Distance metrics; Effective tool; Empirical evaluations; High-level features; Identification techniques; Key assignment; Music recording; Musical audio; Musical features; Number of clusters; Rand index; Unsupervised approaches; Unsupervised clustering; Audio recordings","Y. Liu; Department of Computer Science and Technology, Tsinghua University, Beijing, China; email: liuyuxiang06@mails.tsinghua.edu.cn","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kim Y.E.; Schmidt E.; Emelle L.","Kim, Youngmoo E. (24724623000); Schmidt, Erik (36053813000); Emelle, Lloyd (55582708600)","24724623000; 36053813000; 55582708600","MoodSwings: A collaborative game for music mood label collection","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","86","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445412&partnerID=40&md5=afb938c6b23796dd9e345229a207e96a","Electrical and Computer Engineering, Drexel University, United States","Kim Y.E., Electrical and Computer Engineering, Drexel University, United States; Schmidt E., Electrical and Computer Engineering, Drexel University, United States; Emelle L., Electrical and Computer Engineering, Drexel University, United States","There are many problems in the field of music information retrieval that are not only difficult for machines to solve, but that do not have well-defined answers. In labeling and detecting emotions within music, this lack of specificity makes it difficult to train systems that rely on quantified labels for supervised machine learning. The collection of such ""ground truth"" data for these subjectively perceived features necessarily requires human subjects. Traditional methods of data collection, such as the hiring of subjects, can be flawed, since labeling tasks are time-consuming, tedious, and expensive. Recently, there have been many initiatives to use customized online games to harness so-called ""Human Computation"" for the collection of label data, and several such games have been proposed to collect labels spanning an excerpt of music. We present a new game, MoodSwings (http://schubert.ece. drexel.edu/moodswings), which differs in that it records dynamic (per-second) labels of players' mood ratings of music, in keeping with the unique time-varying quality of musical mood. As in prior collaborative game approaches, players are partnered to verify each others' results, and the game is designed to maximize consensus-building between users. We present preliminary results from an initial set of game play data.","","Data collection; Game approach; Ground truth; Human computation; Human subjects; Music information retrieval; On-line games; Supervised machine learning; Time varying; Train systems; Information retrieval","Y.E. Kim; Electrical and Computer Engineering, Drexel University, United States; email: ykim@drexel.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Meintanis K.A.; Shipman F.M.","Meintanis, Konstantinos A. (13007096700); Shipman, Frank M. (7004203343)","13007096700; 7004203343","Creating and evaluating multi-phrase music summaries","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480268&partnerID=40&md5=fca813bcf946ae3e9f316e4580364fe0","Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States","Meintanis K.A., Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States; Shipman F.M., Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States","Music summarization involves the process of identifying and presenting melody snippets carrying sufficient information for highlighting and remembering a song. In many commercial applications, the problem of finding those snippets is addressed by having humans select the most salient parts of the song or by extracting a few seconds from the song's introduction. Research in the automatic creation of music summaries has focused mainly on the extraction of one or more highly repetitive phrases to represent the whole song. This paper explores whether the composition of multiple ""characteristic"" phrases that are selected to be highly dissimilar to one another will increase the summary's effectiveness. This paper presents three variations of this multi-phrase music summarization approach and a human-centered evaluation comparing these algorithms. Results showed that the resulting multi-phrase summaries performed well in describing the songs. People preferred the multi-phrase summaries over presentations of the introductions of the songs.","","Automatic creations; Commercial applications; Human-centered evaluation; Multi-phrase; Music summarization; Information retrieval","K.A. Meintanis; Center for the Study of Digital Libraries, Department of Computer Science, Texas A and M University, United States; email: kam2959@cs.tamu.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Gasser M.; Flexer A.; Widmer G.","Gasser, Martin (18037419600); Flexer, Arthur (7004555682); Widmer, Gerhard (7004342843)","18037419600; 7004555682; 7004342843","Streamcatcher: Integrated visualization of music clips and online audio streams","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873474373&partnerID=40&md5=406d013b1e75498683161d68bc6ea695","Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We propose a content-based approach to explorative visualization of online audio streams (e.g., web radio streams). The visualization space is defined by prototypical instances of musical concepts taken from personal music collections. Our system shows the relation of prototypes to each other and generates an animated visualization that places representations of audio streams in the vicinity of their most similar prototypes. Both computation of music similarity and visualization are formulated for online real time performance. A software implementation of these ideas is presented and evaluated.","","Information retrieval; Audio stream; Content-based approach; Music clips; Music similarity; Musical concepts; Personal music collection; Real time performance; Software implementation; Visualization","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Cancela P.; Rocamora M.; López E.","Cancela, Pablo (6508290965); Rocamora, Martín (55347707700); López, Ernesto (55566520900)","6508290965; 55347707700; 55566520900","An efficient multi-resolution spectral transform for music analysis","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349251543&partnerID=40&md5=f7a0602dcfa1d0e313fbe5edd3ddce5c","Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay","Cancela P., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay; Rocamora M., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay; López E., Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay","In this paper we focus on multi-resolution spectral analysis algorithms for music signals based on the FFT. Two previously devised efficient algorithms (efficient constant- Q transform [1] and multiresolution FFT [2]) are reviewed and compared with a new proposal based on the IIR filtering of the FFT. Apart from its simplicity, the proposed method shows to be a good compromise between design flexibility and reduced computational effort. Additionally, it was used as a part of an effective melody extraction algorithm. © 2009 International Society for Music Information Retrieval.","","Algorithms; IIR filters; Information retrieval; Spectrum analysis; Analysis algorithms; Computational effort; Design flexibility; IIR filtering; Melody extractions; Multi-resolutions; Music analysis; Music signals; Spectral transform; Fast Fourier transforms","P. Cancela; Instituto de Ingeniería Eléctrica, Universidad de La República, Montevideo, Uruguay; email: pcancela@fing.edu.uy","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Deliège F.; Chua B.Y.; Pedersen T.B.","Deliège, François (23476708400); Chua, Bee Yong (8690659000); Pedersen, Torben Bach (7202189773)","23476708400; 8690659000; 7202189773","High-level audio features: Distributed extraction and similarity search","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872864607&partnerID=40&md5=01a742ec7f05330b6a0833a04f47ef62","Department of Computer Science, Aalborg University, Denmark","Deliège F., Department of Computer Science, Aalborg University, Denmark; Chua B.Y., Department of Computer Science, Aalborg University, Denmark; Pedersen T.B., Department of Computer Science, Aalborg University, Denmark","Today, automatic extraction of high-level audio features suffers from two main scalability issues. First, the extraction algorithms are very demanding in terms of memory and computation resources. Second, copyright laws prevent the audio files to be shared among computers, limiting the use of existing distributed computation frameworks and reducing the transparency of the methods evaluation process. The iSound Music Warehouse (iSoundMW), presented in this paper, is a framework to collect and query high-level audio features. It performs the feature extraction in a two-step process that allows distributed computations while respecting copyright laws. Using public computers, the extraction can be performed on large scale music collections. However, to be truly valuable, data management tools to search among the extracted features are needed. The iSoundMW enables similarity search among the collected high-level features and demonstrates its flexibility and efficiency by using a weighted combination of high-level features and constraints while showing good search performance results.","","Copyrights; Feature extraction; Information management; Audio features; Audio files; Automatic extraction; Computation resources; Copyright law; Data management tools; Distributed computation framework; Distributed computations; Extraction algorithms; High-level features; Music collection; Public computers; Scalability issue; Search performance; Similarity search; Two-step process; Information retrieval","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Woodruff J.; Li Y.; Wang D.","Woodruff, John (15841051800); Li, Yipeng (18042281500); Wang, DeLiang (7407070944)","15841051800; 18042281500; 7407070944","Resolving overlapping harmonics for monaural musical sound separation using pitch and common amplitude modulation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873415211&partnerID=40&md5=98b7115f8c1433fbe86a66c32ded76dd","Dept. of Computer Science and Engineering, Ohio State University, United States; Dept. of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","Woodruff J., Dept. of Computer Science and Engineering, Ohio State University, United States; Li Y., Dept. of Computer Science and Engineering, Ohio State University, United States; Wang D., Dept. of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","In mixtures of pitched sounds, the problem of overlapping harmonics poses a significant challenge to monaural musical sound separation systems. In this paper we present a new algorithm for sinusoidal parameter estimation of overlapping harmonics for pitched instruments. Our algorithm is based on the assumptions that harmonics of the same source have correlated amplitude envelopes and the phase change of harmonics can be accurately predicted from an instrument's pitch. We exploit these two assumptions in a leastsquares estimation framework to resolve overlapping harmonics. This new algorithm is incorporated into a separation system and quantitative evaluation shows that the resulting system performs significantly better than an existing monaural music separation system for mixtures of harmonic instruments.","","Algorithms; Information retrieval; Instruments; Parameter estimation; Separation; Amplitude envelope; Least-squares estimation; Musical sounds; Phase Change; Quantitative evaluation; Separation systems; Sinusoidal parameters; Harmonic analysis","J. Woodruff; Dept. of Computer Science and Engineering, Ohio State University, United States; email: woodrufj@cse.ohio-state.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Zhang X.; Gerhard D.","Zhang, Xinglin (35753982800); Gerhard, David (7004060122)","35753982800; 7004060122","Chord recognition using instrument voicing constraints","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445921&partnerID=40&md5=dcf2a154353a7f7ca7f5e3697e665217","Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada","Zhang X., Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada; Gerhard D., Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada","This paper presents a technique of disambiguation for chord recognition based on a-priori knowledge of probabilities of chord voicings in the specific musical medium. The main motivating example is guitar chord recognition, where the physical layout and structure of the instrument, along with human physical and temporal constraints, make certain chord voicings and chord sequences more likely than others. Pitch classes are first extracted using the Pitch Class Profile (PCP) technique, and chords are then recognized using Artificial Neural Networks. The chord information is then analyzed using an array of voicing vectors (VV) indicating likelihood for chord voicings based on constraints of the instrument. Chord sequence analysis is used to reinforce accuracy of individual chord estimations. The specific notes of the chord are then inferred by combining the chord information and the best estimated voicing of the chord.","","Information retrieval; Neural networks; Chord sequence; Physical layout; Temporal constraints; Instruments","X. Zhang; Dept. of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada; email: zhang46x@cs.uregina.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Scholz R.; Ramalho G.","Scholz, Ricardo (15770403100); Ramalho, Geber (6602152013)","15770403100; 6602152013","COCHONUT: Recognizing complex chords from MIDI guitar sequences","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949575165&partnerID=40&md5=6eb0aa68c6e0eff5c7d923508d45c8b0","Federal University of Pernambuco, Brazil","Scholz R., Federal University of Pernambuco, Brazil; Ramalho G., Federal University of Pernambuco, Brazil","Chord recognition from symbolic data is a complex task, due to its strong context dependency and the large number of possible combinations of the intervals which the chords are made of, specially when dealing with dissonances, such as 7ths, 9ths, 13ths and suspended chords. None of the current approaches deal with such complexity. Most of them consider only simple chord patterns, in the best cases, including sevenths. In addition, when considering symbolic data captured from a MIDI guitar, we need to deal with non quantized and noisy data, which increases the difficulty of the task. The current symbolic approaches deal only with quantized data, with no automatic technique to reduce noise. This paper proposes a new approach to recognize chords, from symbolic MIDI guitar data, called COCHONUT (Complex Chords Nutting). The system uses contextual harmonic information to solve ambiguous cases, integrated with other techniques, such as decision theory, optimization, pattern matching and rule-based recognition. The results are encouraging and provide strong indications that the use of harmonic contextual information, integrated with other techniques, can actually improve the results currently found in literature.","Bossa nova; Chord recognition; MIDI Guitar; Music analysis; Music information retrieval; Music segmenting","Information retrieval; Pattern matching; Bossa nova; Chord recognition; MIDI Guitar; Music analysis; Music information retrieval; Music segmenting; Musical instruments","R. Scholz; Federal University of Pernambuco, Brazil; email: reps@cin.ufpe.br","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Manzagol P.-A.; Bertin-Mahieux T.; Eck D.","Manzagol, Pierre-Antoine (25652216600); Bertin-Mahieux, Thierry (49060926500); Eck, Douglas (12141444300)","25652216600; 49060926500; 12141444300","On the use of sparse time-relative auditory codes for music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465396&partnerID=40&md5=c7bc98091843d2c7522dd9d56a33d31f","Universitéde Montréal, Department of Computer Science, Montreal, Canada","Manzagol P.-A., Universitéde Montréal, Department of Computer Science, Montreal, Canada; Bertin-Mahieux T., Universitéde Montréal, Department of Computer Science, Montreal, Canada; Eck D., Universitéde Montréal, Department of Computer Science, Montreal, Canada","Many if not most audio features used in MIR research are inspired by work done in speech recognition and are variations on the spectrogram. Recently, much attention has been given to new representations of audio that are sparse and time-relative. These representations are efficient and able to avoid the time-frequency trade-off of a spectrogram. Yet little work with music streams has been conducted and these features remain mostly unused in the MIR community. In this paper we further explore the use of these features for musical signals. In particular, we investigate their use on realistic music examples (i.e. released commercial music) and their use as input features for supervised learning. Furthermore, we identify three specific issues related to these features which will need to be further addressed in order to obtain the full benefit for MIR applications.","","Spectrographs; Speech recognition; Audio features; Input features; Musical signals; Spectrograms; Time frequency; Information retrieval","P.-A. Manzagol; Universitéde Montréal, Department of Computer Science, Montreal, Canada; email: manzagop@iro.umontreal.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Lemström K.; Mikkilä N.; Mäkinen V.","Lemström, Kjell (7006564183); Mikkilä, Niko (35422792400); Mäkinen, Veli (35616993400)","7006564183; 35422792400; 35616993400","Fast index based filters for music retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418049&partnerID=40&md5=d7be1fc7bfd980e07fa8cca98bd8e539","University of Helsinki, Department of Computer Science, Finland","Lemström K., University of Helsinki, Department of Computer Science, Finland; Mikkilä N., University of Helsinki, Department of Computer Science, Finland; Mäkinen V., University of Helsinki, Department of Computer Science, Finland","We consider two content-based music retrieval problems where the music is modeled as sets of points in the Euclidean plane, formed by the (on-set time, pitch) pairs. We introduce fast filtering methods based on indexing the underlying database. The filters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practically lossless filters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders of a magnitude faster than previous solutions.","","Geometry; Information retrieval; Content-based music retrieval; Euclidean planes; Fast indices; Filter-based; Filtering method; Index structure; Lossless; Music database; Music retrieval; Running time; Search spaces; Sublinear time; Database systems","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Godfrey M.T.; Chordia P.","Godfrey, Mark T. (24724011900); Chordia, Parag (24723695700)","24724011900; 24723695700","Hubs and homogeneity: Improving content-based music modeling","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416546&partnerID=40&md5=bec4a2bb5715b9315ea55ab1d24f6482","Georgia Institute of Technology, Music Technology Group, United States","Godfrey M.T., Georgia Institute of Technology, Music Technology Group, United States; Chordia P., Georgia Institute of Technology, Music Technology Group, United States","We explore the origins of hubs in timbre-based song modeling in the context of content-based music recommendation and propose several remedies. Specifically, we find that a process of model homogenization, in which certain components of a mixture model are systematically removed, improves performance as measured against several ground-truth similarity metrics. Extending the work of Aucouturier, we introduce several new methods of homogenization. On a subset of the uspop data set, model homogenization improves artist R-precision by a maximum of 3.5% and agreement to user collection co-occurrence data by 7.4%. We also explore differences in the effectiveness of the various homogenization methods for hub reduction. Further, we extend the modeling of frame-based MFCC features by using a kernel density estimation approach to non-parametric modeling. We find that such an approach significantly reduces the number of hubs (by 2.6% of the dataset) while improving agreement to ground-truth by 5% and slightly improving artist R-precision as compared with the standard parametric model.","","Homogenization method; Information retrieval; Co-occurrence; Content-based; Data sets; Kernel Density Estimation; Mixture model; Music recommendation; Non-parametric modeling; Parametric models; Similarity metrics; Computer music","M.T. Godfrey; Georgia Institute of Technology, Music Technology Group, United States; email: mark.godfrey@gatech.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Holzapfel A.; Stylianou Y.","Holzapfel, Andre (18041818000); Stylianou, Yannis (6601991415)","18041818000; 6601991415","Rhythmic similarity in traditional turkish music","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952350713&partnerID=40&md5=f7624fc8fdd495518707c67710907488","Institute of Computer Science, FORTH, University of Crete, Greece","Holzapfel A., Institute of Computer Science, FORTH, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, University of Crete, Greece","In this paper, the problemof automatically assigning a piece of traditional Turkish music into a class of rhythm referred to as usul is addressed. For this, an approach for rhythmic similarity measurement based on scale transforms has been evaluated on a set of MIDI data. Because this task is related to time signature estimation, the accuracy of the proposed method is evaluated and compared with a state of the art time signature estimation approach. The results indicate that the proposed method can be successfully applied to audio signals of Turkish music and that it captures relevant properties of the individual usul. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Audio signal; Estimation approaches; Scale transform; Similarity measurements; State of the art; Turkishs; Audio signal processing","A. Holzapfel; Institute of Computer Science, FORTH, University of Crete, Greece; email: hannover@csd.uoc.gr","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Duan Z.; Lu L.; Zhang C.","Duan, Zhiyao (24450312900); Lu, Lie (7403963024); Zhang, Changshui (7405490589)","24450312900; 7403963024; 7405490589","Collective annotation of music from multiple semantic categories","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431927&partnerID=40&md5=9f1959de6ebfbffa5975e4ddbe4625b1","Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China; State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China","Duan Z., Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China, State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China; Lu L., Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China; Zhang C., State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory for Information Science and Technology (TNList), Tsinghua University, Beijing 100084, China","Music semantic annotation aims to automatically annotate a music signal with a set of semantic labels (words or tags). Existing methods on music semantic annotation usually take it as a multi-label binary classification problem, and model each semantic label individually while ignoring their relationships. However, there are usually strong correlations between some labels. Intuitively, investigating this correlation can be helpful to improve the overall annotation performance. In this paper, we report our attempts to collective music semantic annotation, which not only builds a model for each semantic label, but also builds models for the pairs of labels that have significant correlations. Two methods are exploited in this paper, one based on a generative model (Gaussian Mixture Model), and another based on a discriminative model (Conditional Random Field). Experiments show slight but consistent improvement in terms of precision and recall, compared with the individual-label modeling methods.","","Image segmentation; Information retrieval; Annotation performance; Binary classification problems; Conditional random field; Discriminative models; Gaussian Mixture Model; Generative model; Multi-label; Music signals; Precision and recall; Semantic annotations; Semantic category; Semantic labels; Strong correlation; Semantics","Z. Duan; Microsoft Research Asia (MSRA), Sigma Center, Beijing 100080, Haidian District, China; email: duanzhiyao00@mails.tsinghua.edu.cn","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Pachet F.; Roy P.","Pachet, François (6701441655); Roy, Pierre (55506419000)","6701441655; 55506419000","Hit song science is not yet a science","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","46","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445246&partnerID=40&md5=d47055227f115b5af5ce670a5c899745","Sony CSL, France","Pachet F., Sony CSL, France; Roy P., Sony CSL, France","We describe a large-scale experiment aiming at validating the hypothesis that the popularity of music titles can be predicted from global acoustic or human features. We use a 32.000 title database with 632 manually-entered labels per title including 3 related to the popularity of the title. Our experiment uses two audio feature sets, as well as the set of all the manually-entered labels but the popularity ones. The experiment shows that some subjective labels may indeed be reasonably well-learned by these techniques, but not popularity. This contradicts recent and sustained claims made in the MIR community and in the media about the existence of ""Hit Song Science"".","","Information retrieval; Audio features; Large scale experiments; Experiments","F. Pachet; Sony CSL, France; email: pachet@csl.sony.fr","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kako T.; Ohishi Y.; Kameoka H.; Kashino K.; Takeda K.","Kako, Tatsuya (56940339600); Ohishi, Yasunori (14018275300); Kameoka, Hirokazu (7006771405); Kashino, Kunio (6701924954); Takeda, Kazuya (7404334995)","56940339600; 14018275300; 7006771405; 6701924954; 7404334995","Automatic identification for singing style based on sung melodic contour characterized in phase plane","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952429496&partnerID=40&md5=fa57cddf30f516cf893b581c7e8f89de","Graduate School of Information Science, Nagoya University, Japan; Communication Science Laboratories, NTT Corporation, Japan","Kako T., Graduate School of Information Science, Nagoya University, Japan; Ohishi Y., Communication Science Laboratories, NTT Corporation, Japan; Kameoka H., Communication Science Laboratories, NTT Corporation, Japan; Kashino K., Communication Science Laboratories, NTT Corporation, Japan; Takeda K., Graduate School of Information Science, Nagoya University, Japan","A stochastic representation of singing styles is proposed. The dynamic property of melodic contour, i.e., fundamental frequency (F0) sequence, is assumed to be the main cue for singing styles because it can characterize such typical ornamentations as vibrato . F0 signal trajectories in the phase plane are used as the basic representation. By fitting Gaussian mixture models to the observed F0 trajectories in the phase plane, a parametric representation is obtained by a set of GMM parameters. The effectiveness of our proposed method is confirmed through experimental evaluation where 94.1% accuracy for singer-class discrimination was obtained. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automatic identification; Dynamic property; Experimental evaluation; Fundamental frequencies; Gaussian Mixture Model; In-phase; Parametric representations; Phase plane; Signal trajectories; Singing styles; Stochastic representations; Automation","T. Kako; Graduate School of Information Science, Nagoya University, Japan; email: kako@sp.m.is.nagoya-u.ac.jp","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Peeters G.; Fenech D.; Rodet X.","Peeters, Geoffroy (22433836000); Fenech, David (55582169300); Rodet, Xavier (6603779613)","22433836000; 55582169300; 6603779613","MCIPA: A music content information player and annotator for discovering music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873432129&partnerID=40&md5=42919ea220cefe0eeb1342c72f742f59","Ircam, CNRS STMS, France","Peeters G., Ircam, CNRS STMS, France; Fenech D., Ircam, CNRS STMS, France; Rodet X., Ircam, CNRS STMS, France","In this paper, we present a new tool for intra-document browsing of musical pieces. This tool is a multimedia player which represents the content of a musical piece visually. Each type of musical content (structure, chords, downbeats/beats, notes, events) is associated which a distinct visual representation. The user sees what he/she is listening too. He can also browse inside the music according to the visual content. For this, each type of visual object has a dedicated feedback, either as an audio-feedback or as a playhead feedback. Content information can be extracted automatically from audio (using signal processing algorithms) or annotated by hand by the user. This multimedia player can also be used as an annotator tool guided by the content.","","Signal processing; Content information; Multimedia player; Music contents; Musical pieces; Signal processing algorithms; Visual content; Visual objects; Visual representations; Information retrieval","G. Peeters; Ircam, CNRS STMS, France; email: peeters@ircam.fr","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Corthaut N.; Govaerts S.; Verbert K.; Duval E.","Corthaut, Nik (35748332800); Govaerts, Sten (7801629780); Verbert, Katrien (13605498800); Duval, Erik (7006487422)","35748332800; 7801629780; 13605498800; 7006487422","Connecting the dots: Music metadata generation, schemas and applications","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955914360&partnerID=40&md5=22f909b970f23ce7b6d7c3db56b0777c","Katholieke Universiteit Leuven, Dept. Computer Science, Belgium","Corthaut N., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Govaerts S., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Verbert K., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; Duval E., Katholieke Universiteit Leuven, Dept. Computer Science, Belgium","With the ever-increasing amount of digitized music becoming available, metadata is a key driver for different music related application domains. A service that combines different metadata sources should be aware of the existence of different schemas to store and exchange music metadata. The user of a metadata provider could benefit from knowledge about the metadata needs for different music application domains. In this paper, we present how we can compare the expressiveness and richness of a metadata schema for an application. To cope with different levels of granularity in metadata fields we defined clusters of semantically related metadata fields. Similarly, application domains were defined to tackle the fine-grained functionality space in music applications. Next is shown to what extent music application domains and metadata schemas make use of the metadata field clusters. Finally, we link the metadata schemas with the application domains. A decision table is presented that assists the user of a metadata provider in choosing the right metadata schema for his application.","","Decision tables; Information retrieval; Metadata generation; Metadata schema; Music applications; Metadata","N. Corthaut; Katholieke Universiteit Leuven, Dept. Computer Science, Belgium; email: nik.corthaut@cs.kuleuven.be","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kirlin P.B.","Kirlin, Phillip B. (55582044600)","55582044600","Using harmonic and melodic analyses to automate the initial stages of schenkerian analysis","2009","Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249234110&partnerID=40&md5=80142499fb46734bd029c6cbde1e936a","Department of Computer Science, University of Massachusetts, Amherst, United States","Kirlin P.B., Department of Computer Science, University of Massachusetts, Amherst, United States","Structural music analysis is used to reveal the inner workings of a musical composition by recursively applying reductions to the music, resulting in a series of successively more abstract views of the composition. Schenkerian analysis is the most well-developed type of structural analysis, and while there is a wide body of research on the theory, there is no well-defined algorithm to perform such an analysis. A automated algorithm for Schenkerian analysis would be extremely useful to music scholars and researchers studying music from a computational standpoint. The first major step in producing a Schenkerian analysis involves selecting notes from the composition in question for the primary soprano and bass parts of the analysis. We present an algorithmfor this that uses harmonic andmelodic analyses to accomplish this task. © 2009 International Society for Music Information Retrieval.","","Information retrieval; Automated algorithms; Initial stages; Music analysis; Musical composition; Algorithms","P.B. Kirlin; Department of Computer Science, University of Massachusetts, Amherst, United States; email: pkirlin@cs.umass.edu","","10th International Society for Music Information Retrieval Conference, ISMIR 2009","26 October 2009 through 30 October 2009","Kobe","95395"
"Fujihara H.; Goto M.; Ogata J.","Fujihara, Hiromasa (16068753300); Goto, Masataka (7403505330); Ogata, Jun (7102408269)","16068753300; 7403505330; 7102408269","Hyperlinking lyrics: A method for creating hyperlinks between phrases in song lyrics","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049411004&partnerID=40&md5=9f59a906f0c7f03dc08bda3f1abc44c2","National Institute of Advanced Industrial Science and Technology (AIST), Japan","Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Ogata J., National Institute of Advanced Industrial Science and Technology (AIST), Japan","We describe a novel method for creating a hyperlink from a phrase in the lyrics of a song to the same phrase in the lyrics of another song. This method can be applied to various applications, such as song clustering based on the meaning of the lyrics and a music playback interface that will enable a user to browse and discover songs on the basis of lyrics. Given a song database consisting of songs with their text lyrics and songs without their text lyrics, our method first extracts appropriate keywords (phrases) from the text lyrics without using audio signals. It then finds these keywords in audio signals by estimating the keywords' start and end times. Although the performance obtained in our experiments has room for improvement, the potential of this new approach is shown.","","Information retrieval; Audio signal; Hyperlinking; Hyperlinks; Music-playback interfaces; Hypertext systems","H. Fujihara; National Institute of Advanced Industrial Science and Technology (AIST), Japan; email: h.fujihara@aist.go.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Molina-Solana M.; Arcos J.L.; Gomez E.","Molina-Solana, Miguel (24385556000); Arcos, Josep Lluís (56131974300); Gomez, Emilia (14015483200)","24385556000; 56131974300; 14015483200","Using expressive trends for identifying violin performers","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956613822&partnerID=40&md5=ad541bee7334785e310a64f7b97b6579","Comp. Science and AI Dep., University of Granada, 18071 Granada, Spain; IIIA, AI Research Institute, CSIC, Spanish National Res. Council, 08193 Bellaterra, Spain; Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Spain","Molina-Solana M., Comp. Science and AI Dep., University of Granada, 18071 Granada, Spain; Arcos J.L., IIIA, AI Research Institute, CSIC, Spanish National Res. Council, 08193 Bellaterra, Spain; Gomez E., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Spain","This paper presents a new approach for identifying professional performers in commercial recordings. We propose a Trend-based model that, analyzing the way Narmour's Implication-Realization patterns are played, is able to characterize performers. Concretely, starting from automatically extracted descriptors provided by state-of-the-art extraction tools, the system performs a mapping to a set of qualitative behavior shapes and constructs a collection of frequency distributions for each descriptor. Experiments were conducted in a data-set of violin recordings from 23 different performers. Reported results show that our approach is able to achieve high identification rates.","","Information retrieval; Data sets; Descriptors; Frequency distributions; Identification rates; Qualitative behavior; Audio recordings","M. Molina-Solana; Comp. Science and AI Dep., University of Granada, 18071 Granada, Spain; email: miguelmolina@ugr.es","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Doll T.M.; Migneco R.V.; Kim Y.E.","Doll, Travis M. (24723924900); Migneco, Ray V. (35311311700); Kim, Youngmoo E. (24724623000)","24723924900; 35311311700; 24724623000","Online activities for music information and acoustics education and psychoacoustic data collection","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873442840&partnerID=40&md5=f60e90a8604b51ce6d3b16a0c35e54a2","Drexel University, Electrical and Computer Engineering, United States","Doll T.M., Drexel University, Electrical and Computer Engineering, United States; Migneco R.V., Drexel University, Electrical and Computer Engineering, United States; Kim Y.E., Drexel University, Electrical and Computer Engineering, United States","Online collaborative activities provide a powerful platform for the collection of psychoacoustic data on the perception of audio and music from a very large numbers of subjects. Furthermore, these activities can be designed to simultaneously educate users about aspects of music information and acoustics, particularly for younger students in grades K-12. We have created prototype interactive activities illustrating aspects of two different sound and acoustics concepts: musical instrument timbre and the cocktail party problem (sound source isolation within mixtures) that also provide a method of collecting perceptual data related to these problems with a range of parameter variation that is difficult to achieve for large subject populations using traditional psychoacoustic evaluation. We present preliminary data from a pilot study where middle school students were engaged with the two activities to demonstrate the potential benefits as an education and data collection platform.","","Data acquisition; Information retrieval; Population statistics; Students; Cocktail party problems; Collaborative activities; Data collection; Middle school students; Music information; Online activities; Pilot studies; Potential benefits; Preliminary data; Sound and acoustic; Sound source; Audio acoustics","T.M. Doll; Drexel University, Electrical and Computer Engineering, United States; email: tmd47@drexel.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Flexer A.; Schnitzer D.; Gasser M.; Widmer G.","Flexer, Arthur (7004555682); Schnitzer, Dominik (23996271700); Gasser, Martin (18037419600); Widmer, Gerhard (7004342843)","7004555682; 23996271700; 18037419600; 7004342843","Playlist generation using start and end songs","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","81","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443934&partnerID=40&md5=19685a322ed42c8f337e27f7d8fdb88c","Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Schnitzer D., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","A new algorithm for automatic generation of playlists with an inherent sequential order is presented. Based on a start and end song it creates a smooth transition allowing users to discover new songs in a music collection. The approach is based on audio similarity and does not require any kind of meta data. It is evaluated using both objective genre labels and subjective listening tests. Our approach allows users of the website of a public radio station to create their own digital ""mixtapes"" online.","","Radio broadcasting; Radio stations; Automatic Generation; Music collection; Smooth transitions; Subjective listening test; Information retrieval","A. Flexer; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; email: arthur.flexer@ofai.at","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Ono N.; Miyamoto K.; Kameoka H.; Sagayama S.","Ono, Nobutaka (7202472899); Miyamoto, Kenichi (55619319407); Kameoka, Hirokazu (7006771405); Sagayama, Shigeki (7004859104)","7202472899; 55619319407; 7006771405; 7004859104","A real-time equalizer of harmonic and percussive components in music signals","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","63","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873465086&partnerID=40&md5=7d7a0359d4b42a01e3e4690ba5bea8a3","Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","Ono N., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Miyamoto K., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Kameoka H., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Sagayama S., Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","In this paper, we present a real-time equalizer to control a volume balance of harmonic and percussive components in music signals without a priori knowledge of scores or included instruments. The harmonic and percussive components of music signals have much different structures in the power spectrogram domain, the former is horizontal, while the latter is vertical. Exploiting the anisotropy, our methods separate input music signals into them based on the MAP estimation framework. We derive two kind of algorithm based on a I-divergence-based mixing model and a hard mixing model. Although they include iterative update equations, we realized the real-time processing by a sliding analysis technique. The separated harmonic and percussive components are finally remixed in an arbitrary volume balance and played. We show the prototype system implemented on Windows environment.","","Equalizers; Information retrieval; Iterative methods; Separation; Analysis techniques; Different structure; Iterative update; MAP estimation; Mixing models; Music signals; Priori knowledge; Prototype system; Realtime processing; Spectrograms; Windows environment; Harmonic analysis","N. Ono; Department of Information Physics and Computing, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; email: onono@hil.t.u-tokyo.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Mayer R.; Neumayer R.; Rauber A.","Mayer, Rudolf (23397787500); Neumayer, Robert (23012858400); Rauber, Andreas (57074846700)","23397787500; 23012858400; 57074846700","Rhyme and style features for musical genre classification by song lyrics","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","68","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470329&partnerID=40&md5=417df1262fcdc8abbedee76ac13fafac","Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria; Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway","Mayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria; Neumayer R., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria, Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria","How individuals perceive music is influenced by many different factors. The audible part of a piece of music, its sound, does for sure contribute, but is only one aspect to be taken into account. Cultural information influences how we experience music, as does the songs' text and its sound. Next to symbolic and audio based music information retrieval, which focus on the sound of music, song lyrics, may thus be used to improve classification or similarity ranking of music. Song lyrics exhibit specific properties different from traditional text documents - many lyrics are for example composed in rhyming verses, and may have different frequencies for certain parts-of-speech when compared to other text documents. Further, lyrics may use 'slang' language or differ greatly in the length and complexity of the language used, which can be measured by some statistical features such as word / verse length, and the amount of repetative text. In this paper, we present a novel set of features developed for textual analysis of song lyrics, and combine them with and compare them to classical bag-of-words indexing approaches. We present results for musical genre classification on a test collection in order to demonstrate our analysis.","","Information retrieval; Audio-based; Bag of words; Different frequency; Music information retrieval; Musical genre classification; Similarity rankings; Statistical features; Test Collection; Text document; Textual analysis; Audio acoustics","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Fiebrink R.; Wang G.; Cook P.","Fiebrink, Rebecca (36095844900); Wang, Ge (56144195500); Cook, Perry (57203105418)","36095844900; 56144195500; 57203105418","Support for MIR prototyping and real-time applications in the chuck programming language","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421464&partnerID=40&md5=a74d79a605ba99b0bd2f3c8f21123690","Princeton University, United States; Stanford University, United States","Fiebrink R., Princeton University, United States; Wang G., Stanford University, United States; Cook P., Princeton University, United States","In this paper, we discuss our recent additions of audio analysis and machine learning infrastructure to the ChucK music programming language, wherein we provide a complementary system prototyping framework for MIR researchers and lower the barriers to applying many MIR algorithms in live music performance. The new language capabilities preserve ChucK's breadth of control - from high-level control using building block components to sample-level manipulation - and on-the-fly reprogrammability, allowing the programmer to experiment with new features, signal processing techniques, and learning algorithms with ease and flexibility. Furthermore, our additions integrate tightly with ChucK's synthesis system, allowing the programmer to apply the results of analysis and learning to drive real-time music creation and interaction within a single framework. In this paper, we motivate and describe our recent additions to the language, outline a ChucK-based approach to rapid MIR prototyping, present three case studies in which we have applied ChucK to audio analysis and MIR tasks, and introduce our new toolkit to facilitate experimentation with analysis and learning in the language.","","Audio acoustics; Experiments; High level languages; Historic preservation; Information retrieval; Learning algorithms; Signal processing; Audio analysis; Building blockes; Complementary systems; Language capability; Music creation; Music performance; On-the-fly; Re-programmability; Real-time application; Signal processing technique; Chucks","R. Fiebrink; Princeton University, United States; email: fiebrink@princeton.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Chuan C.-H.; Chew E.","Chuan, Ching-Hua (15050129500); Chew, Elaine (8706714000)","15050129500; 8706714000","Evaluating and visualizing effectiveness of style emulation in musical accompaniment","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856497995&partnerID=40&md5=237e57a7e42de526e723484fa8169f62","Department of Computer Science, University of Southern California, Viterbi School of Engineering, United States; Epstein Department of Industrial and Systems Engineering, University of Southern California, Viterbi School of Engineering, United States; Radcliffe Institute for Advanced Study, Harvard University, United States","Chuan C.-H., Department of Computer Science, University of Southern California, Viterbi School of Engineering, United States; Chew E., Epstein Department of Industrial and Systems Engineering, University of Southern California, Viterbi School of Engineering, United States, Radcliffe Institute for Advanced Study, Harvard University, United States","We propose general quantitative methods for evaluating and visualizing the results of machine-generated style-specific accompaniment. The evaluation of automated accompaniment systems, and the degree to which they emulate a style, has been based primarily on subjective opinion. To quantify style similarity between machine-generated and original accompaniments, we propose two types of measures: one based on transformations in the neo-Riemannian chord space, and another based on the distribution of melody-chord intervals. The first set of experiments demonstrate the methods on an automatic style-specific accompaniment (ASSA) system. They test the effect of training data choice on style emulation effectiveness, and challenge the assumption that more data is better. The second set of experiments compare the output of the ASSA system with those of a rule-based system, and random chord generator. While the examples focus primarily on machine emulation of Pop/Rock accompaniment, the methods generalize to music of other genres.","","Information retrieval; On-machines; Quantitative method; Training data; Experiments","C.-H. Chuan; Department of Computer Science, University of Southern California, Viterbi School of Engineering, United States; email: chinghuc@usc.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Slaney M.; Weinberger K.; White W.","Slaney, Malcolm (6701855101); Weinberger, Kilian (8279937900); White, William (57198982894)","6701855101; 8279937900; 57198982894","Learning a metric for music similarity","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","68","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873426251&partnerID=40&md5=dbb9f2c8110745f954929ae37eb7f071","Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; Yahoo Media Innovation, Berkeley, CA 94704, 1950 University Ave., United States","Slaney M., Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; Weinberger K., Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; White W., Yahoo Media Innovation, Berkeley, CA 94704, 1950 University Ave., United States","This paper describe five different principled ways to embed songs into a Euclidean metric space. In particular, we learn embeddings so that the pairwise Euclidean distance between two songs reflects semantic dissimilarity. This allows distance-based analysis, such as for example straightforward nearest-neighbor classification, to detect and potentially suggest similar songs within a collection. Each of the six approaches (baseline, whitening, LDA, NCA, LMNN and RCA) rotate and scale the raw feature space with a linear transform. We tune the parameters of these models using a song-classification task with content-based features.","","Information retrieval; Semantics; Content-based features; Distance-based; Embeddings; Euclidean distance; Euclidean metrics; Feature space; Linear transform; Music similarity; Nearest-neighbors; Mathematical transformations","M. Slaney; Yahoo Research, Santa Clara, CA 95054, 2821 Mission College Blvd., United States; email: malcolm@ieee.org","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Fremerey C.; Müller M.; Kurth F.; Clausen M.","Fremerey, Christian (23396821400); Müller, Meinard (7404689873); Kurth, Frank (56240850200); Clausen, Michael (56225233200)","23396821400; 7404689873; 56240850200; 56225233200","Automatic mapping of scanned sheet music to audio recordings","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","27","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76949101999&partnerID=40&md5=4db3e86a737d4fb1e4187a029442b9a2","Computer Science III, University of Bonn, Bonn, Germany; Max-Planck-Institut (MPI) for Informatics, Saarbrücken, Germany; Research Establishment for Applied Science (FGAN), Wachtberg, Germany","Fremerey C., Computer Science III, University of Bonn, Bonn, Germany; Müller M., Max-Planck-Institut (MPI) for Informatics, Saarbrücken, Germany; Kurth F., Research Establishment for Applied Science (FGAN), Wachtberg, Germany; Clausen M., Computer Science III, University of Bonn, Bonn, Germany","Significant digitization efforts have resulted in large multimodal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are first transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven's piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material.","","Audio recordings; Information retrieval; Mapping; Signal processing; Systematic errors; Acoustic materials; Audio clips; Automatic mapping; Bavarians; Feature representation; High precision; Multi-modal; Music collection; Music data; Music recognition; Real-world application; Scanned images; Systematic experiment; Audio acoustics","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Sordo M.; Celma O.; Blech M.; Guaus E.","Sordo, Mohamed (43462170300); Celma, Óscar (12804596800); Blech, Martín (35075689500); Guaus, Enric (6508320412)","43462170300; 12804596800; 35075689500; 6508320412","The quest for musical genres: Do the experts and the wisdom of crowds agree?","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155212713&partnerID=40&md5=685feb84a6d47c196a79b1945189ee3b","Music Technology Group, Universitat Pompeu Fabra, Spain","Sordo M., Music Technology Group, Universitat Pompeu Fabra, Spain; Celma O., Music Technology Group, Universitat Pompeu Fabra, Spain; Blech M., Music Technology Group, Universitat Pompeu Fabra, Spain; Guaus E., Music Technology Group, Universitat Pompeu Fabra, Spain","This paper presents some findings around musical genres. The main goal is to analyse whether there is any agreement between a group of experts and a community, when defining a set of genres and their relationships. For this purpose, three different experiments are conducted using two datasets: the MP3.com expert taxonomy, and last.fm tags at artist level. The experimental results show a clear agreement for some components of the taxonomy (Blues, HipHop), whilst in other cases (e.g. Rock) there is no correlations. Interestingly enough, the same results are found in the MIREX2007 results for audio genre classification task. Therefore, a multi-faceted approach for musical genre using expert based classifications, dynamic associations derived from the wisdom of crowds, and content-based analysis can improve genre classification, as well as other relevant MIR tasks such as music similarity or music recommendation.","","Information retrieval; Content-based analysis; Data sets; Genre classification; Last.fm; Multi-faceted approach; Music recommendation; Music similarity; Musical genre; Taxonomies","M. Sordo; Music Technology Group, Universitat Pompeu Fabra, Spain; email: msordo@iua.upf.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Little D.; Pardo B.","Little, David (57197561056); Pardo, Bryan (10242155400)","57197561056; 10242155400","Learning musical instruments from mixtures of audio with weak labels","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428417&partnerID=40&md5=1d6fb0c99ed5f06dab5106796fbb1d61","EECS Department, Northwestern University, Evanston, IL 60208, United States","Little D., EECS Department, Northwestern University, Evanston, IL 60208, United States; Pardo B., EECS Department, Northwestern University, Evanston, IL 60208, United States","We are interested in developing a system that learns to recognize individual sound sources in an auditory scene where multiple sources may be occurring simultaneously. We focus here on sound source recognition in music audio mixtures. Many researchers have made progress by using isolated training examples or very strongly labeled training data. We consider an alternative approach: the learner is presented with a variety of weaky-labeled mixtures. Positive examples include the target instrument at some point in a mixture of sounds, and negative examples are mixtures that do not contain the target. We show that it not only possible to learn from weakly-labeled mixtures of instruments, but that it works significantly better (78% correct labeling compared to 55%) than learning from isolated examples when the task is identification of an instrument in novel mixtures.","","Acoustic generators; Audio acoustics; Information retrieval; Alternative approach; Labeled training data; Multiple source; Negative examples; Positive examples; Sound source; Training example; Weak labels; Mixtures","D. Little; EECS Department, Northwestern University, Evanston, IL 60208, United States; email: d-little@northwestern.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Pugin L.; Hockman J.; Burgoyne J.A.; Fujinaga I.","Pugin, Laurent (23009752900); Hockman, Jason (36730968100); Burgoyne, John Ashley (23007865600); Fujinaga, Ichiro (9038140900)","23009752900; 36730968100; 23007865600; 9038140900","Gamera versus aruspix two optical music recognition approaches","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954257763&partnerID=40&md5=998ed84568831a09e3736950c46d6a68","Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Hockman J., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Optical music recognition (OMR) applications are predominantly designed for common music notation and as such, are inherently incapable of adapting to specialized notation forms within early music. Two OMR systems, namely Gamut (a Gamera application) and Aruspix, have been proposed for early music. In this paper, we present a novel comparison of the two systems, which use markedly different approaches to solve the same problem, and pay close attention to the performance and learning rates of both applications. In order to obtain a complete comparison of Gamut and Aruspix, we evaluated the core recognition systems and the pitch determination processes separately. With our experiments, we were able to highlight the advantages of both approaches as well as causes of problems and possibilities for future improvements.","","Learning rates; Music notation; Music recognition; Recognition systems; Information retrieval","L. Pugin; Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; email: laurent@music.mcgill.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Hu X.; Downie J.S.; Laurier C.; Bay M.; Ehmann A.F.","Hu, Xiao (55496358400); Downie, J. Stephen (7102932568); Laurier, Cyril (26031025000); Bay, Mert (56259607500); Ehmann, Andreas F. (8988651500)","55496358400; 7102932568; 26031025000; 56259607500; 8988651500","The 2007 mirex audio mood classification task: Lessons learned","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","135","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873433681&partnerID=40&md5=91b5862fb8cf0e2cd524eb281f0f0cd8","International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Music Technology Group, Universitat Pompeu Fabra, Spain","Hu X., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Laurier C., Music Technology Group, Universitat Pompeu Fabra, Spain; Bay M., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States","Recent music information retrieval (MIR) research pays increasing attention to music classification based on moods expressed by music pieces. The first Audio Mood Classification (AMC) evaluation task was held in the 2007 running of the Music Information Retrieval Evaluation eXchange (MIREX). This paper describes important issues in setting up the task, including dataset construction and ground-truth labeling, and analyzes human assessments on the audio dataset, as well as system performances from various angles. Interesting findings include system performance differences with regard to mood clusters and the levels of agreement amongst human judgments regarding mood labeling. Based on these analyses, we summarize experiences learned from the first community scale evaluation of the AMC task and propose recommendations for future AMC and similar evaluation tasks.","","Insecticides; Classification tasks; Data sets; Human assessment; Human judgments; Music classification; Music information retrieval; Information retrieval","X. Hu; International Music Information Retrieval System Evaluation Laboratory, University of Illinois, Urbana-Champaign, United States; email: xiaohu@uiuc.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Skalak M.; Han J.; Pardo B.","Skalak, Michael (55582540700); Han, Jinyu (55938782100); Pardo, Bryan (10242155400)","55582540700; 55938782100; 10242155400","Speeding melody search with vantage point trees","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958197457&partnerID=40&md5=297de878f9f4ad8b47853a2e31e5296e","Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States","Skalak M., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States; Han J., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States; Pardo B., Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States","Melodic search engines let people find music in online collections by specifying the desired melody. Comparing the query melody to every item in a large database is prohibitively slow. If melodies can be placed in a metric space, search can be sped by comparing the query to a limited number of vantage melodies, rather than the entire database. We describe a simple melody metric that is customizable using a small number of example queries. This metric allows use of a generalized vantage point tree to organize the database. We show on a standard melodic database that the general vantage tree approach achieves superior search results for query-by-humming compared to an existing vantage point tree method. We then show this method can be used as a preprocessor to speed search for non-metric melodic comparison.","","Database systems; Forestry; Information retrieval; Query processing; Search engines; Customizable; Large database; Metric spaces; Online collection; Search results; Vantage-point trees; Trees (mathematics)","M. Skalak; Electrical Engineering and Computer Science, Ford Engineering Design Center, Northwestern University, Evanston, IL 60208, 2133 Sheridan Road, United States; email: mskalak13@gmail.com","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Cunningham S.J.; Zhang Y.","Cunningham, Sally Jo (7201937110); Zhang, Yiwen (55583701100)","7201937110; 55583701100","Development of a music organizer for children","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873469614&partnerID=40&md5=deb42e772d1000ca1ee5e8bedf123c4b","Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Zhang Y., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","Software development for children is challenging; children have their own needs, which often are not met by 'grown up' software. We focus on software for playing songs and managing a music collection - tasks that children take great interest in, but for which they have few or inappropriate tools. We address this situation with the design of a new music management system, created with children as design partners: the Kids Music Box.","","Software engineering; Management systems; Music boxes; Music collection; Information retrieval","S.J. Cunningham; Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Ogihara M.; Li T.","Ogihara, Mitsunori (54420747900); Li, Tao (55553727585)","54420747900; 55553727585","N-gram chord profiles for composer style representation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416168&partnerID=40&md5=a9a14642a8f4356d684131099872d14e","Department of Computer Science, University of Miami, United States; School of Computer Science, Florida International University, United States","Ogihara M., Department of Computer Science, University of Miami, United States; Li T., School of Computer Science, Florida International University, United States","This paper studies the problem of using weighted N-grams of chord sequences to construct the profile of a composer. The N-gram profile of a chord sequence is the collection of all N-grams appearing in a sequence where each N-gram is given a weight proportional to its beat count. The N-gram profile of a collection of chord sequences is the simple average of the N-gram profile of all the chord sequences in the collection. Similarity of two composers is measured by the cosine of their respective profiles, which has a value in the range [0, 1]. Using the cosine-based similarity, a group of composers is clustered into a hierarchy, which appears to be explicable. Also, the composition style can be identified using N-gram signatures.","","Chord sequence; N-grams; Information retrieval","M. Ogihara; Department of Computer Science, University of Miami, United States; email: ogihara@cs.miami.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Hashida M.; Matsui T.; Katayose H.","Hashida, Mitsuyo (18934839900); Matsui, Toshie (53878021100); Katayose, Haruhiro (8341539100)","18934839900; 53878021100; 8341539100","A new music database describing deviation information of performance expressions","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873455062&partnerID=40&md5=90b597877cd25d78ae00ad383e67339f","Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan","Hashida M., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan; Matsui T., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan; Katayose H., Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan","We introduce the CrestMuse Performance Expression Database (CrestMusePEDB), a music database that describes music performance expression and is available for academic research. While music databases are being provided as MIR technologies continue to progress, few databases deal with performance expression. We constructed a music expression database, CrestMusePEDB. It may be utilized in the research fields of music informatics, music perception and cognition, and musicology. It will contain music expression information on virtuosis' expressive performances, including those of 3 to 10 players at a time, on about 100 pieces of classical Western music. The latest version of the database, CrestMusePEDB Ver. 2.0, is available. The paper gives an overview of CrestMusePEDB.","","Information retrieval; Academic research; Informatics; Music database; Music perception; Music performance; Research fields; Database systems","M. Hashida; Research Center for Human and Media, Kwansei Gakuin University, CrestMuse Project, JST, Japan; email: hashida@kwansei.ac.jp","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Mandel M.I.; Ellis D.P.W.","Mandel, Michael I. (14060460000); Ellis, Daniel P.W. (13609089200)","14060460000; 13609089200","Multiple-instance learning for music information retrieval","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","73","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473825&partnerID=40&md5=c12cb3c41f747ef556bbee2b5fe37a74","LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States","Mandel M.I., LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States","Multiple-instance learning algorithms train classifiers from lightly supervised data, i.e. labeled collections of items, rather than labeled items. We compare the multiple-instance learners mi-SVM and MILES on the task of classifying 10-second song clips. These classifiers are trained on tags at the track, album, and artist levels, or granularities, that have been derived from tags at the clip granularity, allowing us to test the effectiveness of the learners at recovering the clip labeling in the training set and predicting the clip labeling for a held-out test set. We find that mi-SVM is better than a control at the recovery task on training clips, with an average classification accuracy as high as 87% over 43 tags; on test clips, it is comparable to the control with an average classification accuracy of up to 68%. MILES performed adequately on the recovery task, but poorly on the test clips.","","Classification (of information); Learning algorithms; Recovery; Classification accuracy; Multiple-instance learning; Music information retrieval; Test sets; Training sets; Information retrieval","M.I. Mandel; LabROSA, Dept. Elec. Eng., Columbia University, New York, NY, United States; email: mim@ee.columbia.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Symeonidis P.; Ruxanda M.; Nanopoulos A.; Manolopoulos Y.","Symeonidis, Panagiotis (13006105800); Ruxanda, Maria (25423092800); Nanopoulos, Alexandros (6603555418); Manolopoulos, Yannis (54397260800)","13006105800; 25423092800; 6603555418; 54397260800","Ternary semantic analysis of social tags for personalized music recommendation","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","60","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956776436&partnerID=40&md5=10a8a979e95a953ed77e2796f9dd53ce","Department of Informatics, Aristotle Univ. of Thessaloniki, Greece; Department of Computer Science, Aalborg University, Denmark","Symeonidis P., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece; Ruxanda M., Department of Computer Science, Aalborg University, Denmark; Nanopoulos A., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece; Manolopoulos Y., Department of Informatics, Aristotle Univ. of Thessaloniki, Greece","Social tagging is the process by which many users add metadata in the form of keywords, to annotate information items. In case of music, the annotated items can be songs, artists, albums. Current music recommenders which employ social tagging to improve the music recommendation, fail to always provide appropriate item recommendations, because: (i) users may have different interests for a musical item, and (ii) musical items may have multiple facets. In this paper, we propose an approach that tackles the problem of the multimodal use of music. We develop a unified framework, represented by a 3-order tensor, to model altogether users, tags, and items. Then, we recommend musical items according to users multimodal perception of music, by performing latent semantic analysis and dimensionality reduction using the Higher Order Singular Value Decomposition technique. We experimentally evaluate the proposed method against two state-of-the-art recommendations algorithms using real Last.fm data. Our results show significant improvements in terms of effectiveness measured through recall/precision.","","Information retrieval; Metadata; Dimensionality reduction; Higher order singular value decomposition; Information items; Last.fm; Latent Semantic Analysis; Multi-modal; Multimodal perception; Music recommendation; Semantic analysis; Social tagging; Unified framework; Semantics","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Burgoyne J.A.; Devaney J.; Pugin L.; Fujinaga I.","Burgoyne, John Ashley (23007865600); Devaney, Johanna (35766484200); Pugin, Laurent (23009752900); Fujinaga, Ichiro (9038140900)","23007865600; 35766484200; 23009752900; 9038140900","Enhanced bleedthrough correction for early music documents with recto-verso registration","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71249093284&partnerID=40&md5=27ef79d92140b68fec21a56dd3303bd7","Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Devaney J., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada","Ink bleedthrough is common problem in early music documents. Even when such bleedthrough does not pose problems for human perception, it can inhibit the performance of optical music recognition (OMR). One way to reduce the amount of bleedthrough is to take into account what is printed on the reverse of the page. In order to do so, the reverse of the page must be registered to match the front of the page on a pixel-by-pixel basis. This paper describes our approach to registering scanned early music scores as well as our modifications to two robust binarization approaches to take into account bleedthrough and the information available from the registration process. We determined that although the information from registration itself often makes little difference in recognition performance, other modifications to binarization algorithms for correcting bleedthrough can yield dramatic increases in OMR results.","","Information retrieval; Binarization algorithm; Binarizations; Human perception; Music recognition; Recognition performance; Registration process; Pixels","J.A. Burgoyne; Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal (QC), Canada; email: ashley@music.mcgill.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Tindale A.R.; Sprague D.; Tzanetakis G.","Tindale, Adam R. (14058891800); Sprague, David (22958713400); Tzanetakis, George (6602262192)","14058891800; 22958713400; 6602262192","Strike-a-tune: Fuzzy music navigation using a drum interface","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601019&partnerID=40&md5=eb6553a79e79fcb4e0ce57b1af81a6a1","Department of Computer Science, University of Victoria, Canada","Tindale A.R., Department of Computer Science, University of Victoria, Canada; Sprague D., Department of Computer Science, University of Victoria, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, Canada","A traditional music library system controlled by a mouse and keyboard is precise, allowing users to select their desired song. Alternatively, randomized playlist or shuffles are used when users have no particular music in mind. We present a new interface and visualization system called Strike- A-Tune for fuzzy music navigation. Fuzzy navigation is an imprecise navigation approach allowing users to choose preference related items. We believe this will help users to play music they want to hear and re-discover infrequently played songs in their music library, thus combining the best aspects of precision navigation and shuffles. We have designed an interface using an electronic drum to communicate with a visualization and playback system. ©2007 Austrian Computer Society (OCG).","","Electronic musical instruments; Information retrieval; Visualization; Imprecise navigations; Music library; Precision navigation; Visualization system; Navigation","A.R. Tindale; Department of Computer Science, University of Victoria, Canada; email: art@csc.uvic.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Burred J.J.; Sikora T.","Burred, Juan Jośe (16201737900); Sikora, Thomas (36777640600)","16201737900; 36777640600","Monaural source separation from musical mixtures based on time-frequency timbre models","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957855900&partnerID=40&md5=8c6bb8d3d1802617d4dfb24093071fbe","Communication Systems Group, Technical University of Berlin, Germany","Burred J.J., Communication Systems Group, Technical University of Berlin, Germany; Sikora T., Communication Systems Group, Technical University of Berlin, Germany","We present a system for source separation from monaural musical mixtures based on sinusoidal modeling and on a library of timbre models trained a priori. The models, which rely on Principal Component Analysis, serve as time-frequency probabilistic templates of the spectral envelope. They are used to match groups of sinusoidal tracks and assign them to a source, as well as to reconstruct overlapping partials. The proposed method does not make any assumptions on the harmonicity of the sources, and does not require a previous multipitch estimation stage. Since the timbre matching stage detects the instruments present on the mixture, the system can also be used for classification and segmentation. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Principal component analysis; Harmonicity; Multi-pitch estimations; Sinusoidal modeling; Spectral envelopes; Time frequency; Mixtures","J.J. Burred; Communication Systems Group, Technical University of Berlin, Germany; email: burred@nue.tu-berlin.de","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mardirossian A.; Chew E.","Mardirossian, Arpi (15769566400); Chew, Elaine (8706714000)","15769566400; 8706714000","Visualizing music: Tonal progressions and distributions","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867912258&partnerID=40&md5=73507de2963ef3960f6f22508baf5809","Epstein Department of Industrial and Systems Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089, United States","Mardirossian A., Epstein Department of Industrial and Systems Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089, United States; Chew E., Epstein Department of Industrial and Systems Engineering, Viterbi School of Engineering, University of Southern California, Los Angeles, CA 90089, United States","This paper presents a music visualization tool that shows the tonal progression in, and tonal distribution of, a piece of music on Lerdahl's two-dimensional tonal pitch space. The method segments a piece into uniform time slices, and determines the most likely key in each slice. It then generates the visualization by dynamically showing the sequence of keys as translucent, growing discs on the twodimensional plane. The frequency of a key is indicated by the size of its colored disc. Each color and position corresponds to a key, and related keys are shown in proximity with related colors. The visual result effectively presents the changing distribution of the keys employed. The proposed visualization is an improvement over more basic charting methods, such as histograms, and it maintains standards of information design in the form of added dimensionality, color, and animation. We show that the visualization is invariant under music transformations that preserve the piece's identity. We conclude by illustrating how this method may be used to visually distinguish between tonal progression and distribution patterns in western classical versus Armenian folk music. ©2007 Austrian Computer Society (OCG).","","Animation; Color; Information retrieval; Changing distributions; Distribution patterns; Information design; Most likely; Music visualization; Related keys; Time slice; Tonal progression; Two-dimensional planes; Visualization","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Bello J.P.","Bello, Juan Pablo (7102889110)","7102889110","Audio-based cover song retrieval using approximate chord sequences: Testing shifts, gaps, swaps and beats","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","56","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605856&partnerID=40&md5=e8b7dd8c88a88d64eb8ed1622399d57f","New York University, Music Technology, United States","Bello J.P., New York University, Music Technology, United States","This paper presents a variation on the theme of using string alignment for MIR in the context of cover song identification in audio collections. Here, the strings are derived from audio by means of HMM-based chord estimation. The characteristics of the cover-song ID problem and the nature of common chord estimation errors are carefully considered. As a result strategies are proposed and systematically evaluated for key shifting, the cost of gap insertions and character swaps in string alignment, and the use of a beat-synchronous feature set. Results support the view that string alignment, as a mechanism for audiobased retrieval, cannot be oblivious to the problems of robustly estimating musically-meaningful data from audio. ©2007 Austrian Computer Society (OCG).","","Estimation; Information retrieval; Audio-based; Chord sequence; Cover song identifications; Cover songs; Estimation errors; Feature sets; HMM-based; String alignment; Alignment","J.P. Bello; New York University, Music Technology, United States; email: jpbello@nyu.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mesaros A.; Virtanen T.; Klapuri A.","Mesaros, Annamaria (14632356500); Virtanen, Tuomas (35504028500); Klapuri, Anssi (6602945099)","14632356500; 35504028500; 6602945099","Singer identification in polyphonic music using vocal separation and pattern recognition methods","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","75","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582138&partnerID=40&md5=bac17055638184a80b9a5bd4ebbdaa8c","Institute of Signal Processing, Tampere University Technology, Finland","Mesaros A., Institute of Signal Processing, Tampere University Technology, Finland; Virtanen T., Institute of Signal Processing, Tampere University Technology, Finland; Klapuri A., Institute of Signal Processing, Tampere University Technology, Finland","This paper evaluates methods for singer identification in polyphonic music, based on pattern classification together with an algorithm for vocal separation. Classification strategies include the discriminant functions, Gaussian mixture model (GMM)-based maximum likelihood classifier and nearest neighbour classifiers using Kullback-Leibler divergence between the GMMs. A novel method of estimating the symmetric Kullback-Leibler distance between two GMMs is proposed. Two different approaches to singer identification were studied: one where the acoustic features were extracted directly from the polyphonic signal and one where the vocal line was first separated from the mixture using a predominant melody transcription system. The methods are evaluated using a database of songs where the level difference between the singing and the accompaniment varies. It was found that vocal line separation enables robust singer identification down to 0dB and -5dB singer-to- accompaniment ratios. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Pattern recognition; Separation; Acoustic features; Discriminant functions; Gaussian Mixture Model; Kullback Leibler divergence; Kullback-Leibler distance; Level difference; Maximum likelihood classifiers; Nearest-neighbour classifier; Pattern recognition method; Polyphonic music; Polyphonic signals; Computer music","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Izmirli Ö.","Izmirli, Özgür (6507754258)","6507754258","Localized key finding from audio using non-negative matrix factorization for segmentation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857473577&partnerID=40&md5=7dc2f055c761cd498a45f593a5b971f1","Center for Arts and Technology, Computer Science Connecticut College, United States","Izmirli Ö., Center for Arts and Technology, Computer Science Connecticut College, United States","A model for localized key finding from audio is proposed. Besides being able to estimate the key in which a piece starts, the model can also identify points of modulation and label multiple sections with their key names throughout a single piece. The front-end employs an adaptive tuning stage prior to spectral analysis and calculation of chroma features. The segmentation stage uses groups of contiguous chroma vectors as input and identifies sections that are candidates for unique local keys in relation to their neighboring key centers. Non-negative matrix factorization with additional sparsity constraints and additive updates is used for segmentation. The use of segmentation is demonstrated for single and multiple key estimation problems. A correlational model of key finding is applied to the candidate segments to estimate the local keys. Evaluation is given on three different data sets and a range of analysis parameters. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Spectrum analysis; Adaptive tuning; Analysis and calculations; Chroma features; Estimation problem; Nonnegative matrix factorization; Sparsity constraints; Estimation","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Diet J.; Kurth F.","Diet, Jürgen (26430892100); Kurth, Frank (56240850200)","26430892100; 56240850200","The probado music repository at the bavarian state library","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603281&partnerID=40&md5=68f0cf3a9ff13771008e857978db389f","Bavarian State Library, Ludwigstraße 16 80539 Munich, Germany; University of Bonn, Römerstraße 164 53117 Bonn, Germany","Diet J., Bavarian State Library, Ludwigstraße 16 80539 Munich, Germany; Kurth F., University of Bonn, Römerstraße 164 53117 Bonn, Germany","In this paper, we describe the Probado music repository which is currently set up at the Bavarian State Library, Munich, as part of the larger German Probado digital library initiative. Based on the FRBR approach, we propose a novel work-centric metadata model for organizing the document collection. The primary data contained in the repository currently consists of scanned sheet music and digitized audio recordings. The repository can be searched using both classical and content-based retrieval mechanisms. To this end, we propose a workflow for automated content-based document analysis and indexing. ©2007 Austrian Computer Society (OCG).","","Content based retrieval; Digital libraries; Metadata; Bavarians; Content-based; Document analysis; Document collection; Metadata model; Primary data; Information retrieval","J. Diet; Bavarian State Library, Ludwigstraße 16 80539 Munich, Germany; email: juergen.diet@bsb-muenchen.de","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Turnbull D.; Lanckriet G.; Pampalk E.; Goto M.","Turnbull, Douglas (8380095700); Lanckriet, Gert (7801431767); Pampalk, Elias (6507297042); Goto, Masataka (7403505330)","8380095700; 7801431767; 6507297042; 7403505330","A supervised approach for detecting boundaries in music using difference features and boosting","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","56","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873575037&partnerID=40&md5=a1f8f81c34e88f42ca8c7ea9c229529f","Computer Science and Engineering, University of California, San Diego, CA 92093, United States; Electrical and Computer Engineering, University of California, San Diego, CA 92093, United States; National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Ibaraki 305-8568, Japan","Turnbull D., Computer Science and Engineering, University of California, San Diego, CA 92093, United States; Lanckriet G., Electrical and Computer Engineering, University of California, San Diego, CA 92093, United States; Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Ibaraki 305-8568, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST) Tsukuba, Ibaraki 305-8568, Japan","A musical boundary is a transition between two musical segments such as a verse and a chorus. Our goal is to automatically detect musical boundaries using temporallylocal audio features. We develop a set of difference features that indicate when there are changes in perceptual aspects (e.g., timbre, harmony, melody, rhythm) of the music. We show that many individual difference features are useful for detecting boundaries. By combining these features and formulating the problem as a supervised learning problem, we can further improve performance. This is an alternative to previous work on music segmentation which has focused on unsupervised approaches based on notions of self-similarity computed over an entire song. We evaluate performance using a publicly available data set of 100 copyright-cleared pop/rock songs, each of which has been segmented by a human expert. ©2007 Austrian Computer Society (OCG).","","A transitions; Audio features; Data set; Human expert; Improve performance; Individual Differences; Music segmentations; Perceptual aspects; Self-similarities; Supervised learning problems; Unsupervised approaches; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Davis E.","Davis, Elizabeth (57213522436)","57213522436","Finding music in scholarly sets and series: The index to printed music (IPM)","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602231&partnerID=40&md5=9ab649de2b1959612cce2996913f6450","Head of Library, Wiener Music and Arts Library, Columbia University, United States","Davis E., Head of Library, Wiener Music and Arts Library, Columbia University, United States","The Index to Printed Music (IPM) provides access to sets and series of music published beginning in the 19th century. Prepared by scholars and researchers, these titles vary considerably in length (single to multiple volumes), types (topical, pedagogical, historical, etc.), format (treatises, dissertations, editions, etc.), and geographic origin (chiefly Europe and North America). Bibliographical access to their contents is not readily available through library cataloging or reference works. IPM provides title, format, genre, instrumentation, and other metadata access to these publications in three inter-connected databases: Bibliography, Index, and Names, available by subscription through NISC International, Inc. The Index Database now contains over 255,000 entries, the Bibliography Database over 10,000 entries, and the Names Database over 15,000 authority records. Having built this groundwork, future steps involve linking to full-text score images where available through non-commercial projects, and partnerships with publishers and commercial vendors. ©2007 Austrian Computer Society (OCG).","","Bibliographies; Metadata; 19th century; Geographic origins; Index database; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Gatzsche G.; Mehnert M.; Gatzsche D.; Brandenburg K.","Gatzsche, G. (35310025900); Mehnert, M. (17434929600); Gatzsche, D. (55585974300); Brandenburg, K. (7007147554)","35310025900; 17434929600; 55585974300; 7007147554","A symmetry based approach for musical tonality analysis","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873592290&partnerID=40&md5=e4ef60d7756a886006254db815c3f6c6","Fraunhofer IDMT, Ilmenau, Germany; Technische Universität, Ilmenau, Germany; Hochschule für Musik, Franz Liszt Weimar, Germany","Gatzsche G., Fraunhofer IDMT, Ilmenau, Germany; Mehnert M., Technische Universität, Ilmenau, Germany; Gatzsche D., Hochschule für Musik, Franz Liszt Weimar, Germany; Brandenburg K., Technische Universität, Ilmenau, Germany","We present a geometric approach for tonality analysis called symmetry model. To derive the symmetry model, Carol L.Krumhansl and E.J. Kessler's toroidal Multi Dimensional Scaling (MDS) solution is separated into a key spanning and a key related component. While the key spanning component represents relationships between different keys, the key related component is suitable for the analysis of inner relationships of diatonic keys, for example tension or resolution tendencies, or functional relationships. These features are directly related to the symmetric organisation of tones around the tonal center, which is particularly visualized by the key related component. ©2007 Austrian Computer Society (OCG).","","Functional relationship; Geometric approaches; Multi-dimensional scaling; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Silla Jr. C.N.; Koerich A.L.; Kaestner C.A.A.","Silla Jr., Carlos N. (8707999600); Koerich, Alessandro L. (6602596292); Kaestner, Celso A.A. (56121741400)","8707999600; 6602596292; 56121741400","The latin music database","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","88","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949101023&partnerID=40&md5=063fd01af0aae0a1e743789bb549967b","University of Kent, Computing Laboratory, United Kingdom; Pontifical Catholic, University of Paranä, Brazil; Federal University of Technology of Paranä, Brazil","Silla Jr. C.N., University of Kent, Computing Laboratory, United Kingdom; Koerich A.L., Pontifical Catholic, University of Paranä, Brazil; Kaestner C.A.A., Federal University of Technology of Paranä, Brazil","In this paper we present the Latin Music Database, a novel database of Latin musical recordings which has been developed for automatic music genre classification, but can also be used in other music information retrieval tasks. The method for assigning genres to the musical recordings is based on human expert perception and therefore capture their tacit knowledge in the genre labeling process. We also present the ethnomusicology of the genres available in the database as it might provide important information for the analysis of the results of any experiment that employs the database.","","Database systems; Information retrieval; Human expert; Music database; Music genre classification; Music information retrieval; Tacit knowledge; Audio recordings","C.N. Silla Jr.; University of Kent, Computing Laboratory, United Kingdom; email: cns2@kent.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Geleijnse G.; Korst J.","Geleijnse, Gijs (24174165400); Korst, Jan (7004696252)","24174165400; 7004696252","Tool play live: Dealing with ambiguity in artist similarity mining from the web","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596773&partnerID=40&md5=c2503c3054f8c830ccd4b73fa87c7610","Philips Research, Eindhoven, Netherlands","Geleijnse G., Philips Research, Eindhoven, Netherlands; Korst J., Philips Research, Eindhoven, Netherlands","As methods in artist similarity identification using Web Music Information Retrieval perform well on known evaluation sets, we investigate the application of such a method to a more realistic data set. We notice that ambiguous artist names lead to unsatisfying results. We present a simple, efficient and unsupervised method to deal with ambiguous artist names. ©2007 Austrian Computer Society (OCG).","","Artist similarities; Data set; Music information retrieval; Unsupervised method; Information retrieval","G. Geleijnse; Philips Research, Eindhoven, Netherlands; email: fgijs.geleijnse@philips.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Miotto R.; Orio N.","Miotto, Riccardo (57202034402); Orio, Nicola (6507928255)","57202034402; 6507928255","A methodology for the segmentation and identification of music works","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580062&partnerID=40&md5=7a968e56a6a12e51d3263ef7afc4be83","Department of Information Engineering, University of Padova, Italy","Miotto R., Department of Information Engineering, University of Padova, Italy; Orio N., Department of Information Engineering, University of Padova, Italy","The identification of unknown recordings is a challenging problem that has several applications. In this paper, we focus on the identification of alternative releases of a given music work. To this end, a statistical model of the possible performances of a given score is built from the recording of a single performance. The methodology is based on the automatic segmentation of audio recordings, exploiting a technique that has been proposed for text segmentation. The segmentation is followed by the automatic extraction of a set of relevant audio features from each segment. Identification is then carried out using an application of hidden Markov models. The approach has been tested with a collection of orchestral music, showing good results in the identification of acoustic performances. ©2007 Austrian Computer Society (OCG).","","Hidden Markov models; Information retrieval; Acoustic performance; Audio features; Automatic extraction; Automatic segmentations; Identification of unknowns; Statistical models; Text segmentation; Audio recordings","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Han Y.; Raphael C.","Han, Yushen (24477451100); Raphael, Christopher (7004214964)","24477451100; 7004214964","Desoloing monaural audio using mixture models","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873588067&partnerID=40&md5=547d994ea623ec5546270ae3a205fb90","School of Informaitcs, Indiana Univ., United States","Han Y., School of Informaitcs, Indiana Univ., United States; Raphael C., School of Informaitcs, Indiana Univ., United States","We describe a new approach to the ""desoloing"" problem, in which one tries to isolate the accompanying instruments from a monaural recording of a soloist with accompaniment. Our approach is based on explicit knowledge of the audio in the form of a score match - A correspondence between a symbolic score and the music audio, giving the times of all musical events. We employ the familiar idea of masking the short time Fourier transform to eliminate the solo part. The ideal mask is estimated by fitting a model to the data, whose note-based components are derived from the score match. The parameters for our probabilistic model are estimated using the EM algorithm. ©2007 Austrian Computer Society (OCG).","","Algorithms; Information retrieval; EM algorithms; Explicit knowledge; Mixture model; Musical events; New approaches; Probabilistic models; Short time Fourier transforms; Audio acoustics","Y. Han; School of Informaitcs, Indiana Univ., United States; email: yushan@indiana.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Jensen J.H.; Ellis D.P.W.; Christensen M.G.; Jensen S.H.","Jensen, Jesper Højvang (57199943098); Ellis, Daniel P.W. (13609089200); Christensen, Mads G. (8507457400); Jensen, Søren Holdt (9279416600)","57199943098; 13609089200; 8507457400; 9279416600","Evaluation of distance measures between gaussian mixture models of MFCCS","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","47","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606815&partnerID=40&md5=9798d4ffeefb2e10465732a57fcf8935","Dept. Electron. Syst., Aalborg University, Denmark; LabROSA, Columbia University, United States","Jensen J.H., Dept. Electron. Syst., Aalborg University, Denmark; Ellis D.P.W., LabROSA, Columbia University, United States; Christensen M.G., Dept. Electron. Syst., Aalborg University, Denmark; Jensen S.H., Dept. Electron. Syst., Aalborg University, Denmark","In music similarity and in the related task of genre classification, a distance measure between Gaussian mixture models is frequently needed. We present a comparison of the Kullback-Leibler distance, the earth movers distance and the normalized L2 distance for this application. Although the normalized L2 distance was slightly inferior to the Kullback-Leibler distance with respect to classification performance, it has the advantage of obeying the triangle inequality, which allows for efficient searching. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Classification performance; Distance measure; Earth Movers Distance; Gaussian Mixture Model; Genre classification; Kullback-Leibler distance; L2 distances; Music similarity; Triangle inequality; Filter banks","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Leveau P.; Sodoyer D.; Daudet L.","Leveau, Pierre (24724767600); Sodoyer, David (6506991707); Daudet, Laurent (6602438418)","24724767600; 6506991707; 6602438418","Automatic instrument recognition in a polyphonic mixture using sparse representations","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605208&partnerID=40&md5=7d1b9b2ded599c0a5bee55f4872bac96","GET-ENST (Télécom Paris), 75014 Paris, 46, rue Dareau, France; Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France","Leveau P., GET-ENST (Télécom Paris), 75014 Paris, 46, rue Dareau, France, Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France; Sodoyer D., Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France; Daudet L., Institut Jean Le Rond D'Alembert, University Pierre et Marie Curie, 75015 Paris, 11, rue de Lourmel, France","In this paper, we introduce a method to address automatic instrument recognition in polyphonic music. It is based on the decomposition of the music signal with instrumentspecific harmonic atoms, yielding an approximate object representation of the signal. A post-processing is then applied to exhibit ensemble saliences that give clues about the number of instruments and their labels. The whole algorithm is then applied on artificial mixes of solo performances. The identification of the number of instrument reaches 73 % on 10-s segments and the fully blind problem of identification of the ensemble label without prior knowledge on the number of instruments is 17 %. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Instrument recognition; Music signals; Object representations; Polyphonic music; Post processing; Prior knowledge; Sparse representation; Instruments","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Allan H.; Ullensiefen D.M̈.; Geraintwiggins","Allan, Hamish (55586735900); Ullensiefen, Daniel M̈ (23019169400); Geraintwiggins (55586560700)","55586735900; 23019169400; 55586560700","Methodological considerations in studies of musical similarity","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873603426&partnerID=40&md5=108f814ec820182b87e44135d6e3e231","University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom","Allan H., University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom; Ullensiefen D.M̈., University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom; Geraintwiggins, University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom","There are many different aspects of musical similarity [7]. Some relate to acoustic properties, such as melodic [5], rhythmic [10], harmonic [9] and timbral [2]. Others are bound up in cultural aspects: artists involved in creation, year of first release, subject matter of lyrics, demographics of listeners, etc. In judgments about musical similarity, the relative importance of each of these aspects will change, not only for different listeners, but also for the same listener in different contexts [11]. Extra care must therefore be taken when designing studies in musical similarity to ensure that the context is an explicit variable. This paper describes the methodology behind our work in context-based musical similarity; introduces a novel system through which users can specify by example the context and focus of their retrieval needs; and details the design of a study to find parameters for our system which can also be adapted to test the system as a whole. ©2007 Austrian Computer Society (OCG).","","Acoustic properties; Context-based; Cultural aspects; Musical similarity; Subject matters; Information retrieval","H. Allan; University of London, Goldsmiths, London SE14 6NW, New Cross, United Kingdom; email: hamish.allan@gold.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Weyde T.; Wissmann J.; Neubarth K.","Weyde, Tillman (24476899500); Wissmann, Jens (23399186800); Neubarth, Kerstin (25925322600)","24476899500; 23399186800; 25925322600","An experiment on the role of pitch intervals in melodic segmentation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585075&partnerID=40&md5=ea597e4afbf613a52fb1735ef98310eb","Department of Computing, City University London, United Kingdom","Weyde T., Department of Computing, City University London, United Kingdom; Wissmann J., Department of Computing, City University London, United Kingdom; Neubarth K., Department of Computing, City University London, United Kingdom","This paper presents the results of an experiment to test the influence of IOI, dynamics, pitch change, and pitch direction change on melodic segmentation, extending an an earlier experiment [4]. The new results show little to no significant influence of pitch, when evaluated by a linear or log-linear statistical model with regression. This supports the earlier findings, which are in contrast to the commonly made assumption that greater pitch intervals lead to melodic segmentation. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Direction change; New results; Pitch changes; Statistical models; Experiments","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Nichols E.; Raphael C.","Nichols, Eric (57196921182); Raphael, Christopher (7004214964)","57196921182; 7004214964","Automatic transcription of music audio through continuous parameter tracking","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605686&partnerID=40&md5=88f7b0c467e5eae6cf4de6bc526ce8e9","Dept. of Computer Science, Indiana Univ., United States; School of Informatics, Indiana Univ., United States","Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a method for transcribing arbitrary pitched music into a piano-roll-like representation that also tracks the amplitudes of the notes over time. We develop a probabilistic model that gives the likelihood of a frame of audio data given a vector of amplitudes for the possible notes. Using an approximation of the log likelihood function, we develop an objective function that is quadratic in the timevarying amplitude variables, while also depending on the discrete piano-roll variables. We optimize this function using a variant of dynamic programming, by repeatedly growing and pruning our histories. We present results on a variety of different examples using several measures of performance including an edit-distance measure as well as a frame-by-frame measure. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio data; Automatic transcription; Continuous parameters; Edit distance; Growing and pruning; Log-likelihood functions; Measures of performance; Objective functions; Probabilistic models; Time varying; Transcription","E. Nichols; Dept. of Computer Science, Indiana Univ., United States; email: epnichol@indiana.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pugin L.; Burgoyne J.A.; Fujinaga I.","Pugin, Laurent (23009752900); Burgoyne, John Ashley (23007865600); Fujinaga, Ichiro (9038140900)","23009752900; 23007865600; 9038140900","Map adaptation to improve optical music recognition of early music documents using hidden markov models","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861078564&partnerID=40&md5=c45c6bc6c17133f6d01521825277a083","Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Pugin L., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Burgoyne J.A., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Despite steady improvement in optical music recognition (OMR), early documents remain challenging because of the high variability in their contents. In this paper, we present an original approach using maximum a posteriori (MAP) adaptation to improve an OMR tool for early typographic prints dynamically based on hidden Markov models. Taking advantage of the fact that during the normal usage of any OMR tool, errors will be corrected, and thus ground-truth produced, the system can be adapted in real-time. We experimented with five 16th-century music prints using 250 pages of music and two procedures in applying MAP adaptation. With only a handful of pages, both recall and precision rates improved even when the baseline was above 95 percent. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Early musics; High variability; MAP adaptation; Maximum a posteriori; Optical music recognition; Recall and precision; Hidden Markov models","L. Pugin; Centre for Interdisciplinary Research in Music Media and Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; email: laurent@music.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Knees P.","Knees, Peter (8219023200)","8219023200","Search & select - Intuitively retrieving music from large collections","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873571882&partnerID=40&md5=53eaca6291fbfb48c58553bdb29e9577","Department of Computational Perception, Johannes Kepler University, Linz, Austria","Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria","A retrieval system for large-scale collections that allows users to search for music using natural language queries and relevance feedback is presented. In contrast to existing music search engines that are either restricted to manually annotated meta-data or based on a query-by-example variant, the presented approach describes audio pieces via a traditional term vector model and allows therefore to retrieve relevant music pieces by issuing simple free-form text queries. Term vector descriptors for music pieces are derived by applying Web-based and audio-based similarity measures. Additionally, as the user selects music pieces that he/she likes, the subsequent results are adapted to accommodate to the user's preferences. Real-world performance of the system is indicated by a small user study. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Search engines; Audio-based; Descriptors; Natural language queries; Query-by-example; Real-world performance; Relevance feedback; Retrieval systems; Similarity measure; Text query; User study; User's preferences; Vector models; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Ahmaniemi T.","Ahmaniemi, Teemu (26424920100)","26424920100","Influence of tempo and subjective rating of music in step frequency of running","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578675&partnerID=40&md5=9cac8006122267585224a9ff673bf765","Nokia Research Center, 00180 Helsinki, Itamerenkatu 11-13, Finland","Ahmaniemi T., Nokia Research Center, 00180 Helsinki, Itamerenkatu 11-13, Finland","The objective of this work was to study how the tempo and subjective motivational rating of personal music influence in step frequency during an exercise session. The participants (n=8) were requested to bring their own music to the test and rate it according to the motivational effect of each song. The test was conducted on a sports field where the participants were asked to perform a 30 minute exercise without paying attention to the test setup. Significant correlation was found between the subjective motivational rating of music and step frequency, while tempo did not have any influence. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Step frequency; Subjective rating; Test setups; Rating","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Teodoru G.; Raphael C.","Teodoru, Gabi (55028245200); Raphael, Christopher (7004214964)","55028245200; 7004214964","Pitch spelling with conditionally independent voices","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956595929&partnerID=40&md5=8ef788890efced984503821f1570e799","School of Informatics, Indiana University, United States","Teodoru G., School of Informatics, Indiana University, United States; Raphael C., School of Informatics, Indiana University, United States","We introduce a new approach for pitch spelling from MIDI data based on a probabilistic model. The model uses a hidden sequence of variables, one for each measure, describing the local key of the music. The spellings in the voices evolve as conditionally independent Markov chains, given the hidden keys. The model represents both vertical relations through the shared key and horizontal voice-leading relations through the explicit Markov models for the voices. This conditionally independent voice model leads to an efficient dynamic programming algorithm for finding the most likely configuration of hidden variables . spellings and harmonic sequence. The model is also straightforward to train from unlabeled data, though we have not been able to demonstrate any improvement in performance due to training. Our results compare favorably with others when tested on Meredith's corpus, designed specifically for this problem. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Dynamic programming algorithm; Hidden key; Hidden variable; Markov model; Model use; Most likely; New approaches; Pitch spelling; Probabilistic models; Unlabeled data; Voice model; Markov processes","G. Teodoru; School of Informatics, Indiana University, United States; email: ateodoru@indiana.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Deliège F.; Pedersen T.B.","Deliège, François (23476708400); Pedersen, Torben Bach (7202189773)","23476708400; 7202189773","Fuzzy song sets for music warehouses","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599356&partnerID=40&md5=b8854ff45882b4708f82873ded8d9501","Department of Computer Science, Aalborg University, Denmark","Deliège F., Department of Computer Science, Aalborg University, Denmark; Pedersen T.B., Department of Computer Science, Aalborg University, Denmark","The emergence of music recommendation systems calls for the development of new data management technologies able to query vast music collections. In this paper, we define fuzzy song sets and an algebra to manipulate them. We present a music warehouse prototype able to perform efficient nearest neighbor searches in an arbitrary song similarity space. Using fuzzy song sets, the music warehouse offers a practical solution to the all musical data management scenarios provided: song comparisons, user musical preferences and user feedback. We investigate three practical approaches to tackle the storage issues of fuzzy song sets: Tables, arrays and bitmaps. Finally, we confront theoretical estimates to concrete implementation results and prove that, from a storage perspective, arrays and bitmaps are both effective data structure solutions. ©2007 Austrian Computer Society (OCG).","","Data structures; Information management; Information retrieval; Bit maps; Management technologies; Music collection; Music Recommendation System; Musical data management; Nearest Neighbor search; Practical solutions; Similarity spaces; Structure solutions; User feedback; Warehouses","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Fujihara H.; Goto M.","Fujihara, Hiromasa (16068753300); Goto, Masataka (7403505330)","16068753300; 7403505330","A music information retrieval system based on singing voice timbre","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585146&partnerID=40&md5=7a15c1c15043fc7bd960bddafa81cd61","National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","Fujihara H., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","We developed a music information retrieval system based on singing voice timbre, i.e., a system that can search for songs in a database that have similar vocal timbres. To achieve this, we developed a method for extracting feature vectors that represent characteristics of singing voices and calculating the vocal-timbre similarity between two songs by using a mutual information content of their feature vectors. We operated the system using 75 songs and confirmed that the system worked appropriately. According to the results of a subjective experiment, 80% of subjects judged that compared with a conventional method using MFCC, our method finds more appropriate songs that have similar vocal timbres. ©2007 Austrian Computer Society (OCG).","","Conventional methods; Extracting features; Feature vectors; Music information retrieval; Mutual informations; Singing voices; Subjective experiments","H. Fujihara; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; email: h.fujihara@aist.go.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Raczyński S.A.; Ono N.; Sagayama S.","Raczyński, Stanisław A. (57190605458); Ono, Nobutaka (7202472899); Sagayama, Shigeki (7004859104)","57190605458; 7202472899; 7004859104","Multipitch analysis with harmonic nonnegative matrix approximation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","75","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578051&partnerID=40&md5=d39561e3570d5a188cc62e8acfe3f704","Graduate School of Information Science and Engineering, University of Tokyo, Japan","Raczyński S.A., Graduate School of Information Science and Engineering, University of Tokyo, Japan; Ono N., Graduate School of Information Science and Engineering, University of Tokyo, Japan; Sagayama S., Graduate School of Information Science and Engineering, University of Tokyo, Japan","This paper presents a new approach to multipitch analysis by utilizing the Harmonic Nonnegative Matrix Approximation, a harmonically-constrained and penalized version of the Nonnegative Matrix Approximation (NNMA) method. It also includes a description of a note onset, offset and amplitude retrieval procedure based on that technique. Compared with the previous NNMA approaches, specific initialization of the basis matrix is employed - the basis matrix is initialized with zeros everywhere but at positions corresponding to harmonic frequencies of consequent notes of the equal temperament scale. This results in the basis containing nothing but harmonically structured vectors, even after the learning process, and the activity matrix's rows containing peaks corresponding to note onset times and amplitudes. Furthermore, additional penalties of mutual uncorrelation and sparseness of rows are placed upon the activity matrix. The proposed method is able to uncover the underlying musical structure better than the previous NNMA approaches and makes the note detection process very straightforward. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Amplitude retrievals; Basis matrix; Detection process; Harmonic frequency; Learning process; Multi pitches; Musical structures; New approaches; Non-negative matrix approximations; Harmonic analysis","S.A. Raczyński; Graduate School of Information Science and Engineering, University of Tokyo, Japan; email: raczynski@hil.t.u-tokyo.ac.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lee J.H.; Downie J.S.; Jones M.C.","Lee, Jin Ha (57190797465); Downie, J. Stephen (7102932568); Jones, M. Cameron (57199028615)","57190797465; 7102932568; 57199028615","Preliminary analyses of information features provided by users for identifying music","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445080&partnerID=40&md5=08357cab9974fd27f00aa16ae23c8b1c","Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Lee J.H., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper presents preliminary findings based on the analyses of user-provided information features found in 566 queries seeking help in the identification of particular music works or artists. Queries were drawn from the answers.google.com (Google Answers) website. The types and frequency of occurrences of different information features are compared with the results from previous studies of music queries. New feature types have also been developed to obtain a more comprehensive understanding of the kinds of information present in queries including such things as indications of uncertainty, associated use, and the ""aboutness"" of the underlying musical work. The presence of erroneous information in the queries is also discussed. ©2007 Austrian Computer Society (OCG).","","Feature types; Information feature; Preliminary analysis; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Ehmann A.F.; Downie J.S.; Jones M.C.","Ehmann, Andreas F. (8988651500); Downie, J. Stephen (7102932568); Jones, M. Cameron (57199028615)","8988651500; 7102932568; 57199028615","The music information retrieval evaluation exchange ""Do-it- yourself"" web service","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595747&partnerID=40&md5=99620d17e4252d7d35326b8d46d05e6b","International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","The Do-It-Yourself (DIY) web service of the Music Information Retrieval Evaluation eXchange (MIREX) represents a means by which researchers can remotely submit, execute, and evaluate their Music Information Retrieval (MIR) algorithms against standardized datasets that are not otherwise freely distributable. Since its inception in 2005 at the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL), MIREX has, to date, required heavy interaction by IMIRSEL team members in the execution, debugging, and validation of submitted code. The goal of the MIREX DIY web service is to put such responsibilities squarely into the hands of submitters, and also enable the evaluations of algorithms yearround, as opposed to annual exchanges. ©2007 Austrian Computer Society (OCG).","","Algorithms; Data processing; Information retrieval systems; Insecticides; Web services; Music information retrieval; Team members; Websites","A.F. Ehmann; International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; email: aehmann@uiuc.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Balkema W.","Balkema, Wietse (35723785300)","35723785300","Variable-size gaussian mixture models for music similarity measures","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605158&partnerID=40&md5=f67acfb91951359124ea1ea1eda100cb","Robert Bosch GmbH - Corporate Research, Hildesheim, Po. Box 77 77 77 31137, Germany","Balkema W., Robert Bosch GmbH - Corporate Research, Hildesheim, Po. Box 77 77 77 31137, Germany","An algorithm to efficiently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efficient and prevents themodel from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classification and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classification precision and is approximately 40% less computationally expensive. ©2007 Austrian Computer Society (OCG).","","Cluster analysis; Communication channels (information theory); Information retrieval; Object recognition; Clustering methods; Computationally efficient; Gaussian Mixture Model; Genre classification; Iterative procedures; Mixture model; Music similarity; Number of components; Optimal model; Strong correlation; Variable sizes; Iterative methods","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Sapp C.S.","Sapp, Craig Stuart (14032002500)","14032002500","Comparative analysis of multiple musical performances","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","65","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584053&partnerID=40&md5=095714ef953e4399a2269c5bcb9151b5","Centre for History and Analysis of Recorded Music (CHARM), University of London Royal Holloway, United Kingdom","Sapp C.S., Centre for History and Analysis of Recorded Music (CHARM), University of London Royal Holloway, United Kingdom","A technique for comparing numerous performances of an identical selection of music is described. The basic methodology is to split a one-dimensional sequence into all possible sequential sub-sequences, perform some operation on these sequences, and then display a summary of the results as a two-dimensional plot; the horizontal axis being time and the vertical axis being sub-sequence length (longer lengths on top by convention). Most types of timewise data extracted from performances can be compared with this technique, although the current focus is on beat-level information for tempo and dynamics as well as commixtures of the two. The primary operation used on each sub-sequence is correlation between a reference performance and analogous segments of other performances, then selecting the best correlated performances for the summary display. The result is a useful navigational aid for coping with large numbers of performances of the same piece of music and for searching for possible influence between performances. ©2007 Austrian Computer Society (OCG).","","Comparative analysis; Horizontal axis; Musical performance; Vertical axis; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Aucouturier J.-J.; Pachet F.; Roy P.; Beuriv́e A.","Aucouturier, Jean-Julien (9943558100); Pachet, François (6701441655); Roy, Pierre (55506419000); Beuriv́e, Anthony (14032993800)","9943558100; 6701441655; 55506419000; 14032993800","Signal + Context = Better classification","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873604587&partnerID=40&md5=05371a7e8d68500300626473d38d5dc4","Grad. School of Arts and Sciences, University of Tokyo, Japan; SONY CSL Paris, 75005 Paris, 6 rue Amyot, France","Aucouturier J.-J., Grad. School of Arts and Sciences, University of Tokyo, Japan; Pachet F., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France; Roy P., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France; Beuriv́e A., SONY CSL Paris, 75005 Paris, 6 rue Amyot, France","Typical signal-based approaches to extract musical descriptions from audio only have limited precision. A possible explanation is that they do not exploit context, which provides important cues in human cognitive processing of music: e.g. electric guitar is unlikely in 1930s music, children choirs rarely perform heavy metal, etc. We propose an architecture to train a large set of binary classifiers simultaneously, formany differentmusicalmetadata (genre, instrument, mood, etc.), in such a way that correlation between metadata is used to reinforce each individual classifier. The system is iterative: it uses classification decisions it made on some classification problems as new features for new, harder problems; and hybrid: it uses a signal classifier based on timbre similarity to bootstrap symbolic inference with decision trees. While further work is needed, the approach seems to outperform signal-only algorithms by 5% precision on average, and sometimes up to 15% for traditionally difficult problems such as cultural and subjective categories. ©2007 Austrian Computer Society (OCG).","","Decision trees; Heavy metals; Information retrieval; Iterative methods; Metadata; Binary classifiers; Classification decision; Cognitive processing; Electric guitar; Further works; Individual classifiers; Timbre similarities; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Little D.; Raffensperger D.; Pardo B.","Little, David (57197561056); Raffensperger, David (23968223000); Pardo, Bryan (10242155400)","57197561056; 23968223000; 10242155400","A query by humming system that learns from experience","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605798&partnerID=40&md5=b014d941f1ab66ed1373c6a07bed36ce","EECS Department Northwestern, University Evanston, IL 60201, United States","Little D., EECS Department Northwestern, University Evanston, IL 60201, United States; Raffensperger D., EECS Department Northwestern, University Evanston, IL 60201, United States; Pardo B., EECS Department Northwestern, University Evanston, IL 60201, United States","Query-by-Humming (QBH) systems transcribe a sung or hummed query and search for related musical themes in a database, returning the most similar themes. Since it is not possible to predict all individual singer profiles before system deployment, a robust QBH system should be able to adapt to different singers after deployment. Currently deployed systems do not have this capability. We describe a new QBH system that learns from user provided feedback on the search results, letting the system improve while deployed, after only a few queries. This is made possible by a trainable note segmentation system, an easily parameterized singer error model and a straight-forward genetic algorithm. Results show significant improvement in performance given only ten example queries from a particular user. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Deployed systems; Error model; Note segmentation; Parameterized; Query and search; Query by humming systems; Query-by-humming; Search results; System deployment; Query processing","D. Little; EECS Department Northwestern, University Evanston, IL 60201, United States; email: d-little@northwestern.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Wei B.; Zhang C.; Ogihara M.","Wei, Bin (55586064100); Zhang, Chengliang (55703982000); Ogihara, Mitsunori (54420747900)","55586064100; 55703982000; 54420747900","Keyword generation for lyrics","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439114&partnerID=40&md5=7ef255fa08a21afa5fff891b2633103a","Comp. Sci. Dept., U. Rochester, United States","Wei B., Comp. Sci. Dept., U. Rochester, United States; Zhang C., Comp. Sci. Dept., U. Rochester, United States; Ogihara M., Comp. Sci. Dept., U. Rochester, United States","This paper proposes a scheme for content based keyword generation of song lyrics. Syntactic as well semantic similarity is used for sentence level clustering to separate the topic from the background of a song. A method is proposed to search for a center in the semantic graph ofWord- Net for generating keywords not contained in original text. ©2007 Austrian Computer Society (OCG).","","Semantics; A-center; Content-based; Semantic graphs; Semantic similarity; Sentence level; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Moreau A.; Flexer A.","Moreau, Arnaud (55586128300); Flexer, Arthur (7004555682)","55586128300; 7004555682","Drum transcription in polyphonic music using non-negative matrix factorisation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607102&partnerID=40&md5=7052a7ac8e3abbc17e81362e3d24ebba","Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Center for Brain Research Medical, Institute of Medical Cybernetics and Artificial Intelligence, University of Vienna, Austria","Moreau A., Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Flexer A., Center for Brain Research Medical, Institute of Medical Cybernetics and Artificial Intelligence, University of Vienna, Austria","We present a system that is based on the non-negative matrix factorisation (NMF) algorithm and is able to transcribe drum onset events in polyphonic music. The magnitude spectrogram representation of the input music is divided by the NMF algorithm into source spectra and corresponding time-varying gains. Each of these source components is classified as a drum instrument or non-drum sound and a peak-picking algorithm determines the onset times. ©2007 Austrian Computer Society (OCG).","","Algorithms; Information retrieval; Transcription; Non-negative matrix factorisation; Peak picking; Polyphonic music; Spectrograms; Time-varying gains; Factorization","A. Moreau; Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; email: a.moreau@gmx.net","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Seyerlehner K.; Widmer G.; Schnitzer D.","Seyerlehner, Klaus (23996001400); Widmer, Gerhard (7004342843); Schnitzer, Dominik (23996271700)","23996001400; 7004342843; 23996271700","From rhythm patterns to perceived tempo","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873574366&partnerID=40&md5=7a38c57ce68bce92009864458de6be67","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Schnitzer D., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","There are many MIR applications for which we would like to be able to determine the perceived tempo of a song automatically. However, automatic tempo extraction itself is still an open problem. In general there are two tempo extraction methods, either based on the estimation of interonset intervals or based on self similarity computations. To predict a tempo the most significant time-lag or the most significant inter-onset-interval is used. We propose to use existing rhythm patterns and reformulate the tempo extraction problem in terms of a nearest neighbor classification problem. Our experiments, based on three different datasets, show that this novel approach performs at least comparably to state-of-the-art tempo extraction algorithms and could be useful to get a deeper insight into the relation between perceived tempo and rhythm patterns. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Extraction algorithms; Extraction method; Nearest neighbor classification; Self-similarities; Time lag; Extraction","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lanzelotte R.S.G.; Ballesté A.O.; Ulhoa M.","Lanzelotte, Rosana S.G. (6505944118); Ballesté, Adriana O. (55585888000); Ulhoa, Martha (55586714000)","6505944118; 55585888000; 55586714000","A digital collection of brazilian lundus","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649402017&partnerID=40&md5=8281827d07dda7c11dcd561df21a457a","Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil; LNCC - Laboratório Nacional de Computação Científica, Rio de Janeiro, Brazil","Lanzelotte R.S.G., Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil; Ballesté A.O., LNCC - Laboratório Nacional de Computação Científica, Rio de Janeiro, Brazil; Ulhoa M., Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil","Lundu is a typical Brazilian popular musical form at the 19th century. The distinguished musicologist Mozart de Araújo devoted himself to studying lundus and other forms of that period. He collected 48 lundus, which are nowadays stored in a private library, unavailable to public access. The present work describes the implementation of a digital collection of those lundus, using Dspace as the repository. Dspace is chosen in order to guarantee interoperability through the OAIPMH protocol. Metadata is generated using Dublin Core elements, fully compatible with Dspace. The digital collection provides access to the lundu score images, incipits and midi files, as well as metadata. It is the first time such a rare collection of 19th Brazilian popular music will be available on the web. As Dspace enables interoperation among repositories, a broad community may access the collection. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Internet protocols; Metadata; 19th century; D-space; Digital collections; Dublin Core; Fully compatible; Interoperations; MIDI files; OAI-PMH; Popular music; Public Access; Electronic document exchange","R.S.G. Lanzelotte; Unirio - Universidade Federal do Estado do Rio de Janeiro, Graduate Music Department, Rio de Janeiro, Brazil; email: rosana@unirio.br","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Marsden A.","Marsden, Alan (15054644600)","15054644600","Automatic derivation of musical structure: A tool for research on schenkerian analysis","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249245688&partnerID=40&md5=366f4bc9f1424a545025bfe7be8bf854","Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","Marsden A., Lancaster Institute for Contemporary Arts, Lancaster University, United Kingdom","This paper describes software to facilitate research on the automatic derivation of hierarchical (Schenkerian) musical structures from a musical surface. Many MIR tasks require information about musical structure, or would perform better if such information were available. Automatic derivation of musical structure faces two significant obstacles. Firstly, the solution space of possible structural analyses of a piece is very large. Secondly, pieces can have more than one valid structural analysis, and there is little firm agreement among music theorists about how to distinguish a good analysis. To circumvent the first of these obstacles, software has been developed which derives a tractable 'matrix' of possibilities from a musical surface (i.e., MIDI-like note-time information). The matrix is somewhat like the intermediate results of a dynamic-programming algorithm, and in a similar way it is possible to extract a particular structural analysis from the matrix by following the appropriate path from the top level to the surface. It therefore provides a tool to facilitate research on the second obstacle by allowing candidate 'goodness' metrics to be incorporated into the software and tested on actual music. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Research; Software testing; Structural analysis; Automatic derivation; Intermediate results; Musical structures; Solution space; Musical instruments","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lartillot O.; Toiviainen P.","Lartillot, Olivier (6507137446); Toiviainen, Petri (6602829513)","6507137446; 6602829513","Mir in matlab (II): A toolbox for musical feature extraction from audio","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","190","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572465&partnerID=40&md5=89ea118d8500d0103149df6f975a181b","University of Jyv̈askyl̈a, PL 35(M) 40014, Finland","Lartillot O., University of Jyv̈askyl̈a, PL 35(M) 40014, Finland; Toiviainen P., University of Jyv̈askyl̈a, PL 35(M) 40014, Finland","We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio files. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches - including new strategies we have developed -, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio file, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio files, batches of files, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture. ©2007 Austrian Computer Society (OCG).","","Feature extraction; Information retrieval; Syntactics; Adaptiveness; Alternative approach; Audio files; Feature extraction methods; Input type; Modular framework; Multi-channel; Musical features; Object-oriented architectures; Segmentation and clustering; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mandel M.I.; Ellis D.P.W.","Mandel, Michael I. (14060460000); Ellis, Daniel P.W. (13609089200)","14060460000; 13609089200","A web-based game for collecting music metadata","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873594016&partnerID=40&md5=9ff6c3f605cda20a75b96bf5cef46f65","Dept. Electrical Engineering, LabROSA, Columbia University, United States","Mandel M.I., Dept. Electrical Engineering, LabROSA, Columbia University, United States; Ellis D.P.W., Dept. Electrical Engineering, LabROSA, Columbia University, United States","We have designed a web-based game to make collecting descriptions of musical excerpts fun, easy, useful, and objective. Participants describe 10 second clips of songs and score points when their descriptions match those of other participants. The rules were designed to encourage users to be thorough and the clip length was chosen to make judgments more objective and specific. Analysis of preliminary data shows that we are able to collect objective and specific descriptions of clips and that players tend to agree with one another. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Metadata; Preliminary data; Websites","M.I. Mandel; Dept. Electrical Engineering, LabROSA, Columbia University, United States; email: mim@ee.columbia.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Peeling P.; Cemgil A.T.; Godsill S.","Peeling, Paul (16199969100); Cemgil, A. Taylan (15130945100); Godsill, Simon (7004018739)","16199969100; 15130945100; 7004018739","A probabilistic framework for matching music representations","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873572185&partnerID=40&md5=d1c7102532bfef8343f8a299a8f10cac","Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom","Peeling P., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom; Cemgil A.T., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom; Godsill S., Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom","In this paper we introduce a probabilistic framework for matching different music representations (score, MIDI, audio) by incorporating models of how one musical representation might be rendered from another. We propose a dynamical hidden Markov model for the score pointer as a prior, and two observation models, the first based on matching spectrogram data to a trained template, the second detecting damped sinusoids within a frame of audio by subspace methods. The resulting Bayesian framework is robust to local variations in tempo, and can be used for a wide variety of applications. We evaluate both methods in a score alignment context by inferring the posterior distribution of the current position in the score exactly. The spectrogram method is shown to infer the score position reliably with minimal computation, and the damped sinusoid model is able to pinpoint the positions of score events in the audio with a high level of timing accuracy. ©2007 Austrian Computer Society (OCG).","","Hidden Markov models; Information retrieval; Spectrographs; Bayesian frameworks; Damped sinusoids; Local variations; Music representation; Musical representations; Observation model; Posterior distributions; Probabilistic framework; Spectrograms; Sub-space methods; Audio acoustics","P. Peeling; Department of Engineering, Cambridge University, Cambridge CB2 1PZ, Trumpington Street, United Kingdom; email: php23@eng.cam.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Gillet O.; Richard G.","Gillet, Olivier (56616219800); Richard, Gäel (57195915952)","56616219800; 57195915952","Supervised and unsupervised sequence modelling for DRUM transcription","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650867380&partnerID=40&md5=4a20c242ce72e06ee3139abeec766989","GET / Télécom Paris (ENST), CNRS LTCI, 75014 Paris, 37 rue Dareau, France","Gillet O., GET / Télécom Paris (ENST), CNRS LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., GET / Télécom Paris (ENST), CNRS LTCI, 75014 Paris, 37 rue Dareau, France","We discuss in this paper two post-processings for drum transcription systems, which aim to model typical properties of drum sequences. Both methods operate on a symbolic representation of the sequence, which is obtained by quantizing the onsets of drum strokes on an optimal tatum grid, and by fusing the posterior probabilities produced by the drum transcription system. The first proposed method is a generalization of the N-gram model. We discuss several training and recognition strategies (style-dependent models, local models) in order to maximize the reliability and the specificity of the trained models. Alternatively, we introduce a novel unsupervised algorithm based on a complexity criterion, which finds the most regular and wellstructured sequence compatible with the acoustic scores produced by the transcription system. Both approaches are evaluated on a subset of the ENST-drums corpus, and yield performance improvements. ©2007 Austrian Computer Society (OCG).","","Complexity criteria; Local model; N-gram models; Performance improvements; Post-processing; Posterior probability; Recognition strategies; Symbolic representation; Typical properties; Unsupervised algorithms; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Feng L.; Nielsen A.B.; Hansen L.K.","Feng, Ling (58440948800); Nielsen, Andreas Brinch (16068846600); Hansen, Lars Kai (35493380300)","58440948800; 16068846600; 35493380300","Vocal segment classification in popular music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71349088259&partnerID=40&md5=bd24537dc81b9271241cda30fa417712","Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark","Feng L., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark; Nielsen A.B., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark; Hansen L.K., Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark","This paper explores the vocal and non-vocal music classification problem within popular songs. A newly built labeled database covering 147 popular songs is announced. It is designed for classifying signals from 1sec time windows. Features are selected for this particular task, in order to capture both the temporal correlations and the dependencies among the feature dimensions. We systematically study the performance of a set of classifiers, including linear regression, generalized linear model, Gaussian mixture model, reduced kernel orthonormalized partial least squares and K-means on cross-validated training and test setup. The database is divided in two different ways: with/without artist overlap between training and test sets, so as to study the so called 'artist effect'. The performance and results are analyzed in depth: from error rates to sample-to-sample error correlation. A voting scheme is proposed to enhance the performance under certain conditions.","","Information retrieval; Error correlation; Error rate; Feature dimensions; Gaussian Mixture Model; Generalized linear model; K-means; Music classification; Partial least square (PLS); Popular music; Popular song; Temporal correlations; Test sets; Test setups; Time windows; Voting schemes; Classification (of information)","L. Feng; Technical University of Denmark, Department of Informatics and Mathematical Modelling, Denmark; email: lf@imm.dtu.dk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Law E.L.M.; Ahn L.V.; Dannenberg R.B.; Crawford M.","Law, Edith L. M. (35173013200); Ahn, Luis Von (55586561600); Dannenberg, Roger B. (7003266250); Crawford, Mike (57638199700)","35173013200; 55586561600; 7003266250; 57638199700","Tagatune: A game for music and sound annotation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","150","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600416&partnerID=40&md5=61047ba55e1fa95ce6b0d25271e341a3","School of Computer Science, Carnegie Mellon University, United States","Law E.L.M., School of Computer Science, Carnegie Mellon University, United States; Ahn L.V., School of Computer Science, Carnegie Mellon University, United States; Dannenberg R.B., School of Computer Science, Carnegie Mellon University, United States; Crawford M., School of Computer Science, Carnegie Mellon University, United States","Annotations of audio files can be used to search and index music and sound databases, provide data for system evaluation, and generate training data for machine learning. Unfortunately, the cost of obtaining a comprehensive set of annotations manually is high. One way to lower the cost of labeling is to create games with a purpose that people will voluntarily play, producing useful metadata as a by-product. TagATune is an audio-based online game that aims to extract descriptions of sounds and music from human players. This paper presents the rationale, design and preliminary results from a pilot study using a prototype of TagATune to label a subset of the FreeSound database. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Metadata; Search engines; Audio files; Audio-based; Games with a purpose; Human players; On-line games; Pilot studies; Sound database; System evaluation; Training data; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Novello A.; Mckinney M.","Novello, Alberto (19337515900); Mckinney, Martin (57225339869)","19337515900; 57225339869","Assessment of perceptual music similarity","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601901&partnerID=40&md5=7d8e3d6ecedca3ed8dff4134bff7bda0","Philips Research Laboratories, Eindhoven, Netherlands","Novello A., Philips Research Laboratories, Eindhoven, Netherlands; Mckinney M., Philips Research Laboratories, Eindhoven, Netherlands","This paper extends a study on music similarity perception presented at ISMIR last year, in which subjects ranked the similarity of excerpt-pairs presented in triads [1]. The larger number of subjects and stimuli in the current study required a modification of the methodological strategy. We use here two nested incomplete block designs in order to cover the full set of song-excerpts comparisons (triads) while limiting the experimental time per subject. In addition to the two variable factors of the previous experiment, tempo and genre, we examine here the effect of prevalent instrument timbre. We found that 69 of 78 subjects where significantly consistent in their judgments of repeated triads. Furthermore, we found significant acrosssubject consistency on all 10 repeated triads. A significant difference was found in the distributions of interand intra-genre excerpt distances. The stress values in the Shepard's plot shows evidence of increased complexity in the present study compared to the previous smaller study. ©2007 Austrian Computer Society (OCG).","","Incomplete block designs; Methodological strategies; Music similarity; Music similarity perception; Significant differences; Stress values; Variable factors; Information retrieval","A. Novello; Philips Research Laboratories, Eindhoven, Netherlands; email: alberto.novello@philips.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Duggan B.; O'Shea B.; Gainza M.; Cunningham P.","Duggan, Bryan (24823868500); O'Shea, Brendan (23390273100); Gainza, Mikel (6506332560); Cunningham, Pâdraig (7201579809)","24823868500; 23390273100; 6506332560; 7201579809","Machine annotation of sets of traditional irish dance tunes","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949091555&partnerID=40&md5=13e1bf3a275020547b181a785f1cd1ad","DIT School of Computing, Dublin 8, Kevin St., Ireland; Audio Research Group, DIT, Dublin 8, Kevin St., Ireland; School of Informatics and Computer Science, UCD, Dublin, Ireland","Duggan B., DIT School of Computing, Dublin 8, Kevin St., Ireland; O'Shea B., DIT School of Computing, Dublin 8, Kevin St., Ireland; Gainza M., Audio Research Group, DIT, Dublin 8, Kevin St., Ireland; Cunningham P., School of Informatics and Computer Science, UCD, Dublin, Ireland","A set in traditional Irish music is a sequence of two or more dance tunes in the same time signature, where each tune is repeated an arbitrary number of times. A turn in a set represents the point at which either a tune repeats or a new tune is introduced. Tunes in sets are played in a segue (without a pause) and so detecting the turn is a significant challenge. This paper presents the MATS algorithm, a novel algorithm for identifying turns in sets of traditional Irish music. MATS works on digitised audio files of monophonic flute and tin-whistle music. Previous work on machine annotation of traditional music is summarised and experimental results validating the MATS algorithm are presented.","","Algorithms; Information retrieval; Tin; Arbitrary number; Audio files; Novel algorithm; On-machines; Audio acoustics","B. Duggan; DIT School of Computing, Dublin 8, Kevin St., Ireland; email: bryan.duggan@comp.dit.ie","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Tzanetakis G.; Jones R.; McNally K.","Tzanetakis, George (6602262192); Jones, Randy (55722600700); McNally, Kirk (36117989000)","6602262192; 55722600700; 36117989000","Stereo panning features for classifying recording production style","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873605702&partnerID=40&md5=582ae21fb3ae24a2da38ea70eb512330","University of Victoria, Canada","Tzanetakis G., University of Victoria, Canada; Jones R., University of Victoria, Canada; McNally K., University of Victoria, Canada","Recording engineers, mixers and producers play important yet often overlooked roles in defining the sound of a particular record, artist or group. The placement of different sound sources in space using stereo panning information is an important component of the production process. Audio classification systems typically convert stereo signals to mono and to the best of our knowledge have not utilized information related to stereo panning. In this paper we propose a set of audio features that can be used to capture stereo information. These features are shown to provide statistically important information for non-trivial audio classification tasks and are compared with the traditional Mel-Frequency Cepstral Coefficients. The proposed features can be viewed as a first attempt to capture extra-musical information related to the production process through music information retrieval techniques. ©2007 Austrian Computer Society (OCG).","","Audio acoustics; Information retrieval; Production engineering; Audio classification; Audio features; Mel-frequency cepstral coefficients; Music information retrieval; Non-trivial; Production process; Sound source; Stereo information; Audio recordings","G. Tzanetakis; University of Victoria, Canada; email: gtzan@cs.uvic.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Tiemann M.; Pauws S.; Vignoli F.","Tiemann, Marco (57512613000); Pauws, Steffen (11240480500); Vignoli, Fabio (11240995100)","57512613000; 11240480500; 11240995100","Ensemble learning for hybrid music recommendation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580126&partnerID=40&md5=b5403e2e3d2ab85590960184179dd30b","Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","Tiemann M., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Pauws S., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Vignoli F., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","We investigate ensemble learning methods for hybrid music recommenders, combining a social and a content-based recommender algorithm in an initial experiment by applying a simple combination rule to merge recommender results. A first experiment suggests that such a combination can reduce the mean absolute prediction error compared to the used recommenders' individual errors. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Combination rules; Content-based; Ensemble learning; Mean absolute prediction error; Music recommendation; Recommender algorithms; Experiments","M. Tiemann; Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; email: marco.tiemann@philips.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Varewyck M.; Martens J.-P.","Varewyck, Matthias (35114044600); Martens, Jean-Pierre (7201836932)","35114044600; 7201836932","Assessment of state-of-the-art meter analysis systems with an extended meter description model","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582302&partnerID=40&md5=18358dda0bf3bdd16460041d032ac8d2","Department of Electronics and Information Systems, Ghent University (Belgium), Belgium","Varewyck M., Department of Electronics and Information Systems, Ghent University (Belgium), Belgium; Martens J.-P., Department of Electronics and Information Systems, Ghent University (Belgium), Belgium","An extended meter description model capturing the hierarchical metrical structure of Western music is proposed. The model is applied for the quantitative evaluation of four state-of-the-art automatic meter analysis algorithms of musical audio. Evaluation results suggest that the best beat trackers reach a reasonable level of performance, but that none of the tested algorithms has the potential to perform a reliable bar onset tracking. Moreover, the frontends of the best over-all systems not necessarily seem to have the front-ends best encoding the time signature in their output. Therefore, further improvements of these systems should be attainable by a better combination of ideas that can be borrowed from existing algorithms. ©2007 Austrian Computer Society (OCG).","","Audio acoustics; Information retrieval; Analysis algorithms; Analysis system; Description model; Evaluation results; Musical audio; Quantitative evaluation; Algorithms","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Peng W.; Li T.; Ogihara M.","Peng, Wei (12241508300); Li, Tao (55553727585); Ogihara, Mitsunori (54420747900)","12241508300; 55553727585; 54420747900","Music clustering with constraints","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606875&partnerID=40&md5=39d58f530d85f03d6d5f05f5851e25d1","School of Computer Science, Florida International University, United States; Department of Computer Science, University of Rochester, United States","Peng W., School of Computer Science, Florida International University, United States; Li T., School of Computer Science, Florida International University, United States; Ogihara M., Department of Computer Science, University of Rochester, United States","This paper studies the problem of building clusters of music tracks in a collection of popular music in the presence of constraints. The constraints come naturally in the context of music applications. For example, constraints can be generated from the background knowledge (e.g., two artists share similar styles) and the user access patterns (e.g., two pieces of music share similar access patterns across multiple users). We present an approach based on the generalized constraint clustering algorithm by incorporating the constraints for grouping music by ""similar"" artists. The approach is evaluated on a data set consisting of 53 albums covering 41 popular artists. The ""correctness"" of the clusters generated is tested using artist similarity provided by All Music Guide. ©2007 Austrian Computer Society (OCG).","","Clustering algorithms; Artist similarities; Back-ground knowledge; Data set; Generalized constraint; Multiple user; Music applications; Popular music; Similar access; User access patterns; Information retrieval","W. Peng; School of Computer Science, Florida International University, United States; email: wpeng002@cs.fiu.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"You W.; Dannenberg R.B.","You, Wei (55585868400); Dannenberg, Roger B. (7003266250)","55585868400; 7003266250","Polyphonic music note onset detection using semi-supervised learning","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873601587&partnerID=40&md5=e8ae87d7e732aaed3082566d4341a1d4","Schools of Computer Science and Music, Carnegie Mellon University, United States","You W., Schools of Computer Science and Music, Carnegie Mellon University, United States; Dannenberg R.B., Schools of Computer Science and Music, Carnegie Mellon University, United States","Automatic note onset detection is particularly difficult in orchestral music (and polyphonic music in general). Machine learning offers one promising approach, but it is limited by the availability of labeled training data. Score-toaudio alignment, however, offers an economical way to locate onsets in recorded audio, and score data is freely available for many orchestral works in the form of standard MIDI files. Thus, large amounts of training data can be generated quickly, but it is limited by the accuracy of the alignment, which in turn is ultimately related to the problem of onset detection. Semi-supervised or bootstrapping techniques can be used to iteratively refine both onset detection functions and the data used to train the functions. We show that this approach can be used to improve and adapt a general purpose onset detection algorithm for use with orchestral music. ©2007 Austrian Computer Society (OCG).","","Alignment; Information retrieval; Learning algorithms; Signal detection; Supervised learning; General purpose; Labeled training data; Large amounts; MIDI files; Note onset detections; Onset detection; Polyphonic music; Semi-supervised; Semi-supervised learning; Training data; Iterative methods","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Hamanaka M.; Hirata K.; Tojo S.","Hamanaka, Masatoshi (35253968400); Hirata, Keiji (55730620800); Tojo, Satoshi (7103319884)","35253968400; 55730620800; 7103319884","ATTA: Implementing gttm on a computer","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436870&partnerID=40&md5=80fc17d2af5368c175b2172e95e69486","University of Tsukuba, Japan; NTT Communication Sience Laboratories, Japan; Japan Advanced Institute of Science and Technology, Japan","Hamanaka M., University of Tsukuba, Japan; Hirata K., NTT Communication Sience Laboratories, Japan; Tojo S., Japan Advanced Institute of Science and Technology, Japan","We have been discussing the design principle for the implementation of GTTM and presented the semiautomatic generation techniques of grouping structure, metrical structure, and time-span tree, and the searching method for the optimal parameter value assignments. In ISMIR2007, we organize a tutorial session on the techniques for implementing music theory GTTM for summarizing our work and report it to relevant participants of the conference. Since the time of the tutorial session is not enough, we demonstrate a working automatic timespan tree analyzer ATTA in a demo session. ATTA is an integration of our work done so far; by looking at the ATTA demonstration or using ATTA, people will be able to understand the techniques for implementing GTTM as well as GTTM itself in more detail. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Design Principles; Music theory; Optimal parameter; Searching methods; Semi-automatic generation; Forestry","M. Hamanaka; University of Tsukuba, Japan; email: hamanaka@iit.tsukuba.ac.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Cunningham S.J.; Bainbridge D.; Mckay D.","Cunningham, Sally Jo (7201937110); Bainbridge, David (8756864800); Mckay, Dana (17435193400)","7201937110; 8756864800; 17435193400","Finding new music: A diary study of everyday encounters with novel songs","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873595056&partnerID=40&md5=38212dd6a78cec56444db3a2d0857b8c","Department of Computer Science, University of Waikato, Hamilton, New Zealand; University of Technology, Information Resources Swinburne, Melbourne, Australia","Cunningham S.J., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, Hamilton, New Zealand; Mckay D., University of Technology, Information Resources Swinburne, Melbourne, Australia","This paper explores how we, as individuals, purposefully or serendipitously encounter ""new music"" (that is, music that we haven't heard before) and relates these behaviours to music information retrieval activities such as music searching and music discovery via use of recommender systems. 41 participants participated in a three-day diary study, in which they recorded all incidents that brought them into contact with new music. The diaries were analyzed using a Grounded Theory approach. The results of this analysis are discussed with respect to location, time, and whether the music encounter was actively sought or occurred passively. Based on these results, we outline design implications for music information retrieval software, and suggest an extension of ""laid back"" searching. ©2007 Austrian Computer Society (OCG).","","Design implications; Diary study; Grounded theory approach; Music information retrieval; Information retrieval","S.J. Cunningham; Department of Computer Science, University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Volk A.; Van Kranenburg P.; Garbers J.; Wiering F.; Veltkamp R.C.; Grijp L.P.","Volk, Anja (30567849900); Van Kranenburg, Peter (35108158000); Garbers, Jörg (35107028500); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646); Grijp, Louis P. (28067929100)","30567849900; 35108158000; 35107028500; 8976178100; 7003421646; 28067929100","A manual annotation method for melodic similarity and the study of melody feature sets","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350483637&partnerID=40&md5=4cd05c7fddc820f3e03562f671c94549","Department of Information and Computing Sciences, Utrecht University, Netherlands; Meertens Institute, Amsterdam, Netherlands","Volk A., Department of Information and Computing Sciences, Utrecht University, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands; Garbers J., Department of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Department of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Netherlands; Grijp L.P., Department of Information and Computing Sciences, Utrecht University, Netherlands, Meertens Institute, Amsterdam, Netherlands","This paper describes both a newly developed method for manual annotation for aspects of melodic similarity and its use for evaluating melody features concerning their contribution to perceived similarity. The second issue is also addressed with a computational evaluation method. These approaches are applied to a corpus of folk song melodies. We show that classification of melodies could not be based on single features and that the feature sets from the literature are not sufficient to classify melodies into groups of related melodies. The manual annotations enable us to evaluate various models for melodic similarity.","","Computational evaluation; Feature sets; Folk songs; Manual annotation; Melodic similarity; Information retrieval","A. Volk; Department of Information and Computing Sciences, Utrecht University, Netherlands; email: volk@cs.uu.nl","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Jones M.C.; Downie J.S.; Ehmann A.F.","Jones, M. Cameron (57199028615); Downie, J. Stephen (7102932568); Ehmann, Andreas F. (8988651500)","57199028615; 7102932568; 8988651500","Human similarity judgments: Implications for the design of formal evaluations","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607335&partnerID=40&md5=02af5d41e116237628cc5f93faaf7c4b","International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Jones M.C., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","This paper presents findings of a series of analyses of human similarity judgments from the Symbolic Melodic Similarity, and Audio Music Similarity tasks from the Music Information Retrieval Evaluation Exchange (MIREX) 2006. The categorical judgment data generated by the evaluators is analyzed with regard to judgment stability, inter-grader reliability, and patterns of disagreement, both within and between the two tasks. An exploration of this space yields implications for the design of MIREX-like evaluations. ©2007 Austrian Computer Society (OCG).","","Insecticides; Audio music; Melodic similarity; Music information retrieval; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Flexer A.","Flexer, Arthur (7004555682)","7004555682","A closer look on artist filters for musical genre classification","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","53","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580369&partnerID=40&md5=02600c30869d345cf76b457bba8cfda5","Center for Brain Research, Institute of Medical Cybernetics and Artificial Intelligence, Medical University of Vienna Austria, Freyung 6/2, A-1010 Vienna, Austria","Flexer A., Center for Brain Research, Institute of Medical Cybernetics and Artificial Intelligence, Medical University of Vienna Austria, Freyung 6/2, A-1010 Vienna, Austria","Musical genre classification is the automatic classification of audio signals into user defined labels describing pieces of music. A problem inherent to genre classification experiments in music information retrieval research is the use of songs from the same artist in both training and test sets. We show that this does not only lead to overoptimistic accuracy results but also selectively favours particular classification approaches. The advantage of using models of songs rather than models of genres vanishes when applying an artist filter. The same holds true for the use of spectral features versus fluctuation patterns for preprocessing of the audio files. ©2007 Austrian Computer Society (OCG).","","Audio files; Audio signal; Automatic classification; Classification approach; Genre classification; Music information retrieval; Musical genre classification; Spectral feature; Test sets; Information retrieval","A. Flexer; Center for Brain Research, Institute of Medical Cybernetics and Artificial Intelligence, Medical University of Vienna Austria, Freyung 6/2, A-1010 Vienna, Austria; email: arthur.flexer@meduniwien.ac.at","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Hurley N.J.; Balado F.; Mccarthy E.P.; Silvestre G.C.M.","Hurley, Neil J. (7004435214); Balado, F́elix (6603125254); Mccarthy, Elizabeth P. (8605313400); Silvestre, Gúenoĺe C.M. (7003631074)","7004435214; 6603125254; 8605313400; 7003631074","Performance of philips audio fingerprinting under desynchronisation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873576154&partnerID=40&md5=1e2ac6ce29dcc8f759c994ff0d3ab5ab","School of Computer Science and Informatics, University College Dublin, Ireland","Hurley N.J., School of Computer Science and Informatics, University College Dublin, Ireland; Balado F., School of Computer Science and Informatics, University College Dublin, Ireland; Mccarthy E.P., School of Computer Science and Informatics, University College Dublin, Ireland; Silvestre G.C.M., School of Computer Science and Informatics, University College Dublin, Ireland","An audio fingerprint is a compact representation (robust hash) of an audio signal which is linked to its perceptual content. Perceptually equivalent instances of the signal must lead to the same hash value. Fingerprinting finds application in efficient indexing of music databases. We present a theoretical analysis of the Philips audio fingerprinting method under desynchronisation for correlated stationary Gaussian sources. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio fingerprint; Audio fingerprinting; Audio signal; Compact representation; Gaussian sources; Hash value; Music database; Philips; Robust hash; Pattern matching","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Yoshii K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","7103400120; 7403505330; 35577813100; 7402000772; 7102397930","Improving efficiency and scalability of model-based music recommender system based on incremental training","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587838&partnerID=40&md5=a4dde72002fc086c4c50d9a49b3172d9","JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Ogata T., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan","We aimed at improving the efficiency and scalability of a hybrid music recommender system based on a probabilistic generative model that integrates both collaborative data (rating scores provided by users) and content-based data (acoustic features of musical pieces). Although the hybrid system was proved to make accurate recommendations, it lacks efficiency and scalability. In other words, the entire model needs to be re-trained from scratch whenever a new score, user, or piece is added. Furthermore, the system cannot deal with practical numbers of users and pieces on an enterprise scale. To improve efficiency, we propose an incremental method that partially updates the model at low computational cost. To enhance scalability, we propose a method that first constructs a small ""core"" model over fewer virtual representatives created from real users and pieces, and then adds the real users and pieces to the core model by using the incremental method. The experimental results revealed that the proposed system was not only efficient and scalable but also outperformed the original system in terms of accuracy. ©2007 Austrian Computer Society (OCG).","","Efficiency; Hybrid systems; Scalability; Acoustic features; Computational costs; Content-based; Core model; Generative model; Improving efficiency; Incremental method; Incremental training; Music recommender systems; Musical pieces; Original systems; Recommender systems","K. Yoshii; JSPS Research Fellow (DC1), Graduate School of Informatics, Kyoto University, Japan; email: yoshii@kuis.kyoto-u.ac.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Doraisamy S.; Golzari S.; Norowi N.M.; Sulaiman M.N.B.; Udzir N.I.","Doraisamy, Shyamala (24765904400); Golzari, Shahram (15042056400); Norowi, Noris Mohd. (24766399900); Sulaiman, Md. Nasir B. (22434244300); Udzir, Nur Izura (8662597200)","24765904400; 15042056400; 24766399900; 22434244300; 8662597200","A study on feature selection and classification techniques for automatic genre classification of traditional malay music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","66","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949108828&partnerID=40&md5=50fcc1ce99ad86f4f8eea749d234faab","Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia","Doraisamy S., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Golzari S., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Norowi N.M., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Sulaiman M.N.B., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia; Udzir N.I., Faculty of Computer Science and Information Technology, University Putra Malaysia, Malaysia","Machine learning techniques for automated musical genre classification is currently widely studied. With large collections of digital musical files, one approach to classification is to classify by musical genres such as pop, rock and classical in Western music. Beat, pitch and temporal related features are extracted from audio signals and various machine learning algorithms are applied for classification. Features that resulted in better classification accuracies for Traditional Malay Music (TMM), in comparison to western music, in a previous study were beat related features. However, only the J48 classifier was used and in this study we perform a more comprehensive investigation on improving the classification of TMM. In addition, feature selection was performed for dimensionality reduction. Classification accuracies using classifiers of varying paradigms on a dataset comprising ten TMM genres were obtained. Results identify potentially useful classifiers and show the impact of adding a feature selection phase for TMM genre classification.","","Information retrieval; Learning algorithms; Learning systems; Audio signal; Automatic genre classification; Classification accuracy; Data sets; Dimensionality reduction; Feature selection and classification; Genre classification; Machine learning techniques; Musical genre; Musical genre classification; Classification (of information)","","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"De Haas W.B.; Veltkamp R.C.; Wiering F.","De Haas, W. Bas (51160955300); Veltkamp, Remco C. (7003421646); Wiering, Frans (8976178100)","51160955300; 7003421646; 8976178100","Tonal pitch step distance: A similarity measure for chord progressions","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450287180&partnerID=40&md5=0b2869dd4855c5b9846f9f86145af52c","Departement of Information and Computing Sciences, Utrecht University, Netherlands","De Haas W.B., Departement of Information and Computing Sciences, Utrecht University, Netherlands; Veltkamp R.C., Departement of Information and Computing Sciences, Utrecht University, Netherlands; Wiering F., Departement of Information and Computing Sciences, Utrecht University, Netherlands","The computational analysis of musical harmony has received a lot of attention the last decades. Although it is widely recognized that extracting symbolic chord labels from music yields useful abstractions, and the number of chord labeling algorithms for symbolic and audio data is steadily growing, surprisingly little effort has been put into comparing sequences of chord labels. This study presents and tests a new distance function that measures the difference between chord progressions. The presented distance function is based on Lerdahl's Tonal Pitch Space [10]. It compares the harmonic changes of two sequences of chord labels over time. This distance, named the Tonal Pitch Step Distance (TPSD), is shown to be effective for retrieving similar jazz standards found in the Real Book [3]. The TPSD matches the human intuitions about harmonic similarity which is demonstrated on a set of blues variations.","","Information retrieval; Audio data; Computational analysis; Distance functions; Jazz standards; Labeling algorithms; Similarity measure; Audio acoustics","W.B. De Haas; Departement of Information and Computing Sciences, Utrecht University, Netherlands; email: Bas.deHaas@cs.uu.nl","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Fremerey C.; Kurth F.; Müller M.; Clausen M.","Fremerey, Christian (23396821400); Kurth, Frank (56240850200); Müller, Meinard (7404689873); Clausen, Michael (56225233200)","23396821400; 56240850200; 7404689873; 56225233200","A demonstration of the syncplayer system","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956333233&partnerID=40&md5=b7d0c6bb3d8d1cabb5a9c4089c00eac2","Department of Computer Science III, Bonn University, Germany","Fremerey C., Department of Computer Science III, Bonn University, Germany; Kurth F., Department of Computer Science III, Bonn University, Germany; Müller M., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","The SyncPlayer system is an advanced audio player for multimodal presentation, browsing, and retrieval of music data. The system has been extended significantly in the last few years. In this contribution, we describe the current state of the system and demonstrate the functionalities and interactions of the novel SyncPlayer components including combined inter- and intra-document music browsing. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio players; Multi-modal; Music data; Search engines","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Ohishi Y.; Goto M.; Itou K.; Takeda K.","Ohishi, Yasunori (14018275300); Goto, Masataka (7403505330); Itou, Katunobu (8855763800); Takeda, Kazuya (7404334995)","14018275300; 7403505330; 8855763800; 7404334995","A stochastic representation of the dynamics of sung melody","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455186199&partnerID=40&md5=402e6988f8d0a1b34560ebe55226f080","Graduate School of Information Science, Nagoya University, Japan; National Institute of Advanced, Industrial Science and Technology (AIST), Japan; Faculty of Computer and Information Sciences, Hosei University, Japan","Ohishi Y., Graduate School of Information Science, Nagoya University, Japan; Goto M., National Institute of Advanced, Industrial Science and Technology (AIST), Japan; Itou K., Faculty of Computer and Information Sciences, Hosei University, Japan; Takeda K., Graduate School of Information Science, Nagoya University, Japan","In this paper, we propose a stochastic representation of a sung melodic contour, called stochastic phase representation (SPR), which can characterize both musical-note information and the dynamics of singing behaviors included in the melodic contour. The SPR is constructed by fitting probability distribution functions to F0 trajectories in the F0-¢F0 phase plane. Since fluctuations in singing can be easily separated by using SPR, we applied SPR to a melodic similarity measure for query-by-humming (QBH) applications. Our experimental results showed that the SPR-based similarity measure was superior to a conventional dynamic-programming-based method. ©2007 Austrian Computer Society (OCG).","","Distribution functions; Dynamics; Information retrieval; Probability distributions; Fitting probability distributions; Melodic similarity; Phase plane; Query-by-humming; Similarity measure; Stochastic representations; Transport properties","Y. Ohishi; Graduate School of Information Science, Nagoya University, Japan; email: ohishi@sp.m.is.nagoya-u.ac.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Kurth F.; Müller M.; Fremerey C.; Chang Y.-H.; Clausen M.","Kurth, Frank (56240850200); Müller, Meinard (7404689873); Fremerey, Christian (23396821400); Chang, Yoon-Ha (55586883800); Clausen, Michael (56225233200)","56240850200; 7404689873; 23396821400; 55586883800; 56225233200","Automated synchronization of scanned sheet music with audio recordings","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","39","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596210&partnerID=40&md5=4c6ac9cf9134365a5594bc60102e2456","Department of Computer Science III, Bonn University, Germany","Kurth F., Department of Computer Science III, Bonn University, Germany; Müller M., Department of Computer Science III, Bonn University, Germany; Fremerey C., Department of Computer Science III, Bonn University, Germany; Chang Y.-H., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","In this paper, we present a procedure for automatically synchronizing scanned sheet music with a corresponding CD audio recording, where suitable regions (given in pixels) of the scanned digital images are linked to time positions of the audio file. In a first step, we extract note parameters and 2D position information from the scanned images using standard software for optical music recognition (OMR).We then use a chroma-based synchronization algorithm to align the note parameters to the given audio recording. Our experiments show that even though the output of current OMR software is often erroneous, the music parameters extracted from the digital images still suffice to derive a reasonable alignment with the audio data stream. The resulting link structure can be used to highlight the current position in the scanned score or to automatically turn pages during playback of an audio recording. Such functionalities have been realized as plug-in for the SyncPlayer, which is a free prototypical software framework for bringing together various MIR techniques and applications. ©2007 Austrian Computer Society (OCG).","","Audio recordings; Computer programming; Information retrieval; Synchronization; Audio data; Audio files; Digital image; Link structure; Optical music recognition; Plug-ins; Position information; Scanned images; Software frameworks; Standard software; Synchronization algorithm; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Duda A.; Nürnberger A.; Stober S.","Duda, Alexander (57526399400); Nürnberger, Andreas (14027288100); Stober, Sebastian (14027561800)","57526399400; 14027288100; 14027561800","Towards query by singing/humming on audio databases","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053096604&partnerID=40&md5=76c2402ed0707dc71cf0036d0fa6dd90","Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany","Duda A., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany; Nürnberger A., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany; Stober S., Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany","Current work on Query-by-Singing/Humming (QBSH) focusses mainly on databases that contain MIDI files. Here, we present an approach that works on real audio recordings that bring up additional challenges. To tackle the problem of extracting the melody of the lead vocals from recordings, we introduce a method inspired by the popular ""karaoke effect"" exploiting information about the spatial arrangement of voices and instruments in the stereo mix. The extracted signal time series are aggregated into symbolic strings preserving the local approximated values of a feature and revealing higher-level context patterns. This allows distance measures for string pattern matching to be applied in the matching process. A series of experiments are conducted to assess the discrimination and robustness of this representation. They show that the proposed approach provides a viable baseline for further development and point out several possibilities for improvement. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio database; Context patterns; Distance measure; Karaoke; Matching process; MIDI files; Query-by-singing; Signal time; Spatial arrangements; String pattern matching; Audio recordings","A. Nürnberger; Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany; email: nuernb@iws.cs.uni-magdeburg.de","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Martins L.G.; Burred J.J.; Tzanetakis G.; Lagrange M.","Martins, Luis Gustavo (16245475200); Burred, Juan Jośe (16201737900); Tzanetakis, George (6602262192); Lagrange, Mathieu (18042165200)","16245475200; 16201737900; 6602262192; 18042165200","Polyphonic instrument recognition using spectral clustering","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606114&partnerID=40&md5=cf802ae42995fd874130b85301d52038","Telecommunications and Multimedia Unit, INESC Porto Porto, Portugal; Communication Systems Group, Technical University of Berlin, Berlin, Germany; Computer Science Department, University of Victoria, Victoria BC, Canada","Martins L.G., Telecommunications and Multimedia Unit, INESC Porto Porto, Portugal; Burred J.J., Communication Systems Group, Technical University of Berlin, Berlin, Germany; Tzanetakis G., Computer Science Department, University of Victoria, Victoria BC, Canada; Lagrange M., Computer Science Department, University of Victoria, Victoria BC, Canada","The identification of the instruments playing in a polyphonic music signal is an important and unsolved problem in Music Information Retrieval. In this paper, we propose a framework for the sound source separation and timbre classification of polyphonic, multi-instrumental music signals. The sound source separation method is inspired by ideas from Computational Auditory Scene Analysis and formulated as a graph partitioning problem. It utilizes a sinusoidal analysis front-end and makes use of the normalized cut, applied as a global criterion for segmenting graphs. Timbre models for six musical instruments are used for the classification of the resulting sound sources. The proposed framework is evaluated on a dataset consisting of mixtures of a variable number of simultaneous pitches and instruments, up to a maximum of four concurrent notes. ©2007 Austrian Computer Society (OCG).","","Image segmentation; Information retrieval; Computational auditory scene analysis; Global criteria; Graph partitioning problems; Instrument recognition; Music information retrieval; Music signals; Normalized cuts; Polyphonic music; Sinusoidal analysis; Sound source; Sound source separation; Spectral clustering; Timbre classification; Unsolved problems; Variable number; Acoustic generators","L.G. Martins; Telecommunications and Multimedia Unit, INESC Porto Porto, Portugal; email: lmartins@inescporto.pt","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Holzapfel A.; Stylianou Y.","Holzapfel, Andre (18041818000); Stylianou, Yannis (6601991415)","18041818000; 6601991415","Beat tracking using group delay based onset detection","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349448865&partnerID=40&md5=d33bfc6ac2c42a60117a2c50ea9f5ef5","Institute of Computer Science, FORTH, Greece; Multimedia Informatics Lab., Computer Science Department, University of Crete, Greece","Holzapfel A., Institute of Computer Science, FORTH, Greece, Multimedia Informatics Lab., Computer Science Department, University of Crete, Greece; Stylianou Y., Institute of Computer Science, FORTH, Greece, Multimedia Informatics Lab., Computer Science Department, University of Crete, Greece","This paper introduces a novel approach to estimate onsets in musical signals based on the phase spectrum and specifically using the average of the group delay function. A frame-by-frame analysis of a music signal provides the evolution of group delay over time, referred to as phase slope function. Onsets are then detected simply by locating the positive zero-crossings of the phase slope function. The proposed approach is compared to an amplitude-based onset detection approach in the framework of a state-of-the-art system for beat tracking. On a data set of music with less percussive content, the beat tracking accuracy achieved by the system is improved by 82% when the suggested phase-based onset detection approach is used instead of the amplitude-based approach, while on a set of music with stronger percussive characteristics both onset detection approaches provide comparable results of accuracy.","","Group delay; Information retrieval; Beat tracking; Data sets; Frame-by-frame analysis; Group delay functions; Music signals; Musical signals; Onset detection; Phase slope; Phase spectra; State-of-the-art system; Zero-crossings; Signal detection","A. Holzapfel; Institute of Computer Science, FORTH, Greece; email: hannover@csd.uoc.gr","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Garbers J.; Wiering F.","Garbers, Jörg (35107028500); Wiering, Frans (8976178100)","35107028500; 8976178100","Towards structural alignment of folk songs","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350486119&partnerID=40&md5=6292af4e416837fca41cc536bdd9506d","Utrecht University, Department of Information and Computing Sciences, Netherlands","Garbers J., Utrecht University, Department of Information and Computing Sciences, Netherlands; Wiering F., Utrecht University, Department of Information and Computing Sciences, Netherlands","We describe an alignment-based similarity framework for folk song variation research. The framework makes use of phrase and meter information encoded in Humdrum scores. Local similarity measures are used to compute match scores, which are combined with gap scores to form increasingly larger alignments and higher-level similarity values. We discuss the effects of some similarity measures on the alignment of four groups of melodies that are variants of each other.","","Information retrieval; Folk songs; Four-group; Local similarity measure; Match score; Similarity measure; Alignment","J. Garbers; Utrecht University, Department of Information and Computing Sciences, Netherlands; email: garbers@cs.uu.nl","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Slaney M.; White W.","Slaney, Malcolm (6701855101); White, William (57198982894)","6701855101; 57198982894","Similarity based on rating data","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573578&partnerID=40&md5=647b299409d3cc0aa35e14394b34374d","Yahoo Research 2821 Mission College Blvd., Santa Clara, CA 95054, United States; Yahoo Media Innovation 1950 University Ave., Berkeley CA 94704, United States","Slaney M., Yahoo Research 2821 Mission College Blvd., Santa Clara, CA 95054, United States; White W., Yahoo Media Innovation 1950 University Ave., Berkeley CA 94704, United States","This paper describes an algorithm to measure the similarity of two multimedia objects, such as songs or movies, using users' preferences. Much of the previous work on query-by-example (QBE) or music similarity uses detailed analysis of the object's content. This is difficult and it is often impossible to capture how consumers react to the music. We argue that a large collection of user's preferences is more accurate, at least in comparison to our benchmark system, at finding similar songs. We describe an algorithm based the song's rating data, and show how this approach works by measuring its performance using an objective metric based on whether the same artist performed both songs. Our similarity results are based on 1.5 million musical judgments by 380,000 users. We test our system by generating playlists using a content-based system, our rating-based system, and a random list of songs. Music listeners greatly preferred the ratings-based playlists over the content-based and random playlists. ©2007 Austrian Computer Society (OCG).","","Algorithms; Information retrieval; Benchmark system; Content-based; Content-based systems; Multimedia object; Music similarity; Query-by-example; User's preferences; Rating","M. Slaney; Yahoo Research 2821 Mission College Blvd., Santa Clara, CA 95054, United States; email: malcolm@ieee.org","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lamere P.; Eck D.","Lamere, Paul (15765776900); Eck, Douglas (12141444300)","15765776900; 12141444300","Using 3D visualizations to explore and discover music","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873598638&partnerID=40&md5=d88cde757154d8dcb813312e65243bd6","Sun Labs Sun Microsystems, Burlington, MA, United States","Lamere P., Sun Labs Sun Microsystems, Burlington, MA, United States; Eck D., Sun Labs Sun Microsystems, Burlington, MA, United States","This paper presents Search Inside the Music an application for exploring and discovering new music. Search Inside the Music uses a music similarity model and 3D visualizations to provide a user with new tools for exploring and interacting with a music collection. With Search Inside the Music, a music listener can find new music, generate interesting playlists, and interact with their music collection. ©2007 Austrian Computer Society (OCG).","","Audio recordings; Information retrieval; Visualization; 3D Visualization; Music collection; Music similarity; Three dimensional computer graphics","P. Lamere; Sun Labs Sun Microsystems, Burlington, MA, United States; email: Paul.Lamere@sun.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Ellis D.P.W.","Ellis, Daniel P.W. (13609089200)","13609089200","Classifying music audio with timbral and chroma features","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","83","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607165&partnerID=40&md5=a11847e986c3ebd3de5479a9f5b66273","Dept. Elec. Eng., LabROSA, Columbia University, United States","Ellis D.P.W., Dept. Elec. Eng., LabROSA, Columbia University, United States","Music audio classification has most often been addressed by modeling the statistics of broad spectral features, which, by design, exclude pitch information and reflect mainly instrumentation. We investigate using instead beat-synchronous chroma features, designed to reflect melodic and harmonic content and be invariant to instrumentation. Chroma features are less informative for classes such as artist, but contain information that is almost entirely independent of the spectral features, and hence the two can be profitably combined: Using a simple Gaussian classifier on a 20-way pop music artist identification task, we achieve 54% accuracy with MFCCs, 30% with chroma vectors, and 57% by combining the two. All the data and Matlab code to obtain these results are available. ©2007 Austrian Computer Society (OCG).","","Computer music; Information retrieval; Audio classification; Broad spectral; Chroma features; Gaussian classifier; Harmonic contents; Matlab code; Spectral feature; Audio acoustics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Ramirez R.; Perez A.; Kersten S.","Ramirez, Rafael (35280935600); Perez, Alfonso (16302159000); Kersten, Stefan (35279974100)","35280935600; 16302159000; 35279974100","Performer identification in celtic violin recordings","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949092827&partnerID=40&md5=952918b434b1d6ae6e49273e2bf16db4","Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain","Ramirez R., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain; Perez A., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain; Kersten S., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain","We present an approach to the task of identifying performers from their playing styles. We investigate how violinists express and communicate their view of the musical content of Celtic popular pieces and how to use this information in order to automatically identify performers. We study notelevel deviations of parameters such as timing and amplitude. Our approach to performer identification consists of inducing an expressive performance model for each of the interpreters (essentially establishing a performer dependent mapping of inter-note features to a timing and amplitude expressive transformations). We present a successful performer identification case study.","","Performance Model; Playing style; Information retrieval","R. Ramirez; Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Ocata 1, Spain; email: fael@iua.upf.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Anglade A.; Tiemann M.; Vignoli F.","Anglade, Amélie (24070116200); Tiemann, Marco (57512613000); Vignoli, Fabio (11240995100)","24070116200; 57512613000; 11240995100","Virtual communities for creating shared music channels","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873585271&partnerID=40&md5=c4a60a4f3dd91b28280f7faa1c295b2c","Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","Anglade A., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Tiemann M., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; Vignoli F., Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands","We present an approach to automatically create virtual communities of users with similar music tastes. Our goal is to create personalized music channels for these communities in a distributed way, so that they can for example be used in peer-to-peer networks. To find suitable techniques for creating these communities we analyze graphs created from real-world recommender datasets and identify specific properties of these datasets. Based on these properties we select and evaluate different graph-based community-extraction techniques. We select a technique that exploits identified properties to create clusters of music listeners. We validate the suitability of this technique using a music dataset and a large movie dataset. On a graph of 6,040 peers, the selected technique assigns at least 85% of the peers to optimal communities, and obtains a mean classification error of less than 0.05 over the remaining peers that are not assigned to the best community. ©2007 Austrian Computer Society (OCG).","","Distributed computer systems; Information retrieval; Graph-based; Mean classification; Optimal community; Real-world; Specific properties; Virtual community; Virtual reality","A. Anglade; Philips Research Europe, High Tech Campus, 34 5656 AE Eindhoven, Netherlands; email: amelie.anglade@elec.qmul.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Roy P.; Pachet F.; Krakowski S.","Roy, Pierre (55506419000); Pachet, François (6701441655); Krakowski, Sergio (35305196600)","55506419000; 6701441655; 35305196600","Improving the classification of percussive sounds with analytical features: A case study","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873583017&partnerID=40&md5=eedd341e4568b397df6fa11afeab315d","Sony CSL 6, Paris, rue Amyot 75 005, France; Sony CSL Paris, Paris, 6, rue Amyot 75 005, France","Roy P., Sony CSL 6, Paris, rue Amyot 75 005, France; Pachet F., Sony CSL Paris, Paris, 6, rue Amyot 75 005, France; Krakowski S., Sony CSL Paris, Paris, 6, rue Amyot 75 005, France","There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we conducted experiments with so-called analytical features, an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. These analytical features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attack portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with standard approaches. ©2007 Austrian Computer Society (OCG).","","Audio acoustics; Audio classification; Interactive music; Specific problems; Supervised classification; Information retrieval","P. Roy; Sony CSL 6, Paris, rue Amyot 75 005, France; email: roy@csl.sony.fr","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Knopke I.; Byrd D.","Knopke, Ian (24381706300); Byrd, Donald (55041494100)","24381706300; 55041494100","Towards musicdiff: A foundation for improved optical music recognition using multiple recognizers","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578917&partnerID=40&md5=ba9f1b07ad840c0c39f8d641b73614a2","School of Informatics, Indiana University, United States; School of Informatics and Jacobs School of Music, Indiana University, United States","Knopke I., School of Informatics, Indiana University, United States; Byrd D., School of Informatics and Jacobs School of Music, Indiana University, United States","This paper presents work towards a ""musicdiff"" program for comparing files representing different versions of the same piece, primarily in the context of comparing versions produced by different optical music recognition (OMR) programs. Previous work by the current authors and others strongly suggests that using multiple recognizers will make it possible to improve OMR accuracy substantially. The basicmethodology requires several stages: documents must be scanned and submitted to several OMR programs, programs whose strengths and weaknesses have previously been evaluated in detail. We discuss techniques we have implemented for normalization, alignment and rudimentary error correction. We also describe a visualization tool for comparing multiple versions on a measure-by-measure basis. ©2007 Austrian Computer Society (OCG).","","Error correction; Optical music recognition; Visualization tools; Information retrieval","I. Knopke; School of Informatics, Indiana University, United States; email: iknopke@indiana.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Hu X.; Bay M.; Downie J.S.","Hu, Xiao (55496358400); Bay, Mert (56259607500); Downie, J. Stephen (7102932568)","55496358400; 56259607500; 7102932568","Creating a simplified music mood classification ground-truth set","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602017&partnerID=40&md5=12b70bd3c0609b588a510f145e509183","International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Hu X., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Bay M., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","A standardized mood classification testbed is needed for formal cross-algorithm comparison and evaluation. In this poster, we present a simplification of the problems associated with developing a ground-truth set for the evaluation of mood-based Music Information Retrieval (MIR) systems. Using a dataset derived from Last.fm tags and the USPOP audio collection, we have applied a K-means clustering method to create a simple yet meaningful cluster-based set of high-level mood categories as well as a ground-truth dataset. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Cluster-based; Ground-truth dataset; K-means clustering method; Last.fm; Music information retrieval; Cluster analysis","X. Hu; International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; email: xiaohu@uiuc.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Peeters G.","Peeters, Geoffroy (22433836000)","22433836000","Sequence representation of music structure using higher-order similarity matrix and maximum-likelihood approach","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873584066&partnerID=40&md5=1fa4beab9b5e2c789a5e179d68a0cde6","CNRS STMS, 75004 Paris, 1, pl. Igor Stranvinsky, France","Peeters G., CNRS STMS, 75004 Paris, 1, pl. Igor Stranvinsky, France","In this paper, we present a novel method for the automatic estimation of the structure of music tracks using a sequence representation. A set of timbre-related (MFCC and Spectral Contrast) and pitch-related (Pitch Class Profile) features are first extracted from the signal leading to three similarity matrices which are then combined. We then introduce the use of higher-order (2nd and 3rd order) similarity matrices in order to reinforce the diagonals corresponding to common repetitions and reduce the background noise. Segments are then detected and a maximum-likelihood approach is proposed in order to derive simultaneously the underlying sequence representation of the music track and the most representative segment of each sequence. The proposed method is evaluated positively on the MPEG-7 ""melody repetition"" test set. ©2007 Austrian Computer Society (OCG).","","Motion Picture Experts Group standards; Automatic estimation; Background noise; Higher-order; Maximum-likelihood approach; Music structures; Similarity matrix; Test sets; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Barbedo J.G.A.; Lopes A.; Wolfe P.J.","Barbedo, Jayme Garcia Arnal (9279436800); Lopes, Amauri (57196887128); Wolfe, Patrick J. (7103083673)","9279436800; 57196887128; 7103083673","High time-resolution estimation of multiple fundamental frequencies","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957728966&partnerID=40&md5=154992ca13a308fc4406a37d51fac8c2","School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States; School of Electrical and Computer Engineering, State University of Campinas Cidade Universitária Zeferino Vaz, Campinas, SP, C.P. 6101, CEP: 13083-970, Brazil","Barbedo J.G.A., School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States, School of Electrical and Computer Engineering, State University of Campinas Cidade Universitária Zeferino Vaz, Campinas, SP, C.P. 6101, CEP: 13083-970, Brazil; Lopes A., School of Electrical and Computer Engineering, State University of Campinas Cidade Universitária Zeferino Vaz, Campinas, SP, C.P. 6101, CEP: 13083-970, Brazil; Wolfe P.J., School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States","This paper presents a high time-resolution strategy to estimate multiple fundamental frequencies in musical signals. The signal is first divided into overlapping blocks, and a high-resolution estimate made of the short-term spectrum. The resulting spectrum is modified such that only the most relevant spectral components are considered, and an iterative algorithm based on earlier work by Klapuri is used to identify candidate fundamental frequencies. Finally, a context-based rule is used to improve the accuracy of fundamental frequency estimates. The performance of this technique is investigated under both noiseless and noisy conditions, and its accuracy is examined in cases where the polyphony is known and unknown a priori. ©2007 Austrian Computer Society (OCG).","","Algorithms; Estimation; Information retrieval; Signal processing; Context-based rules; Fundamental frequencies; High resolution; Iterative algorithm; Musical signals; Noisy conditions; Short-term spectrum; Spectral components; Time-resolution; Natural frequencies","J.G.A. Barbedo; School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138-2901, 33 Oxford Street, United States; email: jbarbedo@seas.harvard.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Hu X.; Downie J.S.","Hu, Xiao (55496358400); Downie, J. Stephen (7102932568)","55496358400; 7102932568","Exploring mood metadata: Relationships with genre, artist and usage metadata","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","81","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596722&partnerID=40&md5=cf8bb2a806ae4857dad1a1da75ef7650","International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States","Hu X., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Downie J.S., International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States","There is a growing interest in developing and then evaluating Music Information Retrieval (MIR) systems that can provide automated access to the mood dimension of music. Mood as a music access feature, however, is not well understood in that the terms used to describe it are not standardized and their application can be highly idiosyncratic. To better understand how we might develop methods for comprehensively developing and formally evaluating useful automated mood access techniques, we explore the relationships that mood has with genre, artist and usage metadata. Statistical analyses of term interactions across three metadata collections (AllMusicGuide.com, epinions.com and Last.fm) reveal important consistencies within the genre-mood and artist-mood relationships. These consistencies lead us to recommend a cluster-based approach that overcomes specific term-related problems by creating a relatively small set of data-derived ""mood spaces"" that could form the ground-truth for a proposed MIREX ""Automated Mood Classification"" task. ©2007 Austrian Computer Society (OCG).","","Automation; Information retrieval; Access features; Access techniques; Cluster based approach; Last.fm; Music information retrieval; Metadata","X. Hu; International Music Information Retrieval Systems Evaluation Laboratory, Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; email: xiaohu@uiuc.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lebosse J.; Brun L.","Lebosse, Jerome (23397527500); Brun, Luc (56227707000)","23397527500; 56227707000","Audio fingerprint identification by approximate string matching","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593056&partnerID=40&md5=d588599cc72fb54e54f23c1a58bece84","France Telecom RandD, 14000 Caen, 42 rue des coutures, France; GREYC UMR 6072, 14050 Caen, 6 boulevard du Marchal Juin, France","Lebosse J., France Telecom RandD, 14000 Caen, 42 rue des coutures, France; Brun L., GREYC UMR 6072, 14050 Caen, 6 boulevard du Marchal Juin, France","An audio fingerprint is a small digest of an audio file which allows to identify it among a database of candidates. This paper first presents a fingerprint extraction algorithm. The identification task is performed by a new identification scheme which combines string matching algorithms and q-grams filtration. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Approximate string matching; Audio files; Audio fingerprint; Fingerprint extractions; Identification scheme; Q-grams; String matching algorithm; Pattern matching","J. Lebosse; France Telecom RandD, 14000 Caen, 42 rue des coutures, France; email: jerome.lebosse@orange-ftgroup.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Romming C.A.; Selfridge-Field E.","Romming, Christian André (57213844410); Selfridge-Field, Eleanor (6507573143)","57213844410; 6507573143","Algorithms for polyphonic music retrieval: The hausdorff metric and geometric hashing","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587765&partnerID=40&md5=93fe216502faf417e6c08cbe35616e60","Dept. of Computer Science, Stanford University, United States; Dept. of Music, Stanford University, United States","Romming C.A., Dept. of Computer Science, Stanford University, United States; Selfridge-Field E., Dept. of Music, Stanford University, United States","We consider two formulations of the computational problem of transposition-invariant, time-offset tolerant, meterinvariant, and time-scale invariant polyphonic music retrieval. We provide algorithms for both that are scalable in the sense that space requirements are asymptotically linear and queries are efficient for large databases of music. The focus is on cases where a query patternM consisting of m events is to be matched against a database N consisting of n events, and m ≪ n. The database is assumed to be polyphonic, and the algorithms support polyphonic queries. We are interested in finding exact and proximate occurrences of the query pattern. The first problem considered is that of finding the minimum directed Hausdorff distance from M to N. We give a (2 + ε)-approximation algorithm that solves this problem in O (nm) query time and O (n) space. The second problem is that of finding all maximal subset matches of M in N, and we give an algorithm that solves this problem in O (m3 (k + 1)) query time and O (w2n) space, where w represents the maximum window size and k is the number of matches. Using the same method, the problem can be solved in O(m(k + 1)) query time and O(wn) space if we do not require the time-scale invariance property. The latter query time is asymptotically optimal for the given problem. ©2007 Austrian Computer Society (OCG).","","Approximation algorithms; Information retrieval; Query processing; Asymptotically linear; Asymptotically optimal; Computational problem; Geometric hashing; Hausdorff distance; Hausdorff metric; Large database; Polyphonic music; Query patterns; Query time; Space requirements; Time-scales; Window Size; Query languages","C.A. Romming; Dept. of Computer Science, Stanford University, United States; email: romming@cs.stanford.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Paulus J.; Klapuri A.","Paulus, Jouni (16068963200); Klapuri, Anssi (6602945099)","16068963200; 6602945099","Music structure analysis using a probabilistic fitness measure and an integrated musicological model","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149101230&partnerID=40&md5=1e6eed10ff330505ac0da729124c8e18","Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","Paulus J., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland; Klapuri A., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","This paper presents a system for recovering the sectional form of a musical piece: segmentation and labelling of musical parts such as chorus or verse. The system uses three types of acoustic features: mel-frequency cepstral coefficients, chroma, and rhythmogram. An analysed piece is first subdivided into a large amount of potential segments. The distance between each two segments is then calculated and the value is transformed to a probability that the two segments are occurrences of a same musical part. Different features are combined in the probability space and are used to define a fitness measure for a candidate structure description. Musicological knowledge of the temporal dependencies between the parts is integrated into the fitness measure. A novel search algorithm is presented for finding the description that maximises the fitness measure. The system is evaluated with a data set of 557 manually annotated popular music pieces. The results suggest that integrating the musicological model to the fitness measure leads to a more reliable labelling of the parts than performing the labelling as a post-processing step.","","Information retrieval; Acoustic features; Data sets; Fitness measures; Mel-frequency cepstral coefficients; Music structure analysis; Musical pieces; Popular music; Post processing; Probability spaces; Rhythmogram; Search Algorithms; System use; Health","J. Paulus; Institute of Signal Processing, Tampere University of Technology, Tampere, Finland; email: jouni.paulus@tut.fi","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Raimond Y.; Abdallah S.; Sandler M.; Giasson F.","Raimond, Yves (23135967900); Abdallah, Samer (11540522000); Sandler, Mark (7202740804); Giasson, Frederick (55586669300)","23135967900; 11540522000; 7202740804; 55586669300","The music ontology","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","272","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596917&partnerID=40&md5=e4f05178a2fc7a53d37d4711d8a3e8f0","Centre for Digital Music, University of London Queen Mary, United Kingdom; Zitgist LLC, United States","Raimond Y., Centre for Digital Music, University of London Queen Mary, United Kingdom; Abdallah S., Centre for Digital Music, University of London Queen Mary, United Kingdom; Sandler M., Centre for Digital Music, University of London Queen Mary, United Kingdom; Giasson F., Zitgist LLC, United States","In this paper, we overview some Semantic Web technologies and describe the Music Ontology: a formal framework for dealing with music-related information on the Semantic Web, including editorial, cultural and acoustic information. We detail how this ontology can act as a grounding for more domain-specific knowledge representation. In addition, we describe current projects involving the Music Ontology and interlinked repositories of musicrelated knowledge. ©2007 Austrian Computer Society (OCG).","","Knowledge representation; Acoustic information; Current projects; Domain-specific knowledge; Formal framework; Music ontology; Semantic Web technology; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Betser M.; Collen P.; Rault J.-B.","Betser, Michaël (23570581800); Collen, Patrice (23570791300); Rault, Jean-Bernard (7005804024)","23570581800; 23570791300; 7005804024","Audio identification using sinusoidal modeling and application to jingle detection","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606846&partnerID=40&md5=2a869060681d2fd62739a54922004752","France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France","Betser M., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France; Collen P., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France; Rault J.-B., France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France","This article presents a new descriptor dedicated to Audio Identification (audioID), based on sinusoidal modeling. The core idea is an appropriate selection of the sinusoidal components of the signal to be detected. This new descriptor is robust against usual distortions found in audioID tasks. It has several advantages compared to classical subband-based descriptors including an increased robustness to additive noise, especially non-random noise such as additional speech, and a robust detection of short audio events. This descriptor is compared to a classical subband-based feature for a jingle detection task on broadcast radio. It is shown that the new introduced descriptor greatly improves the performance in terms of recall/precision. ©2007 Austrian Computer Society (OCG).","","Audio acoustics; Information retrieval; Audio events; Audio identification; Descriptors; Detection tasks; Robust detection; Sinusoidal components; Sinusoidal modeling; Acoustic noise","M. Betser; France Télécom RandD, 35510 Cesson-Sévigné, 4 rue du clos courtel, France; email: betser.michael@orange-ftgroup.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Wright M.; Schloss W.A.; Tzanetakis G.","Wright, Matthew (58833893000); Schloss, W. Andrew (6506579732); Tzanetakis, George (6602262192)","58833893000; 6506579732; 6602262192","Analyzing Afro-Cuban rhythm using rotation-aware clave template matching with dynamic programming","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949113360&partnerID=40&md5=9ba58e7bca15c82f9e8e4ff8a800fd58","University of Victoria, Computer Science and Music Departments, Canada","Wright M., University of Victoria, Computer Science and Music Departments, Canada; Schloss W.A., University of Victoria, Computer Science and Music Departments, Canada; Tzanetakis G., University of Victoria, Computer Science and Music Departments, Canada","The majority of existing research in Music Information Retrieval (MIR) has focused on either popular or classical music and frequently makes assumptions that do not generalize to other music cultures. We use the term Computational Eth-nomusicology (CE) to describe the use of computer tools to assist the analysis and understanding of musics from around the world. Although existing MIR techniques can serve as a good starting point for CE, the design of effective tools can benefit from incorporating domain-specific knowledge about the musical style and culture of interest. In this paper we describe our realization of this approach in the context of studying Afro-Cuban rhythm. More specifically we show how computer analysis can help us characterize and appreciate the complexities of tracking tempo and analyzing micro-timing in these particular music styles. A novel template-based method for tempo tracking in rhythmically complex Afro-Cuban music is proposed. Although our approach is domain-specific, we believe that the concepts and ideas used could also be used for studying other music cultures after some adaptation.","","Template matching; Computer analysis; Computer tools; Domain specific; Domain-specific knowledge; Effective tool; Music information retrieval; Template-based method; Tempo tracking; Information retrieval","M. Wright; University of Victoria, Computer Science and Music Departments, Canada; email: mattwrig@uvic.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Kasimi A.A.; Nichols E.; Raphael C.","Kasimi, Alia Al (6505453118); Nichols, Eric (57196921182); Raphael, Christopher (7004214964)","6505453118; 57196921182; 7004214964","A simple algorithm f generation of polyphonic piano fingerings","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863252675&partnerID=40&md5=44ef7c3996c9a523698352562089bad8","Dept. of Computer Science, Indiana Univ., United States; School of Informatics, Indiana Univ., United States","Kasimi A.A., Dept. of Computer Science, Indiana Univ., United States; Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a novel method for assigning fingers to notes in a polyphonic piano score. Such a mapping (called a ""fingering"") is of great use to performers. To accommodate performers' unique hand shapes and sizes, our method relies on a simple, user-adjustable cost function. We use dynamic programming to search the space of all possible fingerings for the optimal fingering under this cost function. Despite the simplicity of the algorithm we achieve reasonable and useful results. ©2007 Austrian Computer Society (OCG).","","Cost functions; Information retrieval; Hand shape; SIMPLE algorithm; Algorithms","A.A. Kasimi; Dept. of Computer Science, Indiana Univ., United States; email: alia.alkasimi@gmail.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Volk A.; Garbers J.; Van Kranenburg P.; Franswiering; Veltkamp R.C.; Grijp L.P.","Volk, Anja (30567849900); Garbers, J̈org (35107028500); Van Kranenburg, Peter (35108158000); Franswiering (55585883700); Veltkamp, Remco C. (7003421646); Grijp, Louis P. (28067929100)","30567849900; 35107028500; 35108158000; 55585883700; 7003421646; 28067929100","Applying rhythmic similarity based on inner metric analysis to folksong research","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873573999&partnerID=40&md5=06af5bc947f390785e4c7b57a31aad27","Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Meertens Institute, Amsterdam, Netherlands","Volk A., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Garbers J., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Franswiering, Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Grijp L.P., Meertens Institute, Amsterdam, Netherlands","In this paper we investigate the role of rhythmic similarity as part of melodic similarity in the context of Folksong research. We define a rhythmic similarity measure based on Inner Metric Analysis and apply it to groups of similar melodies. The comparison with a similarity measure of the SIMILE software shows that the two models agree on the number of melodies that are considered very similar, but disagree on the less similar melodies. In general, we achieve good results with the retrieval of melodies using rhythmic information, which demonstrates that rhythmic similarity is an important factor to consider in melodic similarity. ©2007 Austrian Computer Society (OCG).","","Melodic similarity; Metric analysis; Rhythmic similarity measures; Similarity measure; Information retrieval","A. Volk; Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; email: volk@cs.uu.nl","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lidy T.; Rauber A.; Pertusa A.; Iñesta J.M.","Lidy, Thomas (23035315800); Rauber, Andreas (57074846700); Pertusa, Antonio (8540257800); Iñesta, José Manuel (6701387099)","23035315800; 57074846700; 8540257800; 6701387099","Improving genre classification by combination of audio and symbolic descriptors using a transcription system","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","59","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873596118&partnerID=40&md5=4610477846f494257dc309f5cae9d0d8","Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Departamento de Lenguajes Y Sistemas Inforḿaticos, University of Alicante, Spain","Lidy T., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Rauber A., Department of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Pertusa A., Departamento de Lenguajes Y Sistemas Inforḿaticos, University of Alicante, Spain; Iñesta J.M., Departamento de Lenguajes Y Sistemas Inforḿaticos, University of Alicante, Spain","Recent research in music genre classification hints at a glass ceiling being reached using timbral audio features. To overcome this, the combination of multiple different feature sets bearing diverse characteristics is needed. We propose a new approach to extend the scope of the features: We transcribe audio data into a symbolic form using a transcription system, extract symbolic descriptors from that representation and combine them with audio features. With this method, we are able to surpass the glass ceiling and to further improve music genre classification, as shown in the experiments through three reference music databases and comparison to previously published performance results. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio data; Audio features; Descriptors; Feature sets; Genre classification; Glass ceiling; Music database; Music genre classification; New approaches; Recent researches; Glass","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Kuuskankare M.; Laurson M.","Kuuskankare, Mika (56301483200); Laurson, Mikael (15519696600)","56301483200; 15519696600","VIVO - Visualizing harmonic progressions and voice-leading in PWGL","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873579210&partnerID=40&md5=02874aed12520ca9d69295a8a9771bcd","CMT, Sibelius Academy, Finland","Kuuskankare M., CMT, Sibelius Academy, Finland; Laurson M., CMT, Sibelius Academy, Finland","This paper describes a novel tool called VIVO (VIsual VOice-leading) that allows to visually define harmonic progressions and voice-leading rules. VIVO comprises of a compiler and a collection of specialized visualization devices. VIVO takes advantage of several music related applications collected under the umbrella of PWGL (PWGL is a free cross-platform visual programming language for music and sound related applications). Our music notation application-Expressive Notation Package or ENP-is used here to build the user-interface used to visually define harmony and voice-leading rules. These visualizations are converted to textual rules by the VIVO compiler. Finally, our rule-based compositional system, PWGLConstraints, is used generate the final musical output using these rules. ©2007 Austrian Computer Society (OCG).","","Program compilers; User interfaces; Visual languages; Visualization; Cross-platform; Music notation; Rule based; Visual programming languages; Visualization devices; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Li B.; Leon S.D.; Fujinaga I.","Li, Beinan (22980588500); Leon, Simon De (55586299100); Fujinaga, Ichiro (9038140900)","22980588500; 55586299100; 9038140900","Alternative digitization approach for stereo phonograph records using optical audio reconstruction","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873600618&partnerID=40&md5=45ca36106864739e736ca89f0173f30e","Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada","Li B., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada; Leon S.D., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada; Fujinaga I., Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada","This paper presents the first Optical Audio Reconstruction (OAR) approach for the long-term digital preservation of stereo phonograph records. OAR uses precision metrology and digital image processing to obtain and convert groove contour data into digital audio for access and preservation. This contactless and imaging-based approach has considerable advantages over the traditional mechanical methods, such as being the only optical method with the potential to restore broken stereo records. Although past efforts on monophonic phonograph records have been successful, no attempts on 33rpm long-playing stereo records (LPs) have been reported. By using a white-light interferometry optical profiler, we are able to extract stereo audio information encoded in the 3D profile of the phonograph record grooves. ©2007 Austrian Computer Society (OCG).","","Data handling; Digital storage; Image processing; Information retrieval; 3-d profiles; Contact less; Contour datum; Digital audio; Digital preservation; Mechanical methods; Optical methods; Optical profiler; Precision metrology; Stereo audio; White-light interferometry; Phonograph records","B. Li; Schulich School of Music, Music Technology Area, McGill University, Montreal QC, Canada; email: beinan.li@mail.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Bergeron M.; Conklin D.","Bergeron, Mathieu (23992413700); Conklin, Darrell (57220096325)","23992413700; 57220096325","Structured polyphonic patterns","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649914202&partnerID=40&md5=f186b41d232fc0887df5e39487d379a2","Department of Computing, City University London, United Kingdom","Bergeron M., Department of Computing, City University London, United Kingdom; Conklin D., Department of Computing, City University London, United Kingdom","This paper presents a new approach to polyphonic music retrieval, based on a structured pattern representation. Polyphonic patterns are formed by joining and layering pattern components into sequences and simultaneities. Pattern components are conjunctions of features which encode event properties or relations with other events. Relations between events that overlap in time but are not simultaneous are supported, enabling patterns to express many of the temporal relations encountered in polyphonic music. The approach also provides a mechanism for defining new features. It is illustrated and evaluated by querying for three musicological patterns in a corpus of 185 chorale harmonizations by J.S. Bach.","","Polyphonic music; Structured patterns; Temporal relation; Information retrieval","M. Bergeron; Department of Computing, City University London, United Kingdom; email: bergeron@soi.city.ac.uk","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Burgoyne J.A.; Pugin L.; Eustace G.; Fujinaga I.","Burgoyne, John Ashley (23007865600); Pugin, Laurent (23009752900); Eustace, Greg (57213911686); Fujinaga, Ichiro (9038140900)","23007865600; 23009752900; 57213911686; 9038140900","A comparative survey of image binarisation algorithms for optical recognition on degraded musical sources","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549252338&partnerID=40&md5=3b7b92dc5cab0669cdd90ea65cc66de4","Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Eustace G., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Binarisation of greyscale images is a critical step in optical music recognition (OMR) preprocessing. Binarising music documents is particularly challenging because of the nature of music notation, even more so when the sources are degraded, e.g., with ink bleed-through from the other side of the page. This paper presents a comparative evaluation of 25 binarisation algorithms tested on a set of 100 music pages. A real-world OMR infrastructure for early music (Aruspix) was used to perform an objective, goaldirected evaluation of the algorithms' performance. Our results differ significantly from the ones obtained in studies on non-music documents, which highlights the importance of developing tools specific to our community. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Binarisation; Bleed-through; Comparative evaluations; Critical steps; Early musics; Goal-directed; Grey scale images; Music notation; Optical music recognition; Optical recognition; Real-world; Algorithms","J.A. Burgoyne; Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; email: ashley@music.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Chordia P.; Godfrey M.; Rae A.","Chordia, Parag (24723695700); Godfrey, Mark (24724011900); Rae, Alex (24725486000)","24723695700; 24724011900; 24725486000","Extending content-based recommendation: The case of Indian classical music","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949096247&partnerID=40&md5=2832ca21ab6ec2c0d7196da0b3e62954","Georgia Tech., United States","Chordia P., Georgia Tech., United States; Godfrey M., Georgia Tech., United States; Rae A., Georgia Tech., United States","We describe a series of experiments that attempt to create a content-based similarity model suitable for making recommendations about North Indian classical music (NICM). We introduce a dataset (nicm2008) consisting of 897 tracks of NICM along with substantial ground-truth annotations, including artist, predominant instrument, tonic pitch, raag, and parent scale (thaat). Using a timbre-based similarity model derived from short-time MFCCs we find that artist R-precision is 32.69% and that the predominant instrument is correctly classified 90.30% of the time. Consistent with previous work, we find that certain tracks (""hubs"") appear falsely similar to many other tracks. We find that this problem can be attenuated by model homogenization. We also introduce the use of pitch-class distribution (PCD) features to measure melodic similarity. Its effectiveness is evaluated by raag R-precision (16.97%), thaat classification accuracy (75.83%), and comparison to reference similarity metrics. We propose that a hybrid timbral-melodic similarity model may be effective for Indian classical music recommendation. Further, this work suggests that ""hubs"" are a general features of such similarity modeling that may be partially alleviated by model homogenization.","","Classification accuracy; Content-based; Content-based recommendation; Data sets; Indian classical music; Melodic similarity; Similarity metrics; Similarity models; Information retrieval","P. Chordia; Georgia Tech., United States; email: ppc@gatech.edu","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Thul E.; Toussaint G.T.","Thul, Eric (25723896600); Toussaint, Godfried T. (7006305131)","25723896600; 7006305131","Rhythm complexity measures: A comparison of mathematical models of human perception and performance","2008","ISMIR 2008 - 9th International Conference on Music Information Retrieval","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149155605&partnerID=40&md5=51861b5cd9b11053250dd102f4288750","School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada","Thul E., School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada; Toussaint G.T., School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada","Thirty two measures of rhythm complexity are compared using three widely different rhythm data sets. Twenty-two of these measures have been investigated in a limited context in the past, and ten new measures are explored here. Some of these measures are mathematically inspired, some were designed to measure syncopation, some were intended to predict various measures of human performance, some are based on constructs from music theory, such as Pressing's cognitive complexity, and others are direct measures of different aspects of human performance, such as perceptual complexity, meter complexity, and performance complexity. In each data set the rhythms are ranked either according to increasing complexity using the judgements of human subjects, or using calculations with the computational models. Spearman rank correlation coefficients are computed between all pairs of rhythm rankings. Then phylogenetic trees are used to visualize and cluster the correlation coefficients. Among the many conclusions evident from the results, there are several observations common to all three data sets that are worthy of note. The syncopation measures form a tight cluster far from other clusters. The human performance measures fall in the same cluster as the syncopation measures. The complexity measures based on statistical properties of the inter-onset-interval histograms are poor predictors of syncopation or human performance complexity. Finally, this research suggests several open problems.","","Information retrieval; Mathematical models; Cognitive complexity; Complexity measures; Computational model; Correlation coefficient; Data sets; Direct measures; Human perception; Human performance; Human subjects; Music theory; Phylogenetic trees; Spearman rank correlation; Statistical properties; Statistical methods","E. Thul; School of Computer Science, Schulich School of Music, McGill University, Montréal, Canada; email: ethul@cs.mcgill.ca","","9th International Conference on Music Information Retrieval, ISMIR 2008","14 September 2008 through 18 September 2008","Philadelphia, PA","95394"
"Paulus J.; Klapuri A.","Paulus, Jouni (16068963200); Klapuri, Anssi (6602945099)","16068963200; 6602945099","Combining temporal and spectral features in HMM-Based Drum transcription","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863405897&partnerID=40&md5=ea9c086858ac3110adfce37e9407ec34","Institute of Signal Processing Tampere, University of Technology, Finland","Paulus J., Institute of Signal Processing Tampere, University of Technology, Finland; Klapuri A., Institute of Signal Processing Tampere, University of Technology, Finland","To date several methods for transcribing drums from polyphonic music have been published. Majority of the features used in the transcription systems are ""spectral"": parameterising some property of the signal spectrum in a relatively short time frames. It has been shown that utilising narrow-band features describing long-term temporal evolution in conjunction with the more traditional features can improve the overall performance in speech recognition. We investigate similar utilisation of temporal features in addition to the HMM baseline. The effect of the proposed extension is evaluated with simulations on acoustic data, and the results suggest that temporal features do improve the result slightly. Demonstrational signals of the transcription results are available at http://www.cs.tut.fi/sgn/arg/paulus/demo/. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Speech recognition; Acoustic data; HMM-based; Narrow bands; Polyphonic music; Short time frames; Signal spectrum; Spectral feature; Temporal evolution; Temporal features; Transcription","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Gruhne M.; Schmidt K.; Dittmar C.","Gruhne, Matthias (24461733300); Schmidt, Konstantin (55586274900); Dittmar, Christian (15051598000)","24461733300; 55586274900; 15051598000","Phoneme recognition in popular music","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581013&partnerID=40&md5=806ab14d0c5b0608626a087a2c2c1a34","Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany","Gruhne M., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Schmidt K., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Dittmar C., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany","Automatic lyrics synchronization for karaoke applications is a major challenge in the field of music information retrieval. An important pre-requisite in order to precisely synchronize the music and corresponding text is the detection of single phonemes in the vocal part of polyphonic music. This paper describes a system, which detects the phonemes based on a state-of-the-art audio information retrieval system with harmonics extraction and synthesizing as pre-processing method. The extraction algorithm is based on common speech recognition low-level features, such as MFCC and LPC. In order to distinguish phonemes, three different classification techniques (SVM, GMM and MLP) have been used and their results are depicted in the paper. ©2007 Austrian Computer Society (OCG).","","Speech recognition; Audio information retrievals; Classification technique; Extraction algorithms; Harmonics extraction; Karaoke; Low-level features; Music information retrieval; Phoneme recognition; Polyphonic music; Popular music; Pre-processing method; Extraction","M. Gruhne; Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; email: fghe@idmt.fraunhofer.de","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"De Léon P.J.P.; Rizo D.; Ñesta J.M.I.","De Léon, Pedro J. Ponce (14042292100); Rizo, David (14042169500); Ñesta, Jośe M. I (55585922800)","14042292100; 14042169500; 55585922800","Towards a human-friendly melody characterization by automatically induced rules","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873602996&partnerID=40&md5=93d0ffde866aa99e1f8fa6b383116862","Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain","De Léon P.J.P., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain; Rizo D., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain; Ñesta J.M.I., Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain","There is an increasing interest in music information retrieval for reference, motive, or thumbnail extraction from a piece in order to have a compact and representative representation of the information to be retrieved. One of the main references for music is its melody. In a practical environment of symbolic format collections the information can be found in standard MIDI file format, structured as a number of tracks, usually one of them containing the melodic line, while the others contain the accompaniment. The goal of this work is to analyse how statistical rules can be used to characterize a melody in such a way that one can understand the solution of an automatic system for selecting the track containing the melody in such files. ©2007 Austrian Computer Society (OCG).","","Automatic systems; Human-friendly; MIDI files; Music information retrieval; Statistical rules; Information retrieval","P.J.P. De Léon; Departamento de Lenguajes Y Sistemas Informáticos, Universidad de Alicante, Spain; email: pierre@dlsi.ua.es","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Proutskova P.","Proutskova, Polina (55586627700)","55586627700","Musical memory of the world - Data infrastructure in ethnomusicological archives","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873589894&partnerID=40&md5=98640079fa3b8c8cd0261e1c9805112b","Computing department, Goldsmiths, University of London, United Kingdom","Proutskova P., Computing department, Goldsmiths, University of London, United Kingdom","Ethnomusicological archives build the musical memory of the world, covering the geographical and the historical aspects of music worldwide. This article gives a brief description of the nature and the functionality of ethnomusicological archives. It reflects the current state of data infrastructure (policy and technology), addressing issues of access to archives' holdings, of online visibility of music collections and of interoperability between archives. An outlook of a mutual involvement and a resulting influence at each others work between MIR community and ethnomusicological archives is given. ©2007 Austrian Computer Society (OCG).","","Data infrastructure; Music collection; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Schedl M.; Gerhardwidmer; Pohle T.; Seyerlehner K.","Schedl, Markus (8684865900); Gerhardwidmer (55585962700); Pohle, Tim (14036302300); Seyerlehner, Klaus (23996001400)","8684865900; 55585962700; 14036302300; 23996001400","Web-Based detection of music band members and line-up","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873606799&partnerID=40&md5=139ab51a322bd4d3b440f67396318185","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Gerhardwidmer, Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Seyerlehner K., Department of Computational Perception, Johannes Kepler University, Linz, Austria","We present first steps towards the automatic detection of music band members and instrumentation using web content mining techniques. To this end, we combine a named entity detection method with rule-based linguistic text analysis. We report on preliminary evaluation results and discuss limitations of the current method. ©2007 Austrian Computer Society (OCG).","","Automatic Detection; Evaluation results; Named entities; Rule based; Text analysis; Web content mining; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pikrakis A.; Theodoridis S.","Pikrakis, Aggelos (6507232714); Theodoridis, Sergios (7004236721)","6507232714; 7004236721","An application of empirical mode decomposition on tempo induction from music recordings","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856462785&partnerID=40&md5=094c8a48557ab69f39d2f571beea7997","Dept. of Informatics and Telecommunications, University of Athens, Greece","Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, Greece","This paper presents an application of Empirical Mode Decomposition (EMD) on the induction of notated tempo from music recordings. At a first stage, EMD is employed as a means to segment music recordings into segments that exhibit similar rhythmic characteristics. At a second stage, EMD is used in order to analyze the diagonals of the Self-Similarity Matrix of each segment, so as to estimate the tempo of the recording. The proposed method has been employed on various music genres with music meters of 2/4 , 3/4 and 4/4 . Tempo has been assumed to remain approximately constant throughout each recording, ranging from 60bpm up to 220bpm. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Signal processing; Empirical Mode Decomposition; Music genre; Music recording; Self-similarity matrix; Audio recordings","A. Pikrakis; Dept. of Informatics and Telecommunications, University of Athens, Greece; email: pikrakis@di.uoa.gr","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Cont A.; Schwarz D.; Schnell N.; Raphael C.","Cont, Arshia (12344985300); Schwarz, Diemo (7102731246); Schnell, Norbert (12344577800); Raphael, Christopher (7004214964)","12344985300; 7102731246; 12344577800; 7004214964","Evaluation of real-time audio-to-score alignment","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","48","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873578650&partnerID=40&md5=93c4a0507c2b864031df6d513bf7299f","Ircam UMR CNRS 9912, CRCA, UCSD, France; Ircam-Centre Pompidou Paris, UMR CNRS 9912, France; Indiana University, Bloomington, IN, United States","Cont A., Ircam UMR CNRS 9912, CRCA, UCSD, France; Schwarz D., Ircam-Centre Pompidou Paris, UMR CNRS 9912, France; Schnell N., Ircam-Centre Pompidou Paris, UMR CNRS 9912, France; Raphael C., Indiana University, Bloomington, IN, United States","This article explains evaluation methods for real-time audio to score alignment, or score following, that allow for the quantitative assessment of the robustness and preciseness of an algorithm. The published ground truth data base and the evaluation framework, including file formats for the score and the reference alignments, are presented. The work, started forMIREX 2006, is meant as a first step towards a standardized evaluation process contributing to the exchange and progress in this field. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Evaluation framework; Evaluation methods; File formats; Ground truth data; Quantitative assessments; Real-time audio; Score-following; Alignment","A. Cont; Ircam UMR CNRS 9912, CRCA, UCSD, France; email: cont@ircam.fr","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Dressler K.; Streich S.","Dressler, Karin (54390973100); Streich, Sebastian (25423069500)","54390973100; 25423069500","Tuning frequency estimation using circular statistics","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955836763&partnerID=40&md5=83cd79c4771cfe794950c44897cd0355","Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Music Technology Group, Pompeu Fabra University, Barcelona, Spain","Dressler K., Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; Streich S., Music Technology Group, Pompeu Fabra University, Barcelona, Spain","In this document a new approach on tuning frequency estimation based on circular statistics is presented. Two methods are introduced: the calculation of the tuning frequency over an entire audio piece, and the estimation of an adapting reference frequency for a single voice. The results for the tuning frequency estimation look very good for audio pieces where the dominant voices are tuned close to the equal-temperament scale and exhibit only moderate frequency dynamics. For the analysis of popular western music, the method does not achieve very robust results due to the strong frequency dynamics of the human singing voice. Nevertheless, the method could be improved by excluding the singing voice from the calculation taking only the accompaniment into account. The main advantage of the proposed method lies especially in the easy computation of an adaptive reference frequency using an exponential moving average. This adaptive reference can for example be used in the quantization of the singing voice into a note representation. ©2007 Austrian Computer Society (OCG).","","Dynamics; Information retrieval; Circular statistics; Exponential moving averages; Frequency dynamics; New approaches; Reference frequency; Singing voices; Tuning frequency; Frequency estimation","K. Dressler; Fraunhofer IDMT, 98693 Ilmenau, Langewiesener Str. 22, Germany; email: dresslkn@idmt.fraunhofer.de","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Klapuri A.","Klapuri, Anssi (6602945099)","6602945099","Multiple fundamental frequency estimation by summing harmonic amplitudes","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","136","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444806&partnerID=40&md5=1932db44a18aba8fdce7e44ff4bd0156","Institute of Signal Processing, Tampere University of Technology, 33720 Tampere, Korkeakoulunkatu 1, Finland","Klapuri A., Institute of Signal Processing, Tampere University of Technology, 33720 Tampere, Korkeakoulunkatu 1, Finland","This paper proposes a conceptually simple and computationally efficient fundamental frequency (F0) estimator for polyphonic music signals. The studied class of estimators calculate the salience, or strength, of a F0 candidate as a weighted sum of the amplitudes of its harmonic partials. A mapping from the Fourier spectrum to a ""F0 salience spectrum"" is found by optimization using generated training material. Based on the resulting function, three different estimators are proposed: a ""direct"" method, an iterative estimation and cancellation method, and a method that estimates multiple F0s jointly. The latter two performed as well as a considerably more complex reference method. The number of concurrent sounds is estimated along with their F0s. © 2006 University of Victoria.","F0 estimation; Music transcription; Pitch","Audio signal processing; Information retrieval; Iterative methods; Natural frequencies; Computationally efficient; Concurrent sounds; Fourier spectra; Fundamental frequencies; Fundamental frequency estimation; Harmonic amplitude; Iterative estimation; Music transcription; Pitch; Polyphonic music; Reference method; Training material; Weighted Sum; Estimation","A. Klapuri; Institute of Signal Processing, Tampere University of Technology, 33720 Tampere, Korkeakoulunkatu 1, Finland; email: anssi.klapuri@tut.fi","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Noland K.; Sandler M.","Noland, Katy (18042247700); Sandler, Mark (7202740804)","18042247700; 7202740804","Key estimation using a Hidden Markov Model","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","41","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873438765&partnerID=40&md5=aae51db651b332aa73d878b11fd56c4f","Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","Noland K., Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","A novel technique to estimate the predominant key in a musical excerpt is proposed. The key space is modelled by a 24-state Hidden Markov Model (HMM), where each state represents one of the 24 major and minor keys, and each observation represents a chord transition, or pair of consecutive chords. The use of chord transitions as the observations models a greater temporal dependency between consecutive chords than would observations of single chords. The key transition and chord emission probabilities are initialised using the results of perceptual tests in order to reflect the human expectation of harmonic relationships. HMM parameters are then trained on a per-song basis using handannotated chord symbols, before the model for each song is decoded to give the likelihood of each key at each time frame. Examples of the algorithm as a segmentation technique are given, and its capability to estimate the overall key of a song is evaluated using a data set of 110 Beatles songs, of which 91% were correctly classified. An extension to include operation from audio data instead of chord symbols is planned, which will enable application to general music retrieval purposes. © 2006 University of Victoria.","Chords; Harmony; HMM; Key estimation","Audio acoustics; Hidden Markov models; Information retrieval; Audio data; Chords; Data sets; Emission probabilities; Harmony; HMM; Music retrieval; Novel techniques; Perceptual test; Segmentation techniques; Time frame; Estimation","K. Noland; Centre for Digital Music, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; email: katy.noland@elec.qmul.ac.uk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Gillet O.; Richard G.","Gillet, Olivier (56616219800); Richard, Gaël (57195915952)","56616219800; 57195915952","ENST-Drums: An extensive audio-visual database for drum signals processing","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","70","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439973&partnerID=40&md5=ab18c35af260aa6e1b6c94bd0329785e","GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France","Gillet O., GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France; Richard G., GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France","One of the main bottlenecks in the progress of the Music Information Retrieval (MIR) research field is the limited access to common, large and annotated audio databases that could serve for technology development and/or evaluation. The aim of this paper is to present in detail the ENST-Drums database, emphasizing on both the content and the recording process. This audiovisual database of drum performances by three professional drummers was recorded on 8 audio channels and 2 video channels. The drum sequences are fully annotated and will be, for a large part, freely distributed for research purposes. The large variety in its content should serve research in various domains of audio signal processing involving drums, ranging from single drum event classification to complex multimodal drum track transcription and extraction from polyphonic music. © 2006 University of Victoria.","Automatic drum transcription; Drum event detection in polyphonic music; Multimodal music transcription; Research database; Source separation","Audio signal processing; Information retrieval; Research; Source separation; Transcription; Audio channels; Audio database; Audio-visual database; Event classification; Large parts; Multi-modal; Music information retrieval; Music transcription; Polyphonic music; Recording process; Research database; Research fields; Technology development; Video channels; Database systems","O. Gillet; GET / ENST, CNRS LTCI, 75014 Paris, 37 Rue Dareau, France; email: olivier.gillet@enst.fr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Eichner M.; Wolff M.; Hoffmann R.","Eichner, Matthias (7006804128); Wolff, Matthias (55869793600); Hoffmann, Rüdiger (7401622646)","7006804128; 55869793600; 7401622646","Instrument classification using Hidden Markov Models","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873430088&partnerID=40&md5=5221c68619da4ca05648fb25db233a24","Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany","Eichner M., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany; Wolff M., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany; Hoffmann R., Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany","In this paper we present first results on musical instrument classification using an HMM based recognizer. The final goal of our work is to automatically evaluate instruments and to classify them according to their characteristics. The first step in this direction was to train a system that is able to recognize a particular instrument among others of the same kind (e.g. guitars). The recognition is based on solo music pieces played on the instrument under various conditions. For this purpose a database was designed and is currently being recorded that comprises four instrument types: classical guitar, violin, trumpet and clarinet. We briefly describe the classifier and give first experimental results on the classification of acoustic guitars. © 2006 University of Victoria.","Automatic musical instrument recognition; Multimedia content description; Music content processing","Hidden Markov models; Information retrieval; Acoustic guitar; Classical guitar; Instrument recognition; Multimedia content description; Music contents; Musical instruments","M. Eichner; Technische Universität Dresden, Laboratory of Acoustics and Speech Communication, Germany; email: eichner@ias.et.tu-dresden.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Decoro C.; Barutcuoglu Z.; Fiebrink R.","Decoro, Christopher (10142295900); Barutcuoglu, Zafer (12794175200); Fiebrink, Rebecca (36095844900)","10142295900; 12794175200; 36095844900","Bayesian aggregation for hierarchical genre classification","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-74849106455&partnerID=40&md5=f25c902fc9a8ba1b3cffc5b652777856","Department of Computer Science, Princeton University, United States","Decoro C., Department of Computer Science, Princeton University, United States; Barutcuoglu Z., Department of Computer Science, Princeton University, United States; Fiebrink R., Department of Computer Science, Princeton University, United States","Hierarchical taxonomies of classes arise in the analysis of many types of musical information, including genre, as a means of organizing overlapping categories at varying levels of generality. However, incorporating hierarchical structure into conventional machine learning systems presents a challenge: the use of independent binary classifiers for each class in the hierarchy can produce hierarchically inconsistent predictions. That is, an example may be assigned to a class, and not assigned to the parent of that class. This paper applies a Bayesian framework to combine, or aggregate, a hierarchy of multiple binary classifiers in a principled manner, and consequently improves performance over the hierarchy as a whole. Furthermore, such an approach allows for an arbitrarily complex hierarchy, and does not suffer from classes that are too broad or too refined. Experiments on theMIREX 2005 symbolic genre classification dataset show that our Bayesian Aggregation algorithm provides significant improvement over independent classifiers, and demonstrates superior performance compared to previous work. Our method also improves similarity search by ranking songs by similarity of hierarchical predictions to those of a query song. ©2007 Austrian Computer Society (OCG).","","Forecasting; Information retrieval; Learning systems; Aggregation algorithms; Bayesian; Bayesian frameworks; Binary classifiers; Complex hierarchy; Conventional machines; Genre classification; Hierarchical structures; Hierarchical taxonomy; Independent classifiers; Musical information; Similarity search; Classification (of information)","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pampalk E.; Goto M.","Pampalk, Elias (6507297042); Goto, Masataka (7403505330)","6507297042; 7403505330","Musicsun: A new approach to artist recommendation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954634752&partnerID=40&md5=807c8f003d43852866b2acbf761978f6","National Institute of Advanced Industrial Science and Technology (AIST), IT AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","MusicSun is a graphical user interface to discover artists. Artists are recommended based on one or more artists selected by the user. The recommendations are computed by combining 3 different aspects of similarity. The users can change the impact of each of these aspects. In addition words are displayed which describe the artists selected by the user. The user can select one of these words to focus the search on a specific direction. In this paper we present the techniques used to compute the recommendations and the graphical user interface. Furthermore, we present the results of an evaluation with 33 users. We asked them, for example, to judge the usefulness of the different interface components and the quality of the recommendations. ©2007 Austrian Computer Society (OCG).","","Information retrieval; New approaches; Techniques used; Graphical user interfaces","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Leue I.; Izmirli O.","Leue, Ian (55582826500); Izmirli, Ozgur (6507754258)","55582826500; 6507754258","Tempo tracking with a periodicity comb kernel","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459191&partnerID=40&md5=946f5795737e4137291d8e46fe72b613","Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States","Leue I., Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States; Izmirli O., Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States","Automatic tempo extraction and beat tracking from audio is an important ability, with many applications in music information retrieval. This paper describes a method for tempo tracking which builds on current research in the field. In this algorithm, an autocorrelation surface is calculated from the output of a spectral energy flux onset novelty function. The most salient repetition rate is calculated by cross-correlating dilations of a comb-like prototype spanning multiple frames and the autocorrelation surface. The method addresses tempo tracking through time to account for pieces with variable tempos. In order to compare the performance of our method on music with strong and weak percussive onsets we have evaluated it on both classical music with and without percussion and popular music with percussion. Additionally, beats are phase-aligned and superimposed on the signal for aural evaluation. Results show the comb kernel to be a useful feature in determining the correct beat level. © 2006 University of Victoria.","Beat; Onset detection; Tempo tracking","Autocorrelation; Beat; Beat tracking; Comb-like; Multiple-frame; Music information retrieval; On currents; Onset detection; Popular music; Repetition rate; Spectral energy; Tempo tracking; Information retrieval","I. Leue; Center for Arts and Technology, Connecticut College, New London, CT, 270 Mohegan Ave., United States; email: ipleu@conncoll.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Garbers J.","Garbers, Jörg (35107028500)","35107028500","An integrated MIR programming and testing environment","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873420675&partnerID=40&md5=a87ac8c64acaedf69a9fb14bde17b068","Department of Computer and Information Sciences, Utrecht University, Netherlands","Garbers J., Department of Computer and Information Sciences, Utrecht University, Netherlands","The process of shaping a music information retrieval algorithm is highly connected with implementing it and testing suitable parameterizations. Often music information retrieval scientists do not have a programmer at hand and must implement their experimental setup themselves. This paper describes an integrated tool setup OHR consisting of the music (analysis) systems OpenMusic, Humdrum and Rubato and a system for form based parametrization and comparison of algorithms. These packages and their programming environments provide the scientist with frameworks and existing libraries for implementing and testing algorithms. They differ in the programming languages that they support and in the type of testing user interfaces that they allow the scientist to build easily. The systems and their components are integrated by using their scripting languages. We sketch an example of the integrated use of these systems. © 2006 University of Victoria.","Computational music analysis; Development environments; Scientific programming; Software integration","Algorithms; Computer programming; User interfaces; Development environment; Integrated tools; Music analysis; Music information retrieval; Parametrizations; Programming environment; Scientific programming; Scripting languages; Software integration; Testing algorithm; Testing environment; Information retrieval","J. Garbers; Department of Computer and Information Sciences, Utrecht University, Netherlands; email: garbers@cs.uu.nl","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Turnbull D.; Barrington L.; Lanckriet G.","Turnbull, Douglas (8380095700); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","8380095700; 14041197300; 7801431767","Modeling music and words using a multi-class naïve Bayes approach","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873439183&partnerID=40&md5=2ff68a3a8d9628349512397c936e99ac","UC San Diego, San Diego, CA 92093, United States","Turnbull D., UC San Diego, San Diego, CA 92093, United States; Barrington L., UC San Diego, San Diego, CA 92093, United States; Lanckriet G., UC San Diego, San Diego, CA 92093, United States","We propose a query-by-text system for modeling a heterogeneous data set of music and words. We quantitatively show that our system can both annotate a novel song with semantically meaningful words and retrieve relevant unla-beled songs from a database given a text-based query. We explain two feature extraction methods useful for summarizing the audio content of a song. We describe a supervised multi-class naïve Bayes model and compare two parameter estimation techniques. Our approach is influenced by recent computer vision research on the related tasks of image annotation and retrieval. © 2006 University of Victoria.","Heterogeneous data; Music annotation; Music retrieval; Query-by-text","Computer vision; Feature extraction; Information retrieval; Parameter estimation; Query processing; Audio content; Bayes approach; Bayes models; Estimation techniques; Feature extraction methods; Heterogeneous data; Image annotation; Multi-class; Music annotation; Music retrieval; Query-by-text; Text-based queries; Two parameter; Computer music","D. Turnbull; UC San Diego, San Diego, CA 92093, United States; email: dturnbul@cs.ucsd.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Skowronek J.; McKinney M.F.; Van De Par S.","Skowronek, Janto (15050961900); McKinney, Martin F. (57225339869); Van De Par, Steven (6701627290)","15050961900; 57225339869; 6701627290","Ground truth for automatic music mood classification","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476490&partnerID=40&md5=5098afabc72f46be93f82bcabce86ba5","Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands","Skowronek J., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands; McKinney M.F., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands; Van De Par S., Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands","Automatic music classification based on audio signals provides a core technology for tools that help users to manage and browse their music collections. Since ""mood"" is also used as a browsing criterium, automatic mood classification could support the creation of the necessary metadata. We have developed a method to obtain a reliable ""ground truth"" database for automatic music mood classification. Our results confirm that excerpt selection is a non-trivial issue and that there are some mood labels that are relatively consistent across subjects. © 2006 University of Victoria.","Ground truth; Music mood classification","Metadata; Audio signal; Core technology; Ground truth; Music classification; Music collection; Non-trivial; Information retrieval","J. Skowronek; Philips Research Laboratories, 5656 AE Eindhoven, Hightech Campus 36, Netherlands; email: janto.skowronek@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hoffman M.; Cook P.R.","Hoffman, Matt (17434675700); Cook, Perry R. (57203105418)","17434675700; 57203105418","Feature-based synthesis: A tool for evaluating, designing, and interacting with music IR systems","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448602&partnerID=40&md5=35975dbb56f530977891e9b80bf0eb97","Princeton University, Computer Science Department, Princeton, NJ 08544, 35 Olden Street, United States; Princeton University, Computer Science and Music Departments, Princeton, NJ 08544, 35 Olden Street, United States","Hoffman M., Princeton University, Computer Science Department, Princeton, NJ 08544, 35 Olden Street, United States; Cook P.R., Princeton University, Computer Science and Music Departments, Princeton, NJ 08544, 35 Olden Street, United States","We present a general framework for performing feature-based synthesis - that is, for producing audio characterized by arbitrarily specified sets of perceptually motivated, quantifiable acoustic features of the sort used in many music information retrieval systems. © 2006 University of Victoria.","","Information retrieval systems; Acoustic features; Feature-based; Music information retrieval; Audio acoustics","M. Hoffman; Princeton University, Computer Science Department, Princeton, NJ 08544, 35 Olden Street, United States; email: mdhoffma@cs.princeton.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Selfridge-Field E.","Selfridge-Field, Eleanor (6507573143)","6507573143","Social cognition and melodic persistence: Where metadata and content diverge","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873480101&partnerID=40&md5=4b973dfc835f0f287182747fa8418c34","CCARH, Braun Music Center 129, Stanford University, Stanford, CA 94305-3076, United States","Selfridge-Field E., CCARH, Braun Music Center 129, Stanford University, Stanford, CA 94305-3076, United States","The automatic retrieval of members of a tune family from a database of melodies is potentially complicated by well documented divergences between textual metadata and musical content. We examine recently reported cases of such divergences in search of musical features which persist even when titles change or the melodies themselves vary. We find that apart from meter and mode, the rate of preservation of searchable musical features is low. Social and gestural factors appear to play a varying role in establishing the ""melodic"" identity of widely transmitted songs. The rapid growth of social computing bring urgency to better understanding the different ways in which ""same"" or ""similar"" can be defined. © 2006 University of Victoria.","Melodic similarity; Musical features; Social cognition; Tune families","Information retrieval; Automatic retrieval; Melodic similarity; Musical features; Rapid growth; Social cognition; Social computing; Textual metadata; Tune families; Metadata","E. Selfridge-Field; CCARH, Braun Music Center 129, Stanford University, Stanford, CA 94305-3076, United States; email: esfield@stanford.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Sordo M.; Laurier C.; Celma Ò.","Sordo, Mohamed (43462170300); Laurier, Cyril (26031025000); Celma, Òscar (12804596800)","43462170300; 26031025000; 12804596800","Annotating music collections: Howcontent-based similarity helps to propagate labels","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049115166&partnerID=40&md5=315f6b84374a0587d373146aa6567f3c","Music Technology Group, Universitat Pompeu, Fabra, Spain","Sordo M., Music Technology Group, Universitat Pompeu, Fabra, Spain; Laurier C., Music Technology Group, Universitat Pompeu, Fabra, Spain; Celma Ò., Music Technology Group, Universitat Pompeu, Fabra, Spain","In this paper we present a way to annotate music collections by exploiting audio similarity. Similarity is used to propose labels (tags) to yet unlabeled songs, based on the content-based distance between them. The main goal of our work is to ease the process of annotating huge music collections, by using content-based similarity distances as a way to propagate labels among songs. We present two different experiments. The first one propagates labels that are related with the style of the piece, whereas the second experiment deals with mood labels. On the one hand, our approach shows that using a music collection annotated at 40% with styles, the collection can be automatically annotated up to 78% (that is, 40% already annotated and the rest, 38%, only using propagation), with a recall greater than 0.4. On the other hand, for a smaller music collection annotated at 30% with moods, the collection can be automatically annotated up to 65% (e.g. 30% plus 35% using propagation). ©2007 Austrian Computer Society (OCG).","","Experiments; Information retrieval; Audio similarities; Content-based; Music collection; Similarity distance; Audio recordings","M. Sordo; Music Technology Group, Universitat Pompeu, Fabra, Spain; email: msordo@iua.upf.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Flexer A.; Gouyon F.; Dixon S.; Widmer G.","Flexer, Arthur (7004555682); Gouyon, Fabien (8373002800); Dixon, Simon (57203056378); Widmer, Gerhard (7004342843)","7004555682; 8373002800; 57203056378; 7004342843","Probabilistic combination of features for music classification","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428796&partnerID=40&md5=cbb0cbafd056991eae3bd49c9f5da67f","Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Flexer A., Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Gouyon F., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Dixon S., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","We describe an approach to the combination of music similarity feature spaces in the context of music classification. The approach is based on taking the product of posterior probabilities obtained from separate classifiers for the different feature spaces. This allows for a different influence of the classifiers per song and an overall classification accuracy improving those resulting from individual feature spaces alone. This is demonstrated by combining spectral and rhythmic similarity for classification of ballroom dance music. © 2006 University of Victoria.","Combination; Music classification","Classification accuracy; Combination; Feature space; Music classification; Music similarity; Posterior probability; Information retrieval","A. Flexer; Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; email: arthur.flexer@meduniwien.ac.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Donaldson J.; Knopke I.","Donaldson, Justin (24070691000); Knopke, Ian (24381706300)","24070691000; 24381706300","Music recommendation mapping and interface based on structural network entropy","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62249195186&partnerID=40&md5=b23b2d81c4bfb5f2d250026fb38ac4ed","School of Informatics, Human Computer Interaction/Design, Indiana University, United States; School of Informatics Music Informatics, Indiana University, United States","Donaldson J., School of Informatics, Human Computer Interaction/Design, Indiana University, United States; Knopke I., School of Informatics Music Informatics, Indiana University, United States","Recommendation systems generally produce the results of their output to their users in the form of an ordinal list. In the interest of simplicity, these lists are often obscure, abstract, or omit many relevant metrics pertaining to the measured strength of the recommendations or the relationships the recommended items share with each other. This information is often useful for coming to a better understanding of the nature of how the items are structured according to the recommendation data. This paper describes the ZMDS algorithm, a novel way of analyzing the fundamental network structure of recommendation results. Furthermore, it also describes a dynamic plot interaction method as a recommendation browsing utility. A novel ""Recommendation Map"" web application implements both the ZMDS algorithm and the plot interface and are offered as an example of both components working together. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Interaction methods; Music recommendation; Network structures; Structural networks; WEB application; Algorithms","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Suzuki M.; Hosoya T.; Ito A.; Makino S.","Suzuki, Motoyuki (56037004700); Hosoya, Toru (15765019200); Ito, Akinori (7403722531); Makino, Shozo (7403067655)","56037004700; 15765019200; 7403722531; 7403067655","Music information retrieval from a singing voice based on verification of recognized hypotheses","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461837&partnerID=40&md5=663eea786556f68e1ea22290a5c52471","Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan","Suzuki M., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Hosoya T., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Ito A., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; Makino S., Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan","Several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user's singing voice. All of these systems use only melody information for retrieval, although lyrics information is also useful for retrieval. In this paper, we propose an MIR system that uses both melody and lyrics information in the singing voice. The MIR system verifies hypotheses output by a lyrics recognizer from a melodic point of view. Each hypothesis has time alignment information between the singing voice and recognized text, and the boundaries of each note can be estimated using the information. As a result, melody information is extracted from the singing voice. On the other hand, the melody information can be calculated from the musical score of the song because the recognized text must be a part of the lyrics of the song. The hypothesis is verified by calculating the similarity between the two types of melody information. From the experimental results, the verification method increased the retrieval accuracy. Especially, it was very effective when the number of words in the user's singing voice was small. The proposed method increased the retrieval accuracy from 81.3% to 87.4% when the number of words was only three. © 2006 University of Victoria.","Lyrics recognition; MIR from singing voice; Verification of recognized hypotheses","Character recognition; Information retrieval; Information use; Lyrics recognition; Music information retrieval; Musical pieces; Musical score; Retrieval accuracy; Time alignment; Verification method; Search engines","M. Suzuki; Graduate School of Engineering, Tohoku University, Aoba-ku, Sendai, 980-8579, 6-6-05, Aramaki-Aza-Aoba, Japan; email: moto@makino.ecei.tohoku.ac.jp","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lai C.; Fujinaga I.","Lai, Catherine (8972569700); Fujinaga, Ichiro (9038140900)","8972569700; 9038140900","Data dictionary: Metadata for phonograph records","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428314&partnerID=40&md5=5234951b7b3955c49ad6664d45725b20","Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada","Lai C., Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada; Fujinaga I., Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada","The creation and maintenance of a metadata data dictionary is essential to large-scale digital repositories. It assists the process of data entry, ensures consistency of records, facilitates semantic compatibility and interoperability between systems, and, most importantly, forms the foundation for efficient and effective information retrieval infrastructure. In this paper we explain in detail the necessity of metadata data dictionaries to digitization projects and digital library retrieval services. We also describe the development process of our Data Dictionary for phonograph records. We then present the underlying data model of our Data Dictionary and provide information about the meaning and use of semantic units defined in the Data Dictionary. We stress the usefulness of the generation and maintenance of our Data Dictionary for MIR as it provides a means to ensure accurate, consistent, and comprehensive metadata annotation. For maximum interoperability between systems, digital repositories not only need to agree on the same metadata fields, but also the meanings of the fields. To this end, we believe our Data Dictionary is the cornerstone of optimal retrieval of music information about phonograph records. © 2006 University of Victoria.","Data dictionary; Digitization; Management; Metadata; Phonograph records; Standardization","Analog to digital conversion; Database systems; Digital libraries; Information retrieval; Interoperability; Maintenance; Management; Phonograph records; Semantics; Standardization; Development process; Digital repository; Music information; Semantic compatibility; Semantic units; Metadata","C. Lai; Schulich School of Music, McGill University, Montreal, QC H3A 1E3, Canada; email: lai@music.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lai C.; Fujinaga I.; Descheneau D.; Frishkopf M.; Riley J.; Hafner J.; Mcmillan B.","Lai, Catherine (8972569700); Fujinaga, Ichiro (9038140900); Descheneau, David (55586075000); Frishkopf, Michael (39963012700); Riley, Jenn (57197504438); Hafner, Joseph (36554277800); Mcmillan, Brian (57212696015)","8972569700; 9038140900; 55586075000; 39963012700; 57197504438; 36554277800; 57212696015","Metadata infrastructure for sound recordings","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949120080&partnerID=40&md5=4ee69ed002152268928cc0c48489a4f1","Music Technology, McGill University, Canada; Department of Music, University of Alberta, Canada; Digital Library Program, Indiana University, United States; McGill University Libraries, McGill University, Canada","Lai C., Music Technology, McGill University, Canada; Fujinaga I., Music Technology, McGill University, Canada; Descheneau D., Department of Music, University of Alberta, Canada; Frishkopf M., Department of Music, University of Alberta, Canada; Riley J., Digital Library Program, Indiana University, United States; Hafner J., McGill University Libraries, McGill University, Canada; Mcmillan B., McGill University Libraries, McGill University, Canada","This paper describes the first iteration of a working model for searching heterogeneous distributed metadata repositories for sound recording collections, focusing on techniques used for real-time querying and harmonizing diverse metadata models. The initial model for a metadata infrastructure presented here is the first of its kind for sound recordings. ©2007 Austrian Computer Society (OCG).","","Audio recordings; Information retrieval; Iterative methods; Sound recording; Distributed metadata; Metadata model; Techniques used; Working models; Metadata","C. Lai; Music Technology, McGill University, Canada; email: lai@music.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"McEnnis D.; McKay C.; Fujinaga I.","McEnnis, Daniel (16234097400); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","16234097400; 14033215600; 9038140900","Overview of OMEN","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873475383&partnerID=40&md5=ae4d3801412396470263cfa2ec814fcd","Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada","McEnnis D., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada; McKay C., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada; Fujinaga I., Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada","This paper introduces OMEN (On-demand Metadata Extraction Network), which addresses a fundamental problem in MIR: the lack of universal access to a large dataset containing significant amounts of copyrighted music. This is accomplished by utilizing the large collections of digitized music available at many libraries. Using OMEN, libraries will be able to perform on-demand feature extraction on site, returning feature values to researchers instead of providing direct access to the recordings themselves. This avoids copyright difficulties, since the underlying music never leaves the library that owns it. The analysis is performed using grid-style computation on library machines that are otherwise underused (e.g., devoted to patron web and catalogue use). © 2006 University of Victoria.","Datasets; Distributed computing; Feature extraction; Music database","Copyrights; Distributed computer systems; Information retrieval; Libraries; Metadata; Data sets; Feature values; Metadata extraction; Music database; Universal access; Feature extraction","D. McEnnis; Music Technology, Schulich School of Music, McGill University, Montreal, QC H3A 1E3, 555 Sherbrooke Street West, Canada; email: daniel.mcennis@mail.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hitchner S.; Murdoch J.; Tzanetakis G.","Hitchner, Stephen (55586753300); Murdoch, Jennifer (16245564400); Tzanetakis, George (6602262192)","55586753300; 16245564400; 6602262192","Music browsing using a tabletop display","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349581274&partnerID=40&md5=ad0eaad605f6f3394c13e7c9521de012","Computer Engineering, Univeristy of Victoria, Canada","Hitchner S., Computer Engineering, Univeristy of Victoria, Canada; Murdoch J., Computer Engineering, Univeristy of Victoria, Canada; Tzanetakis G., Computer Engineering, Univeristy of Victoria, Canada","The majority of work in Music Information Retrieval (MIR) follows a search/retrieval paradigm. More recently, the importance of browsing as an interaction paradigm has been realized, and several novel interfaces have been proposed. In this paper, we describe two novel interaction schemes for content-aware browsing of music collections that use a graphical tabletop interface. We further present findings from qualitative user studies. We describe our work in the context of two primary themes: music collection browsing, and collaborative (multiple simultaneous users) interaction and involvement during the browsing/ selection process. ©2007 Austrian Computer Society (OCG).","","Audio recordings; Content-aware; Interaction paradigm; Interaction schemes; Music collection; Music information retrieval; Selection process; Tabletop displays; Tabletop interfaces; User study; Information retrieval","S. Hitchner; Computer Engineering, Univeristy of Victoria, Canada; email: hitchner@engr.uvic.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"McEnnis D.; McKay C.; Fujinaga I.","McEnnis, Daniel (16234097400); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","16234097400; 14033215600; 9038140900","jAudio: Additions and improvements","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873437546&partnerID=40&md5=f142d1a8f0f1740fcccdc167e89b66ca","Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada","McEnnis D., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada; McKay C., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada; Fujinaga I., Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada","jAudio is an application designed to extract features for use in a variety of MIR tasks. It eliminates the need for re-implementing existing feature extraction algorithms and provides a framework that greatly facilitates the development and deployment of new features. Three classes of features are presented and explained-features, metafeatures, and aggregators. A detailed description of jAudio's dependency resolution algorithm is also discussed. Finally, ways in which jAudio can be embedded in and integrated with new systems are discussed, along with a description of jAudio's ability to add new features or aggregators, potentially at runtime. © 2006 University of Victoria.","Audio feature extraction; Java audio environment; Music information retrieval","Algorithms; Embedded systems; Feature extraction; Audio feature extraction; Feature extraction algorithms; Java audio environment; Meta-features; Music information retrieval; Resolution algorithms; Runtimes; Information retrieval","D. McEnnis; Music Technology Area, McGill University, Montreal, QC, 555 Sherbrooke West, Canada; email: daniel.mcennis@mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Kim Y.E.; Williamson D.S.; Pilli S.","Kim, Youngmoo E. (24724623000); Williamson, Donald S. (57225803782); Pilli, Sridhar (55257773400)","24724623000; 57225803782; 55257773400","Towards quantifying the ""album effect"" in artist identification","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476754&partnerID=40&md5=fc1efe579c3c40103aaee6b10fada39d","Drexel University, Electrical and Computer Engineering, United States","Kim Y.E., Drexel University, Electrical and Computer Engineering, United States; Williamson D.S., Drexel University, Electrical and Computer Engineering, United States; Pilli S., Drexel University, Electrical and Computer Engineering, United States","Recent systems for automatically identifying the performing artist from the acoustic signal of music have demonstrated reasonably high accuracy when discriminating between hundreds of known artists. A well-documented issue, however, is that the performance of these systems degrades when music from different albums is used for training and evaluation. Conversely, accuracy improves when systems are trained and evaluated using music from the same album. This performance characteristic has been labeled the ""album effect"". The unfortunate corollary to this result is that the classification results of these systems are based not entirely on the music itself, but on other audio features common to the album that may be unrelated to the underlying music. We hypothesize that one of the primary reasons for this phenomenon is the production process of commercial recordings, specifically, post-production. Understanding the primary aspects of post-production, we can attempt to model its effect on the acoustic features used for classification. By quantifying and accounting for this transformation, we hope to improve future systems for automatic artist identification. © 2006 University of Victoria.","Album effect; Artist identification; Music production; Song classification","Acoustic features; Acoustic signals; Album effect; Audio features; Classification results; Music production; Performance characteristics; Post-production; Production process; Information retrieval","Y.E. Kim; Drexel University, Electrical and Computer Engineering, United States; email: ykim@drexel.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Burgoyne J.A.; Pugin L.; Kereliuk C.; Fujinaga I.","Burgoyne, John Ashley (23007865600); Pugin, Laurent (23009752900); Kereliuk, Corey (12778640700); Fujinaga, Ichiro (9038140900)","23007865600; 23009752900; 12778640700; 9038140900","A cross-validated study of modelling strategies for automatic chord recognition in audio","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955824923&partnerID=40&md5=fc4af43b5f9a6ea4f50086b359baacf5","Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Burgoyne J.A., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Pugin L., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Kereliuk C., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; Fujinaga I., Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada","Although automatic chord recognition has generated a number of recent papers in MIR, nobody to date has done a proper cross validation of their recognition results. Cross validation is the most common way to establish baseline standards and make comparisons, e.g., for MIREX competitions, but a lack of labelled aligned training data has rendered it impractical. In this paper, we present a comparison of several modelling strategies for chord recognition, hiddenMarkov models (HMMs) and conditional random fields (CRFs), on a new set of aligned ground truth for the Beatles data set of Sheh and Ellis (2003). Consistent with previous work, our models use pitch class profile (PCP) vectors for audio modelling. Our results show improvement over previous literature, provide precise estimates of the performance of both old and new approaches to the problem, and suggest several avenues for future work. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Chord recognition; Conditional Random Fields(CRFs); Cross validation; Data set; Ground truth; Modelling strategies; New approaches; Training data; Computer music","J.A. Burgoyne; Centre for Interdisciplinary Research in Music and Media Technology, Schulich School of Music, McGill University, Montréal, QC, Canada; email: ashley@music.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Turnbull D.; Liu R.; Barrington L.; Lanckriet G.","Turnbull, Douglas (8380095700); Liu, Ruoran (14035688600); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","8380095700; 14035688600; 14041197300; 7801431767","A game-based approach for collecting semantic annotations of music","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","76","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049092472&partnerID=40&md5=96cfa26de02b44ade8f3a83c73af6282","Dept. of Computer Science and Engineering, University of California, San Diego, United States; Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","Turnbull D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Liu R., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","Games based on human computation are a valuable tool for collecting semantic information about images. We show how to transfer this idea into the music domain in order to collect high-quality semantic information about songs. We present Listen Game, a online, multiplayer game that measures the semantic relationship between music and words. In the normal mode, a player sees a list of semantically related words (e.g., instruments, emotions, usages, genres) and is asked to pick the best and worst word to describe a song. In the freestyle mode, a user is asked to suggest a new word that describes the music. Each player receives realtime feedback about the agreement amongst all players. We show that we can use the data collected during a two-week pilot study of Listen Game to learn a supervised multiclass labeling (SML) model. We show that this SML model can annotate a novel song with meaningful words and retrieve relevant songs from a database of audio content. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio content; Game-based approaches; High quality; Human computation; Multi-class labeling; Multiplayer games; Normal modes; Pilot studies; Real-time feedback; Semantic annotations; Semantic information; Semantic relationships; Semantically-related words; Semantics","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Lee K.; Slaney M.","Lee, Kyogu (8597995500); Slaney, Malcolm (6701855101)","8597995500; 6701855101","Automatic chord recognition from audio using an HMM with supervised learning","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873462366&partnerID=40&md5=df86c3f34262a49265182c5ba3dd86cf","Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; Yahoo Research, Sunnyvale, CA 94089, United States","Lee K., Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; Slaney M., Yahoo Research, Sunnyvale, CA 94089, United States","In this paper, we propose a novel method for obtaining labeled training data to estimate the parameters in a supervised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to estimate model parameters. Experimental results show higher performance in frame-level chord recognition than the previous approaches. © 2006 University of Victoria.","Chord recognition; Hidden Markov model; Supervised learning","Computer music; Hidden Markov models; Information retrieval; Supervised learning; Audio data; Chord recognition; Estimate model; Labeled training data; Symbolic data; Learning algorithms","K. Lee; Center for Computer Research in Music and Acoustics, Department of Music, Stanford University, United States; email: kglee@ccrma.stanford.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Müller M.; Clausen M.","Müller, Meinard (7404689873); Clausen, Michael (56225233200)","7404689873; 56225233200","Transposition-invariant self-similarity matrices","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149182149&partnerID=40&md5=c27447834fd01d147e77990997d566c5","Department of Computer Science III, Bonn University, Germany","Müller M., Department of Computer Science III, Bonn University, Germany; Clausen M., Department of Computer Science III, Bonn University, Germany","Self-similarity matrices have become an important tool for visualizing the repetitive structure of a music recording. Transforming an audio data stream into a feature sequence, one obtains a self-similarity matrix by pairwise comparing all features of the sequence with respect to a local cost measure. The basic idea is that similar audio segments are revealed as paths of low cost along diagonals in the resulting self-similarity matrix. It is often the case, in particular for classical music, that certain musical parts are repeated in another key. In this paper, we introduce the concept of a transposition- invariant self-similarity matrix, which reveals the repetitive structure even in the presence of key transpositions. Furthermore, we introduce an associated transposition index matrix displaying harmonic relations within the music recording. As an application, we sketch how our concept can be used for the task of audio structure analysis. ©2007 Austrian Computer Society (OCG).","","Information retrieval; AS paths; Audio data; Classical musics; Feature sequence; Low costs; Music recording; Repetitive structure; Self-similarities; Self-similarity matrix; Structure analysis; Audio recordings","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Li B.; Burgoyne J.A.; Fujinaga I.","Li, Beinan (22980588500); Burgoyne, John Ashley (23007865600); Fujinaga, Ichiro (9038140900)","22980588500; 23007865600; 9038140900","Extending Audacity for audio annotation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444078&partnerID=40&md5=b527b94c313655b1ec49018fce773021","Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada","Li B., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada; Burgoyne J.A., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada","By implementing a cached region selection scheme and automatic label completion, we extended an open-source audio editor to become a more convenient audio annotation tool for tasks such as ground-truth annotation for audio and music classification. A usability experiment was conducted with encouraging preliminary results. © 2006 University of Victoria.","Audio annotation; Classification; Usability","Classification (of information); Information retrieval; Annotation tool; Audio annotation; Music classification; Open-source; Selection scheme; Usability; Audio acoustics","B. Li; Music Technology Area, Schulich School of Music, McGill University, Montreal, QC, Canada; email: beinan.li@mail.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Landone C.; Harrop J.; Reiss J.","Landone, Christian (6506420847); Harrop, Joseph (55586226700); Reiss, Josh (10140139100)","6506420847; 55586226700; 10140139100","Enabling access to sound archives through integration, enrichment and retrieval: The easaier project","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949103833&partnerID=40&md5=d289c360a30af2b988c0a6104e1356c3","Centre for Digital Music, Queen Mary University of London, London E14NS, Mile End Road, United Kingdom; Royal Scottish Academy of Music and Drama, Glasgow G23DB, 100 Renfrew St, United Kingdom","Landone C., Centre for Digital Music, Queen Mary University of London, London E14NS, Mile End Road, United Kingdom; Harrop J., Royal Scottish Academy of Music and Drama, Glasgow G23DB, 100 Renfrew St, United Kingdom; Reiss J., Centre for Digital Music, Queen Mary University of London, London E14NS, Mile End Road, United Kingdom","Many digital sound archives suffer from problems concerning on-line access: sound materials are often held separately from other related media, they are not easily browsed and little opportunity to search the actual audio content of the material is provided. The EASAIER project aims to alleviate these problems, offering a number of solutions to support sound archive managers and users. EASAIER will enable enhanced access to sound archives, providing multiple methods of retrieval, integration with other media archives, content enrichment and enhanced access tools. ©2007 Austrian Computer Society (OCG).","","Access tools; Audio content; Digital sound; Media archives; Multiple methods; On-line access; Sound archives; Sound materials; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Kapur A.; Singer E.","Kapur, Ajay (13609187300); Singer, Eric (35613610900)","13609187300; 35613610900","A retrieval approach for human/robotic musical performance","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431117&partnerID=40&md5=043353a42200648e2d7bc965602e33a1","University of Victoria, Victoria, BC, Canada; LEMUR, Brooklyn, NY, United States","Kapur A., University of Victoria, Victoria, BC, Canada; Singer E., LEMUR, Brooklyn, NY, United States","This paper describes a MIR-based system for live musical performance between a human and a robot. This project involves combining human computer interface with musical robotics using query/retrieval architecture to create musical rhythmic phrases towards improvisation. © 2006 University of Victoria.","Electronic sitar; Interface-based MIR; KiOm; MIR live performance system; Musical robotics","Human computer interaction; Information retrieval; Robotics; Electronic sitar; Human computer interfaces; Interface-based MIR; KiOm; Musical performance; Performance system; Electronic musical instruments","A. Kapur; University of Victoria, Victoria, BC, Canada; email: ajay@uvic.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Pampalk E.; Goto M.","Pampalk, Elias (6507297042); Goto, Masataka (7403505330)","6507297042; 7403505330","MusicRainbow: A new user interface to discover artists using audio-based similarity and web-based labeling","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","53","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470483&partnerID=40&md5=ac0495e3d0607a8ef8b10890aeac1fc7","National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","In this paper we present MusicRainbow which is a simple interface for discovering artists where colors encode different types of music. MusicRainbow is based on a new audio-based approach to compute artist similarity. This approach scores 15 percentage points higher in a genre classification task than the similarity computed on track level. Using a traveling salesman algorithm, similar artists are mapped near each other on a circular rainbow. Furthermore, we present a new approach of combining this audio-based information with information from the web. In particular, we label the rainbow and summarize the artists with words extracted from web pages related to the artists. We use different vocabularies for different hierarchical levels and heuristics to select the most descriptive labels. We conclude with a discussion of the results. The first impressions are very promising. © 2006 University of Victoria.","","Information retrieval; Labels; User interfaces; Websites; Audio-based; Genre classification; Hierarchical level; Percentage points; Traveling salesman algorithm; Motion compensation","","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hamanaka M.; Lee S.","Hamanaka, Masatoshi (35253968400); Lee, Seunghee (53164279300)","35253968400; 53164279300","Music scope headphones: Natural user interface for selection of music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873431596&partnerID=40&md5=8db61f25017c1649b8978f91c6dbc490","Presto, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 305-8568, Mbox 604 1-1-1 Umezono, Japan; University of Tsukuba, Tsukuba, Ibaraki, 305-8574, Tennoudai 1-1-1, Japan","Hamanaka M., Presto, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 305-8568, Mbox 604 1-1-1 Umezono, Japan; Lee S., University of Tsukuba, Tsukuba, Ibaraki, 305-8574, Tennoudai 1-1-1, Japan","This paper describes a novel audio only interface for selecting music which enables us to select songs without having to click a mouse. Using previous music players with normal headphones, we can hear only one song at a time and we thus have to play pieces individually to select the one we want to hear from numerous new music files, which involves a large number of mouse operations. The main advantage of our headphones is that they detect natural movements, such as the head or hand moving when users are listening to music and they can focus on a particular musical source that they want to hear. By moving their head left or right, listeners can hear the source from a frontal position as the digital compass detects the change in the direction they are facing. By looking up or down, the tilt sensor will detect the change in the face's angle of elevation; they can better hear the source that is allocated to a more distant or closer position. By putting their hand behind their ear, listeners can adjust the focus sensor on the headphones to focus on a particular musical source that they want to hear. © 2006 University of Victoria.","Digital compass; Headphones; Infrared distance sensor; Music interface; Tilt sensor","Headphones; Information retrieval; Loudspeakers; Mammals; Sensors; User interfaces; Digital compass; Focus sensors; Infrared distance sensors; Mouse operations; Music files; Music interfaces; Music players; Natural movements; Tilt sensor; Audio acoustics","M. Hamanaka; Presto, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 305-8568, Mbox 604 1-1-1 Umezono, Japan; email: m.hamanaka@aist.go.jp","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Leitich S.; Topf M.","Leitich, Stefan (8401178700); Topf, Martin (55586122000)","8401178700; 55586122000","Globe of music - Music library visualization using geosom","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349574669&partnerID=40&md5=94fc79ea1e268b43c86226ff0ac8e4f4","Department of Distributed and Multimedia Systems, University of Vienna, 1010 Vienna, AT, Liebiggasse 4/3-4, Austria","Leitich S., Department of Distributed and Multimedia Systems, University of Vienna, 1010 Vienna, AT, Liebiggasse 4/3-4, Austria; Topf M., Department of Distributed and Multimedia Systems, University of Vienna, 1010 Vienna, AT, Liebiggasse 4/3-4, Austria","Music collections are commonly represented as plain textual lists of artist, title, album etc. for each contained music track. The large volume of personal music libraries makes them difficult to browse and access for users. In respect to possible information visualization techniques, no established convenient user interfaces exist. By using a spherical self-organizing map algorithm on low level audio features and processing the resulting map data, a Geographic Information System is used to visualize a music collection. This results in an aspiring music library visualization, which can be handled intuitively by the user and even provides new possibilities for accessing a music collection in the digital domain. ©2007 Austrian Computer Society (OCG).","","Audio recordings; Conformal mapping; Geographic information systems; Information retrieval; Information systems; User interfaces; Visualization; Audio features; Digital domain; Information visualization; Large volumes; Low level; Map data; Music collection; Music library; Digital libraries","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Gomez E.; Herrera P.","Gomez, Emilia (14015483200); Herrera, Perfecto (24824250300)","14015483200; 24824250300","The song remains the same: Identifying versions of the same piece using tonal descriptors","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472146&partnerID=40&md5=92f34c1ada84eb98242fcc69d7e394b5","Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain","Gomez E., Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain","Identifying versions of the same song by means of automatically extracted audio features is a complex task for a music information retrieval system, even though it may seem very simple for a human listener. The design of a system to perform this task gives the opportunity to analyze which features are relevant for music similarity. This paper focuses on the analysis of tonal similarity and its application to the identification of different versions of the same piece. This work formulates the situations where a song is ver-sioned and several musical aspects are transformed with respect to the canonical version. A quantitative evaluation is made using tonal descriptors, including chroma representations and tonality. A simple similarity measure, based on Dynamic Time Warping over transposed chroma features, yields around 55% accuracy, which exceeds by far the expected random baseline rate. © 2006 University of Victoria.","Audio description; Chroma; Cover versions; Pitch class profile; Tonality; Version identification","Audio description; Chroma; Cover versions; Pitch class profile; Tonality; Version identification","E. Gomez; Music Technology Group, Universitat Pompeu Fabra, 08003, Barcelona, Ocata, 1, Spain; email: emilia.gomez@iua.upf.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Reed J.; Lee C.-H.","Reed, Jeremy (34969598900); Lee, Chin-Hui (7410147008)","34969598900; 7410147008","A study on music genre classification based on universal acoustic models","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","41","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444148&partnerID=40&md5=8314720916bbd01fe3e44363b00a9a62","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States","Reed J., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States; Lee C.-H., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States","Classification of musical genres gives a useful measure of similarity and is often the most useful descriptor of a musical piece. Previous techniques to use hidden Markov models (HMMs) for automatic genre classification have used a single HMM to model an entire song or genre. This paper provides a framework to give finer segmentation of HMMs through acoustic segment modeling. Modeling each of these acoustic segments with an HMM builds a timbral dictionary in the same fashion that one would create a phonetic dictionary for speech. A symbolic transcription is created by finding the most likely sequence of symbols. These transcriptions then serve as inputs into an efficient text classifier utilized to provide a solution to the genre classification problem. This paper demonstrates that language-ignorant approaches provide results that are consistent with the current state-of-the-art for the genre classification problem. However, the finer segmentation potentially allows for ""musical language""-based syntactic rules to enhance performance. © 2006 University of Victoria.","Acoustic segment models; Hidden Markov models; Latent-semantic indexing; Musical genres","Hidden Markov models; Information retrieval; Transcription; Acoustic model; Automatic genre classification; Descriptors; Genre classification; Hidden markov models (HMMs); Music genre classification; Musical genre; Musical pieces; Phonetic dictionary; Segment modeling; Segment models; Syntactic rules; Text classifiers; Text processing","J. Reed; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States; email: jeremy.reed@gatech.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Clifford R.; Christodoulakis M.; Crawford T.; Meredith D.; Wiggins G.","Clifford, Raphaël (56260226800); Christodoulakis, Manolis (14029839000); Crawford, Tim (15054056900); Meredith, David (57125703100); Wiggins, Geraint (14032393700)","56260226800; 14029839000; 15054056900; 57125703100; 14032393700","A fast, randomised, maximal subset matching algorithm for document-level music retrieval","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873440226&partnerID=40&md5=c4306bcb49809bdacf244f0fcac4b842","University of Bristol, Merchant Venturers' Building, Bristol BS8 1UB, Woodland Road, United Kingdom; King's College, London, London WC2R 2LS, Strand, United Kingdom; Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom","Clifford R., University of Bristol, Merchant Venturers' Building, Bristol BS8 1UB, Woodland Road, United Kingdom; Christodoulakis M., King's College, London, London WC2R 2LS, Strand, United Kingdom; Crawford T., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom; Meredith D., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom; Wiggins G., Goldsmiths College, University of London, London SE14 6NW, New Cross, United Kingdom","We present MSM, a new maximal subset matching algorithm, for MIR at score level with polyphonic texts and patterns. First, we argue that the problem MSM and its ancestors, the SIA family of algorithms, solve is 3SUM-hard and, therefore, subquadratic solutions must involve approximation. MSM is such a solution; we describe it, and argue that, at O(n log n) time with no large constants, it is orders of magnitude more time-efficient than its closest competitor. We also evaluate MSM's performance on a retrieval problem addressed by the OMRAS project, and show that it outperforms OMRAS on this task by a considerable margin. © 2006 University of Victoria.","Pattern matching; Point set representation","Information retrieval; Pattern matching; Matching algorithm; Music retrieval; Orders of magnitude; Point set; Approximation algorithms","R. Clifford; University of Bristol, Merchant Venturers' Building, Bristol BS8 1UB, Woodland Road, United Kingdom; email: clifford@compsci.bristol.ac.uk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Ryynänen M.; Klapuri A.","Ryynänen, Matti (14632464600); Klapuri, Anssi (6602945099)","14632464600; 6602945099","Transcription of the singing melody in polyphonic music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873440865&partnerID=40&md5=ec910df7d50c6015fb37ab6c441de3a3","Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland","Ryynänen M., Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland; Klapuri A., Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland","This paper proposes a method for the automatic transcription of singing melodies in polyphonic music. The method is based on multiple-F0 estimation followed by acoustic and musicological modeling. The acoustic model consists of separate models for singing notes and for no-melody segments. The musicological model uses key estimation and note bigrams to determine the transition probabilities between notes. Viterbi decoding produces a sequence of notes and rests as a transcription of the singing melody. The performance of the method is evaluated using the RWC popular music database for which the recall rate was 63% and precision rate 46%. A significant improvement was achieved compared to a baseline method from MIREX05 evaluations. © 2006 University of Victoria.","Acoustic modeling; HMM; Key estimation; Musi-cological modeling; Singing transcription","Estimation; Information retrieval; Viterbi algorithm; Acoustic model; Acoustic modeling; Automatic transcription; Baseline methods; Bigrams; HMM; Polyphonic music; Popular music; Precision rates; Recall rate; Transition probabilities; Viterbi decoding; Transcription","M. Ryynänen; Institute of Signal Processing, Tampere University of Technology, FI-33101 Tampere, P.O. Box 553, Finland; email: matti.ryynanen@tut.fi","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Peeters G.","Peeters, Geoffroy (22433836000)","22433836000","Chroma-based estimation of musical key from audio-signal analysis","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","42","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873421782&partnerID=40&md5=7faffa64c548ea676267693841726bc9","Ircam - Sound Analysis/Synthesis Team, CNRS - STMS, F-75004 Paris, 1, pl. Igor Stravinsky, France","Peeters G., Ircam - Sound Analysis/Synthesis Team, CNRS - STMS, F-75004 Paris, 1, pl. Igor Stravinsky, France","This paper deals with the automatic estimation of key (keynote and mode) of a music track from the analysis of its audio signal. Such a system usually relies on a succession of processes, each one making hypotheses about either the signal content or the music content: spectral representation, mapping to chroma, decision about the global key of the music piece. We review here the underlying hypotheses, compare them and propose improvements over current state of the art. In particular, we propose the use of a Harmonic Peak Subtraction algorithm as a front-end of the system and evaluate the performance of an approach based on hidden Markov models. We then compare our approach with other approaches in an evaluation using a database of 302 baroque, classical and romantic music tracks. © 2006 University of Victoria.","Harmonic Peak Subtraction; Hidden Markov model; Key estimation; Pitch representation","Estimation; Hidden Markov models; Information retrieval; Photomapping; Signal analysis; Audio signal; Automatic estimation; Harmonic peaks; Music contents; Over current; Pitch representation; Signal content; Spectral representations; State of the art; Subtraction algorithms; Audio acoustics","G. Peeters; Ircam - Sound Analysis/Synthesis Team, CNRS - STMS, F-75004 Paris, 1, pl. Igor Stravinsky, France; email: peeters@ircam.fr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Eck D.; Bertin-Mahieux T.; Lamere P.","Eck, Douglas (12141444300); Bertin-Mahieux, Thierry (49060926500); Lamere, Paul (15765776900)","12141444300; 49060926500; 15765776900","Autotagging music using supervised machine learning","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049167362&partnerID=40&md5=8cae4575c10c19514ec3d4421998a816","Sun Labs Sun Microsystems, Burlington, MA, United States; Dept. of Comp. Sci. Montreal, Univ. of Montreal, QC, Canada","Eck D., Sun Labs Sun Microsystems, Burlington, MA, United States; Bertin-Mahieux T., Dept. of Comp. Sci. Montreal, Univ. of Montreal, QC, Canada; Lamere P., Sun Labs Sun Microsystems, Burlington, MA, United States","Social tags are an important component of ""Web2.0"" music recommendation websites. In this paper we propose a method for predicting social tags using audio features and supervised learning. These automatically- generated tags (or ""autotags"") can furnish information about music that is untagged or poorly tagged. The tags can also serve to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio features; Music recommendation; Social Tags; Supervised machine learning; Supervised learning","D. Eck; Sun Labs Sun Microsystems, Burlington, MA, United States; email: douglas.eck@umontreal.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Govaerts S.; Corthaut N.; Duval E.","Govaerts, Sten (7801629780); Corthaut, Nik (35748332800); Duval, Erik (7006487422)","7801629780; 35748332800; 7006487422","Mood-ex-machina: Towards automation of moody tunes","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949525631&partnerID=40&md5=f7f0757345f187a417126be52a304971","Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium","Govaerts S., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium; Corthaut N., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium; Duval E., Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium","In 2006, the rockanango system was developed for music annotation by music experts. The system allows these experts to create new musical parameters within a flat data structure [1]. Rockanango is deployed in a commercial environment of hotels, restaurants and cafés. One of the main concerns is the time it takes to manually annotate the music and to introduce new parameters. In this paper, we investigate the possibilities to assist the experts by means of automatic metadata generation. Two case studies are described. One focuses on the use of association rules, in combination with lower level metadata like mode and key. The other case study concerns the generation of a topic or subject marker for songs through harvested lyrics and a keyword generator. From our evaluation, we conclude that the generated keywords are relevant and that the music experts value them higher then laymen. Data mining techniques provide means for monitoring the metadata in terms of interparametric relationships that can be used to generate metadata. ©2007 Austrian Computer Society (OCG).","","Data structures; Information retrieval; Data mining techniques; Metadata generation; Musical parameters; New parameters; Metadata","S. Govaerts; Department of Computer Science, Katholieke Universiteit Leuven, Celestijnenlaan 200A B-3001 Heverlee, Belgium; email: Sten.Govaerts@cs.kuleuven.be","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Dehghani M.; Lovett A.M.","Dehghani, Morteza (23093814000); Lovett, Andrew M. (55582927500)","23093814000; 55582927500","Efficient genre classification using qualitative representations","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873460951&partnerID=40&md5=1e2b601acfa0d6fa0f2db98f2f3a5f0a","Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States","Dehghani M., Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States; Lovett A.M., Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States","We have constructed a system that can compute a qualitative representation of music from high-level features extracted from MusicXML files. We use two cognitively motivated computational models called SME and SEQL to build generalizations of musical genres from these representations. We then categorize novel music pieces according to the generalizations. We demonstrate the feasibility of the system with training sets much smaller than those used in previous systems. © 2006 University of Victoria.","Genre classification; Symbolic representation of music","Computational model; Genre classification; High-level features; Musical genre; Qualitative representation; Symbolic representation; Training sets; Information retrieval","M. Dehghani; Northwestern University, EECS Dept., Evanston, IL 60208-0834, 2145 Sheridan Rd., United States; email: morteza@cs.northwestern.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"McKay C.; Fujinaga I.","McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","14033215600; 9038140900","Musical genre classification: Is it worth pursuing and how can it be improved?","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","102","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873429683&partnerID=40&md5=a219590e3f6a84a31a2b1f4dd29af62a","Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada","McKay C., Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada","Research in automatic genre classification has been producing increasingly small performance gains in recent years, with the result that some have suggested that such research should be abandoned in favor of more general similarity research. It has been further argued that genre classification is of limited utility as a goal in itself because of the ambiguities and subjectivity inherent to genre. This paper presents a number of counterarguments that emphasize the importance of continuing research in automatic genre classification. Specific strategies for overcoming current performance limitations are discussed, and a brief review of background research in musicology and psychology relating to genre is presented. Insights from these highly relevant fields are generally absent from discourse within the MIR community, and it is hoped that this will help to encourage a more multi-disciplinary approach to automatic genre classification in the future. © 2006 University of Victoria.","Classification; Genre; Improvements; Music","Classification (of information); Information retrieval; Automatic genre classification; Current performance; Genre; Genre classification; Improvements; Multi-disciplinary approach; Music; Musical genre classification; Performance Gain; Research","C. McKay; Music Technology, Schulich School of Music, McGill University, Montreal, QC, Canada; email: cory.mckay@mail.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Goto M.","Goto, Masataka (7403505330)","7403505330","AIST annotation for the RWC music database","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","64","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873448585&partnerID=40&md5=fe680101994ad8086a4549a595172e9a","National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","In this paper, we introduce our activities regarding the manual annotation of the musical pieces of the RWC Music Database. Although the RWC Music Database is widely used, its annotated descriptions are not widely available. We therefore annotated a set of music-scene descriptions consisting of the beat structure, melody line, and chorus sections. We call this AIST Annotation. We also manually synchronized standard MIDI files with the corresponding audio signals at the beat level. We hope that the AIST Annotation will contribute to further advances in the field of music information processing. © 2006 University of Victoria.","","Information retrieval; Audio signal; Manual annotation; MIDI files; Music database; Music information processing; Musical pieces; Database systems","M. Goto; National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; email: m.goto@aist.go.jp","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Izmirli O.","Izmirli, Özgür (6507754258)","6507754258","Audio key finding using low-dimensional spaces","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482586&partnerID=40&md5=7e09958ea7e58453b0d3942b4015f1a8","Center for Arts and Technology Computer Science, Connecticut College, New London, CT, 06320, United States","Izmirli O., Center for Arts and Technology Computer Science, Connecticut College, New London, CT, 06320, United States","This paper presents two models of audio key finding: a template based correlational model and a template based model that uses a low-dimensional tonal representation. The first model uses a confidence weighted correlation to find the most probable key. The second model is distance based and employs dimensionality reduction to the tonal representation before generating a key estimate. Experiments to determine the dependence of key finding accuracy on dimensionality are presented. Results show that low dimensional representations, compared to commonly used 12 dimensions, may be utilized for key finding without sacrificing accuracy. The first model's independently verified performance enabled it to be used as a benchmark for evaluation of the second model. Key finding accuracies for both models are given together with detailed results of the second model's performance as a function of the number of dimensions used. © 2006 University of Victoria.","Chroma based representations; Dimensionality reduction; Key finding","Information retrieval; Audio key finding; Chroma based representations; Dimensionality reduction; Distance-based; Key finding; Low-dimensional representation; Low-dimensional spaces; Template-based; Weighted correlation; Benchmarking","O. Izmirli; Center for Arts and Technology Computer Science, Connecticut College, New London, CT, 06320, United States; email: oizm@conncoll.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Pohle T.; Knees P.; Schedl M.; Widmer G.","Pohle, Tim (14036302300); Knees, Peter (8219023200); Schedl, Markus (8684865900); Widmer, Gerhard (7004342843)","14036302300; 8219023200; 8684865900; 7004342843","Meaningfully browsing music services","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62249181919&partnerID=40&md5=b36edb460e76732a835b8838884eefc7","Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Pohle T., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Schedl M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","We present a browser application that offers the user an enhanced access to the content of music web services. Most importantly, the technique we apply aims at making it feasible to add to the automated suggestion of similar artists some intentional spin, or direction. At the heart of the algorithm, automatically derived artist descriptions are analyzed for common topics or aspects, and each artist is described by the extent to which it is associated with each of these topics. The browser application enables the user to formulate a query by means of these underlying topics by simply adjusting slider positions. The best matching artist is shown, and its web page found on the web music service is displayed. ©2007 Austrian Computer Society (OCG).","","Web services; Websites; Best matching; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Levy M.; Sandler M.","Levy, Mark (14021515000); Sandler, Mark (7202740804)","14021515000; 7202740804","A semantic space for music derived from social tags","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","67","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049175658&partnerID=40&md5=265ad3561f749b45f47ea3ef85c4c290","Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom","Levy M., Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom; Sandler M., Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom","In this paper we investigate social tags as a novel highvolume source of semanticmetadata formusic, using techniques from the fields of information retrieval and multivariate data analysis. We show that, despite the ad hoc and informal language of tagging, tags define a low-dimensional semantic space that is extremely well-behaved at the track level, in particular being highly organised by artist and musical genre. We introduce the use of Correspondence Analysis to visualise this semantic space, and show how it can be applied to create a browse-by-mood interface for a psychologically-motivated two-dimensional subspace representing musical emotion. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Multivariant analysis; Correspondence analysis; High volumes; Multivariate data analysis; Musical emotion; Musical genre; Semantic Space; Social Tags; Two-dimensional subspaces; Semantics","M. Levy; Centre for Digital Music Queen Mary, University of London Mile End Road, London E1 4NS, United Kingdom; email: mark.levy@elec.qmul.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Antonopoulos I.; Pikrakis A.; Theodoridis S.; Cornelis O.; Moelants D.; Leman M.","Antonopoulos, Iasonas (18041773400); Pikrakis, Aggelos (6507232714); Theodoridis, Sergios (7004236721); Cornelis, Olmo (27267474100); Moelants, Dirk (6507495086); Leman, Marc (6603703642)","18041773400; 6507232714; 7004236721; 27267474100; 6507495086; 6603703642","Music retrieval by rhythmic similarity applied on greek and african traditional music","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949124063&partnerID=40&md5=5e7e45504368876e35d702510a787f6b","Dept. of Informatics and Telecommunications, University of Athens, Greece; IPEM - Dept. of Musicology, Ghent University, Belgium","Antonopoulos I., Dept. of Informatics and Telecommunications, University of Athens, Greece; Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, Greece; Cornelis O., IPEM - Dept. of Musicology, Ghent University, Belgium; Moelants D., IPEM - Dept. of Musicology, Ghent University, Belgium; Leman M., IPEM - Dept. of Musicology, Ghent University, Belgium","This paper presents a method for retrieving music recordings by means of rhythmic similarity in the context of traditional Greek and African music. To this end, Self Similarity Analysis is applied either on the whole recording or on instances of a music thumbnail that can be extracted from the recording with an optional thumbnailing scheme. This type of analysis permits the extraction of a rhythmic signature per music recording. Similarity between signatures is measured with a standard Dynamic TimeWarping technique. The proposed method was evaluated on corpora of Greek and African traditional music where human improvisation plays a key role and music recordings exhibit a variety of music meters, tempi and instrumentation. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Music recording; Music retrieval; Self-similarities; Time warping; Audio recordings","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Müller M.; Mattes H.; Kurth F.","Müller, Meinard (7404689873); Mattes, Henning (55583412300); Kurth, Frank (56240850200)","7404689873; 55583412300; 56240850200","An efficient multiscale approach to audio synchronization","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","87","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873428975&partnerID=40&md5=3d878889115bc44fd9ce29aafca926db","Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","Müller M., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Mattes H., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Kurth F., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","We present an efficient and robust multiscale DTW (Ms-DTW) approach to music synchronization for time-aligning CD recordings of different interpretations of the same piece. The general strategy is to recursively project an alignment path computed at a coarse resolution level to the next higher level and then to refine the projected path. As main contributions, we address several crucial issues including the design and specification of robust and scalable audio features, suitable local cost measures, MsDTW levels, constraint regions, as well as sampling rate adaptation and structural enhancement strategies. Extensive experiments on Western classical music show that our MsDTW-based algorithm yields the same alignment result as the classical DTW-based strategy while significantly reducing the running time and memory requirements. Even for pieces of a duration of 10 to 15 minutes, the alignment (based on previously extracted feature sequences) can be computed in less than a second. © 2006 University of Victoria.","Alignment; Audio synchronization; Chroma feature; Multiscale","Alignment; Information retrieval; Audio features; Chroma features; Feature sequence; Memory requirements; Multi-scale approaches; Multiscales; Resolution level; Running time; Sampling rates; Structural enhancements; Synchronization","M. Müller; Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; email: meinard@cs.uni-bonn.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Serrà J.","Serrà, Joan (35749172500)","35749172500","A qualitative assessment of measures for the evaluation of a cover song identification system","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149090114&partnerID=40&md5=54c69fd31236d9e1d9bc76c6450e043f","Music Technology Group, Universitat Pompeu Fabra, Spain","Serrà J., Music Technology Group, Universitat Pompeu Fabra, Spain","The evaluation of effectiveness in InformationRetrieval systems has been developed in parallel to its evolution, generating a great amount of proposals to achieve this process. This paper focuses on a particular task of Music Information Retrieval: a system for Cover Song Identification. We present a concrete example and then try to elucidate which metrics work best to evaluate such a system. We end up with two evaluation measures suitable for this problem: bpref and Normalized Lift Curves. ©2007 Austrian Computer Society (OCG).","","Cover song identifications; Evaluation measures; Lift curves; Music information retrieval; Qualitative assessments; Information retrieval","J. Serrà; Music Technology Group, Universitat Pompeu Fabra, Spain; email: jserra@iua.upf.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Yoshii K.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.","Yoshii, Kazuyoshi (7103400120); Goto, Masataka (7403505330); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","7103400120; 7403505330; 35577813100; 7402000772; 7102397930","Hybrid collaborative and content-based music recommendation using probabilistic model with latent user preferences","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","117","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459337&partnerID=40&md5=66e35f4a38a0c4a5fa9ff4dc900c1ae0","Graduate School of Informatics, Kyoto University, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","Yoshii K., Graduate School of Informatics, Kyoto University, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Japan; Komatani K., Graduate School of Informatics, Kyoto University, Japan; Ogata T., Graduate School of Informatics, Kyoto University, Japan; Okuno H.G., Graduate School of Informatics, Kyoto University, Japan","This paper presents a hybrid music recommendation method that solves problems of two prominent conventional methods: collaborative filtering and content-based recommendation. The former cannot recommend musical pieces that have no ratings because recommendations are based on actual user ratings. In addition, artist variety in recommended pieces tends to be poor. The latter, which recommends musical pieces that are similar to users' favorites in terms of music content, has not been fully investigated. This induces unreliability in modeling of user preferences; the content similarity does not completely reflect the preferences. Our method integrates both rating and content data by using a Bayesian network called an aspect model. Unobservable user preferences are directly represented by introducing latent variables, which are statistically estimated. To verify our method, we conducted experiments by using actual audio signals of Japanese songs and the corresponding rating data collected from Amazon. The results showed that our method outperforms the two conventional methods in terms of recommendation accuracy and artist variety and can reasonably recommend pieces even if they have no ratings. © 2006 University of Victoria.","Collaborative filtering; Content-based recommendation; Hybrid method; Probabilistic model","Bayesian networks; Information retrieval; Aspect model; Audio signal; Collaborative filtering; Content data; Content similarity; Content-based; Content-based recommendation; Conventional methods; Hybrid method; Latent variable; Music contents; Music recommendation; Musical pieces; Probabilistic models; Recommendation accuracy; Unobservable; User rating; Quality of service","K. Yoshii; Graduate School of Informatics, Kyoto University, Japan; email: yoshii@kuis.kyoto-u.ac.jp","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Marolt M.","Marolt, Matija (6603601816)","6603601816","A mid-level melody-based representation for calculating audio similarity","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459578&partnerID=40&md5=19abd5d1514e3f186e41679f96bbb55c","University of Ljubljana, 1000 Ljubljana, Trzaska 25, Slovenia","Marolt M., University of Ljubljana, 1000 Ljubljana, Trzaska 25, Slovenia","We propose a mid-level melody-based representation that incorporates melodic, rhythmic and structural aspects of a music signal and is useful for calculating audio similarity measures. Most current approaches to music similarity use either low-level signal features, such as MFCCs that mostly capture timbral characteristics of music and contain little semantic information, or require symbolic representations, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythmic and structural representation of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and contains information on repetitions of short melodic phrases within the analyzed piece. We show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for discovering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection. © 2006 University of Victoria.","Melody-based representation; Mid-level representation; Music similarity; Searching audio","Audio signal processing; Information retrieval; Audio signal; Melodic similarity; Melody-based representation; Mid-level representation; Music collection; Music signals; Music similarity; Searching audio; Semantic information; Signal features; Similarity measure; Structural aspects; Structural representation; Symbolic representation; Audio acoustics","M. Marolt; University of Ljubljana, 1000 Ljubljana, Trzaska 25, Slovenia; email: matija.marolt@fri.uni-lj.si","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Pauws S.; Verhaegh W.; Vossen M.","Pauws, Steffen (11240480500); Verhaegh, Wim (6701740914); Vossen, Mark (22954923400)","11240480500; 6701740914; 22954923400","Fast generation of optimal music playlists using local search","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873470750&partnerID=40&md5=8b1f0afa8932d40328984f934287a152","Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands","Pauws S., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands; Verhaegh W., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands; Vossen M., Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands","We present an algorithm for use in an interactive music system that automatically generates music playlists that fit the music preferences given by a user. To this end, we introduce a formal model, define the problem of automatic playlist generation (APG) and indicate its NP-hardness. We use a local search (LS) procedure based on simulated annealing (SA) to solve the APG problem. In order to employ this LS procedure, we introduce an optimization variant of the APG problem, which includes the definition of penalty functions and a neighborhood structure. To improve upon the performance of the standard SA algorithm, we incorporated three heuristics referred to as song domain reduction, partial constraint voting, and two-level neighborhood structure. In tests, LS performed better than a constraint satisfaction (CS) solution in terms of run time, scalability and playlist quality. © 2006 University of Victoria.","Local search; Music playlist generation; Music retrieval; Simulated annealing","Algorithms; Information retrieval; Simulated annealing; Automatic playlist generation; Constraint Satisfaction; Formal model; Interactive music; Local search; Music playlist generation; Music retrieval; Neighborhood structure; NP-hardness; Partial constraints; Penalty function; Runtimes; SA algorithm; Problem solving","S. Pauws; Philips Research Europe, 5656 AE Eindhoven, High Tech Campus 34, Netherlands; email: teffen.pauws@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Sandvold V.; Aussenac T.; Celma O.; Herrera P.","Sandvold, Vegard (36476580800); Aussenac, Thomas (55583381900); Celma, Òscar (12804596800); Herrera, Perfecto (24824250300)","36476580800; 55583381900; 12804596800; 24824250300","Good vibrations: Music discovery through personal musical concepts","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424941&partnerID=40&md5=5fb0629ca12baa70dffb0bddd2b9bfe4","Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain","Sandvold V., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Aussenac T., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Celma O., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; Herrera P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain","We present here Good Vibrations, a tool for music tagging, exploration and discovery, shaped as a media player plugin, and intended for home users. The plugin allows the quick ""invention"" of concepts and properties that can be tagged to songs. After some hours of active tagging, the plugin starts automatically proposing the proper tags to the user, who is also allowed to correct them. The plugin generates playlists according to the user-defined concepts, and recommends related music either from the user's personal collection or from the Internet (through it's connection to Foafing the Music). The plugin runs, for the moment, in Nullsoft Winamp on Windows XP systems. © 2006 University of Victoria.","Music recommendation; Music tagging; Playlist generation; Software tools","Computer aided software engineering; Information retrieval; Home users; Media players; Music recommendation; Music tagging; Musical concepts; Playlist generation; Plug-ins; Windows XP; User interfaces","V. Sandvold; Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumval lació 8, Spain; email: vsandvold@iua.upf.es","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Pikrakis A.; Giannakopoulos T.; Theodoridis S.","Pikrakis, Aggelos (6507232714); Giannakopoulos, Theodoros (8723549900); Theodoridis, Sergios (7004236721)","6507232714; 8723549900; 7004236721","A computationally efficient speech/music discriminator for radio recordings","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873467474&partnerID=40&md5=fa35d990b471d00711bf35c9877bcb08","University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece","Pikrakis A., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece; Giannakopoulos T., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece; Theodoridis S., University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece","This paper presents a speech/music discriminator for radio recordings, based on a new and computationally efficient region growing technique, that bears its origins in the field of image segmentation. The proposed scheme operates on a single feature, a variant of the spectral entropy, which is extracted from the audio recording by means of a short-term processing technique. The proposed method has been tested on recordings from radio stations broadcasting over the Internet and, despite its simplicity, has proved to yield performance results comparable to more sophisticated approaches. © 2006 University of Victoria.","Region growing techniques; Spectral-entropy; Speech/music discrimination","Computational efficiency; Entropy; Information retrieval; Radio stations; Speech recognition; Computationally efficient; Processing technique; Region-growing techniques; Spectral-entropy; Speech/music discrimination; Audio recordings","A. Pikrakis; University of Athens, Department of Informatics and Telecommunications, 15784, Athens, Panepistimioupolis, Greece; email: pikrakis@di.uoa.gr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Torres D.; Turnbull D.; Barrington L.; Lanckriet G.","Torres, David (57211775350); Turnbull, Douglas (8380095700); Barrington, Luke (14041197300); Lanckriet, Gert (7801431767)","57211775350; 8380095700; 14041197300; 7801431767","Identifying words that are musically meaningful","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049111891&partnerID=40&md5=dae93848751f0f7e5c2fe8c9a80a21d0","Dept. of Computer Science and Engineering, University of California, San Diego, United States; Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","Torres D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Turnbull D., Dept. of Computer Science and Engineering, University of California, San Diego, United States; Barrington L., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States; Lanckriet G., Dept. of Electrical and Computer Engineering, University of California, San Diego, United States","A musically meaningful vocabulary is one of the keystones in building a computer audition system that can model the semantics of audio content. If a word in the vocabulary is inconsistently used by human annotators, or the word is not clearly represented by the underlying acoustic representation, the word can be considered as noisy and should be removed from the vocabulary to denoise the modeling process. This paper proposes an approach to construct a vocabulary of predictive semantic concepts based on sparse canonical component analysis (sparse CCA) . Experimental results illustrate that, by identifying musically meaningful words, we can improve the performance of a previously proposed computer audition system for music annotation and retrieval. ©2007 Austrian Computer Society (OCG).","","Audition; Semantics; Audio content; Component analysis; Computer audition; De-Noise; In-buildings; Modeling process; Semantic concept; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Chuan C.-H.; Chew E.","Chuan, Ching-Hua (15050129500); Chew, Elaine (8706714000)","15050129500; 8706714000","A dynamic programming approach to the extraction of phrase boundaries from tempo variations in expressive performances","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149181116&partnerID=40&md5=e97e2369f402b309edd19fb3dc060ab9","Department of Computer Science, Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States; Integrated Media Systems Center, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA, United States","Chuan C.-H., Department of Computer Science, Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States; Chew E., Integrated Media Systems Center, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA, United States","We present an approach to phrase segmentation that starts with an expressive music performance. Previous research has shown that phrases are delineated by tempo speedups and slowdowns. We propose a dynamic programming algorithm for extracting phrases from tempo information. We test two hypotheses formodeling phrase tempo shapes: a quadratic model, and a spline curve. We test the two models on phrase extraction from performances of entire classical romantic pieces namely, Chopin's Preludes Nos. 1 and 7. The algorithms determined 21 of the 26 phrase boundaries correctly from Arthur Rubinstein's and Evgeny Kissin's performances. We observe that not all tempo slowdowns signify a boundary (some are agogic accents), and multiple levels of phrasing strategies should be considered for detailed interpretation analyses. ©2007 Austrian Computer Society (OCG).","","Algorithms; Extraction; Information retrieval; Dynamic programming algorithm; Expressive music performance; Expressive performance; Multiple levels; Phrase boundary; Phrase extraction; Phrase segmentations; Quadratic models; Spline curve; Dynamic programming","C.-H. Chuan; Department of Computer Science, Viterbi School of Engineering, University of Southern California, Los Angeles, CA, United States; email: chinghuc@usc.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Grund C.M.","Grund, Cynthia M. (7006384605)","7006384605","A philosophical wish list for research in music information retrieval","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423074&partnerID=40&md5=e7fea0fae53e48c0d4dad4a10960dd37","Institute of Philosophy, Education and the Study of Religions - Philosophy, University of Southern Denmark, Odense, Denmark","Grund C.M., Institute of Philosophy, Education and the Study of Religions - Philosophy, University of Southern Denmark, Odense, Denmark","Within a framework provided by the traditional trio consisting of metaphysics, epistemology and ethics, a first stab is made at a wish list for MIR-research from a philosophical point of view. Since the tools of MIR are equipped to study language and its use from a purely sonic standpoint, MIR research could result in another revealing revolution within the linguistic turn in philosophy. © 2006 University of Victoria.","Language as spoken; Memory; Philosophy and MIR","Data storage equipment; Information retrieval; Ontology; Philosophical aspects; Language as spoken; Music information retrieval; Philosophy and MIR; Research","C.M. Grund; Institute of Philosophy, Education and the Study of Religions - Philosophy, University of Southern Denmark, Odense, Denmark; email: cmgrund@ifpr.sdu.dk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Van Kranenburg P.; Garbers J.; Volk A.; Franswiering; Grijp L.; Veltkamp R.C.","Van Kranenburg, Peter (35108158000); Garbers, J̈org (35107028500); Volk, Anja (30567849900); Franswiering (55585883700); Grijp, Louis (28067929100); Veltkamp, Remco C. (7003421646)","35108158000; 35107028500; 30567849900; 55585883700; 28067929100; 7003421646","Towards integration of MIR and folk song research","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949087242&partnerID=40&md5=f96ad1f8c92c7bb02937993bdabd9a46","Utrecht University, Amsterdam, Netherlands; Meertens Institute, Amsterdam, Netherlands","Van Kranenburg P., Utrecht University, Amsterdam, Netherlands; Garbers J., Utrecht University, Amsterdam, Netherlands; Volk A., Utrecht University, Amsterdam, Netherlands; Franswiering, Utrecht University, Amsterdam, Netherlands; Grijp L., Meertens Institute, Amsterdam, Netherlands; Veltkamp R.C., Utrecht University, Amsterdam, Netherlands","Folk song research (FSR) often deals with large collections of tunes that have various types of relations to each other. Computational methods can support the study of the contents of these collections. Music Information Retrieval (MIR) research provides such methods. Yet a fruitful cooperation of both disciplines is difficult to achieve. We present a role-model to structure this cooperation in which tasks and responsibilities are distributed among the roles of MIR, Computational Musicology (CM) and FSR. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Folk songs; Music information retrieval; Types of relations; Research","P. Van Kranenburg; Utrecht University, Amsterdam, Netherlands; email: petervk@cs.uu.nl","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Geleijnse G.; Schedl M.; Knees P.","Geleijnse, Gijs (24174165400); Schedl, Markus (8684865900); Knees, Peter (8219023200)","24174165400; 8684865900; 8219023200","The quest for ground truth in musical artist tagging in the social web era","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649640622&partnerID=40&md5=176058ccd6347a99e2fb72e72b521a21","Philips Research High Tech Campus, 34 Eindhoven, Netherlands; Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Geleijnse G., Philips Research High Tech Campus, 34 Eindhoven, Netherlands; Schedl M., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Dept. of Computational Perception, Johannes Kepler University, Linz, Austria","Research in Web music information retrieval traditionally focuses on the classification, clustering or categorizing of music into genres or other subdivisions. However, current community-based web sites provide richer descriptors (i.e. tags) for all kinds of products. Although tags have no well-defined semantics, they have proven to be an effective mechanism to label and retrieve items. Moreover, these tags are community-based and hence give a description of a product through the eyes of a community rather than an expert opinion. In this work we focus on Last.fm, which is currently the largest music community web service. We investigate whether the tagging of artists is consistent with the artist similarities found with collaborative filtering techniques. As the Last.fm data shows to be both consistent and descriptive, we propose a method to use this community-based data to create a ground truth for artist tagging and artist similarity. ©2007 Austrian Computer Society (OCG).","","Semantics; Web services; Websites; Artist similarities; Collaborative filtering techniques; Community-based; Descriptors; Effective mechanisms; Expert opinion; Ground truth; Last.fm; Music information retrieval; Musical artists; Social webs; Information retrieval","G. Geleijnse; Philips Research High Tech Campus, 34 Eindhoven, Netherlands; email: gijs.geleijnse@philips.com","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pinto A.; Van Leuken R.H.; Demirci M.F.; Wiering F.; Veltkamp R.C.","Pinto, Alberto (55937244900); Van Leuken, Reinier H. (16204544700); Demirci, M. Fatih (7006827854); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","55937244900; 16204544700; 7006827854; 8976178100; 7003421646","Indexing music collections through graph spectra","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350498624&partnerID=40&md5=7567dcb41567fda48c0c3c442d3bd61e","Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Dipartimento di Informatica E Comunicazione, Universit̀a Degli Studi di Milano, Italy","Pinto A., Dipartimento di Informatica E Comunicazione, Universit̀a Degli Studi di Milano, Italy; Van Leuken R.H., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Demirci M.F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Wiering F., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Universiteit Utrecht, Netherlands","Content based music retrieval opens up large collections, both for the general public and music scholars. It basically enables the user to find (groups of) similar melodies, thus facilitating musicological research of many kinds. We present a graph spectral approach, new to the music retrieval field, in which melodies are represented as graphs, based on the intervals between the notes they are composed of. These graphs are then indexed into a database using their laplacian spectra as a feature vector. This laplacian spectrum is known to be very informative about the graph, and is therefore a good representative of the original melody. Consequently, range searching around the query spectrum returns similar melodies. We present an experimental evaluation of this approach, together with a comparison with two known retrieval techniques. On our test corpus, a subset of a well documented and annotated collection of Dutch folk songs, this evaluation demonstrates the effectiveness of the overall approach. ©2007 Austrian Computer Society (OCG).","","Laplace transforms; Query processing; AS graph; Content-based music retrieval; Experimental evaluation; Feature vectors; Folk songs; General publics; Graph spectra; Graph-spectral approach; Laplacian spectra; Music collection; Music retrieval; Range searching; Retrieval techniques; Test corpus; Image retrieval","A. Pinto; Dipartimento di Informatica E Comunicazione, Universit̀a Degli Studi di Milano, Italy; email: pinto@dico.unimi.it","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Gómez C.; Abad-Mota S.; Ruckhaus E.","Gómez, Carlos (57196544462); Abad-Mota, Soraya (15123872500); Ruckhaus, Edna (10242585700)","57196544462; 15123872500; 10242585700","An analysis of the mongeau-sankoff algorithm for music information retrieval","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449105867&partnerID=40&md5=7432135cdeeaa01ceb279d1e8f109430","Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela","Gómez C., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela; Abad-Mota S., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela; Ruckhaus E., Dept. of Computing and Information Technology, Universidad Simón Boĺivar, Venezuela","An essential problem in music information retrieval is to determine the similarity between two given melodies; there are several melodic similarity measures that have been proposed, among others, the Mongeau-Sankoff measure. In this work we implemented a modified version of the Mongeau-Sankoff measure. We conducted an experimental study to compare the implemented measure with other similarity measures; this evaluation was done in the context of the 2005 edition of the MIREX symbolic melodic similarity competition. The most relevant result of our work is an implementation of the Mongeau-Sankoff measure that presents greater effectiveness when compared to other current melodic similarity measures. ©2007 Austrian Computer Society (OCG).","","Essential problems; Experimental studies; Melodic similarity; Music information retrieval; Similarity measure; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Frieler K.","Frieler, Klaus (32667638400)","32667638400","Visualizing music on the metrical circle","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949110503&partnerID=40&md5=810d95829b55736f53f95331541b67dc","Institute for Musicology, University of Hamburg, Germany","Frieler K., Institute for Musicology, University of Hamburg, Germany","In this paper we propose a novel method, called Metrical Circle Map, for exploring the cyclic aspects of musical time. To this end, we give a short formalization introducing the notion of Metrical Markov Chains as transition probabilities of segments on the metrical circle. As an illustration we present a compact visualization of the zeroth- and first order metrical Markov transitions of 61 Irish folk songs. ©2007 Austrian Computer Society (OCG).","","Markov processes; Circle map; First order; Folk songs; Transition probabilities; Information retrieval","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Cao C.; Li M.; Liu J.; Yan Y.","Cao, Chuan (26647978700); Li, Ming (56994217300); Liu, Jian (57207463462); Yan, Yonghong (7404586597)","26647978700; 56994217300; 57207463462; 7404586597","Singing melody extraction in polyphonic music by harmonic tracking","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950237831&partnerID=40&md5=de6779556cd76bcfda316afafc342ecf","Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China","Cao C., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Li M., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Liu J., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; Yan Y., Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China","This paper proposes an effective method for automatic melody extraction in polyphonic music, especially vocal melody songs. The method is based on subharmonic summation spectrum and harmonic structure tracking strategy. Performance of the method is evaluated using the LabROSA database 1 . The pitch extraction accuracy of our method is 82.2% on the whole database, while 79.4% on the vocal part. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Harmonic structures; Melody extractions; Pitch extraction; Polyphonic music; Sub-harmonic summations; Tracking strategies; Extraction","C. Cao; Thinkit Speech Lab., Institute of Acoustics, Chinese Academy of Sciences, China; email: fccao@hccl.ioa.ac.cn","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mayer R.; Lidy T.; Rauber A.","Mayer, Rudolf (23397787500); Lidy, Thomas (23035315800); Rauber, Andreas (57074846700)","23397787500; 23035315800; 57074846700","The map of mozart","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873419255&partnerID=40&md5=3cf4504c053335196e1b6cbda9096840","Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria","Mayer R., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria; Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria","We present a study on using a Mnemonic Self-Organizing Map for clustering a very homogeneous collection of music. In particular, we create a map containing the complete works of Wolfgang Amadeus Mozart. We study and analyze the clustering capabilities of the SOM on this very focused collection. We furthermore present a web-based application for exploring the map and accessing the music it represents. © 2006 University of Victoria.","Clustering; Explorative search; Self-Organizing Map","Information retrieval; AMADEUS; Clustering; Explorative search; Web-based applications; Conformal mapping","R. Mayer; Vienna University of Technology, Department of Software Technology and Interactive Systems, Austria; email: mayer@ifs.tuwien.ac.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hoashi K.; Ishizaki H.; Matsumoto K.; Sugaya F.","Hoashi, Keiichiro (6603899930); Ishizaki, Hiromi (16068647300); Matsumoto, Kazunori (55427576400); Sugaya, Fumiaki (8853329400)","6603899930; 16068647300; 55427576400; 8853329400","Content-based music retrieval using query integration for users with diverse preferences","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949224740&partnerID=40&md5=644c44b54100a504160f2b03952e0e6c","KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan","Hoashi K., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Ishizaki H., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Matsumoto K., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; Sugaya F., KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan","This paper proposes content-based music information retrieval (MIR) methods based on user preferences, which aim to improve the accuracy of MIR for users with .diverse . preferences, i.e., users whose preferences range in songs with a wide variety of features. The proposed MIR method dynamically generates an optimal set of query vectors from the sample set of songs submitted by the user to express their preferences, based on the similarity of the songs in the sample set. Experiments conducted on a music collection with subjective user ratings verify that our proposal is effective to improve the accuracy of contentbased MIR. Furthermore, by implementing a two-step MIR algorithm which utilizes song clustering results, the efficiency of the proposed MIR method is significantly improved. ©2007 Austrian Computer Society (OCG).","","Clustering algorithms; Quality of service; Clustering results; Content-based; Content-based music retrieval; Music collection; Music information retrieval; Optimal sets; Query vectors; Sample sets; User rating; Information retrieval","K. Hoashi; KDDI R and D Laboratories Inc., Saitama 356-8502, 2-1-15 Ohara Fujimino-shi, Japan; email: fhoashi@kddilabs.jp","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mardirossian A.; Chew E.","Mardirossian, Arpi (15769566400); Chew, Elaine (8706714000)","15769566400; 8706714000","Music summarization via key distributions: Analyses of similarity assessment across variations","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445933&partnerID=40&md5=7341f881a2867a57e305a7170237800e","University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States","Mardirossian A., University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States; Chew E., University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States","This paper presents a computationally efficient method for quantifying the degree of tonal similarity between two pieces of music. The properties we examine are key frequencies and average time in key, and we propose two metrics, based on the L1 and L2 norms, for quantifying similarity using these descriptors. The methods are applied to 711 classical themes and variations over 71 variation sets by 10 composers of different genres. Quantile-quantile plots and the Kolmogorov-Smirnov measure show that the proposed metrics exhibit strongly distinct behaviour when assessing pieces from the same variation set, and those that are not. Comparisons across variation sets by the same composer, and comparisons of pieces by different composers although result in similar distributions, are derived from fundamentally different underlying distributions, according to the K-S measure. We present probabilistic analyses of the two methods based on the distributions derived empirically. When the discrimination threshold is set at 55, the probabilities of Type I and Type II errors are 18.41% and 20.56% respectively for Method 1, and 15.72% and 22.94% respectively for Method 2. Method 1 has a success rate of 99.48% when labeling pieces as dissimilar (not from the same variation set), while the corresponding rate for Method 2 is 99.45%. © 2006 University of Victoria.","Key distribution; Music information retrieval; Music representation; Music similarity; Music summarization; Pitch; Similarity assessment","Information retrieval; Key distribution; Music information retrieval; Music representation; Music similarity; Music summarization; Pitch; Similarity assessment; Probability distributions","A. Mardirossian; University of Southern California, Viterbi School of Engineering, Epstein Department of Industrial and Systems Engineering, Los Angeles, CA 90089, United States; email: mardiros@usc.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Yeh C.; Bogaards N.; Roebel A.","Yeh, Chunghsin (35076852400); Bogaards, Niels (55360332100); Roebel, Axel (7801333053)","35076852400; 55360332100; 7801333053","Synthesized polyphonic music database with verifiable ground truth for multiple f0 estimation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349218803&partnerID=40&md5=de06978c486d7d3d800d6ee8ad231a14","IRCAM, CNRS-STMS Paris, France","Yeh C., IRCAM, CNRS-STMS Paris, France; Bogaards N.; Roebel A., IRCAM, CNRS-STMS Paris, France","To study and to evaluate a multiple F0 estimation algorithm, a polyphonic database with verifiable ground truth is necessary. Real recordings with manual annotation as ground truth are often used for evaluation. However, ambiguities arise during manual annotation, which are often set up by subjective judgements. Therefore, in order to have access to verifiable ground truth, we propose a systematic method for creating a polyphonicmusic database. Multiple monophonic tracks are rendered from a given MIDI file, in which rendered samples are separated to prevent overlaps and to facilitate automatic annotation. F0s can then be reliably extracted as ground truth, which are stored using SDIF. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Automatic annotation; Ground truth; Manual annotation; MIDI files; Multiple-F0 estimations; Polyphonic music; Subjective judgement; Systematic method; Database systems","C. Yeh; IRCAM, CNRS-STMS Paris, France; email: Chunghsin.Yeh@ircam.fr","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Bainbridge D.; Bell T.","Bainbridge, David (8756864800); Bell, Tim (7402501556)","8756864800; 7402501556","Identifying music documents in a collection of images","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873464179&partnerID=40&md5=b9388c339f2a9e0dcc386b8282818cac","Department of Computer Science, University of Waikato, New Zealand; Department of Computer Science and Software Engineering, University of Canterbury, New Zealand","Bainbridge D., Department of Computer Science, University of Waikato, New Zealand; Bell T., Department of Computer Science and Software Engineering, University of Canterbury, New Zealand","Digital libraries and search engines are now well-equipped to find images of documentsbased on queries. Many images of music scores are now available, often mixed up with textual documents and images. For example, using the Google ""images"" search feature, a search for ""Beethoven"" will return a number of scores and manuscripts as well as pictures of the composer. In this paper we report on an investigation into methods to mechanically determine if a particular document is indeed a score, so that the user can specify that only musical scores should be returned. The goal is to find a minimal set of features that can be used as a quick test that will be applied to large numbers of documents. A variety of filters were considered, and two promising ones (run-length ratios and Hough transform) were evaluated. We found that a method based around run-lengths in vertical scans (RL) that out-performs a comparable algorithm using the Hough transform (HT). On a test set of 1030 images, RL achieved recall and precision of 97.8% and 88.4% respectively while HT achieved 97.8% and 73.5%. In terms of processor time, RL was more than five times as fast as HT. © 2006 University of Victoria.","Music image; Optical music recognition (OMR); Score classification","Digital libraries; Hough transforms; Information retrieval; Music image; Music recognition; Musical score; Quick tests; Recall and precision; Run length; Test sets; Textual documents; Search engines","D. Bainbridge; Department of Computer Science, University of Waikato, New Zealand; email: davidb@cs.waikato.ac.nz","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Livshin A.; Rodet X.","Livshin, Arie (28567904200); Rodet, Xavier (6603779613)","28567904200; 6603779613","The significance of the non-harmonic ""noise"" versus the harmonic series for musical instrument recognition","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873452800&partnerID=40&md5=79ef1f6c1b88068c7d0fd95ab7626adf","IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France","Livshin A., IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France; Rodet X., IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France","Sound produced by Musical instruments with definite pitch consists of the Harmonic Series and the non-harmonic Residual. It is common to treat the Harmonic Series as the main characteristic of the timbre of pitched musical instruments. But does the Harmonic Series indeed contain the complete information required for discriminating among different musical instruments? Could the non-harmonic Residual, the ""noise"", be used all by itself for instrument recognition? The paper begins by performing musical instrument recognition with an extensive sound collection using a large set of feature descriptors, achieving a high instrument recognition rate. Next, using Additive Analysis/Synthesis, each sound sample is resynthesized using solely its Harmonic Series. These ""Harmonic"" samples are then subtracted from the original samples to retrieve the non-harmonic Residuals. Instrument recognition is performed on the resynthesized and the ""Residual"" sound sets. The paper shows that the Harmonic Series by itself is indeed enough for achieving a high instrument recognition rate; however, the non-harmonic Residuals by themselves can also be used for distinguishing among musical instruments, although with lesser success. Using feature selection, the best 10 feature descriptors for instrument recognition out of our extensive feature set are presented for the Original, Harmonic and Residual sound sets. © 2006 University of Victoria.","Harmonic series; Instrument recognition; Musical instruments; Noise; Pitch; Residual","Acoustic noise; Information retrieval; Musical instruments; Harmonic series; Instrument recognition; Noise; Pitch; Residual; Harmonic analysis","A. Livshin; IRCAM Centre Pompidou, Paris 75004, 1 Place Stravinsky, France; email: arie.livshin@ircam.fr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Geleijnse G.; Korst J.","Geleijnse, Gijs (24174165400); Korst, Jan (7004696252)","24174165400; 7004696252","Efficient lyrics extraction from the Web","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873478738&partnerID=40&md5=f448ec94713610365f3112a1411bf6f4","Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Geleijnse G., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Korst J., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","We present a novel method to extract lyrics from the Web. The aim is to extract a set of multiple versions of the lyrics to a song. Lyrics can be identified within a text by a regular expression. We use a projection of a document to efficiently identify lyrics within the document by mapping it to a regular expression. We describe a method to cluster the multiple versions of the lyrics by filtering out erroneous texts such as lyrics to other songs. For reasons of efficiency, we do this by comparing fingerprints instead of the texts themselves. © 2006 University of Victoria.","Google; Lyrics; Regular expressions; Web","Information retrieval; Google; Lyrics; Regular expressions; Web; Pattern matching","G. Geleijnse; Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; email: gijs.geleijnse@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Schedl M.; Pohle T.; Knees P.; Widmer G.","Schedl, Markus (8684865900); Pohle, Tim (14036302300); Knees, Peter (8219023200); Widmer, Gerhard (7004342843)","8684865900; 14036302300; 8219023200; 7004342843","Assigning and visualizing music genres by web-based co-occurrence analysis","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873459558&partnerID=40&md5=c4be54ad845ca52b29b164dff6f6e5a1","Department of Computational Perception, Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Pohle T., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Knees P., Department of Computational Perception, Johannes Kepler University, Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence, Vienna, Austria","We explore a simple, web-based method for predicting the genre of a given artist based on co-occurrence analysis, i.e. analyzing co-occurrences of artist and genre names on music-related web pages. To this end, we use the page counts provided by Google to estimate the relatedness of an arbitrary artist to each of a set of genres. We investigate four different query schemes for obtaining the page counts and two different probabilistic approaches for predicting the genre of a given artist. Evaluation is performed on two test collections, a large one with a quite general genre taxonomy and a quite small one with rather specific genres. Since our approach yields estimates for the relatedness of an artist to every genre of a given genre set, we can derive genre distributions which incorporate information about artists that cannot be assigned a single genre. This allows us to overcome the inflexible artist-genre assignment usually used in music information systems. We present a simple method to visualize such genre distributions with our Traveller's Sound Player. Finally, we briefly outline how to adapt the presented approach to extract other properties of music artists from the web. © 2006 University of Victoria.","Co-occurrence analysis; Evaluation; Genre classification; User interface; Web mining","Information retrieval; User interfaces; Co-occurrence analysis; Evaluation; Genre classification; Music genre; Probabilistic approaches; Query schemes; SIMPLE method; Sound players; Test Collection; Web Mining; Websites","M. Schedl; Department of Computational Perception, Johannes Kepler University, Linz, Austria; email: markus.schedl@jku.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"McEnnis D.; Cunningham S.J.","McEnnis, Daniel (16234097400); Cunningham, Sally Jo (7201937110)","16234097400; 7201937110","Sociology and music recommendation systems","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749089902&partnerID=40&md5=981bf9a8016726f82e6b01706c112c8e","Waikato University, New Zealand","McEnnis D., Waikato University, New Zealand; Cunningham S.J., Waikato University, New Zealand","Music recommendation systems have centred on two different approaches: content based analysis and collaborative filtering. Little attention has been paid to the reasons why these techniques have been effective. Fortunately, the social sciences have asked these questions. One of the findings of this research is that social context is much more important than previously thought. This paper introduces this body of research from sociology and its relevance to music recommendation algorithms. ©2007 Austrian Computer Society (OCG).","","Recommender systems; Content-based analysis; Music recommendation; Music Recommendation System; Social context; Social sciences","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Schwenninger J.; Brueckner R.; Willett D.; Hennecke M.","Schwenninger, Jochen (35273389600); Brueckner, Raymond (16068050700); Willett, Daniel (7006668712); Hennecke, Marcus (57225388306)","35273389600; 16068050700; 7006668712; 57225388306","Language identification in vocal music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873446853&partnerID=40&md5=33d0b4715a035608679541f9c2eb693c","Department of Electrical Engineering, University of Ulm, Ulm, Germany; Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany","Schwenninger J., Department of Electrical Engineering, University of Ulm, Ulm, Germany; Brueckner R., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany; Willett D., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany; Hennecke M., Harman/Becker Automotive Systems, Speech Dialog Systems, Ulm, Germany","Language identification is an important field in spoken language processing. The identification of the language sung or spoken in music, however, has attracted only minor attention so far. This, however, is an important task when it comes to categorizing, classifying and labelling of music data. In this paper, we review our efforts of transferring well-established techniques from spoken language identification to the area of language identification in music. We present results of distinguishing German and English sung modern music and propose and evaluate techniques designed for improving the classification performance. These techniques involve limiting the classification on song segments that appear to have vocals and on frames that are not distorted by heavy beat onsets. © 2006 University of Victoria.","","Information retrieval; Classification performance; Language identification; Music data; Spoken language processing; Spoken languages; Vocal music; Well-established techniques; Speech recognition","J. Schwenninger; Department of Electrical Engineering, University of Ulm, Ulm, Germany; email: jochen.schwenninger@uni-ulm.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Geleijnse G.; Korst J.","Geleijnse, Gijs (24174165400); Korst, Jan (7004696252)","24174165400; 7004696252","Web-based artist categorization","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424683&partnerID=40&md5=a669d84b6f74b90b663080ae9bf15eff","Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Geleijnse G., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Korst J., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","We present a novel approach in categorizing artists into subjective categories such as genre. We base our method on co-occurrences on the web, found with the Google search engine. A direct mapping between artists and categories proved to be unreliable. We use the categories mapped to closely related artists to obtain a more reliable mapping. The method is tested on a genre classification test set with convincing results. Moreover, mood categorization is explored using the same techniques. © 2006 University of Victoria.","Artist categorization; Google; Web","Search engines; Artist categorization; Direct mapping; Genre classification; Google; Google search engine; Web; Information retrieval","G. Geleijnse; Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; email: gijs.geleijnse@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mckay C.; Fujinaga I.","Mckay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","14033215600; 9038140900","Jwebminer: A web-based feature extractor","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952344720&partnerID=40&md5=5d6ca4fed63cd8f48ef120a88581ed31","Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","Mckay C., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada","jWebMiner is a software package for extracting cultural features from the web. It is designed to be used for arbitrary types of MIR research, either as a stand-alone application or as part of the jMIR suite. It emphasizes extensibility, generality and an easy-to-use interface. At its most basic level, the software operates by using web services to extract hit counts from search engines. Functionality is available for calculating a variety of statistical features based on these counts, for variably weighting web sites or limiting searches only to particular sites, for excluding hits that do not contain particular filter terms, for defining synonym relationships between certain search strings, and for applying a number of additional search configurations. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Search engines; Web services; Hit count; Standalone applications; Statistical features; Web-based features; Websites","C. Mckay; Music Technology Area and CIRMMT, Schulich School of Music, McGill University, Montreal, QC, Canada; email: cory.mckay@mail.mcgill.ca","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Craft A.J.D.; A.wiggins G.; Crawford T.","Craft, Alastair J. D. (55585816000); A.wiggins, Geraint (14032393700); Crawford, Tim (15054056900)","55585816000; 14032393700; 15054056900","Howmany beans make five? The consensus problem in music-genre classification and a new evaluation method for single-genre categorisation systems","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749112518&partnerID=40&md5=585485d7b3b2ad8c1752a365f8954e1c","Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom","Craft A.J.D., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom; A.wiggins G., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom; Crawford T., Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom","Genre definition and attribution is generally considered to be subjective. This makes evaluation of any genrelabelling system intrinsically difficult, as the ground-truth against which it is compared is based upon subjective responses, with little inter-participant consensus. This paper presents a novel method of analysing the results of a genre-labelling task, and demonstrates that there are groups of genre-labelling behaviour which are selfconsistent. It is proposed that the evaluation of any genre classification system uses this modified analysis method. ©2007 Austrian Computer Society (OCG).","","Analysis method; Consensus problems; Genre classification; New evaluation methods; Information retrieval","A.J.D. Craft; Department of Intelligent Sound and Music Systems, Centre for Cognition,Computation and Culture, Goldsmiths University of London, United Kingdom; email: a.craft@gold.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Rhodes C.; Casey M.","Rhodes, Christophe (57196565939); Casey, Michael (15080769900)","57196565939; 15080769900","Algorithms for determining and labelling approximate hierarchical self-similarity","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149101231&partnerID=40&md5=ea7d3c7668b668f7280e33a83111512f","Department of Computing, Goldsmiths University of London, SE14 6NW, United Kingdom","Rhodes C., Department of Computing, Goldsmiths University of London, SE14 6NW, United Kingdom; Casey M., Department of Computing, Goldsmiths University of London, SE14 6NW, United Kingdom","We describe an algorithm for finding approximate sequence similarity at all scales of interest, being explicit about our modelling assumptions and the parameters of the algorithm. We further present an algorithm for producing section labels based on the sequence similarity, and compare these labels with some expert-provided ground truth for a particular set of recordings. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Ground truth; Self-similarities; Sequence similarity; Algorithms","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Sinclair S.; Droettboom M.; Fujinaga I.","Sinclair, Stephen (55412599500); Droettboom, Michael (6506634231); Fujinaga, Ichiro (9038140900)","55412599500; 6506634231; 9038140900","Lilypond for pyScore: Approaching a universal translator for music notation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873472186&partnerID=40&md5=61220ec2b5da2644b4b169a86a247bf3","Schulich School of Music, McGill University, Montreal, QC, Canada; Johns Hopkins University, Baltimore, MD, 21218, 3400 North Charles Street, United States","Sinclair S., Schulich School of Music, McGill University, Montreal, QC, Canada; Droettboom M., Johns Hopkins University, Baltimore, MD, 21218, 3400 North Charles Street, United States; Fujinaga I., Schulich School of Music, McGill University, Montreal, QC, Canada","Several languages for music notation have been defined in recent years. pyScore, a framework for translating between notation formats, and new module for it which can generate input for the LilyPond music engraving system are described. This shows the potential for developing pyScore into a ""universal translator"" for musical scores. © 2006 University of Victoria.","Engraving; Notation; Representation; Score; Translation","Information retrieval; Translation (languages); Engraving; Music notation; Musical score; Notation; Representation; Score; Etching","S. Sinclair; Schulich School of Music, McGill University, Montreal, QC, Canada; email: sinclair@music.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Reed J.; Lee C.-H.","Reed, Jeremy (34969598900); Lee, Chin-Hui (7410147008)","34969598900; 7410147008","A study on attribute-based taxonomy for music information retrieval","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952410453&partnerID=40&md5=28c424efeb8446325f407fbd7e8a1445","Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States","Reed J., Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States; Lee C.-H., Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States","We propose an attribute-based taxonomy approach to providing alternative labels to music. Labels, such as genre, are often used as ground-truth for describing song similarity in music information retrieval (MIR) systems. A consistent labelling scheme is usually a key in determining quality of classifier learning in training and performance in testing of an MIR system. We examine links between conventional genre-based taxonomies and acoustical attributes available in text-based descriptions of songs. We show that the vector representation of each song based on these acoustic attributes enables a framework for unsupervised clustering of songs to produce alternative labels and quantitative measures of similarity between songs. Our experimental results demonstrate that this new set of labels are meaningful and classifiers based on these labels achieve similar or better results than those designed with existing genrebased labels. ©2007 Austrian Computer Society (OCG).","","Taxonomies; Attribute-based; Classifier learning; Labelling schemes; Music information retrieval; Quantitative measures; Unsupervised clustering; Vector representations; Information retrieval","J. Reed; Georgia Institute of Technology Atlanta, School of Electrical and Computer Engineering, GA 30332, United States; email: jeremy.reed@gatech.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Garbers J.; Van Kranenburg P.; Volk A.; Franswiering; Veltkamp R.C.; Grijp L.P.","Garbers, J̈org (35107028500); Van Kranenburg, Peter (35108158000); Volk, Anja (30567849900); Franswiering (55585883700); Veltkamp, Remco C. (7003421646); Grijp, Louis P. (28067929100)","35107028500; 35108158000; 30567849900; 55585883700; 7003421646; 28067929100","Using pitch stability among a group of aligned query melodies to retrieve unidentified variant melodies","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949100249&partnerID=40&md5=72e7fec16de434a08e14a6fdf71931ae","Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Meertens Institute, Amsterdam, Netherlands","Garbers J., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Volk A., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Franswiering, Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Veltkamp R.C., Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; Grijp L.P., Meertens Institute, Amsterdam, Netherlands","Melody identification is an important task in folk song variation research. In this paper we develop methods and tools that support researchers in finding melodies in a database that belong to the same variant group as a set of given melodies. The basic approach is to derive from the pitches of the known variants per onset a weighted pitch distribution, which quantifies pitch stability. We allow for partial matching and AND and OR queries. Technically we do so by defining a distance measure between weighted pitch distribution sequences. It is based on two applications of the Earth Mover's Distance, which is a distribution distance. We set up a distance framework and discuss musically meaningful parameterizations for two tasks: a) Study the inner-group distances between the group as a whole and single members of the group. b) Use the group's weighted pitch distribution sequence to query for variant melodies. The first experimental results seem very promising: a) The inner-group distances correlate to expert assigned subgroups. b) For variant retrieval our method works better than last year's MIREX winner. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Distance measure; Earth Mover's distance; Folk songs; Inner-group distance; Partial matching; Query processing","J. Garbers; Department of Information and Computing Sciences, Utrecht University, Amsterdam, Netherlands; email: garbers@cs.uu.nl","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pohle T.; Knees P.; Schedl M.; Widmer G.","Pohle, Tim (14036302300); Knees, Peter (8219023200); Schedl, Markus (8684865900); Widmer, Gerhard (7004342843)","14036302300; 8219023200; 8684865900; 7004342843","Independent component analysis for music similarity computation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873457860&partnerID=40&md5=4348a03cc29b9f2432fa00dd51813e01","Johannes Kepler University, Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","Pohle T., Johannes Kepler University, Linz, Austria; Knees P., Johannes Kepler University, Linz, Austria; Schedl M., Johannes Kepler University, Linz, Austria; Widmer G., Johannes Kepler University, Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), Vienna, Austria","In the recent years, a number of publications have appeared that deal with automatically calculating the similarity of music tracks. Most of them are based on features that are not intuitively understandable to humans, as they do not have a musically meaningful counterpart, but are merely measures of basic physical properties of the audio signal. Furthermore, most of these algorithms do not take into account the temporal development of the audio signal, which certainly is an important aspect of music. All of them consider the musical signal as a whole, not trying to reconstruct the listening process of dividing the signal into a number of sources. In this work, we present a novel approach to fill this gap by combining a number of existing ideas. At the heart of our approach, Independent Component Analysis (ICA) decomposes an audio signal into individual parts that appear maximally independent from each other. We present one basic algorithm to use these components for similarity computations, and evaluate a number of modifications to it with respect to genre classification accuracy. Our results indicate that this approach is at least of similar quality as many existing feature extraction routines. © 2006 University of Victoria.","Audio feature extraction; Music similarity computation","Algorithms; Feature extraction; Information retrieval; Audio feature extraction; Audio signal; Genre classification; Music similarity; Musical signals; Similarity computation; Temporal development; Independent component analysis","T. Pohle; Johannes Kepler University, Linz, Austria; email: music@jku.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Cannam C.; Landone C.; Sandler M.; Bello J.P.","Cannam, Chris (36730835200); Landone, Christian (6506420847); Sandler, Mark (7202740804); Bello, Juan Pablo (7102889110)","36730835200; 6506420847; 7202740804; 7102889110","The Sonic Visualiser: A visualisation platform for semantic descriptors from musical signals","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","95","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873453261&partnerID=40&md5=d391287f5823910c7260eff470c1ab60","Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom","Cannam C., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Landone C., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; Bello J.P., Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom","Sonic Visualiser is the name for an implementation of a system to assist study and comprehension of the contents of audio data, particularly of musical recordings. It is a C++ application with a Qt4 GUI that runs on Windows, Mac, and Linux. It embodies a number of concepts which are intended to improve interaction with audio data and features, most notably with respect to the representation of time-synchronous information. The architecture of the application allows for easy integration of third party algorithms for the extraction of low and mid-level features from musical audio data. This paper describes some basic principles and functionalities of Sonic Visualiser. © 2006 University of Victoria.","Musical feature; Semantic descriptor; Visualisation","Computer applications; Computer operating systems; Information retrieval; Semantics; Visualization; Audio data; Basic principles; Descriptors; Mid-level features; Musical audio data; Musical features; Musical signals; Third parties; Audio acoustics","C. Cannam; Centre for Digital Music, Queen Mary University of London, London, Mile End Road, United Kingdom; email: chris.cannam@elec.qmul.ac.uk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Chordia P.; Rae A.","Chordia, Parag (24723695700); Rae, Alex (24725486000)","24723695700; 24725486000","Raag recognition using pitch-class and pitch-class dyad distributions","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","80","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73249125605&partnerID=40&md5=3941db1df2b2e182b71c95280bfb7b60","Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States","Chordia P., Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States; Rae A., Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States","We describe the results of the first large-scale raag recognition experiment. Raags are the central structure of Indian classical music, each consisting of a unique set of complex melodic gestures. We construct a system to recognize raags based on pitch-class distributions (PCDs) and pitch-class dyad distributions (PCDDs) calculated directly from the audio signal. A large, diverse database consisting of 20 hours of recorded performances in 31 different raags by 19 different performers was assembled to train and test the system. Classification was performed using support vector machines, maximum a posteriori (MAP) rule using a multivariate likelihood model (MVN), and Random Forests. When classification was done on 60s segments, a maximum classification accuracy of 99.0% was attained in a cross-validation experiment. In a more difficult unseen generalization experiment, accuracy was 75%. The current work clearly demonstrates the effectiveness of PCDs and PCDDs in discriminating raags, even when musical differences are subtle. ©2007 Austrian Computer Society (OCG).","","Decision trees; Experiments; Information retrieval; Audio signal; Classification accuracy; Cross validation; Indian classical music; Maximum a posteriori; Random forests; Maximum likelihood","P. Chordia; Department of Music, Georgia Institute of Technology, Atlanta GA 30332, 840 Mc Millan St., United States; email: ppc@gatech.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Sigurdsson S.; Petersen K.B.; Lehn-Schiøler T.","Sigurdsson, Sigurdur (18438375800); Petersen, Kaare Brandt (8530662700); Lehn-Schiøler, Tue (6507438254)","18438375800; 8530662700; 6507438254","Mel frequency cepstral coefficients: An evaluation of robustness of MP3 encoded music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","113","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873450155&partnerID=40&md5=fca42317054b0fadc534c137aba8c8c8","Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark","Sigurdsson S., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark; Petersen K.B., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark; Lehn-Schiøler T., Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark","In large MP3 databases, files are typically generated with different parameter settings, i.e., bit rate and sampling rates. This is of concern for MIR applications, as encoding difference can potentially confound meta-data estimation and similarity evaluation. In this paper we will discuss the influence of MP3 coding for the Mel frequency cepstral coe-ficients (MFCCs). The main result is that the widely used subset of the MFCCs is robust at bit rates equal or higher than 128 kbits/s, for the implementations we have investigated. However, for lower bit rates, e.g., 64 kbits/s, the implementation of the Mel filter bank becomes an issue. © 2006 University of Victoria.","Mel frequency cepstral coefficients; MFCC; MP3; Robustness","Filter banks; Information retrieval; Robustness (control systems); Bit rates; Cepstral; Mel-filter banks; Mel-frequency cepstral coefficients; MFCC; MP3; Parameter setting; Sampling rates; Similarity evaluation; Speech recognition","S. Sigurdsson; Informatics and Mathematical Modelling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Richard Petersens Plads - Building 321, Denmark; email: siggi@imm.dtu.dk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Casey M.; Slaney M.","Casey, Michael (15080769900); Slaney, Malcolm (6701855101)","15080769900; 6701855101","Song intersection by approximate nearest neighbor search","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873443097&partnerID=40&md5=70037f1134cfb7fc3b47561c4e8f08e6","Goldsmiths College, University of London, United Kingdom; Yahoo Research Inc., Sunnyvale, CA, United States","Casey M., Goldsmiths College, University of London, United Kingdom; Slaney M., Yahoo Research Inc., Sunnyvale, CA, United States","We present new methods for computing inter-song similarities using intersections between multiple audio pieces. The intersection contains portions that are similar, when one song is a derivative work of the other for example, in two different musical recordings. To scale our search to large song databases we have developed an algorithm based on locality-sensitive hashing (LSH) of sequences of audio features called audio shingles. LSH provides an efficient means to identify approximate nearest neighbors in a high-dimensional feature space. We combine these nearest neighbor estimates, each a match from a very large database of audio to a small portion of the query song, to form a measure of the approximate similarity. We demonstrate the utility of our methods on a derivative works retrieval experiment using both exact and approximate (LSH) methods. The results show that LSH is at least an order of magnitude faster than the exact nearest neighbor method and that accuracy is not impacted by the approximate method. © 2006 University of Victoria.","Audio shingling; High dimensions; Music similarity; Nearest neighbors","Approximation theory; Information retrieval; Query processing; Approximate methods; Approximate nearest neighbor; Approximate Nearest Neighbor Search; Audio features; Audio shingling; High dimensions; High-dimensional feature space; Locality sensitive hashing; Music similarity; Nearest neighbor method; Nearest neighbors; Very large database; Audio acoustics","M. Casey; Goldsmiths College, University of London, United Kingdom; email: m.casey@gold.ac.uk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lee K.; Slaney M.","Lee, Kyogu (8597995500); Slaney, Malcolm (6701855101)","8597995500; 6701855101","A unified system for chord transcription and key extraction using hidden markov models","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849168570&partnerID=40&md5=34c4e50352608845ff0d545e9d8af989","Center for Computer Research in Music and Acoustics, Stanford University, United States; Yahoo Research, Sunnyvale, CA 94089, United States","Lee K., Center for Computer Research in Music and Acoustics, Stanford University, United States; Slaney M., Yahoo Research, Sunnyvale, CA 94089, United States","A new approach to acoustic chord transcription and key extraction is presented. As in an isolated word recognizer in automatic speech recognition systems, we treat a musical key as a word and build a separate hidden Markov model for each key in 24 major/minor keys. In order to acquire a large set of labeled training data for supervised training, we first perform harmonic analysis on symbolic data to extract the key information and the chord labels with precise segment boundaries. In parallel, we synthesize audio from the same symbolic data whose harmonic progression are in perfect alignment with the automatically generated annotations. We then estimate the model parameters directly from the labeled training data, and build 24 key-specific HMMs. The experimental results show that the proposed model not only successfully estimates the key, but also yields higher chord recognition accuracy than a universal, key-independentmodel. ©2007 Austrian Computer Society (OCG).","","Extraction; Information retrieval; Transcription; Automatic speech recognition system; Automatically generated; Chord recognition; Isolated words; Labeled training data; Model parameters; New approaches; Supervised trainings; Symbolic data; Unified system; Hidden Markov models","K. Lee; Center for Computer Research in Music and Acoustics, Stanford University, United States; email: kglee@ccrma.stanford.edu","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Pampalk E.; Gasser M.","Pampalk, Elias (6507297042); Gasser, Martin (18037419600)","6507297042; 18037419600","An implementation of a simple playlist generator based on audio similarity measures and user feedback","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444741&partnerID=40&md5=bafce4abed833ba9b0b9ce5a34a5ad8c","National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria","Pampalk E., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Gasser M., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6/6, Austria","This paper presents an implementation of a simple playlist generator. An audio-based music similarity measure and simple heuristics are used to create playlists given minimum user input. The ultimate goal of this work is to conduct a field study, i.e., to run the system on the users' personal collection and study the usage behavior over a longer period of time. The functions include, for example, allowing the user to control the variance of the playlists in terms of how often the same song or songs from the same artists are repeated. © 2006 University of Victoria.","","Audio-based; Field studies; Music similarity; Similarity measure; User feedback; User input; Information retrieval","","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Rizo D.; Ponce De León P.J.; Pérez-Sancho C.; Pertusa A.; Inesta J.M.","Rizo, David (14042169500); Ponce De León, Pedro J. (14042292100); Pérez-Sancho, Carlos (8918093500); Pertusa, Antonio (8540257800); Inesta, José M. (6701387099)","14042169500; 14042292100; 8918093500; 8540257800; 6701387099","A pattern recognition approach for melody track selection in MIDI files","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873444584&partnerID=40&md5=2844018fc498e1d8c03fdb94f88aeae9","Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain","Rizo D., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Ponce De León P.J., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Pérez-Sancho C., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Pertusa A., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; Inesta J.M., Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain","Standard MIDI files contain data that can be considered as a symbolic representation of music (a digital score), and most of them are structured as a number of tracks. One of them usually contains the melodic line of the piece, while the other tracks contain accompaniment music. The goal of this work is to identify the track that contains the melody using statistical properties of the musical content and pattern recognition techniques. Finding that track is very useful for a number of applications, like speeding up melody matching when searching in MIDI databases or motif extraction, among others. First, a set of descriptors from each track of the target file are extracted. These descriptors are the input to a random forest classifier that assigns the probability of being a melodic line to each track. The track with the highest probability is selected as the one containing the melodic line of that MIDI file. Promising results have been obtained testing a number of databases of different music styles. © 2006 University of Victoria.","Melody finding; Music perception; Musical analysis; Symbolic representation","Decision trees; Information retrieval; Descriptors; Digital score; Melody finding; MIDI files; Music perception; Musical analysis; Pattern recognition techniques; Random forest classifier; Statistical properties; Symbolic representation; Pattern recognition","D. Rizo; Departamento de Lenguajes y Sistemas Informáticos, Universidad de Alicante, Spain; email: inesta@dlsi.ua.es","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Karydis I.; Nanopoulos A.; Papadopoulos A.N.; Cambouropoulos E.","Karydis, Ioannis (36950839300); Nanopoulos, Alexandros (6603555418); Papadopoulos, Apostolos N. (7101944599); Cambouropoulos, Emilios (9632551100)","36950839300; 6603555418; 7101944599; 9632551100","VISA: The voice integration/segregation algorithm","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349100739&partnerID=40&md5=debe1519eec08495c087a4202dcbdc73","Department of Informatics Aristotle, University of Thessaloniki, Greece; Dept. of Music Studies Aristotle, Univ. of Thessaloniki, Greece","Karydis I., Department of Informatics Aristotle, University of Thessaloniki, Greece; Nanopoulos A., Department of Informatics Aristotle, University of Thessaloniki, Greece; Papadopoulos A.N., Department of Informatics Aristotle, University of Thessaloniki, Greece; Cambouropoulos E., Dept. of Music Studies Aristotle, Univ. of Thessaloniki, Greece","Listeners are capable to perceive multiple voices in music. Adopting a perceptual view of musical 'voice' that corresponds to the notion of auditory stream, a computational model is developed that splits musical scores (symbolic musical data) into different voices. A single 'voice' may consist of more than one synchronous notes that are perceived as belonging to the same auditory stream; in this sense, the proposed algorithm, may separate a given musical work into fewer voices than the maximum number of notes in the greatest chord. This is paramount, among other, for developing MIR systems that enable pattern recognition and extraction within musically pertinent 'voices' (e.g. melodic lines). The algorithm is tested against a small dataset that acts as groundtruth. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Pattern recognition; Computational model; Musical score; Algorithms","I. Karydis; Department of Informatics Aristotle, University of Thessaloniki, Greece; email: karydis@csd.auth.gr","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Kapur A.; Percival G.; Lagrange M.; Tzanetakis G.","Kapur, Ajay (13609187300); Percival, Graham (23135676800); Lagrange, Mathieu (18042165200); Tzanetakis, George (6602262192)","13609187300; 23135676800; 18042165200; 6602262192","Pedagogical transcription for multimodal sitar performance","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-52649144788&partnerID=40&md5=b2eb6574606a4ff382dba08c535398a6","Department of Computer Science, University of Victoria, BC, Canada","Kapur A., Department of Computer Science, University of Victoria, BC, Canada; Percival G., Department of Computer Science, University of Victoria, BC, Canada; Lagrange M., Department of Computer Science, University of Victoria, BC, Canada; Tzanetakis G., Department of Computer Science, University of Victoria, BC, Canada","Most automatic music transcription research is concerned with producing sheet music from the audio signal alone. However, the audio data does not include certain performance data which is vital for the preservation of instrument performance techniques and the creation of annotated guidelines for students. We propose the use of modified traditional instruments enhanced with sensors which can obtain such data; as a case study we examine the sitar. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio data; Audio signal; Automatic music transcription; Instrument performance; Multi-modal; Performance data; Audio signal processing","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Mauch M.; Dixon S.; Harte C.; Casey M.; Fields B.","Mauch, Matthias (36461512900); Dixon, Simon (7201479437); Harte, Christopher (18036993000); Casey, Michael (15080769900); Fields, Benjamin (35733701400)","36461512900; 7201479437; 18036993000; 15080769900; 35733701400","Discovering chord idioms through beatles and real book songs","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","27","10.1111/josi.12045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873593757&doi=10.1111%2fjosi.12045&partnerID=40&md5=1505ead36e2cf967942b0eca5770c57b","Centre for Digital Music, University of London Queen Mary, United Kingdom; Computing Department, Goldsmiths, University of London, United Kingdom","Mauch M., Centre for Digital Music, University of London Queen Mary, United Kingdom; Dixon S., Centre for Digital Music, University of London Queen Mary, United Kingdom; Harte C., Centre for Digital Music, University of London Queen Mary, United Kingdom; Casey M., Computing Department, Goldsmiths, University of London, United Kingdom; Fields B., Computing Department, Goldsmiths, University of London, United Kingdom","Modern collections of symbolic and audio music content provide unprecedented possibilities for musicological research, but traditional qualitative evaluation methods cannot realistically cope with such amounts of data. We are interested in harmonic analysis and propose key-independent chord idioms derived from a bottom-up analysis of musical data as a new subject of musicological interest. In order to motivate future research on audio chord idioms and on probabilistic models of harmony we perform a quantitative study of chord progressions in two popular music collections. In particular, we extract common subsequences of chord classes from symbolic data, independent of key and context, and order them by frequency of occurrence, thus enabling us to identify chord idioms. We make musicological observations on selected chord idioms from the collections. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Audio music; Bottom-up analysis; Common subsequence; Popular music; Probabilistic models; Qualitative evaluations; Quantitative study; Symbolic data; Audio acoustics","M. Mauch; Centre for Digital Music, University of London Queen Mary, United Kingdom; email: matthias.mauch@elec.qmul.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Müllensiefen D.; Lewis D.; Rhodes C.; Wiggins G.","Müllensiefen, Daniel (23019169400); Lewis, David (55742498400); Rhodes, Christophe (57196565939); Wiggins, Geraint (14032393700)","23019169400; 55742498400; 57196565939; 14032393700","Evaluating a chord-labelling algorithm","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64649098119&partnerID=40&md5=63ddf2066d646a2a163e9027f3897706","Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom","Müllensiefen D., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Lewis D., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Rhodes C., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; Wiggins G., Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom","This paper outlines a method for evaluating a new chordlabelling algorithmusing symbolic data as input. Excerpts from full-score transcriptions of 40 pop songs are used. The accuracy of the algorithm's output is compared with that of chord labels from published song books, as assessed by experts in pop music theory. We are interested not only in the accuracy of the two sets of labels but also in the question of potential harmonic ambiguity as reflected the judges' assessments. We focus, in this short paper, on outlining the general approach of this research project. ©2007 Austrian Computer Society (OCG).","","Information retrieval; General approach; Music theory; Potential harmonics; Symbolic data; Algorithms","D. Müllensiefen; Department of Computing, Goldsmiths University of London, London, SE14 6NW, United Kingdom; email: d.mullensiefen@gold.ac.uk","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Seifert F.; Rasch K.; Rentzsch M.","Seifert, Frank (56998300500); Rasch, Katharina (57496579300); Rentzsch, Michael (36900494100)","56998300500; 57496579300; 36900494100","Tempo induction by stream-based evaluation of musical events","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873450047&partnerID=40&md5=472303e735d5a5bf0fe91a19b45ab9b4","Department of Computer Science, University of Technology, Chemnitz, 09107, Germany","Seifert F., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany; Rasch K., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany; Rentzsch M., Department of Computer Science, University of Technology, Chemnitz, 09107, Germany","We present an approach for tempo induction that is based on a more perception-oriented analysis of inter-onset intervals. Therefore we utilize auditory grouping concepts and define some rules for their formation. Finally, we show preliminary results that confirm our aim of improving the quality of tempo induction by reducing the amount of perceptually irrelevant data. © 2006 University of Victoria.","Stream segregation; Tempo induction","Musical events; Stream segregation; Tempo induction; Information retrieval","F. Seifert; Department of Computer Science, University of Technology, Chemnitz, 09107, Germany; email: fsei@cs.tu-chemnitz.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Gouyon F.; Dixon S.","Gouyon, Fabien (8373002800); Dixon, Simon (57203056378)","8373002800; 57203056378","ISMIR 2006 tutorial: Computational rhythm description","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873464724&partnerID=40&md5=6caa480134399a6fb7b95a20814ef0ba","Austrian Research Institute for Artificial Intelligence, Vienna, Austria","Gouyon F., Austrian Research Institute for Artificial Intelligence, Vienna, Austria; Dixon S., Austrian Research Institute for Artificial Intelligence, Vienna, Austria","[No abstract available]","","","","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mountain R.","Mountain, Rosemary (14421446700)","14421446700","Name that mood! Describe that tune! Invitation to the IMP","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873461576&partnerID=40&md5=9840277a862dfc50b430220ddf3b0702","Concordia University, Montreal, QC, H4A 3C5, 7141 Sherbrooke St. W, Canada","Mountain R., Concordia University, Montreal, QC, H4A 3C5, 7141 Sherbrooke St. W, Canada","The ongoing research project The Interactive Multimedia Playroom (IMP) was established to stimulate discourse about issues relating to our perception and description of sounds in artistic and multimedia contexts. Although it was originally conceived to help develop better analytical tools for music, the unique and playful design is well-adapted to helping establish common references for potential collaborators in media arts. As the team working on the project development includes experts in psychology as well as creative artists and theorists, the format of the project is being designed to maximize its transfer to psychological studies. Unlike most psychological studies, however, we are particularly interested in the reactions of those intimately involved in the arts, and ask participants to comment on the suitability of the terminology, perceived relevance of the questions, etc. It is believed that the issues being addressed by the project are fundamental ones which could have high relevance for MIR research, including descriptors, sound-image associations, and the recognition of salient characteristics of a musical excerpt. © 2006 University of Victoria.","Descriptors; Multimedia; Non-verbal; Play","Association reactions; Information retrieval; Interactive computer systems; Multimedia systems; Analytical tool; Descriptors; Interactive multimedia; Media arts; Multimedia; Multimedia context; Non-verbal; Play; Potential collaborators; Project development; Team working; Research","R. Mountain; Concordia University, Montreal, QC, H4A 3C5, 7141 Sherbrooke St. W, Canada; email: mountain@vax2.concordia.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Cahill M.; Maidín D.O.","Cahill, Margaret (55582788400); Maidín, Donncha Ó. (55582697800)","55582788400; 55582697800","Assessing the performance of melodic similarity algorithms using human judgments of similarity","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423258&partnerID=40&md5=64eecff31242f4d5c96fd68f14dffcb9","Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","Cahill M., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; Maidín D.O., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper outlines a project to identify reliable algorithms for measuring melodic similarity by using melodies extracted from a piece of music in Theme and Variations form, for which human judgements of similarity have been gathered. © 2006 University of Victoria.","Human similarity judgments; Melodic similarity; Scores","Information retrieval; Human judgments; Human similarity judgments; Melodic similarity; Scores; Algorithms","M. Cahill; Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; email: margaret.cahill@ul.ie","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Castro B.M.; Alonso L.B.N.; Ferneda E.; Da Cunha M.B.; Cruz F.W.; Brandão M.D.C.P.","Castro, Beatriz Magalhães (55582205900); Alonso, Luiza Beth Nunes (52363339700); Ferneda, Edilson (57219126974); Da Cunha, Murilo Bastos (55667284400); Cruz, Fernando William (56187236700); Brandão, Márcio Da Costa P. (16634386200)","55582205900; 52363339700; 57219126974; 55667284400; 56187236700; 16634386200","BDB-MUS: A project for the preservation of Brazilian musical heritage","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449229&partnerID=40&md5=a048cfaf1a8f2dd51d0b45ba80ae112a","University of Brasilia, Music Department, Brasília, Brazil; Catholic University of Brasilia, Knowledge Management and IT, Brasília, Brazil; University of Brasilia, Information Science Department, Brasília, Brazil; University of Brasilia, Computer Science Department, Brasília, Brazil","Castro B.M., University of Brasilia, Music Department, Brasília, Brazil; Alonso L.B.N., Catholic University of Brasilia, Knowledge Management and IT, Brasília, Brazil; Ferneda E., Catholic University of Brasilia, Knowledge Management and IT, Brasília, Brazil; Da Cunha M.B., University of Brasilia, Information Science Department, Brasília, Brazil; Cruz F.W., University of Brasilia, Information Science Department, Brasília, Brazil; Brandão M.D.C.P., University of Brasilia, Computer Science Department, Brasília, Brazil","This poster proposes a discussion on concepts evolving from the role of digital libraries on the preservation of tangible and intangible cultural inheritance, including concepts developed in 2003 by UNESCO and the World Summit on the Information Society. It further describes the construction and design process leading to the development of BDB-MUS - Brazilian Digital Music Library, which aims to establish national recommendations on metadata attributions, and to develop means for appropriation and retrieval of musical sources. The poster further explores the concept of digital music or culture within the aims and objectives of the project. © 2006 University of Victoria.","Brazil; Cultural preservation; Digital libraries; Erudite music; Globalization; Indigenous music; Popular music","Information retrieval; Metadata; Brazil; Cultural preservation; Erudite music; Globalization; Indigenous music; Popular music; Digital libraries","B.M. Castro; University of Brasilia, Music Department, Brasília, Brazil; email: beatriz@unb.br","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mohri M.; Moreno P.; Weinstein E.","Mohri, Mehryar (7101967852); Moreno, Pedro (7102550006); Weinstein, Eugene (23399112300)","7101967852; 7102550006; 23399112300","Robust music identification, detection, and analysis","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67651107652&partnerID=40&md5=12c957a9f7fc929c1becb4eb8536a64e","Courant Institute of Mathematical Sciences, New York, NY 10012, 251 Mercer Street, United States; Google Inc., New York, NY 10011, 76 Ninth Avenue, United States","Mohri M., Courant Institute of Mathematical Sciences, New York, NY 10012, 251 Mercer Street, United States, Google Inc., New York, NY 10011, 76 Ninth Avenue, United States; Moreno P., Google Inc., New York, NY 10011, 76 Ninth Avenue, United States; Weinstein E., Courant Institute of Mathematical Sciences, New York, NY 10012, 251 Mercer Street, United States, Google Inc., New York, NY 10011, 76 Ninth Avenue, United States","In previous work, we presented a new approach to music identification based on finite-state transducers and Gaussian mixture models. Here, we expand this work and study the performance of our system in the presence of noise and distortions. We also evaluate a song detection method based on a universal background model in combination with a support vector machine classifier and provide some insight into why our transducer representation allows for accurate identification even when only a short song snippet is available. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Speech recognition; Detection methods; Finite state transducers; Gaussian Mixture Model; Music identification; New approaches; Support vector machine classifiers; Universal background model; Transducers","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Skowronek J.; McKinney M.; Par S.V.D.","Skowronek, Janto (15050961900); McKinney, Martin (57225339869); Par, Steven Van De (6701627290)","15050961900; 57225339869; 6701627290","A demonstrator for automatic music mood estimation","2007","Proceedings of the 8th International Conference on Music Information Retrieval, ISMIR 2007","46","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450256709&partnerID=40&md5=81dae786195d118a65196353c5075de4","Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands","Skowronek J., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands; McKinney M., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands; Par S.V.D., Philips Research Laboratories, Hightech Campus 36, 5656 AE Eindhoven, Netherlands","Interest in automatic music mood classification is increasing because it could enable people to browse and manage their music collections by means of the music's emotional expression complementary to the widely used music genres. We continue our work on designing a well defined ground-truth database for music mood classification and show a demonstrator of automatic mood estimation. While a subjective evaluation of this algorithm on arbitrary music is ongoing, the initial classification results are encouraging and suggest that an automatic predicition of music mood is possible. ©2007 Austrian Computer Society (OCG).","","Information retrieval; Classification results; Emotional expressions; Music collection; Music genre; Subjective evaluations; Computer music","","","8th International Conference on Music Information Retrieval, ISMIR 2007","23 September 2007 through 27 September 2007","Vienna","95393"
"Itoyama K.; Kitahara T.; Komatani K.; Ogata T.; Okuno H.G.","Itoyama, Katsutoshi (18042499100); Kitahara, Tetsuro (7201371361); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","18042499100; 7201371361; 35577813100; 7402000772; 7102397930","Automatic feature weighting in automatic transcription of specified part in polyphonic music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873482135&partnerID=40&md5=21686db8b1986d7f87a0b1c9bdfe7e51","Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","Itoyama K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","We studied the problem of automatic music transcription (AMT) for polyphonic music. AMT is an important task for music information retrieval because AMT results enable retrieving musical pieces, high-level annotation, demixing, etc. We attempted to transcribe a part played by an instrument specified by users (specified part tracking). Only two timbre models are required in the specified part tracking to identify the specified musical instrument even when the number of instruments increases. This transcription is formulated into a time-series classification problem with multiple features. We furthermore attempted to automatically estimate weights of the features, because the importance of these features varies for each musical signal. We estimated quasi-optimal weights of the features using a genetic algorithm for each musical signal. We tested our AMT system using trio stereo musical signals. Accuracies with our feature weighting method were 69.8% on average, whereas those without feature weighting were 66.0%. © 2006 University of Victoria.","Automatic music transcription; Feature weighting; Genetic algorithm; Specified part tracking","Genetic algorithms; Information retrieval; Automatic music transcription; Automatic transcription; De-mixing; Feature weighting; Multiple features; Music information retrieval; Musical pieces; Musical signals; Polyphonic music; Quasi-optimal; Time series classifications; Transcription","K. Itoyama; Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: itoyama@kuis.kyoto-u.ac.jp","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Van De Par S.; McKinney M.; Redert A.","Van De Par, Steven (6701627290); McKinney, Martin (57225339869); Redert, André (6602887438)","6701627290; 57225339869; 6602887438","Musical key extraction from audio using profile training","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872709732&partnerID=40&md5=830dc45e16d7bcf403195a85268e7ad6","Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands","Van De Par S., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands; McKinney M., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands; Redert A., Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands","A new method is presented for extracting the musical key from raw audio data. The method is based on the extraction of chromagrams using a new approach for tonal component selection taking into account auditory masking. The extracted chromagrams were used to train three key profiles for major and three key profiles for minor keys. The three trained key profiles differ in their temporal weighting of information across the duration of the song. One profile is based on uniform weighting while the other two apply emphasis on the beginning and ending of the song, respectively. The actual key extraction is based on comparing the key profiles with three average chromagrams that were extracted from a particular piece of music using the same temporal weighting functions as used for the key profile training. A correct key classification of 98% was achieved using non-overlapping test and training sets drawn from a larger set of 237 CD recordings of classical piano sonatas. © 2006 University of Victoria.","Audio; Chromagram; Key extraction; Music","Information retrieval; Speech enhancement; Audio; Audio data; Auditory masking; Chromagram; Component selection; Music; Temporal weighting functions; Training sets; Audio acoustics","S. Van De Par; Philips Research Laboratories Eindhoven, 5656 AE Eindhoven, High Tech Campus 36, Netherlands; email: Steven.van.de.Par@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Robine M.; Lagrange M.","Robine, Matthias (24559139300); Lagrange, Mathieu (18042165200)","24559139300; 18042165200","Evaluation of the technical level of saxophone performers by considering the evolution of spectral parameters of the sound","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849019751&partnerID=40&md5=f653f2a5a9fa1a3c055019fdf83c55d2","SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France","Robine M., SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France; Lagrange M., SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France","We introduce in this paper a new method to evaluate the technical level of a musical performer, by considering only the evolutions of the spectral parameters during one tone. The proposed protocol may be considered as front end for music pedagogy related softwares that intend to provide feedback to the performer. Although this study only considers alto saxophone recordings, the evaluation protocol intends to be as generic as possible and may surely be considered for wider range of classical instruments from winds to bowed strings. © 2006 University of Victoria.","Music education; Performer skills evaluation; Sinusoidal modeling","Information retrieval; Bowed string; Evaluation protocol; Front end; Music education; Performer skills evaluation; Sinusoidal modeling; Spectral parameters; Technical levels; Acoustic devices","M. Robine; SCRIME, LaBRI, Université Bordeaux 1, F-33405 Talence Cedex, 351 cours de la Libération, France; email: matthias.robine@labri.fr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lidy T.; Rauber A.","Lidy, Thomas (23035315800); Rauber, Andreas (57074846700)","23035315800; 57074846700","Visually profiling radio stations","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873434289&partnerID=40&md5=0a11cab225854c0326788cea8fa51116","Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","The overwhelming number of radio stations, both online and over the air, makes the choice of an appropriate program difficult. By profiling the program content of radio stations using Self-Organizing Maps we provide a reflection of a station's program type and give potential listeners a visual clue for selecting radio stations. Profiles of current broadcasts indicate which program type a station is currently playing. By creating radio station maps it is possible to directly pick a specific program type instead of having to search for a suitable radio station. © 2006 University of Victoria.","Audio feature extraction; Broadcast; Genre discrimination; Online streams; Profiles; Radio stations; Self-Organizing Map","Broadcasting; Conformal mapping; Information retrieval; Radio broadcasting; Radio stations; Audio feature extraction; Genre discrimination; Online streams; Over the airs; Profiles; Program content; Visual clues; Self organizing maps","T. Lidy; Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; email: lidy@ifs.tuwien.ac.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Van Kranenburg P.","Van Kranenburg, Peter (35108158000)","35108158000","Composer attribution by quantifying compositional strategies","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958086951&partnerID=40&md5=ec606548ec7410987799a75891e4e435","Department of Information and Computing Sciences, Utrecht University, Netherlands","Van Kranenburg P., Department of Information and Computing Sciences, Utrecht University, Netherlands","Taking a theory of musical style, developed by Leonard B. Meyer, as a starting point, an experiment is described in which statistical pattern recognition algorithms are used to characterize a particular musical style with respect to other styles. The resulting description can be used in authorship discussions. In the current study, a number of disputed organ works from the Bach catalog is used to illustrate the possibilities of this approach. © 2006 University of Victoria.","Classical music; Composer attribution; Johann Sebastian Bach; Musical style; Pattern recognition","Information retrieval; Classical music; Composer attribution; Compositional strategies; Musical style; Sebastian; Statistical pattern recognition; Pattern recognition","P. Van Kranenburg; Department of Information and Computing Sciences, Utrecht University, Netherlands; email: petervk@cs.uu.nl","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Whiteley N.; Cemgil A.T.; Godsill S.","Whiteley, Nick (55336867600); Cemgil, A. Taylan (15130945100); Godsill, Simon (7004018739)","55336867600; 15130945100; 7004018739","Bayesian modelling of temporal structure in musical audio","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549083232&partnerID=40&md5=899d1bdb7825d18f36285694c9b92080","Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom","Whiteley N., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom; Cemgil A.T., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom; Godsill S., Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom","This paper presents a probabilistic model of temporal structure in music which allows joint inference of tempo, meter and rhythmic pattern. The framework of the model naturally quantifies these three musical concepts in terms of hidden state-variables, allowing resolution of otherwise apparent ambiguities in musical structure. At the heart of the system is a probabilistic model of a hypothetical 'bar-pointer' which maps an input signal to one cycle of a latent, periodic rhythmical pattern. The system flexibly accommodates different input signals via two observation models: a Poisson points model for use with MIDI onset data and a Gaussian process model for use with raw audio signals. The discrete state-space permits exact computation of posterior probability distributions for the quantities of interest. Results are presented for both observation models, demonstrating the ability of the system to correctly detect changes in rhythmic pattern and meter, whilst tracking tempo. © 2006 University of Victoria.","Bayesian inference; Meter recognition; Rhythm recognition; Tempo tracking","Bayesian networks; Inference engines; Information retrieval; Probability distributions; Signal analysis; Audio signal; Bayesian inference; Bayesian modelling; Exact computations; Gaussian process models; Musical audio; Musical concepts; Musical structures; Observation model; Poisson points; Probabilistic models; Rhythm recognition; Rhythmic patterns; State-space; Tempo tracking; Temporal structures; Audio acoustics","N. Whiteley; Signal Processing Group, University of Cambridge, Department of Engineering, Cambridge, CB2 1PZ, Trumpington Street, United Kingdom; email: npw24@eng.cam.ac.uk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Neumayer R.; Dittenbach M.; Rauber A.","Neumayer, Robert (23012858400); Dittenbach, Michael (6602335688); Rauber, Andreas (57074846700)","23012858400; 6602335688; 57074846700","Playsom and PocketSOMPlayer, alternative interfaces to large music collections","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","52","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873540722&partnerID=40&md5=47fee4ba38a2e893fe8da77e3c66386e","Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria; ECommerce Competence Center, ISpaces Group, A-1220 Wien, Donau-City Strasse 1, Austria","Neumayer R., Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria; Dittenbach M., ECommerce Competence Center, ISpaces Group, A-1220 Wien, Donau-City Strasse 1, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria","With the rising popularity of digital music archives the need for new access methods such as interactive exploration or similarity-based search become significant. In this paper we present the PlaySOM, as well as the PocketSOMPlayer, two novel interfaces that enable one to browse a music collection by navigating a map of clustered music tracks and to select regions of interest containing similar tracks for playing. The PlaySOM system is primarily designed to allow interaction via a large-screen device, whereas the PocketSOMPlayer is implemented for mobile devices, supporting both local as well as streamed audio replay. This approach offers content-based organization of music as an alternative to conventional navigation of audio archives, i.e. flat or hierarchical listings of music tracks that are sorted and filtered by meta information. © 2005 Queen Mary, University of London.","Audio clustering; Audio interfaces; Information discovery and retrieval; Mobile devices; Music collections; User interaction","Information retrieval; Mobile devices; Audio clustering; Audio interfaces; Information discovery; Music collection; User interaction; Audio recordings","R. Neumayer; Vienna University of Technology, Department of Software Technology and Interactive Systems, A 1040 Wien, Favoritenstr. 9-11 / 188, Austria; email: robert.neumayer@univie.ac.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Walser R.Y.","Walser, Robert Young (58371899600)","58371899600","Herding folksongs","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873565127&partnerID=40&md5=733ed035f72977533ba44af7d01b26d1","James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen, Aberdeen, AB24 3EB, 24 High Street, United Kingdom","Walser R.Y., James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen, Aberdeen, AB24 3EB, 24 High Street, United Kingdom","Cataloging a large, multi-media collection of traditional song and drama in preparation for online presentation highlights issues of song identity and access in the context of contemporary digitized archives. In the James Madison Carpenter collection a particular folksong sung by a particular individual may exist in multiple manifestations: typed song text, sound recording(s), and/or manuscript music notation. While controlled vocabulary references such as Child and Roud numbers provide a degree of identification, such narrative- and text-centric tools are only partly effective in differentiating folkloric materials. Additional means are needed for identifying and controlling folk materials which are distinguished by other aspects of the song such as melody or non-narrative text. The Carpenter project team's experience with Encoded Archival Description (EAD) illustrates the value of this platform-independent, widely recognized standard and suggests opportunities for further developments particularly suited to locating and retrieving folk music materials. © 2005 Queen Mary, University of London.","EAD; Folklore; Folksong; James Madison Carpenter; Music; Sound recordings; XML","Audio recordings; Sound recording; XML; EAD; Folklore; Folksong; James Madison Carpenter; Music; Information retrieval","R.Y. Walser; James Madison Carpenter Project, Elphinstone Institute, University of Aberdeen, Aberdeen, AB24 3EB, 24 High Street, United Kingdom; email: seasongs@spacestar.net","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bergstra J.; Lacoste A.; Eck D.","Bergstra, James (15080566700); Lacoste, Alexandre (57213369649); Eck, Douglas (12141444300)","15080566700; 57213369649; 12141444300","Predicting genre labels for artists using FreeDB","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049085544&partnerID=40&md5=49f8a5ec5eb3f3b87611bffadc3136c8","Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada","Bergstra J., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada; Lacoste A., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada; Eck D., Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada","This paper explores the value of FreeDB as a source of genre and music similarity information. FreeDB is a public, dynamic, uncurated database for identifying and labelling CDs with album, song, artist and genre information. One quality of FreeDB is that there is high variance in, e.g., the genre labels assigned to a particular disc. We investigate here the ability to use these genre labels to predict a more constrained set of ""canonical"" genres as decided by the curated but private database AllMusic (i.e. multi-class learning). This work is relevant for study in music similarity: we present an automatic, data-driven method for embedding artists in a continuous space that corresponds to genre similarity judgements over a large population of music fans. At the same time, we observe that FreeDB is a valuable resource to researchers developing music classification algorithms; it serves as a reference for what music is popular over a large population, and provides relevant targets for supervised learning algorithms. © 2006 University of Victoria.","FreeDB; Genre recognition; Music classification; Music similarity","Information retrieval; Learning algorithms; CdS; Continuous spaces; Data-driven methods; FreeDB; Genre recognition; Large population; Multi-class; Music classification; Music similarity; Private database; Relevant target; Similarity judgements; Computer music","J. Bergstra; Dept. of Computer Science, Université de Montréal, Montreal, QC H3C 3J7, CP 6128 Succ Centre-Ville, Canada; email: bergstrj@iro.umontreal.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Herrera P.; Celma O.; Massaguer J.; Cano P.; Gómez E.; Gouyon F.; Koppenberger M.; García D.; García J.-P.; Wack N.","Herrera, Perfecto (24824250300); Celma, Òscar (12804596800); Massaguer, Jordi (6603092129); Cano, Pedro (56248848600); Gómez, Emilia (14015483200); Gouyon, Fabien (8373002800); Koppenberger, Markus (8380775000); García, David (57213360830); García, José-Pedro (55584848200); Wack, Nicolas (8373003100)","24824250300; 12804596800; 6603092129; 56248848600; 14015483200; 8373002800; 8380775000; 57213360830; 55584848200; 8373003100","MUCOSA: A music content semantic annotator","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473006&partnerID=40&md5=1b8c3cad5412fabe50881a17e090e69e","Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain","Herrera P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Celma O., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Massaguer J., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Cano P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Gómez E., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Gouyon F., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Koppenberger M., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; García D., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; García J.-P., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; Wack N., Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain","MUCOSA (Music Content Semantic Annotator) is an environment for the annotation and generation of music metadata at different levels of abstraction. It is composed of three tiers: an annotation client that deals with microannotations (i.e. within-file annotations), a collection tagger, which deals with macro-annotations (i.e. acrossfiles annotations), and a collaborative annotation subsystem, which manages large-scale annotation tasks that can be shared among different research centres. The annotation client is an enhanced version of WaveSurfer, a speech annotation tool. The collection tagger includes tools for automatic generation of unary descriptors, invention of new descriptors, and propagation of descriptors across sub-collections or playlists. Finally, the collaborative annotation subsystem, based on Plone, makes possible to share the annotation chores and results between several research institutions. A collection of annotated songs is available, as a ""starter pack"" to all the individuals or institutions that are eager to join this initiative. © 2005 Queen Mary, University of London.","Audio annotations; Audio music content processing; Music databases; Music tagging; Semantic descriptors","Information retrieval; Metadata; Semantics; Societies and institutions; Audio annotations; Audio music; Descriptors; Music database; Music tagging; Audio acoustics","P. Herrera; Universitat Pompeu Fabra, Barcelona, 08003, Pg. Circumvallació 8, Spain; email: pherrera@iua.upf.es","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Degroeve S.; Tanghe K.; De Baets B.; Leman M.; Martens J.-P.","Degroeve, Sven (6505827636); Tanghe, Koen (55545970400); De Baets, Bernard (55664779600); Leman, Marc (6603703642); Martens, Jean-Pierre (7201836932)","6505827636; 55545970400; 55664779600; 6603703642; 7201836932","A simulated annealing optimization of audio features for drum classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550802&partnerID=40&md5=bb01df6bca36fa307670372193bd6b77","Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Department of Musicology (IPEM), Ghent University, Belgium; Department of Electronics and Information Systems (ELIS), Ghent University, Belgium","Degroeve S., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Tanghe K., Department of Musicology (IPEM), Ghent University, Belgium; De Baets B., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Leman M., Department of Musicology (IPEM), Ghent University, Belgium; Martens J.-P., Department of Electronics and Information Systems (ELIS), Ghent University, Belgium","Current methods for the accurate recognition of instruments within music are based on discriminative data descriptors. These are features of the music fragment that capture the characteristics of the audio and suppress details that are redundant for the problem at hand. The extraction of such features from an audio signal requires the user to set certain parameters. We propose a method for optimizing the parameters for a particular task on the basis of the Simulated Annealing algorithm and Support Vector Machine classification. We show that using an optimized set of audio features improves the recognition accuracy of drum sounds in music fragments. © 2005 Queen Mary, University of London.","Drum classification; Mel frequency cepstral coefficients; Simulated annealing; Support vector machine","Audio acoustics; Information retrieval; Support vector machines; Audio features; Audio signal; Descriptors; Mel frequency cepstral co-efficient; Recognition accuracy; Simulated annealing algorithms; Simulated annealing optimization; Support vector machine classification; Simulated annealing","S. Degroeve; Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; email: Sven.Degroeve@UGent.be","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lai C.; Li B.; Fujinaga I.","Lai, Catherine (8972569700); Li, Beinan (22980588500); Fujinaga, Ichiro (9038140900)","8972569700; 22980588500; 9038140900","Preservation digitization of David Edelberg's Handel LP collection: A pilot project","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873476874&partnerID=40&md5=da9e8d5bc7a41fca10bc38d32632fd73","Music Technology, Faculty of Music, McGill University, Montreal, Canada","Lai C., Music Technology, Faculty of Music, McGill University, Montreal, Canada; Li B., Music Technology, Faculty of Music, McGill University, Montreal, Canada; Fujinaga I., Music Technology, Faculty of Music, McGill University, Montreal, Canada","This paper describes the digitization process for building an online collection of LPs and the procedure for creating the ground-truth data essential for developing an automated metadata and content capturing system. © 2005 Queen Mary, University of London.","Analogue sound recordings; Digital library collections; Digitization; Preservation; Use and access","Analog to digital conversion; Digital libraries; Metadata; Wood preservation; Content capturing; Digital library collections; Online collection; Pilot projects; Use and access; Information retrieval","C. Lai; Music Technology, Faculty of Music, McGill University, Montreal, Canada; email: lai@music.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Gerhard D.","Gerhard, David (7004060122)","7004060122","Pitch track target deviation in natural singing","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545056&partnerID=40&md5=ea3fbc53f699bfbb903264e80da77201","Department of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada; Department of Music, University of Regina, Regina, SK S4S 0A2, Canada","Gerhard D., Department of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada, Department of Music, University of Regina, Regina, SK S4S 0A2, Canada","Unlike fixed-pitch instruments such as the piano, human singing can stray from a target pitch by as much as a semitone while still being perceived as a single fixed note. This paper presents a study of the difference between target pitch and actualized pitch in natural singing. A set of 50 subjects singing the same melody and lyric is used to compare utterance styles. An algorithm for alignment of idealized template pitch tracks to measured frequency tracks is presented. Specific examples are discussed, and generalizations are made with respect to the types of deviations typical in human singing. Demographics, including the skill of the singer, are presented and discussed in the context of the pitch track deviation from the ideal. © 2005 Queen Mary, University of London.","Melody alignment; Ornamentation; Pitch track; Singing; Vibrato","Information retrieval; Ornamentation; Pitch track; Singing; Target deviation; Vibrato; Alignment","D. Gerhard; Department of Computer Science, University of Regina, Regina, SK S4S 0A2, Canada; email: david.gerhard@uregina.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Hazan A.; Grachten M.; Ramirez R.","Hazan, Amaury (55916412300); Grachten, Maarten (8974600000); Ramirez, Rafael (35280935600)","55916412300; 8974600000; 35280935600","Evolving performance models by performance similarity: Beyond note-to-note transformations","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950624242&partnerID=40&md5=fe7a2a1deaf5d9f3472fa273c7c8e911","Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain; Artificial Intelligence Research Institute, Spanish Council for Scientific Research (IIIA - CSIC), Campus UAB, 08193 Bellaterra, Spain","Hazan A., Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain; Grachten M., Artificial Intelligence Research Institute, Spanish Council for Scientific Research (IIIA - CSIC), Campus UAB, 08193 Bellaterra, Spain; Ramirez R., Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain","This paper focuses on expressive music performance modeling. We induce a population of score-driven performance models using a database of annotated performances extracted from saxophone acoustic recordings of jazz standards. In addition to note-to-note timing transformations that are invariably introduced in human renditions, more extensive alterations that lead to insertions and deletions of notes are usual in jazz performance. In spite of this, inductive approaches usually treat these latter alterations as artifacts. As a first step, we integrate part of the alterations occurring in jazz performances in an evolutionary regression tree model based on strongly typed genetic programming (STGP). This is made possible (i) by creating a new regression data type that includes a range of melodic alterations and (ii) by using a similarity measurement based on an edit-distance fit to human performance similarity judgments. Finally, we present the results of both learning and generalization experiments using a set of standards from the Real Book. © 2006 University of Victoria.","Evolutionary modeling; Expressive music performance; Melodic similarity","Digital storage; Genetic algorithms; Genetic programming; Information retrieval; Data type; Edit distance; Expressive music performance; Human performance; Insertions and deletions; Jazz standards; Melodic similarity; Performance Model; Regression tree models; Similarity measurements; Strongly-typed genetic programming; Computer music","A. Hazan; Music Technology Group, Pompeu Fabra University, 08001 Barcelona, Ocata 1, Spain; email: ahazan@iua.upf.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hamanaka M.; Hirata K.; Tojo S.","Hamanaka, Masatoshi (35253968400); Hirata, Keiji (55730620800); Tojo, Satoshi (7103319884)","35253968400; 55730620800; 7103319884","ATTA: Automatic time-span tree analyzer based on extended GTTM","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873553238&partnerID=40&md5=0b43a683d4e3a34e252df542a43eeefd","PRESTO, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 1-1-1 Umezono, Japan; NTT Communication Science Laboratories, Keihanna, Science City, Kyoto, 2-4, Hikaridai, Seikacho, Japan; Japan Advanced Institute of Science and Technoloty, Nomi, Ishikawa, 1-1, Asahidai, Japan","Hamanaka M., PRESTO, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 1-1-1 Umezono, Japan; Hirata K., NTT Communication Science Laboratories, Keihanna, Science City, Kyoto, 2-4, Hikaridai, Seikacho, Japan; Tojo S., Japan Advanced Institute of Science and Technoloty, Nomi, Ishikawa, 1-1, Asahidai, Japan","This paper describes a music analyzing system called the automatic time-span tree analyzer (ATTA), which we have developed. The ATTA derives a time-span tree that assigns a hierarchy of 'structural importance' to the notes of a piece of music based on the generative theory of tonal music (GTTM). Although the time-span tree has been applied with music summarization and collaborative music creation systems, these systems use time-span trees manually analyzed by experts in musicology. Previous systems based on GTTM cannot acquire a timespan tree without manual application of most of the rules, because GTTM does not resolve much of the ambiguity that exists with the application of the rules. To solve this problem, we propose a novel computational model of the GTTM that re-formalizes the rules with computer implementation. The main advantage of our approach is that we can introduce adjustable parameters, which enables us to assign priority to the rules. Our analyzer automatically acquires time-span trees by configuring the parameters that cover 26 rules out of 36 GTTM rules for constructing a time-span tree. Experimental results showed that after these parameters were tuned, our method outperformed a baseline performance. We hope to distribute the time-span tree as the content for various musical tasks, such as searching and arranging music. © 2005 Queen Mary, University of London.","ATTA; Generative Theory of Tonal Music (GTTM); Grouping structure; Knowledge acquisition; Metrical structure; Musical knowledge; Time-span tree","Group theory; Information retrieval; Knowledge acquisition; Adjustable parameters; Analyzing system; ATTA; Base-line performance; Computational model; Computer implementations; Music creation; Music summarization; Musical knowledge; Structural importance; Time-span tree; Tonal music; Forestry","M. Hamanaka; PRESTO, Japan Science and Technology Agency, A.I.S.T., Tsukuba, Ibaraki, 1-1-1 Umezono, Japan; email: m.hamanaka@aist.go.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Wen X.; Sandler M.","Wen, Xue (55311828400); Sandler, Mark (7202740804)","55311828400; 7202740804","A partial searching algorithm and its application for polyphonic music transcription","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549899&partnerID=40&md5=8c1db6da15796a65136358d8e235ec12","Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","Wen X., Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom","This paper proposes an algorithm for studying spectral contents of pitched sounds in real-world recordings. We assume that the 2nd-order difference, w.r.t. partial index, of a pitched sound is bounded by some small positive value, rather than equal to 0 in a perfect harmonic case. Given a spectrum and a fundamental frequency f0, the algorithm searches the spectrum for partials that can be associated with f0 by dynamic programming. In section 3 a background-foreground model is plugged into the algorithm to make it work with reverberant background, such as in a piano recording. In section 4 we illustrate an application of the algorithm in which a multipitch scoring machine, which involves special processing for close or shared partials, is coupled with a tree searching method for polyphonic transcription task. Results are evaluated on the traditional note level, as well as on a partial-based sub-note level. © 2005 Queen Mary, University of London.","Dynamic programming; Polyphonic music transcription; Sinusoids; Spectral harmonic model","Algorithms; Dynamic programming; Information retrieval; Trees (mathematics); Fundamental frequencies; ITS applications; Multi pitches; Polyphonic music; Polyphonic transcriptions; Positive value; Real-world; Searching algorithms; Sinusoids; Special processing; Spectral content; Spectral harmonics; Tree searching; Audio recordings","X. Wen; Centre for Digital Music, Department of Electronic Engineering, Queen Mary, University of London, London, E1 4NS, Mile End Road, United Kingdom; email: xue.wen@elec.qmul.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"McKay C.; Fiebrink R.; McEnnis D.; Li B.; Fujinaga I.","McKay, Cory (14033215600); Fiebrink, Rebecca (36095844900); McEnnis, Daniel (16234097400); Li, Beinan (22980588500); Fujinaga, Ichiro (9038140900)","14033215600; 36095844900; 16234097400; 22980588500; 9038140900","ACE: A framework for optimizing music classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873552024&partnerID=40&md5=6909337a7495b05467d61e3caeca1566","Music Technology, McGill University, Montreal, Canada","McKay C., Music Technology, McGill University, Montreal, Canada; Fiebrink R., Music Technology, McGill University, Montreal, Canada; McEnnis D., Music Technology, McGill University, Montreal, Canada; Li B., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","This paper presents ACE (Autonomous Classification Engine), a framework for using and optimizing classifiers. Given a set of feature vectors, ACE experiments with a variety of classifiers, classifier parameters, classifier ensembles and dimensionality reduction techniques in order to arrive at a good configuration for the problem at hand. In addition to evaluating classification methodologies in terms of success rates, functionality is also being incorporated into ACE allowing users to specify constraints on training and classification times as well as on the amount of time that ACE has to arrive at a solution. ACE is designed to facilitate classification for those new to pattern recognition as well as provide flexibility for those with more experience. ACE is packaged with audio and MIDI feature extraction software, although it can certainly be used with existing feature extractors. This paper includes a discussion of ways in which existing general-purpose classification software can be adapted to meet the needs of music researchers and shows how these ideas have been implemented in ACE. A standardized XML format for communicating features and other information to classifiers is proposed. A special emphasis is placed on the potential of classifier ensembles, which have remained largely untapped by the MIR community to date. A brief theoretical discussion of ensemble classification is presented in order to promote this powerful approach. © 2005 Queen Mary, University of London.","Classifier ensembles; Combining classifiers; MIR; Music classification; Optimization","Feature extraction; Information retrieval; Classification methodologies; Classifier ensembles; Combining classifiers; Dimensionality reduction techniques; Ensemble classification; Feature extractor; Feature vectors; MIR; Music classification; XML format; Optimization","C. McKay; Music Technology, McGill University, Montreal, Canada; email: cory.mckay@mail.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Doraisamy S.; Adnan H.; Norowi N.M.","Doraisamy, Shyamala (24765904400); Adnan, Hamdan (55582930600); Norowi, Noris Mohd. (24766399900)","24765904400; 55582930600; 24766399900","Towards a MIR system for Malaysian music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549105544&partnerID=40&md5=1e24eab54589a60cf88b34baf72ab0f2","Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia; Dept. of Music, National Arts Academy, Ministry of Culture, Arts and Heritage, Malaysia","Doraisamy S., Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia; Adnan H., Dept. of Music, National Arts Academy, Ministry of Culture, Arts and Heritage, Malaysia; Norowi N.M., Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia","Systems for the archival of musical documents digitally and development of digital music libraries are currently being researched and developed extensively. However, adapting these systems for the archival and retrieval of Malaysian music materials might not be as straightforward due to the distinct differences in musical structure and modes of non-Western music. This paper covers the motivations for the creation of a MIR system for Malaysian Music and outlines the plans for its development. © 2006 University of Victoria.","Digital libraries; Genre classification; Malaysian music; Music IR; N-grams","Information retrieval; Digital music libraries; Genre classification; Malaysians; Music materials; Musical structures; N-grams; Digital libraries","S. Doraisamy; Dept. of Multimedia, Faculty of Comp. Sc. and IT, University Putra Malaysia, Malaysia; email: shyamala@fsktm.upm.edu.my","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Winget M.","Winget, Megan (23395559800)","23395559800","Heroic frogs save the bow: Performing musician's annotation and interaction behavior with written music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866296333&partnerID=40&md5=8acaf8cb65632c9df6bbd503b3bfffaa","ISchool, University of Texas at Austin, 1 University Station, D7000, Austin, TX 78712-0390, United States","Winget M., ISchool, University of Texas at Austin, 1 University Station, D7000, Austin, TX 78712-0390, United States","Although there have been a number of fairly recent studies in which researchers have explored the information seeking and management behaviors of people interacting with musical retrieval systems, there have been very few published studies of the interaction and use behaviors of musicians themselves. The qualitative research study reported here seeks to correct this deficiency in the literature. Drawing on data collected from nearly 300 annotated parts representing 15 unique works, and 20 musician interviews, we make a number of functionality recommendations for constructive music digital library tool development. For example, all musicians annotate their written music, although this action seems to become more important as the musician becomes more skilled. Musicians' annotations are comprehensible to anyone who can read music, and are valuable as records of interpretation, interaction, and performance. Musicians annotate at the note (rather than at the phrase or movement) level, their annotations are standardized and formal, and are largely non-text. Music digital libraries that cater to musicians should attempt to provide annotation tools that work at the micro level, and extend the symbolic language of the primary document. Furthermore, preserving the annotations for future use would prove valuable for performance students, professionals, and historians alike. © 2006 University of Victoria.","Annotation; Interaction; Musician; Performance","Digital libraries; Information retrieval; Annotation; Annotation tool; Information seeking; Interaction; Interaction behavior; Management behavior; Micro level; Music digital libraries; Musician; Performance; Qualitative research; Retrieval systems; Symbolic languages; Tool development; Behavioral research","M. Winget; ISchool, University of Texas at Austin, 1 University Station, D7000, Austin, TX 78712-0390, United States; email: winget@ischool.utexas.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lehn-Schiøler T.; Arenas-García J.; Petersen K.B.; Hansen L.K.","Lehn-Schiøler, Tue (6507438254); Arenas-García, Jerónimo (6506540366); Petersen, Kaare Brandt (8530662700); Hansen, Lars Kai (35493380300)","6507438254; 6506540366; 8530662700; 35493380300","A genre classification plug-in for data collection","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849113023&partnerID=40&md5=9f7fe7bf0f2185592c6b340f50565ac7","Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark","Lehn-Schiøler T., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Arenas-García J., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Petersen K.B., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; Hansen L.K., Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark","This demonstration illustrates how the methods developed in the MIR community can be used to provide real-time feedback to music users. By creating a genre classifier plugin for a popular media player we present users with relevant information as they play their songs. The plug-in can furthermore be used as a data collection platform. After informed consent from a selected set of users the plug-in will report on music consumption behavior back to a central server. © 2006 University of Victoria.","Data collection; Genre classification; Media player; Plug-in","Information retrieval; Central servers; Data collection; Genre classification; Media players; Plug-ins; Real-time feedback; Data acquisition","T. Lehn-Schiøler; Technical University of Denmark, Informatics and Mathematical Modelling, Kgs. Lyngby DK 2800, Richard Petersens Plads Bld. 321, Denmark; email: tls@imm.dtu.dk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Chai W.; Vercoe B.","Chai, Wei (7005362080); Vercoe, Barry (6506872820)","7005362080; 6506872820","Detection of key change in classical piano music","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873544235&partnerID=40&md5=ae44b88f2168b159b684b7e945588aba","MIT Media Laboratory, Cambridge MA, United States","Chai W., MIT Media Laboratory, Cambridge MA, United States; Vercoe B., MIT Media Laboratory, Cambridge MA, United States","onality is an important aspect of musical structure. Detecting key of music is one of the major tasks in tonal analysis and will benefit semantic segmentation of music for indexing and searching. This paper presents an HMM-based approach for segmenting musical signals based on key change and identifying the key of each segment. Classical piano music was used in the experiment. The performance, evaluated by three proposed measures (recall, precision and label accuracy), demonstrates the promise of the method. © 2005 Queen Mary, University of London.","Hidden Markov Models; Key detection; Music segmentation","Hidden Markov models; Image segmentation; Information retrieval; HMM-based; Music segmentations; Musical signals; Musical structures; Piano music; Semantic segmentation; Musical instruments","W. Chai; MIT Media Laboratory, Cambridge MA, United States; email: chaiwei@media.mit.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"McKay C.; McEnnis D.; Fujinaga I.","McKay, Cory (14033215600); McEnnis, Daniel (16234097400); Fujinaga, Ichiro (9038140900)","14033215600; 16234097400; 9038140900","A large publicly accessible prototype audio database for music research","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849086365&partnerID=40&md5=17933b837dcb39c11bc00d2cda6cbb09","McGill University, Montreal, QC, Canada","McKay C., McGill University, Montreal, QC, Canada; McEnnis D., McGill University, Montreal, QC, Canada; Fujinaga I., McGill University, Montreal, QC, Canada","This paper introduces Codaich, a large and diverse publicly accessible database of musical recordings for use in music information retrieval (MIR) research. The issues that must be dealt with when constructing such a database are discussed, as are ways of addressing these problems. It is suggested that copyright restrictions may be overcome by allowing users to make customized feature extraction queries rather than allowing direct access to recordings themselves. The jMusicMetaManager software is introduced as a tool for improving metadata associated with recordings by automatically detecting inconsistencies and redundancies. © 2006 University of Victoria.","Features; Metadata; MP3s; Music database","Audio acoustics; Feature extraction; Information retrieval; Metadata; Audio database; Features; MP3s; Music database; Music information retrieval; Audio recordings","C. McKay; McGill University, Montreal, QC, Canada; email: cory.mckay@mail.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Burgoyne J.A.; Saul L.K.","Burgoyne, J. Ashley (23007865600); Saul, Lawrence K. (7005701303)","23007865600; 7005701303","Learning harmonic relationships in digital audio with dirichlet-based hidden Markov models","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873542451&partnerID=40&md5=c175cbd2bef47fd1960f1c2e2ec3e825","Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States","Burgoyne J.A., Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States; Saul L.K., Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States","Harmonic analysis is a standard musicological tool for understanding many pieces of Western classical music and making comparisons among them. Traditionally, this analysis is done on paper scores, and most past research in machine-assisted analysis has begun with digital representations of them. Human music students are also taught to hear their musical analyses, however, in both musical recordings and performances. Our approach attempts to teach machines to do the same, beginning with a corpus of recorded Mozart symphonies. The audio files are first transformed into an ordered series of normalized pitch class profile (PCP) vectors. Simplified rules of tonal harmony are encoded in a transition matrix. Classical music tends to change key more frequently than popular music, and so these rules account not only for chords, as most previous work has done, but also for the keys in which they function. A hidden Markov model (HMM) is used with this transition matrix to train Dirichlet distributions for major and minor keys on the PCP vectors. The system tracks chords and keys successfully and shows promise for a real-time implementation. © 2005 Queen Mary, University of London.","Dirichlet; Harmony; HMM; Mozart; PCP","Information retrieval; Real time control; Dirichlet; Harmony; HMM; Mozart; PCP; Hidden Markov models","J.A. Burgoyne; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA 19104, United States; email: burgoyne@cis.upenn.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pampalk E.; Pohle T.; Widmer G.","Pampalk, Elias (6507297042); Pohle, Tim (14036302300); Widmer, Gerhard (7004342843)","6507297042; 14036302300; 7004342843","Dynamic playlist generation based on skipping behavior","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","109","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533505&partnerID=40&md5=687e5aa1151d7b736f5f83820a39a421","Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Pohle T., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","Common approaches to creating playlists are to randomly shuffle a collection (e.g. iPod shuffle) or manually select songs. In this paper we present and evaluate heuristics to adapt playlists automatically given a song to start with (seed song) and immediate user feedback. Instead of rich metadata we use audio-based similarity. The user gives feedback by pressing a skip button if the user dislikes the current song. Songs similar to skipped songs are removed, while songs similar to accepted ones are added to the playlist. We evaluate the heuristics with hypothetical use cases. For each use case we assume a specific user behavior (e.g. the user always skips songs by a particular artist). Our results show that using audio similarity and simple heuristics it is possible to drastically reduce the number of necessary skips. © 2005 Queen Mary, University of London.","","Information retrieval; Metadata; Audio similarities; Audio-based; Simple heuristics; User behaviors; User feedback; Behavioral research","","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Paiement J.-F.; Eck D.; Bengio S.","Paiement, Jean-François (12141594900); Eck, Douglas (12141444300); Bengio, Samy (57203254475)","12141594900; 12141444300; 57203254475","A probabilistic model for chord progressions","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873532933&partnerID=40&md5=d5bbef18face349d7a8b0b70f1aedec1","IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland; Dep. of Computer Science and Operations, Research University of Montreal, Montréal, QC H3C 3J7, CP 6128 succ Centre-Ville, Canada","Paiement J.-F., IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland; Eck D., Dep. of Computer Science and Operations, Research University of Montreal, Montréal, QC H3C 3J7, CP 6128 succ Centre-Ville, Canada; Bengio S., IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland","Chord progressions are the building blocks from which tonal music is constructed. Inferring chord progressions is thus an essential step towards modeling long term dependencies in music. In this paper, a distributed representation for chords is designed such that Euclidean distances roughly correspond to psychoacoustic dissimilarities. Estimated probabilities of chord substitutions are derived from this representation and are used to introduce smoothing in graphical models observing chord progressions. Parameters in the graphical models are learnt with the EM algorithm and the classical Junction Tree algorithm is used for inference. Various model architectures are compared in terms of conditional out-of-sample likelihood. Both perceptual and statistical evidence show that binary trees related to meter are well suited to capture chord dependencies. © 2005 Queen Mary, University of London.","","Acoustics; Algorithms; Binary trees; Graphic methods; Information retrieval; Building blockes; Distributed representation; EM algorithms; Euclidean distance; GraphicaL model; Junction tree algorithms; Long-term dependencies; Model architecture; Probabilistic models; Statistical evidence; Tonal music; Computer music","J.-F. Paiement; IDIAP Research Institute, CH-1920 Martigny, Rue du Simplon 4, Switzerland; email: paiement@idiap.ch","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Oliver N.; Kreger-Stickles L.","Oliver, Nuria (26534738900); Kreger-Stickles, Lucas (15033603500)","26534738900; 15033603500","PAPA: Physiology and purpose-aware automatic playlist generation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873416236&partnerID=40&md5=f8a836a9f21a700bec53e06a9fb8027c","Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States","Oliver N., Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States; Kreger-Stickles L., Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States","In this paper we present PAPA, a novel approach for automatically generating playlists. The proposed framework utilizes the user's physiological response to music, together with traditional song meta-data to generate a playlist the user will not only enjoy, but which will assist him or her in achieving various user-defined goals (""purpose""). In addition to outlining the generic framework, we present an exemplary application named MPTrain that (1) creates a playlist in real-time to assist users in achieving specific exercise goals; and (2) incorporates the user's physiological response to the music to determine the next song to play. © 2006 University of Victoria.","Automatic playlist generation; Physiological monitoring; User modeling","Information retrieval; Patient monitoring; Physiological models; Automatic playlist generation; Generic frameworks; Physiological monitoring; Physiological response; User Modeling; Physiology","N. Oliver; Microsoft Research, Redmond, WA 98052, One Microsoft Way, United States; email: nuria@microsoft.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Ruppin A.; Yeshurun H.","Ruppin, Adi (55583175600); Yeshurun, Hezy (6508241855)","55583175600; 6508241855","MIDI music genre classification by invariant features","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349120625&partnerID=40&md5=3d5d9fe3f00f10a33a65d18d58864b78","School of Computer Science, Tel Aviv University, Israel","Ruppin A., School of Computer Science, Tel Aviv University, Israel; Yeshurun H., School of Computer Science, Tel Aviv University, Israel","MIDI music genre classification methods are largely based on generic text classification techniques. We attempt to leverage music domain knowledge in order to improve classification results. We combine techniques of selection and extraction of musically invariant features with classification using compression distance similarity metric, which is an approximation of the theoretical, yet computationally intractable, Kolmogorov complexity. We introduce several methods for extracting features which are invariant under certain transformations commonly found in music. These methods, combined with data compression, generate a lossy compressed representation which attempts to preserve feature invariance. We analyze the performance of each method, thus gaining insight into the features that are significant to the human perception of music. © 2006 University of Victoria.","Genre classification; Kolmogorov complexity","Data compression; Information retrieval; Knowledge management; Classification results; Domain knowledge; Extracting features; Gaining insights; Genre classification; Human perception; Invariant features; Kolmogorov complexity; Music genre classification; Similarity metrics; Text classification; Classification (of information)","A. Ruppin; School of Computer Science, Tel Aviv University, Israel; email: ruppin@att.biz","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Peeters G.","Peeters, Geoffroy (22433836000)","22433836000","Rhythm classification using spectral rhythm patterns","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","37","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873556216&partnerID=40&md5=aa9903154e56fd5da9b7529ef211ed22","IRCAM, Sound Analysis/Synthesis Team, 75004 Paris, 1, pl. Igor Stravinsky, France","Peeters G., IRCAM, Sound Analysis/Synthesis Team, 75004 Paris, 1, pl. Igor Stravinsky, France","In this paper, we study the use of spectral patterns to represent the characteristics of the rhythm of an audio signal. A function representing the position of onsets over time is first extracted from the audio signal. From this function we compute at each time a vector which represents the characteristics of the local rhythm. Three feature sets are studied for this vector. They are derived from the amplitude of the Discrete Fourier Transform, the Auto- Correlation Function and the product of the DFT and of a Frequency-Mapped ACF. The vectors are then sampled at some specific frequencies, which represents various ratios of the local tempo. The ability of the three feature sets to represent the rhythm characteristics of an audio item is evaluated through a classification task. We show that using such simple spectral representations allows obtaining results comparable to the state of the art. © 2005 Queen Mary, University of London.","Classification; Rhythm representation","Classification (of information); Discrete Fourier transforms; Audio signal; Classification tasks; Correlation function; Feature sets; Rhythm representation; Specific frequencies; Spectral patterns; Spectral representations; State of the art; Information retrieval","G. Peeters; IRCAM, Sound Analysis/Synthesis Team, 75004 Paris, 1, pl. Igor Stravinsky, France; email: peeters@ircam.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lübbers D.","Lübbers, Dominik (55584877900)","55584877900","SoniXplorer: Combining visualization and auralization for content-based exploration of music collections","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549800&partnerID=40&md5=4e1418ea9955aad9705b4d64e786f968","Lehrstuhl Informatik v, RWTH Aachen University, 52072 Aachen, Ahornstr. 55, Germany","Lübbers D., Lehrstuhl Informatik v, RWTH Aachen University, 52072 Aachen, Ahornstr. 55, Germany","Music can be described best by music. However, current research in the design of user interfaces for the exploration of music collections has mainly focused on visualization aspects ignoring possible benefits from spatialized music playback. We describe our first development steps towards two novel user-interface designs: The Sonic Radar arranges a fixed number of prototypes resulting from a content-based clustering process in a circle around the user's standpoint. To derive an auralization of the scene, we introduce the concept of an aural focus of perception that adapts well-known principles from the visual domain. The Sonic SOM is based on Kohonen's Self-Organizing Map. It helps the user in understanding the structure of his music collection by positioning titles on a two-dimensional grid according to their high-dimensional similarity. We show how our auralization concept can be adapted to extend this visualization technique and thereby support multimodal navigation. © 2005 Queen Mary, University of London.","Auralization; Content-based music retrieval; Exploration; User interface; Visualization","Audio recordings; Clustering algorithms; Conformal mapping; Flow visualization; Information retrieval; Natural resources exploration; Visualization; Auralizations; Clustering process; Content-based; Content-based music retrieval; Fixed numbers; High-dimensional; Kohonen; Multi-modal; Music collection; Two-dimensional grids; User interface designs; Visualization technique; User interfaces","D. Lübbers; Lehrstuhl Informatik v, RWTH Aachen University, 52072 Aachen, Ahornstr. 55, Germany; email: luebbers@cs.rwth-aachen.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Liang W.; Zhang S.; Xu B.","Liang, Wei (57682931700); Zhang, Shuwu (35754160500); Xu, Bo (37022633100)","57682931700; 35754160500; 37022633100","A hierarchical approach for audio stream segmentation and classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545843&partnerID=40&md5=8c01bcfdfe1f8f2c2705269f207b6d54","Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","Liang W., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Zhang S., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Xu B., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a hierarchical approach for fast audio stream segmentation and classification. With this approach, the audio stream is firstly segmented into audio clips by MBCR (Multiple sub-Bands spectrum Centroid relative Ratio) based histogram modeling. Then a MGM (Modified Gaussian modeling) based hierarchical classifier is adopted to put the segmented audio clips into six pre-defined categories in terms of discriminative background sounds, which is pure speech, pure music, song, speech with music, speech with noise and silence. The experiments on real TV program recordings showed that this approach has higher accuracy and recall rate for audio classification with a fast speed under noise environments. © 2005 Queen Mary, University of London.","Audio classification; Histogram; MBCR; MGM","Audio acoustics; Computer music; Graphic methods; Information retrieval; Audio classification; Audio clips; Audio stream; Gaussian modeling; Hierarchical approach; Hierarchical classifiers; Histogram; MBCR; MGM; Noise environments; Recall rate; Relative ratios; TV programs; Audio recordings","W. Liang; Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; email: wliang@hitic.ia.ac.cn","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pugin L.","Pugin, Laurent (23009752900)","23009752900","Optical music recognition of early typographic prints using Hidden Markov Models","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","55","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348985923&partnerID=40&md5=905c6d1ff813a1c801deb5bffdbee3df","Music Technology Area, Schulich School of Music, McGill University, Montreal, Canada","Pugin L., Music Technology Area, Schulich School of Music, McGill University, Montreal, Canada","Music printed with movable type (typographic music) from the 16th and 17th centuries contains specific graphic features. In this paper, we present a technique and associated experiments for performing optical music recognition on such music prints using Hidden Markov Models (HMM). Our original approach avoids the difficult and unreliable removal of staff lines usually required before processing. The modeling of symbols on the staff is based on low-level simple features. We show that, using our technique, these features are robust enough to obtain good recognition rates even with poor quality images scanned from microfilm of originals. The music content retrieved by the optical recognition process can be put to significant use in, for example, the creation of searchable digital music libraries. © 2006 University of Victoria.","Early typographic prints; HMM; OMR","Digital libraries; Information retrieval; Digital music libraries; Early typographic prints; HMM; Music contents; Music recognition; OMR; Optical recognition; Quality image; Recognition rates; Hidden Markov models","L. Pugin; Music Technology Area, Schulich School of Music, McGill University, Montreal, Canada; email: laurent@music.mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Arenas-García J.; Larsen J.; Hansen L.K.; Meng A.","Arenas-García, Jerónimo (6506540366); Larsen, Jan (7402980719); Hansen, Lars Kai (35493380300); Meng, Anders (24480225400)","6506540366; 7402980719; 35493380300; 24480225400","Optimal filtering of dynamics in short-time features for music organization","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549082216&partnerID=40&md5=7bfa1fed69b827841248e2d1da19ec18","Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark","Arenas-García J., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Larsen J., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Hansen L.K., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; Meng A., Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark","There is an increasing interest in customizable methods for organizing music collections. Relevant music characterization can be obtained from short-time features, but it is not obvious how to combine them to get useful information. In this work, a novel method, denoted as the Positive Constrained Orthonormalized Partial Least Squares (POPLS), is proposed. Working on the periodograms of MFCCs time series, this supervised method finds optimal filters which pick up the most discriminative temporal information for any music organization task. Two examples are presented in the paper, the first being a simple proof-of-concept, where an altosax with and without vibrato is modelled. A more complex 11 music genre classification setup is also investigated to illustrate the robustness and validity of the proposed method on larger datasets. Both experiments showed the good properties of our method, as well as superior performance when compared to a fixed filter bank approach suggested previously in the MIR literature. We think that the proposed method is a natural step towards a customized MIR application that generalizes well to a wide range of different music organization tasks. © 2006 University of Victoria.","Filter bank model; Music organization; Positive constrained OPLS","Classification (of information); Information retrieval; Optimization; Customizable; Data sets; Music characterization; Music collection; Music genre classification; Natural steps; Optimal filter; Optimal filtering; Partial least square (PLS); Periodograms; Positive constrained OPLS; Proof of concept; Temporal information; Filter banks","J. Arenas-García; Informatics and Mathematical Modelling, Technical University of Denmark, 2800 Kgs. Lyngby, Denmark; email: jag@imm.dtu.dk","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Laplante A.; Downie J.S.","Laplante, Audrey (37110930300); Downie, J. Stephen (7102932568)","37110930300; 7102932568","Everyday life music information-seeking behaviour of young adults","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951199427&partnerID=40&md5=78465f30df1c793cba902aa925c063a8","McGill University, Graduate School of Library and Information Studies, Canada; University of Illinois, Graduate School of Library and Information Science, Urbana-Champaign, United States","Laplante A., McGill University, Graduate School of Library and Information Studies, Canada; Downie J.S., University of Illinois, Graduate School of Library and Information Science, Urbana-Champaign, United States","This poster presents the preliminary results of an ongoing qualitative study on the everyday-life music information seeking behaviour of young adults. The data were collected through in-depth interviews and analyzed following a grounded theory approach. The analysis showed a strong penchant for informal channels (e.g., friends, relative) and, conversely, a distrust of experts. It also emerged that music seeking was mostly motivated by curiosity rather than by actual information needs, which in turn explains why browsing is such a popular strategy. © 2006 University of Victoria.","Music information behaviour; User studies","Grounded theory approach; In-depth interviews; Information need; Information seeking; Music information; Qualitative study; User study; Information retrieval","A. Laplante; McGill University, Graduate School of Library and Information Studies, Canada; email: audrey.laplante@mcgill.ca","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Temperley D.","Temperley, David (6602761758)","6602761758","A probabilistic model of melody perception","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549033114&partnerID=40&md5=8a2429bb4deba735c5b6ce1264a6f2a9","Eastman School of Music, Rochester, NY 14604, 26 Gibbs St., United States","Temperley D., Eastman School of Music, Rochester, NY 14604, 26 Gibbs St., United States","This study presents a probabilistic model of melody perception, which infers the key of a melody and also judges the probability of the melody itself. (A ""melody"" is defined here as a sequence of pitches, without rhythmic information.) The model uses Bayesian reasoning. A generative probabilistic model is proposed, based on three principles: 1) melodies tend to remain within a narrow pitch range; 2) note-to-note intervals within a melody tend to be small; 3) notes tend to conform to a distribution (or ""key-profile"") that depends on the key. The model is tested in three ways: on a key-finding task, on a melodic expectation task, and on an error-detection task. © 2006 University of Victoria.","Error detection; Expectation; Key identification; Music cognition; Probabilistic modeling","Computer music; Error detection; Information retrieval; Bayesian reasoning; Expectation; Music cognition; Probabilistic modeling; Probabilistic models; Probability distributions","D. Temperley; Eastman School of Music, Rochester, NY 14604, 26 Gibbs St., United States; email: dtemperley@esm.rochester.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Essid S.; Richard G.; David B.","Essid, Slim (16033218700); Richard, Gaël (57195915952); David, Bertrand (7103015940)","16033218700; 57195915952; 7103015940","Inferring efficient hierarchical taxonomies for MIR tasks: Application to musical instruments","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873543378&partnerID=40&md5=f3935ed1a38ed5d1b16796551f8a3408","GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France","Essid S., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France; Richard G., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France; David B., GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France","A number of approaches for automatic audio classification are based on hierarchical taxonomies since it is acknowledged that improved performance can be thereby obtained. In this paper, we propose a new strategy to automatically acquire hierarchical taxonomies, using machine learning methods, which are expected to maximize the performance of subsequent classification. It is shown that the optimal hierarchical taxonomy of musical instruments (in the sense of inter-class distances) does not follow the traditional and more intuitive instrument classification into instrument families. © 2005 Queen Mary, University of London.","Clustering; Hierarchical taxonomy; Musical instrument; Probabilistic distance","Audio acoustics; Information retrieval; Learning systems; Musical instruments; Audio classification; Clustering; Hierarchical taxonomy; Inter-class distance; Machine learning methods; Probabilistic distance; Taxonomies","S. Essid; GET-Télécom Paris, CNRS, LTCI, 75014 Paris, 37, rue Dareau, France; email: slim.essid@enst.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Hu X.; Downie J.S.; West K.; Ehmann A.","Hu, Xiao (55496358400); Downie, J. Stephen (7102932568); West, Kris (15766636800); Ehmann, Andreas (8988651500)","55496358400; 7102932568; 15766636800; 8988651500","Mining music reviews: Promising preliminary results","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531799&partnerID=40&md5=e49393d424ab0606f171a86d326fba62","GSLIS, University of Illinois at Urbana-Champaign, United States; School of Computing Sciences, University of East Anglia, United Kingdom; Electrical Engineering, University of Illinois at Urbana-Champaign, United States","Hu X., GSLIS, University of Illinois at Urbana-Champaign, United States; Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; West K., School of Computing Sciences, University of East Anglia, United Kingdom; Ehmann A., Electrical Engineering, University of Illinois at Urbana-Champaign, United States","In this paper we present a system for the automatic mining of information from music reviews. We demonstrate a system which has the ability to automatically classify reviews according to the genre of the music reviewed and to predict the simple one-to-five star rating assigned to the music by the reviewer. This experiment is the first step in the development of a system to automatically mine arbitrary bodies of text, such as weblogs (blogs) for musically relevant information. © 2005 Queen Mary, University of London.","Data mining; Genre; Music reviews; Rating; Text classification","Classification (of information); Data mining; Rating; Arbitrary body; Genre; Relevant informations; Star ratings; Text classification; Weblogs; Information retrieval","X. Hu; GSLIS, University of Illinois at Urbana-Champaign, United States; email: xiaohu@uiuc.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Casagrande N.; Eck D.; Kégl B.","Casagrande, Norman (15080733700); Eck, Douglas (12141444300); Kégl, Balázs (6603766794)","15080733700; 12141444300; 6603766794","Frame-level speech/music discrimination using AdaBoost","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550748&partnerID=40&md5=78788946a659f18f50e31296bbed908f","University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","Casagrande N., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Eck D., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Kégl B., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","In this paper we adapt an AdaBoost-based image processing algorithm to the task of predicting whether an audio signal contains speech or music. We derive a frame-level discriminator that is both fast and accurate. Using a simple FFT and no built-in prior knowledge of signal structure we obtain an accuracy of 88% on frames sampled at 20ms intervals. When we smooth the output of the classifier with the output of the previous 40 frames our forecast rate rises to 93% on the Scheirer-Slaney (Scheirer and Slaney, 1997) database. To demonstrate the efficiency and effectiveness of the model, we have implemented it as a graphical real-time plugin to the popular Winamp audio player. © 2005 Queen Mary, University of London.","","Classification (of information); Image processing; Information retrieval; Speech recognition; Audio players; Audio signal; Efficiency and effectiveness; Image processing algorithm; Plug-ins; Prior knowledge; Signal structures; Speech/music discrimination; Adaptive boosting","N. Casagrande; University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; email: casagran@iro.umontreal.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pickens J.","Pickens, Jeremy (7005535299)","7005535299","Classifier combination for capturing musical variation","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873549357&partnerID=40&md5=2cde14cd73869e0324c637921865feb4","Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","Pickens J., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","At its heart, music information retrieval is characterized by the need to find the similarity between pieces of music. However, .similar. does not mean .the same.. Therefore, techniques for approximate matching are crucial to the development of good music information retrieval systems. Yet as one increases the level of approximation, one finds not only additional similar, relevant music, but also a larger number of not-as-similar, non-relevant music. The purpose of this work is to show that if two different retrieval systems do approximate matching in different manners, and both give decent results, they can be combined to give results better than either system individually. One need not sacrifice accuracy for the sake of fiexibility. © 2005 Queen Mary, University of London.","Approximate matching; Classifier combination","Approximate matching; Classifier combination; Music information retrieval; Retrieval systems; Information retrieval systems","J. Pickens; Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; email: jeremy@dcs.kcl.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kirlin P.B.; Utgoff P.E.","Kirlin, Phillip B. (55582044600); Utgoff, Paul E. (6602417932)","55582044600; 6602417932","VoiSe: Learning to segregate voices in explicit and implicit polyphony","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","31","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873548940&partnerID=40&md5=e35ccfeb0af3b9e58b4540dda29e7dfe","Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","Kirlin P.B., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; Utgoff P.E., Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States","Finding multiple occurrences of themes and patterns in music can be hampered due to polyphonic textures. This is caused by the complexity of music that weaves multiple independent lines of music together. We present and demonstrate a system, VoiSe, that is capable of isolating individual voices in both explicit and implicit polyphonic music. VoiSe is designed to work on a symbolic representation of a music score, and consists of two components: a same-voice predicate implemented as a learned decision tree, and a hard-coded voice numbering algorithm. © 2005 Queen Mary, University of London.","Explicit polyphony; Implicit polyphony; Machine learning; Theme finding; Voice segregation","Decision trees; Learning systems; Explicit polyphony; Implicit polyphony; Music scores; Polyphonic music; Polyphonic texture; Symbolic representation; Theme finding; Two-component; Information retrieval","P.B. Kirlin; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA 01003, United States; email: pkirlin@cs.umass.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Nichols E.; Raphael C.","Nichols, Eric (57196921182); Raphael, Christopher (7004214964)","57196921182; 7004214964","Globally optimal audio partitioning","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956672342&partnerID=40&md5=3ca9984b91ed6dc07ba9fac454ae83a5","Dept. of Computer Science, Indiana Univ., United States; School of Informatics, Indiana Univ., United States","Nichols E., Dept. of Computer Science, Indiana Univ., United States; Raphael C., School of Informatics, Indiana Univ., United States","We present a technique for partitioning an audio file into maximally-sized segments having nearly uniform spectral content, ideally corresponding to notes or chords. Our method uses dynamic programming to globally optimize a measure of simplicity or homogeneity of the intervals in the partition. Here we have focused on an entropy-like measure, though there is considerable flexibility in choosing this measure. Experiments are presented for several musical scenarios1. © 2006 University of Victoria.","Audio partitioning; Dynamic programming","Dynamic programming; Information retrieval; Audio files; Audio partitioning; Spectral content; Optimization","E. Nichols; Dept. of Computer Science, Indiana Univ., United States; email: epnichol@indiana.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lee J.H.; Jones M.C.; Downie J.S.","Lee, Jin Ha (57190797465); Jones, M. Cameron (57199028615); Downie, J. Stephen (7102932568)","57190797465; 57199028615; 7102932568","Factors affecting response rates for real-life MIR queries","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873418427&partnerID=40&md5=b2706fc7b95974c80f8f972c305b44b6","Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","Lee J.H., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Jones M.C., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States","In this poster we present preliminary findings of an exploratory study of natural language music information queries posted to the Google Answers web site. We discuss the proportion of queries answered as a function of time and attempt to identify factors which affect the probability of a query being answered. © 2006 University of Victoria.","Google answers; HUMIRS; Queries; Users","Information retrieval; Exploratory studies; Function of time; Google answers; HUMIRS; Music information; Natural languages; Queries; Response rate; Users; Search engines","J.H. Lee; Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; email: jinlee1@uiuc.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Collins N.","Collins, Nick (24478398900)","24478398900","Using a pitch detector for onset detection","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","34","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873551830&partnerID=40&md5=b9b72631b3c9b82a3bd52d0621333651","University of Cambridge, Centre for Music and Science, Cambridge, CB3 9DP, 11 West Road, United Kingdom","Collins N., University of Cambridge, Centre for Music and Science, Cambridge, CB3 9DP, 11 West Road, United Kingdom","A segmentation strategy is explored for monophonic instrumental pitched non-percussive material (PNP) which proceeds from the assertion that human-like event analysis can be founded on a notion of stable pitch percept. A constant-Q pitch detector following the work of Brown and Puckette provides pitch tracks which are post processed in such a way as to identify likely transitions between notes. A core part of this preparation of the pitch detector signal is an algorithm for vibrato suppression. An evaluation task is undertaken on slow attack and high vibrato PNP source files with human annotated onsets, exemplars of a difficult case in monophonic source segmentation. The pitch track onset detection algorithm shows an improvement over the previous best performing algorithm from a recent comparison study of onset detectors. Whilst further timbral cues must play a part in a general solution, the method shows promise as a component of a note event analysis system. © 2005 Queen Mary, University of London.","Onset detection; Pitch detection; Segmentation","Algorithms; Image segmentation; Information retrieval; Comparison study; Core part; Detector signals; Event analysis; General solutions; Onset detection; Pitch detection; Source files; Detectors","N. Collins; University of Cambridge, Centre for Music and Science, Cambridge, CB3 9DP, 11 West Road, United Kingdom; email: nc272@cam.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Goto M.; Goto T.","Goto, Masataka (7403505330); Goto, Takayuki (55585360000)","7403505330; 55585360000","Musicream: New music playback interface for streaming, sticking, sorting, and recalling musical pieces","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","58","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873539321&partnerID=40&md5=4cff6b96fa3d397c928d57f07d4284d8","National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","Goto M., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; Goto T., National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan","This paper describes a novel music playback interface, called Musicream, which lets a user unexpectedly come across various musical pieces similar to those liked by the user. With most previous ""query-by-example"" interfaces used for similarity-based searching, for the same query and music collection a user will always receive the same list of musical pieces ranked by their similarity and opportunities to encounter unfamiliar musical pieces in the collection are limited. Musicream facilitates active, flexible, and unexpected encounters with musical pieces by providing four functions: the music-disc streaming function which creates a flow of many musical-piece entities (discs) from a (huge) music collection, the similaritybased sticking function which allows a user to easily pick out and listen to similar pieces from the flow, the metaplaylist function which can generate a playlist of playlists (ordered lists of pieces) while editing them with a high degree of freedom, and the time-machine function which automatically records all Musicream activities and allows a user to visit and retrieve a past state as if using a time machine. In our experiments, these functions were used seamlessly to achieve active and creative querying and browsing of music collections, confirming the effectiveness of Musicream. © 2005 Queen Mary, University of London.","Music interface; Music player; Musiccollection browser; Playlist; Query-by-example","Information retrieval; Music interfaces; Music players; Musiccollection browser; Playlist; Query-by-example; Audio recordings","M. Goto; National Institute of Advanced Industrial Science and Technology (AIST), IT, AIST, Tsukuba, Ibaraki 305-8568, 1-1-1 Umezono, Japan; email: m.goto@aist.go.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bruderer M.J.; McKinney M.; Kohlrausch A.","Bruderer, Michael J. (19336918000); McKinney, Martin (57225339869); Kohlrausch, Armin (7004271004)","19336918000; 57225339869; 7004271004","Structural boundary perception in popular music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149149378&partnerID=40&md5=7ccd0272975f24804da7fda17ce980fd","Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands; Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4 (WO-02), Netherlands","Bruderer M.J., Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands; McKinney M., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4 (WO-02), Netherlands; Kohlrausch A., Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands, Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4 (WO-02), Netherlands","The automatic extraction of musical structure from audio is an important aspect for many music information retrieval (MIR) systems. The criteria on which structural elements in music are defined in MIR systems is often not clearly stated but typically stem from (music) theoretical or signal-based properties. In many cases, however, perceptual-based criteria are the most relevant and systems need to be trained on or modeled after the perception of structural elements in music. Here, we investigate the perception of structural boundaries to Western popular music and examine the musical cues responsible for their perception. We make links to music theoretical descriptions of structural boundaries and to computational methods for extracting structure. The methods and data presented here are useful for developing and training systems for the automatic extraction of musical structure as it is perceived by listeners. © 2006 University of Victoria.","Music cognition; Music perception; Music segmentation; Music structure","Information retrieval; Automatic extraction; Music cognition; Music information retrieval; Music perception; Music segmentations; Music structures; Musical structures; Popular music; Structural boundary; Structural elements; Training Systems; Audio systems","M.J. Bruderer; Technische Universiteit Eindhoven, 5600 Eindhoven, Postbus 513, Netherlands; email: m.j.bruderer@tm.tue.nl","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mesaros A.; Astola J.","Mesaros, Annamaria (14632356500); Astola, Jaakko (35232592700)","14632356500; 35232592700","The Mel-Frequency Cepstral Coefficients in the context of singer identification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873530192&partnerID=40&md5=703f059ec550961d0c2091ed95e5bcd6","Technical University of Cluj Napoca, Communications Department, Cluj Napoca, Romania; Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","Mesaros A., Technical University of Cluj Napoca, Communications Department, Cluj Napoca, Romania, Institute of Signal Processing, Tampere University of Technology, Tampere, Finland; Astola J., Institute of Signal Processing, Tampere University of Technology, Tampere, Finland","The singing voice is the oldest and most complex musical instrument. A familiar singer's voice is easily recognizable for humans, even when hearing a song for the first time. On the other hand, for automatic identification this is a difficult task among sound source identification applications. The signal processing techniques aim to extract features that are related to identity characteristics. The research presented in this paper considers 32 Mel-Frequency Cepstral Coefficients in two subsets: the low order MFCCs characterizing the vocal tract resonances and the high order MFCCs related to the glottal wave shape. We explore possibilities to identify and discriminate singers using the two sets. Based on the results we can affirm that both subsets have their contribution in defining the identity of the voice, but the high order subset is more robust to changes in singing style. © 2005 Queen Mary, University of London.","MFCC; Singing voice; Sound source identification","Acoustic generators; Audition; Automation; Information retrieval; Signal processing; Automatic identification; Low order; Mel-frequency cepstral coefficients; MFCC; Order subsets; Signal processing technique; Singing styles; Singing voices; Sound source identification; Vocal tract resonances; Wave shape; Speech recognition","A. Mesaros; Technical University of Cluj Napoca, Communications Department, Cluj Napoca, Romania; email: annamaria.mesaros@com.utcluj.ro","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Novello A.; McKinney M.F.; Kohlrausch A.","Novello, Alberto (19337515900); McKinney, Martin F. (57225339869); Kohlrausch, Armin (7004271004)","19337515900; 57225339869; 7004271004","Perceptual evaluation of music similarity","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960936062&partnerID=40&md5=1c7f568c512bb6f1c858af9f833406bf","Philips Research Laboratories, Eindhoven, Netherlands; Human Technology Interaction, Technische Universiteit Eindhoven, Eindhoven, Netherlands","Novello A., Philips Research Laboratories, Eindhoven, Netherlands; McKinney M.F., Philips Research Laboratories, Eindhoven, Netherlands; Kohlrausch A., Philips Research Laboratories, Eindhoven, Netherlands, Human Technology Interaction, Technische Universiteit Eindhoven, Eindhoven, Netherlands","This paper presents an empirical method for assessing music similarity on a set of stimuli using triadic comparisons in a balanced incomplete block design. We first evaluated the consistency of subjects in their rankings and then the concordance across subjects. The concordance was also evaluated for different subject populations to assess the influence of experience of the subject with the musical material. We finally analysed subjects' ranking by the means of multidimensional scaling. Similarity judgments were found to be rather concordant across subjects. Significant differences between musicians and non-musicians and between subjects being familiar or non-familiar with the music were found for a small number of cases. Multidimensional scaling reveals a proximity of songs belonging to the same genre, congruent with the idea of genre being a perceptual dimension in subjects' similarity ranking. © 2006 University of Victoria.","Music; Music similarity; Perception","Sensory perception; Balanced incomplete block design; Empirical method; Multi-dimensional scaling; Music; Music similarity; Musical materials; Perceptual evaluation; Similarity rankings; Information retrieval","A. Novello; Philips Research Laboratories, Eindhoven, Netherlands; email: alberto.novello@philips.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Vembu S.; Baumann S.","Vembu, Shankar (58000591000); Baumann, Stephan (36674422000)","58000591000; 36674422000","Separation of vocals from polyphonic audio recordings","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","85","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873538214&partnerID=40&md5=26513d2047f384c6853a8358fc002df0","German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany","Vembu S., German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany; Baumann S., German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany","Source separation techniques like independent component analysis and the more recent non-negative matrix factorization are gaining widespread use for the monaural separation of individual tracks present in a music sample. The underlying principle behind these approaches characterises only stationary signals and fails to separate nonstationary sources like speech or vocals. In this paper, we make an attempt to solve this problem and propose solutions to the extraction of vocal tracks from polyphonic audio recordings. We also present techniques to identify vocal sections in a music sample and design a classifier to perform a vocal-nonvocal segmentation task. Finally, we describe an application wherein we try to extract the melody from the separated vocal track using existing monophonic transcription techniques. The experimental work leads us to the conclusion that the quality of vocal source separation, albeit satisfactory, is not sufficient enough for further F0 analysis to extract the melody line from the vocal track. We identify areas that need further investigation to improve the quality of vocal source separation. © 2005 Queen Mary, University of London.","Blind source separation; Independent component analysis; Melody extraction; Non-negativematrix factorization; Vocal-nonvocal discrimination","Blind source separation; Extraction; Factorization; Independent component analysis; Information retrieval; Melody extractions; Non-stationary sources; Nonnegative matrix factorization; Separation techniques; Stationary signal; Underlying principles; Vocal-nonvocal discrimination; Audio recordings","S. Vembu; German Research Centre for AI, 67663 Kaiserslautern, Erwin-Schroedinger-Str. 57, Germany; email: shankar.vembu@dfki.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Downie J.S.; West K.; Ehmann A.; Vincent E.","Downie, J. Stephen (7102932568); West, Kris (15766636800); Ehmann, Andreas (8988651500); Vincent, Emmanuel (14010158800)","7102932568; 15766636800; 8988651500; 14010158800","The 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005): Preliminary overview","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","80","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563532&partnerID=40&md5=ea43331e38649defd76041b719aa6d42","GSLIS, University of Illinois at Urbana-Champaign, United States; School of Computing Sciences, University of East Anglia, United Kingdom; Electrical Engineering, University of Illinois at Urbana-Champaign, United States; Electronic Engineering, Queen Mary University of London, United Kingdom","Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; West K., School of Computing Sciences, University of East Anglia, United Kingdom; Ehmann A., Electrical Engineering, University of Illinois at Urbana-Champaign, United States; Vincent E., Electronic Engineering, Queen Mary University of London, United Kingdom","This paper is an extended abstract which provides a brief preliminary overview of the 2005 Music Information Retrieval Evaluation eXchange (MIREX 2005). The MIREX organizational framework and infrastructure are outlined. Summary data concerning the 10 evaluation contests is provided. Key issues affecting future MIR evaluations are identified and discussed. The paper concludes with a listing of targets items to be undertaken before MIREX 2006 to ensure the ongoing success of the MIREX framework. © 2005 Queen Mary, University of London.","Evaluation; MIREX 2005","Abstracting; Information retrieval; Evaluation; Extended abstracts; Key Issues; MIREX 2005; Music information retrieval; Organizational frameworks; Insecticides","J.S. Downie; GSLIS, University of Illinois at Urbana-Champaign, United States; email: jdownie@uiuc.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lidy T.; Rauber A.","Lidy, Thomas (23035315800); Rauber, Andreas (57074846700)","23035315800; 57074846700","Evaluation of feature extractors and psycho-acoustic transformations for music genre classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","187","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873536955&partnerID=40&md5=a1840af629d22735f583970819d7231a","Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","Lidy T., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; Rauber A., Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria","We present a study on the importance of psycho-acoustic transformations for effective audio feature calculation. From the results, both crucial and problematic parts of the algorithm for Rhythm Patterns feature extraction are identified. We furthermore introduce two new feature representations in this context: Statistical Spectrum Descriptors and Rhythm Histogram features. Evaluation on both the individual and combined feature sets is accomplished through a music genre classification task, involving 3 reference audio collections. Results are compared to published measures on the same data sets. Experiments confirmed that in all settings the inclusion of psycho-acoustic transformations provides significant improvement of classification accuracy. © 2005 Queen Mary, University of London.","Audio feature extraction; Content-based retrieval; Music genre classification; Psycho-acoustic","Content based retrieval; Feature extraction; Information retrieval; Audio feature extraction; Audio features; Classification accuracy; Combined features; Descriptors; Feature extractor; Feature representation; Histogram features; Music genre classification; Psycho-acoustic; Audio acoustics","T. Lidy; Vienna University of Technology, Department of Software Technology and Interactive Systems, A-1040 Vienna, Favoritenstrasse 9-11/188, Austria; email: lidy@ifs.tuwien.ac.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Riley J.","Riley, Jenn (57197504438)","57197504438","Exploiting musical connections: A proposal for support of work relationships in a Digital Music Library","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533846&partnerID=40&md5=3e9a8c0a80a55a785c22b9fe14945ee8","Indiana University Digital Library Program, Bloomington, IN 47404, 1320 E 10th St, E170, United States","Riley J., Indiana University Digital Library Program, Bloomington, IN 47404, 1320 E 10th St, E170, United States","Musical works in the Western art music tradition exist in a complex, inter-related web. Works that are derivative or part of another work are common; however, most music information retrieval systems, including traditional library catalogs, don't use these essential relationships to improve search results or provide information about them to end-users. As part of the NSF-funded Variations2 Digital Music Library project at Indiana University, we have developed a set of functional requirements defining how derivative and whole/part relationships between musical works should be acted upon in search results, and how these results should be displayed. This paper describes recent research into these relationships, provides examples why they are important in Western art music, outlines how Variations2 or any other music information retrieval system could use these relationships in matching user queries, and describes optimal displays of these relationships to end-users. © 2005 Queen Mary, University of London.","Bibliographic relationships; Digital music libraries; Metadata","Metadata; Bibliographic relationships; Digital music libraries; End-users; Functional requirement; Indiana University; Library catalogs; Music information retrieval; Recent researches; Search results; User query; Digital libraries","J. Riley; Indiana University Digital Library Program, Bloomington, IN 47404, 1320 E 10th St, E170, United States; email: jenlrile@indiana.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"West K.; Cox S.","West, Kris (15766636800); Cox, Stephen (7401456306)","15766636800; 7401456306","Finding an optimal segmentation for audio genre classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","46","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531705&partnerID=40&md5=7bd071c4cb768ede45e4dc2fba692ae3","School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom","West K., School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom; Cox S., School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom","In the automatic classification of music many different segmentations of the audio signal have been used to calculate features. These include individual short frames (23 ms), longer frames (200 ms), short sliding textural windows (1 sec) of a stream of 23 ms frames, large fixed windows (10 sec) and whole files. In this work we present an evaluation of these different segmentations, showing that they are sub-optimal for genre classification and introduce the use of an onset detection based segmentation, which appears to outperform all of the fixed and sliding windows segmentation schemes in terms of classification accuracy and model size. © 2005 Queen Mary, University of London.","Classification; Detection; Genre; Onset; Segmentation","Classification (of information); Error detection; Image segmentation; Information retrieval; Audio signal; Automatic classification; Classification accuracy; Genre; Genre classification; Model size; Onset; Onset detection; Optimal segmentation; Segmentation scheme; Sliding Window; Optimization","K. West; School of Computing Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdom; email: kw@cmp.uea.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Hu X.; Downie J.S.; Ehmann A.F.","Hu, Xiao (55496358400); Downie, J. Stephen (7102932568); Ehmann, Andreas F. (8988651500)","55496358400; 7102932568; 8988651500","Exploiting recommended usage metadata: Exploratory analyses","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849091064&partnerID=40&md5=05e826d31b6df7b6dac33a54dfa474a2","Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign, United States","Hu X., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; Ehmann A.F., Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign, United States","In this paper, we conduct a series of exploratory analyses on the user-recommended usages of music as generated by 1,042 reviewers who have posted to www.epinions.com. Using hierarchical clustering methods on data derived from the co-occurrence analyses of usage and genre, usage and artist, and usage and album, we are able to conclude that further investigation of user-recommended usage metadata is warranted, especially with regard to its implications for future iterations of the Music Information Retrieval Evaluation eXchange (MIREX). © 2006 University of Victoria.","Hierarchical clustering; HUMIRS; MIREX; Music reviews; User study; User-recommended usage","Information retrieval; Insecticides; Hier-archical clustering; HUMIRS; MIREX; User study; User-recommended usage; Metadata","X. Hu; Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign, United States; email: xiaohu@uiuc.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Byrd D.; Schindele M.","Byrd, Donald (55041494100); Schindele, Megan (55583258600)","55041494100; 55583258600","Prospects for improving OMR with multiple recognizers","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","33","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56549094127&partnerID=40&md5=9e74f44ca856011c41449eba8d1d3999","School of Informatics, Indiana University, Bloomington, United States; School of Music, Indiana University, Bloomington, United States","Byrd D., School of Informatics, Indiana University, Bloomington, United States, School of Music, Indiana University, Bloomington, United States; Schindele M., School of Music, Indiana University, Bloomington, United States","OMR (Optical Music Recognition) programs have been available for years, but they still leave much to be desired in terms of accuracy. We studied the feasibility of achieving substantially better accuracy by using the output of several programs to ""triangulate"" and get better results than any of the individual programs; this multiple-recognizer approach has had some success with other media but, to our knowledge, has never been tried for music. A major obstacle is that the complexity of music notation is such that evaluating OMR accuracy is difficult for any but the simplest music. Nonetheless, existing programs have serious enough limitations that the multiple-recognizer approach is promising. © 2006 University of Victoria.","Classifier; OMR; Optical Music Recognition; Recognizer","Classifiers; Music notation; Music recognition; OMR; Recognizer; Information retrieval","D. Byrd; School of Informatics, Indiana University, Bloomington, United States; email: donbyrd@indiana.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mandel M.I.; Ellis D.P.W.","Mandel, Michael I. (14060460000); Ellis, Daniel P.W. (13609089200)","14060460000; 13609089200","Song-level features and support vector machines for music classification","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","198","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873528643&partnerID=40&md5=5c3fde2d321ca5571816a8bd54b6eb90","LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States","Mandel M.I., LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States; Ellis D.P.W., LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States","Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplarbased classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we show that this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. © 2005 Queen Mary, University of London.","Artist identification; Kernel spaces; Song classification; Support vector machines","Information retrieval; Automatic classification; Digital music; Exemplar-based; Gaussians; Kernel space; KL-divergence; Mahalanobis distances; Music classification; Support vector machines","M.I. Mandel; LabROSA, Dept. of Elec. Eng., Columbia University, New York, NY, United States; email: mim@ee.columbia.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cantone D.; Cristofaro S.; Faro S.","Cantone, Domenico (55892604800); Cristofaro, Salvatore (8881779200); Faro, Simone (8881779300)","55892604800; 8881779200; 8881779300","Solving the (δ, α)-approximate matching problem under transposition invariance in musical sequences","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563422&partnerID=40&md5=7493d636f9c572501dd4314333bc5825","Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","Cantone D., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Cristofaro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Faro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","The δ-approximate matching problem arises in many questions concerning musical information retrieval and musical analysis. In the case in which gaps are not allowed between consecutive pitches of the melody, transposition invariance is automatically taken care of, provided that the musical melodies are encoded using the pitch interval encoding. However, in the case in which nonnull gaps are allowed between consecutive pitches of the melodies, transposition invariance is not dealt with properly by the algorithms present in literature. In this paper, we propose two slightly different variants of the approximate matching problem under transposition invariance and for each of them provide an algorithm, obtained by adapting an efficient algorithm for the δ-approximate matching problem with α-bounded gaps. © 2005 Queen Mary, University of London.","Approximate string matching; Experimental algorithms; Musical information retrieval","Algorithms; Information retrieval; Approximate matching; Approximate string matching; Experimental algorithms; Musical analysis; Musical information retrieval; Transposition invariance; Pattern matching","D. Cantone; Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; email: cantone@dmi.unict.it","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"McEnnis D.; McKay C.; Fujinaga I.; Depalle P.","McEnnis, Daniel (16234097400); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900); Depalle, Philippe (35577100200)","16234097400; 14033215600; 9038140900; 35577100200","Audio: A feature extraction library","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","151","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533157&partnerID=40&md5=8b39cbf46adc5387120a5207aa33aca5","Faculty of Music, McGill University, Montreal, Canada","McEnnis D., Faculty of Music, McGill University, Montreal, Canada; McKay C., Faculty of Music, McGill University, Montreal, Canada; Fujinaga I., Faculty of Music, McGill University, Montreal, Canada; Depalle P., Faculty of Music, McGill University, Montreal, Canada","jAudio is a new framework for feature extraction designed to eliminate the duplication of effort in calculating features from an audio signal. This system meets the needs of MIR researchers by providing a library of analysis algorithms that are suitable for a wide array of MIR tasks. In order to provide these features with a minimal learning curve, the system implements a GUI that makes the process of selecting desired features straight forward. A command-line interface is also provided to manipulate jAudio via scripting. Furthermore, jAudio provides a unique method of handling multidimensional features and a new mechanism for dependency handling to prevent duplicate calculations. The system takes a sequence of audio files as input. In the GUI, users select the features that they wish to have extracted-letting jAudio take care of all dependency problems-and either execute directly from the GUI or save the settings for batch processing. The output is either an ACE XML file or an ARFF file depending on the user's preference. © 2005 Queen Mary, University of London.","Audio feature extraction; Java audio environment; Music information retrieval","Batch data processing; Graphical user interfaces; Information retrieval; Analysis algorithms; Audio feature extraction; Audio files; Audio signal; Command-line interfaces; Java audio environment; Learning curves; Music information retrieval; New mechanisms; User's preferences; XML files; Feature extraction","D. McEnnis; Faculty of Music, McGill University, Montreal, Canada; email: daniel.mcennis@mail.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cabral G.; Pachet F.; Briot J.-P.","Cabral, Giordano (14041251800); Pachet, François (6701441655); Briot, Jean-Pierre (6603795538)","14041251800; 6701441655; 6603795538","Automatic X traditional descriptor extraction: The case of chord recognition","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560554&partnerID=40&md5=6524c7c671542501318c5967a486c2f7","LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France; Sony CSL Paris, 75005 Paris, 6 Rue Amyot, France","Cabral G., LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France; Pachet F., Sony CSL Paris, 75005 Paris, 6 Rue Amyot, France; Briot J.-P., LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France","Audio descriptor extraction is the activity of finding mathematical models which describe properties of the sound, requiring signal processing skills. The scientific literature presents a vast collection of descriptors (e.g. energy, tempo, tonality) each one representing a significant effort of research in finding an appropriate descriptor for a particular application. The Extractor Discovery System (EDS) [1] is a recent approach for the discovery of such descriptors, which aim is to extract them automatically. This system can be useful for both non experts - who can let the system work fully automatically - and experts - who can start the system with an initial solution expecting it to enhance their results. Nevertheless, EDS still needs to be massively tested. We consider that its comparison with the results of problems already studied would be very useful to validate it as an effective tool. This work intends to perform the first part of this validation, comparing the results from classic approaches with EDS results when operated by a completely naïve user building a guitar chord recognizer. © 2005 Queen Mary, University of London.","Chord recognition; Descriptor extraction","Audio acoustics; Information retrieval; Mathematical models; Signal processing; Chord recognition; Descriptor extractions; Descriptors; Discovery systems; Effective tool; Initial solution; Scientific literature; Extraction","G. Cabral; LIP6 - Paris 6, 75018 Paris, 8 Rue du Capitaine Scott, France; email: Giordano.CABRAL@lip6.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Abdallah S.; Noland K.; Sandler M.; Casey M.; Rhodes C.","Abdallah, Samer (11540522000); Noland, Katy (18042247700); Sandler, Mark (7202740804); Casey, Michael (15080769900); Rhodes, Christophe (57196565939)","11540522000; 18042247700; 7202740804; 15080769900; 57196565939","Theory and evaluation of a Bayesian music structure extractor","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","40","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541730&partnerID=40&md5=a4bf002f6a58afd3977ce55c34557fa5","Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Centre for Cognition, Computation and Culture, Goldsmiths College, University of London, London SE14 6NW, New Cross Gate, United Kingdom","Abdallah S., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Noland K., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; Casey M., Centre for Cognition, Computation and Culture, Goldsmiths College, University of London, London SE14 6NW, New Cross Gate, United Kingdom; Rhodes C., Centre for Cognition, Computation and Culture, Goldsmiths College, University of London, London SE14 6NW, New Cross Gate, United Kingdom","We introduce a new model for extracting classified structural segments, such as intro, verse, chorus, break and so forth, from recorded music. Our approach is to classify signal frames on the basis of their audio properties and then to agglomerate contiguous runs of similarly classified frames into texturally homogenous (or 'self-similar') segments which inherit the classificaton of their consituent frames. Our work extends previous work on automatic structure extraction by addressing the classification problem using using an unsupervised Bayesian clustering model, the parameters of which are estimated using a variant of the expectation maximisation (EM) algorithm which includes deterministic annealing to help avoid local optima. The model identifies and classifies all the segments in a song, not just the chorus or longest segment. We discuss the theory, implementation, and evaluation of the model, and test its performance against a ground truth of human judgements. Using an analogue of a precisionrecall graph for segment boundaries, our results indicate an optimal trade-off point at approximately 80% precision for 80% recall. © 2005 Queen Mary, University of London.","Audio; Boundary; Segmentation; Structure","Algorithms; Image segmentation; Structure (composition); Audio; Automatic structures; Bayesian; Bayesian clustering; Boundary; Deterministic annealing; Ground truth; Local optima; Longest segments; Music structures; Precision-recall graphs; Self-similar; Trade-off point; Information retrieval","S. Abdallah; Centre for Digital Music, Queen Mary, University of London, London E1, Mile End Road, United Kingdom; email: samer.abdallah@elec.qmul.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Harte C.; Sandler M.; Abdallah S.; Gómez E.","Harte, Christopher (18036993000); Sandler, Mark (7202740804); Abdallah, Samer (11540522000); Gómez, Emilia (14015483200)","18036993000; 7202740804; 11540522000; 14015483200","Symbolic representation of musical chords: A proposed syntax for text annotations","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","138","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873531256&partnerID=40&md5=d7d21bce72b2d0c1ea3fb24e8e05a8f1","Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Music Technology Group, IUA, Universitat Pompeu Fabra, Barcelona, Spain","Harte C., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Sandler M., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Abdallah S., Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; Gómez E., Music Technology Group, IUA, Universitat Pompeu Fabra, Barcelona, Spain","In this paper we propose a text represention for musical chord symbols that is simple and intuitive for musically trained individuals to write and understand, yet highly structured and unambiguous to parse with computer programs. When designing feature extraction algorithms, it is important to have a hand annotated test set providing a ground truth to compare results against. Hand labelling of chords in music files is a long and arduous task and there is no standard annotation methodology, which causes difficulties sharing with existing annotations. In this paper we address this problem by defining a rigid, contextindependent syntax for representing chord symbols in text, supported with a new database of annotations using this system. © 2005 Queen Mary, University of London.","Annotation; Chords; Harmony; Music; Notation","Feature extraction; Information retrieval; Annotation; Chords; Harmony; Music; Notation; Syntactics","C. Harte; Centre for Digital Music, Queen Mary, University of London, London, Mile End Road, United Kingdom; email: christopher.harte@elec.qmul.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Celma O.; Cano P.; Herrera P.","Celma, Òscar (12804596800); Cano, Pedro (56248848600); Herrera, Perfecto (24824250300)","12804596800; 56248848600; 24824250300","Search sounds: An audio crawler focused on weblogs","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863753638&partnerID=40&md5=93063ae05ccd4a40d67b4db457be4a2b","Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain","Celma O., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain; Cano P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain","In this paper we present a focused audio crawler that mines audio weblogs (MP3 blogs). This source of semi-structured information contains links to audio files, plus some textual information that is referring to the media file. A retrieval system-that exploits the mined data-fetches relevant audio files related to user's text query. Based on these results, the user can navigate and discover new music by means of content-based audio similarity. The system is available at: http://www.searchsounds.net. © 2006 University of Victoria.","Content-based similarity; Focused audio crawler; Music recommendation; Weblogs","Information retrieval; Social networking (online); Audio files; Content-based; Focused audio crawler; Media files; Music recommendation; Semi-structured information; Text query; Textual information; Weblogs; Audio acoustics","O. Celma; Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval.lació 8, Spain; email: ocelma@iua.upf.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Seppänen J.; Eronen A.; Hiipakka J.","Seppänen, Jarno (7003521196); Eronen, Antti (14010411600); Hiipakka, Jarmo (6508015833)","7003521196; 14010411600; 6508015833","Joint beat & tatum tracking from music signals","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049217163&partnerID=40&md5=1f0694591c69a2ef081c23181a12150e","Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland","Seppänen J., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland; Eronen A., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland; Hiipakka J., Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland","This paper presents a method for extracting two key metrical properties, the beat and the tatum, from acoustic signals of popular music. The method is computationally very efficient while performing comparably to earlier methods. High efficiency is achieved through multirate accent analysis, discrete cosine transform periodicity analysis, and phase estimation by adaptive comb filtering. During analysis, the music signals are first represented in terms of accentuation on four frequency subbands, and then the accent signals are transformed into periodicity domain. Beat and tatum periods and phases are estimated in a probabilistic setting, incorporating primitive musicological knowledge of beat-tatum relations, the prior distributions, and the temporal continuities of beats and tatums. In an evaluation with 192 songs, the beat tracking accuracy of the proposed method was found comparable to the state of the art. Complexity evaluation showed that the computational cost is less than 1% of earlier methods. The authors have written a real-time implementation of the method for the S60 smartphone platform. © 2006 University of Victoria.","Beat tracking; Music meter estimation; Rhythm analysis","Discrete cosine transforms; Information retrieval; Real time control; Acoustic signals; Beat tracking; Computational costs; Multi rate; Music signals; Periodicity analysis; Phase estimation; Popular music; Prior distribution; Real-time implementations; Rhythm analysis; State of the art; Subbands; Temporal continuity; Frequency domain analysis","J. Seppänen; Nokia Research Center, FI-00045 Nokia Group, P.O. Box 407, Finland; email: jarno.seppanen@nokia.com","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Kurth F.; Gehrmann T.; Müller M.","Kurth, Frank (56240850200); Gehrmann, Thorsten (55582130600); Müller, Meinard (7404689873)","56240850200; 55582130600; 7404689873","The cyclic beat spectrum: Tempo-related audio features for time-scale invariant audio identification","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","21","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36549014432&partnerID=40&md5=7a9f49e0f88e871abdc64cf2ff704894","Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","Kurth F., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Gehrmann T., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; Müller M., Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany","In this paper, we present a novel set of tempo-related audio features for applications in audio retrieval. As opposed to existing feature sets commonly used in the retrieval domain which mainly focus on local spectral characteristics of the audio signal, our features capture its local temporal behaviour w.r.t. tempo, rhythm, and meter. As a key component to obtaining a high level of feature robustness we introduce the cyclic beat spectrum (CBS) consisting of residual tempo classes which are constructed similarly to the well-known pitch chroma classes. We illustrate the use of the newly constructed features by applying them to robust time-scale invariant audio identification. © 2006 University of Victoria.","Cyclic beat spectrum; Tempo-related audio features; Time-scale invariant audio identification","Audio features; Audio identification; Audio retrieval; Audio signal; Cyclic beat spectrum; Feature sets; Spectral characteristics; Temporal behaviour; Time-scales; Information retrieval","F. Kurth; Department of Computer Science, University of Bonn, 53117 Bonn, Römerstraße 164, Germany; email: frank@cs.uni-bonn.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Van Gulik R.; Vignoli F.","Van Gulik, Rob (55584995600); Vignoli, Fabio (11240995100)","55584995600; 11240995100","Visual playlist generation on the artist map","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","40","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873529473&partnerID=40&md5=4bbd1b876e60720cd9f77c6e3b4586d8","Institute of Information and Computing Sciences, Utrecht University, 3584 CH, Utrecht, Padualaan 14, Netherlands; Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Van Gulik R., Institute of Information and Computing Sciences, Utrecht University, 3584 CH, Utrecht, Padualaan 14, Netherlands; Vignoli F., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","This paper describes a visual playlist creation method based on a previously designed visualization technique for large music collections. The method gives users high-level control over the contents of a playlist as well as the progression of songs in it, while minimizing the interaction requirements. An interesting feature of the technique is that it creates playlists that are independent of the underlying music collection, making them highly portable. Future work includes an extensive user evaluation to compare the described method with alternative techniques and to measure its qualities, such as the perceived ease of use and perceived usefulness. © 2005 Queen Mary, University of London.","Artist map; Playlist generation; Visualization of music collections","Audio recordings; Information retrieval; Highly-portable; Large music collections; Music collection; Perceived ease of use; Perceived usefulness; Playlist generation; User evaluations; Visualization technique; Sound recording","R. Van Gulik; Institute of Information and Computing Sciences, Utrecht University, 3584 CH, Utrecht, Padualaan 14, Netherlands; email: rob@cs.uu.nl","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Jacobson K.","Jacobson, Kurt (37107739300)","37107739300","A multifaceted approach to music similarity","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449442276&partnerID=40&md5=455012b0ce143ad24560cdbbc685494d","University of Miami, Coral Gables, FL, United States","Jacobson K., University of Miami, Coral Gables, FL, United States","Previous work has explored the concept of music similarity measures and a variety of methods have been proposed for calculating such measures. This paper describes a system for music similarity which attempts to model and compare some of the more musically salient features of a set of audio signals. A model for timbre and a model for rhythm are implemented directly from previous work, and a model for song structure is developed. The different models are weighted and combined to provide an overall music similarity measure. The system is tested on a small set of popular music files spanning eleven different genres. The system is tuned to estimate genre boundaries using multidimensional scaling - a technique that allows for quick visualization of similarity data. An ""automatic DJ"" application, that generates playlists based on the music similarity models, serves as a subjective evaluation for the system. © 2006 University of Victoria.","Automatic DJ; Multidimensional scaling; Music similarity; Playlist generation; Song structure","Information retrieval; Automatic DJ; Multi-dimensional scaling; Music similarity; Playlist generation; Song structure; Data visualization","K. Jacobson; University of Miami, Coral Gables, FL, United States; email: kurtj@miami.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Meng A.; Shawe-Taylor J.","Meng, Anders (24480225400); Shawe-Taylor, John (7003290763)","24480225400; 7003290763","An investigation of feature models for music genre classification using the support vector classifier","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","45","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533162&partnerID=40&md5=95ef28f2c209905d217284f7d2907ee1","Informatics and Mathematical Modelling - B321, Technical University of Denmark, Denmark; University of Southampton, United Kingdom","Meng A., Informatics and Mathematical Modelling - B321, Technical University of Denmark, Denmark; Shawe-Taylor J., University of Southampton, United Kingdom","In music genre classification the decision time is typically of the order of several seconds, however, most automatic music genre classification systems focus on short time features derived from 10-50ms. This work investigates two models, the multivariate Gaussian model and the multivariate autoregressive model for modelling short time features. Furthermore, it was investigated how these models can be integrated over a segment of short time features into a kernel such that a support vector machine can be applied. Two kernels with this property were considered, the convolution kernel and product probability kernel. In order to examine the different methods an 11 genre music setup was utilized. In this setup the Mel Frequency Cepstral Coefficients were used as short time features. The accuracy of the best performing model on this data set was ∼ 44% compared to a human performance of ∼ 52% on the same data set. © 2005 Queen Mary, University of London.","Convolution kernel; Feature integration; Music genre; Product probability kernel; Support vector machine","Convolution; Support vector machines; Convolution kernel; Data set; Decision time; Feature integration; Feature models; Human performance; Mel frequency cepstral co-efficient; Multivariate autoregressive models; Multivariate gaussian models; Music genre; Music genre classification; Product probability kernel; Support vector classifiers; Time features; Image retrieval","A. Meng; Informatics and Mathematical Modelling - B321, Technical University of Denmark, Denmark; email: am@imm.dtu.dk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Paiva R.P.; Mendes T.; Cardoso A.","Paiva, Rui Pedro (7003358437); Mendes, Teresa (8532568800); Cardoso, Amílcar (57204639511)","7003358437; 8532568800; 57204639511","On the detection of melody notes in polyphonic audio","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541859&partnerID=40&md5=3aa155377d51079d7196ac24993be15b","CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal","Paiva R.P., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal; Mendes T., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal; Cardoso A., CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal","This paper describes a method for melody detection in polyphonic musical signals. Our approach starts by obtaining a set of pitch candidates for each time frame, with recourse to an auditory model. Trajectories of the most salient pitches are then constructed. Next, note candidates are obtained by trajectory segmentation (in terms of frequency and pitch salience variations). Too short, lowsalience and harmonically related notes are then eliminated. Finally, the notes comprising the melody are extracted. This is the main topic of this paper. We select the melody notes by making use of note saliences and melodic smoothness. First, we select the notes with highest pitch salience at each moment. Then, by the melodic smoothness principle, we exploit the fact that tonal melodies are usually smooth. Thus, long music intervals indicate the presence of possibly erroneous notes, which are substituted by notes that smooth out the melodic contour. Finally, false positives in the extracted melody should be eliminated. To this end, we remove spurious notes that correspond to abrupt drops in note saliences or durations. Additionally, note clustering is conducted to further discriminate between true melody notes and false positives. © 2005 Queen Mary, University of London.","Feature extraction; Melodic smoothness; Melody detection; Note clustering","Feature extraction; Abrupt drops; Auditory models; False positive; Melodic smoothness; Melody detection; Note clustering; Polyphonic musical signals; Time frame; Trajectory segmentation; Information retrieval","R.P. Paiva; CISUC, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, P 3030, 290 Coimbra, Pólo II, Pinhal de Marrocos, Portugal; email: ruipedro@dei.uc.pt","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Tsai W.-H.; Yu H.-M.; Wang H.-M.","Tsai, Wei-Ho (13104438100); Yu, Hung-Ming (37062812700); Wang, Hsin-Min (8297293300)","13104438100; 37062812700; 8297293300","A query-by-example technique for retrieving cover versions of popular songs with similar melodies","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","36","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535580&partnerID=40&md5=69542e2f3f07fdca142f952b8dbf74bf","Institute of Information Science, Academia Sinica, Taipei, Taiwan","Tsai W.-H., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Yu H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan; Wang H.-M., Institute of Information Science, Academia Sinica, Taipei, Taiwan","Retrieving audio material based on audio queries is an important and challenging issue in the research field of content-based access to popular music. As part of this research field, we present a preliminary investigation into retrieving cover versions of songs specified by users. The technique enables users to listen to songs with an identical tune, but performed by different singers, in different languages, genres, and so on. The proposed system is built on a query-by-example framework, which takes a fragment of the song submitted by the user as input, and returns songs similar to the query in terms of the main melody as output. To handle the likely discrepancies, e.g., tempos, transpositions, and accompaniments between cover versions and the original song, methods are presented to remove the non-vocal portions of the song, extract the sung notes from the accompanied vocals, and compare the similarities between the sung note sequences. © 2005 Queen Mary, University of London.","Accompaniments; Cover version; Main melody; Query-byexample","Information retrieval; Accompaniments; Content-based access; Cover version; Main melody; Material-based; Original songs; Popular music; Popular song; Query-byexample; Research fields; Audio acoustics","W.-H. Tsai; Institute of Information Science, Academia Sinica, Taipei, Taiwan; email: wesley@iis.sinica.edu.tw","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Moelants D.; Cornelis O.; Leman M.; Gansemans J.; De Caluwe R.; De Tré G.; Matthé T.; Hallez A.","Moelants, Dirk (6507495086); Cornelis, Olmo (27267474100); Leman, Marc (6603703642); Gansemans, Jos (14044968100); De Caluwe, Rita (6602090819); De Tré, Guy (6603348266); Matthé, Tom (22734716400); Hallez, Axel (22733976800)","6507495086; 27267474100; 6603703642; 14044968100; 6602090819; 6603348266; 22734716400; 22733976800","Problems and opportunities of applying data-& audio-mining techniques to ethnic music","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949101693&partnerID=40&md5=779f258c444c13d14de4acdfea21f919","IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Dept. of Cultural Anthropology, Royal Museum of Central-Africa, B-3080 Tervuren, Leuvensesteenweg 13, Belgium; TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium","Moelants D., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Cornelis O., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Leman M., IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; Gansemans J., Dept. of Cultural Anthropology, Royal Museum of Central-Africa, B-3080 Tervuren, Leuvensesteenweg 13, Belgium; De Caluwe R., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; De Tré G., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; Matthé T., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium; Hallez A., TELIN, Ghent University, B-9000 Ghent, Sint-Pietersnieuwstraat 41, Belgium","Current research in music information retrieval focuses on Western music. In music from other cultures, both musical structures and thinking about music can be very different. This creates problems for both the analysis of musical features and the construction of databases. On the other hand, a well-documented digitization offers interesting opportunities for the study and spread of 'endangered' music. Here, some general problems regarding the digital indexation of ethnic music are given, illustrated with a method for describing pitch structure, comparing Western standards with African music found in the digitization of the archives of the Royal Museum of Central-Africa in Tervuren (Brussels). © 2006 University of Victoria.","Africa; Archiving; Cultural heritage; Databases; Ethnic music; Pitch","Database systems; Information retrieval; Africa; Archiving; Cultural heritages; Ethnic music; Pitch; Audio acoustics","D. Moelants; IPEM-Dept. of Musicology, Ghent University, B-9000 Ghent, Blandijnberg 2, Belgium; email: Dirk.Moelants@UGent.be","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Chuan C.-H.; Chew E.","Chuan, Ching-Hua (15050129500); Chew, Elaine (8706714000)","15050129500; 8706714000","Fuzzy analysis in pitch class determination for polyphonic audio key finding","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873473031&partnerID=40&md5=cfa65f8fff0342a328def96fe611aad0","Department of Computer Science, University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States; Epstein Dep of Industrial and Systems Eng., University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States","Chuan C.-H., Department of Computer Science, University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States; Chew E., Epstein Dep of Industrial and Systems Eng., University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States","This paper presents a fuzzy analysis technique for pitch class determination that improves the accuracy of key finding from audio information. Errors in audio key finding, typically incorrect assignments of closely related keys, commonly result from imprecise pitch class determination and biases introduced by the quality of the sound. Our technique is motivated by hypotheses on the sources of audio key finding errors, and uses fuzzy analysis to reduce the errors caused by noisy detection of lower pitches, and to refine the biased raw frequency data, in order to extract more correct pitch classes. We compare the proposed system to two others, an earlier one employing only peak detection from FFT results, and another providing direct key finding from MIDI. All three used the same key finding algorithm (Chew's Spiral Array CEG algorithm) and the same 410 classical music pieces (ranging from Baroque to Contemporary). Considering only the first 15 seconds of music in each piece, the proposed fuzzy analysis technique outperforms the peak detection method by 12.18% on average, matches the performance of direct key finding from MIDI 41.73% of the time, and achieves an overall maximum correct rate of 75.25% (compared to 80.34% for MIDI key finding). © 2005 Queen Mary, University of London.","Audio key finding; Fuzzy analysis; Key proximity; Pitch classes","Information retrieval; Audio information; Audio key finding; Classical musics; Finding algorithm; Frequency data; Fuzzy analysis; Key proximity; Peak detection; Pitch classes; Related keys; Errors","C.-H. Chuan; Department of Computer Science, University of Southern California, Integrated Media Systems Center, Los Angeles, CA 90089, United States; email: chinghuc@usc.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Vignoli F.; Pauws S.","Vignoli, Fabio (11240995100); Pauws, Steffen (11240480500)","11240995100; 11240480500","A music retrieval system based on user-driven similarity and its evaluation","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","62","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873536012&partnerID=40&md5=addffb0aeefc850c4cf33a0dcb5d99a4","Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Vignoli F., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Pauws S., Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Large music collections require new ways to let users interact with their music. The concept of finding 'similar' songs, albums, or artists provides handles to users for easy navigation and instant retrieval. This paper presents the realization and user evaluation of a music retrieval music that sorts songs on the basis of similarity to a given seed song. Similarity is based on a userweighted combination of timbre, genre, tempo, year, and mood. A conclusive user evaluation assessed the usability of the system in comparison to two control systems in which the user control of defining the similarity measure was diminished. © 2005 Queen Mary, University of London.","Music similarity; User evaluation","ITS evaluation; Large music collections; Music retrieval; Music retrieval systems; Music similarity; Similarity measure; User control; User evaluations; Information retrieval","F. Vignoli; Philips Research Laboratories, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; email: Fabio.Vignoli@philips.com","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Stenzel R.; Kamps T.","Stenzel, Richard (23037254000); Kamps, Thomas (57196810464)","23037254000; 57196810464","Improving content-based similarity measures by training a collaborative model","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873557073&partnerID=40&md5=b7954a44e993f4db483f57ea1b4f8a37","Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany","Stenzel R., Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany; Kamps T., Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany","We observed that for multimedia data - especially music - collaborative similarity measures perform much better than similarity measures derived from content-based sound features. Our observation is based on a large scale evaluation with <250,000,000 collaborative data points crawled from the web and <190,000 songs annotated with content-based sound feature sets. A song mentioned in a playlist is regarded as one collaborative data point. In this paper we present a novel approach to bridging the performance gap between collaborative and contentbased similarity measures. In the initial training phase a model vector for each song is computed, based on collaborative data. Each vector consists of 200 overlapping unlabelled 'genres' or song clusters. Instead of using explicit numerical voting, we use implicit user profile data as collaborative data source, which is, for example, available as purchase histories in many large scale ecommerce applications. After the training phase, we used support vector machines based on content-based sound features to predict the collaborative model vectors. These predicted model vectors are finally used to compute the similarity between songs. We show that combining collaborative and content-based similarity measures can help to overcome the new item problem in e-commerce applications that offer a collaborative similarity recommender as service to their customers. © 2005 Queen Mary, University of London.","Acoustic measures; Collaborative metadata; Content based sound feature similarity measures; Evaluation; Machine learning; Music similarity; Recommender","Electronic commerce; Information retrieval; Learning systems; Metadata; Collaborative model; Content-based; Data points; E-Commerce applications; Evaluation; Feature sets; Feature similarities; Model vectors; Multimedia data; Music similarity; Performance gaps; Predicted models; Recommender; Similarity measure; Training phase; User profile; Vectors","R. Stenzel; Fraunhofer IPSI, D-64293 Darmstadt, Dolivostr. 15, Germany; email: stenzel@ipsi.fraunhofer.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lartillot O.","Lartillot, Olivier (6507137446)","6507137446","Efficient extraction of closed motivic patterns in multi-dimensional symbolic representations of music","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873559974&partnerID=40&md5=d52cfb87f177ae162c5bbfdf73a322e3","University of Jyväskylä, 40014 Jyväskylä, PL 35(A), Finland","Lartillot O., University of Jyväskylä, 40014 Jyväskylä, PL 35(A), Finland","An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is usually filtered using global selective mechanisms, based on pattern frequency and length. The proposed approach is founded instead on the concept of closed pattern, and insures lossless compression through an adaptive selection of most specific descriptions in the multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling the filtering of another form of combinatorial redundancy provoked by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalising particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but requires instead a complex and detailed modelling of the cognitive system ruling the listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex motivic transformations. © 2005 Queen Mary, University of London.","Closed pattern discovery; Cognitive modelling; Cyclic pattern; Formal concept analysis; Gallois connection","Formal concept analysis; Image compression; Information retrieval; Redundancy; Adaptive selection; Automated analysis; Closed pattern; Cognitive modelling; Cyclic patterns; Gallois connection; Gestalt principles; Lossless compression; Music database; Parametric spaces; Pattern discovery; Repeated patterns; Statistical approach; Symbolic representation; Time compact; Cognitive systems","O. Lartillot; University of Jyväskylä, 40014 Jyväskylä, PL 35(A), Finland; email: lartillo@campus.jyu.fi","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bello J.P.; Pickens J.","Bello, Juan P. (7102889110); Pickens, Jeremy (7005535299)","7102889110; 7005535299","A robust mid-level representation for harmonic content in music signals","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","168","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873553947&partnerID=40&md5=bf44cc6e62eafcd7f15e74da8faeaefa","Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom","Bello J.P., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom; Pickens J., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom","When considering the problem of audio-to-audio matching, determining musical similarity using low-level features such as Fourier transforms and MFCCs is an extremely difficult task, as there is little semantic information available. Full semantic transcription of audio is an unreliable and imperfect task in the best case, an unsolved problem in the worst. To this end we propose a robust mid-level representation that incorporates both harmonic and rhythmic information, without attempting full transcription. We describe a process for creating this representation automatically, directly from multi-timbral and polyphonic music signals, with an emphasis on popular music. We also offer various evaluations of our techniques. Moreso than most approaches working from raw audio, we incorporate musical knowledge into our assumptions, our models, and our processes. Our hope is that by utilizing this notion of a musically-motivated mid-level representation we may help bridge the gap between symbolic and audio research. © 2005 Queen Mary, University of London.","Harmonic description; Music similarity; Segmentation","Fourier transforms; Harmonic analysis; Image segmentation; Information retrieval; Semantics; Transcription; Harmonic contents; Harmonic description; Low-level features; Mid-level representation; Music signals; Music similarity; Musical similarity; Polyphonic music; Popular music; Semantic information; Unsolved problems; Audio acoustics","J.P. Bello; Centre for Digital Music, Queen Mary, University of London, London E1 4NS, United Kingdom; email: juan.bello-correa@elec.qmul.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kuuskankare M.; Laurson M.","Kuuskankare, Mika (56301483200); Laurson, Mikael (15519696600)","56301483200; 15519696600","Annotating musical scores in ENP","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873547151&partnerID=40&md5=6c677df3b8273956fedec8cbc8517953","Department of Doctoral Studies in Musical Performance and Research, Sibelius Academy, Finland; Centre for Music and Technology, Sibelius Academy, Finland","Kuuskankare M., Department of Doctoral Studies in Musical Performance and Research, Sibelius Academy, Finland; Laurson M., Centre for Music and Technology, Sibelius Academy, Finland","The focus of this paper is on ENP-expressions that can be used for annotating ENP scores with user definable information. ENP is a music notation program written in Lisp and CLOS with a special focus on compositional and music analytical applications. We present number of built-in expressions suitable for visualizing, for example, music analytical information as a part of music notation. A Lisp and CLOS based system for creating user-definable annotation information is also presented along with some sample algorithms. Finally, our system for automatically analyzing and annotating an ENP score is illustrated through several examples including some dealing with music information retrieval. © 2005 Queen Mary, University of London.","Annotating; Music representation; Symbolic notation","Application programs; LISP (programming language); Analytical applications; Annotating; Music information retrieval; Music notation; Music representation; Musical score; Symbolic notation; Information retrieval","M. Kuuskankare; Department of Doctoral Studies in Musical Performance and Research, Sibelius Academy, Finland; email: mkuuskan@siba.fi","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Riley J.; Mayer C.A.","Riley, Jenn (57197504438); Mayer, Constance A. (7202232516)","57197504438; 7202232516","Ask a librarian: The role of librarians in the music information retrieval community","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873414891&partnerID=40&md5=fb83dd9adf8107681a6ea48b97e365d5","Indiana University, Bloomington, IN 47405, 1320 E. 10th St. E170, United States; University of Maryland, 2511 Clarice Smith Performing Arts Center, College Park, MD 20742, United States","Riley J., Indiana University, Bloomington, IN 47405, 1320 E. 10th St. E170, United States; Mayer C.A., University of Maryland, 2511 Clarice Smith Performing Arts Center, College Park, MD 20742, United States","Participation from music librarians has been sparse in the first six ISMIR conferences, despite many potential areas of common interest. This paper makes an argument for the benefit to both the library and Music IR communities of increased representation of librarians at ISMIR. An analysis of conference programs and primary publications of two music library organizations to determine topics from the library literature relevant to Music IR research is presented. A discussion follows of expertise music librarians could potentially contribute to Music IR research and the ways in which Music IR research could further the work of music librarians, in each of the topics represented in the library literature. © 2006 University of Victoria.","ISMIR; Music librarians","Information retrieval; Research; Conference programs; ISMIR; Music information retrieval; Music librarians; Music library; Libraries","J. Riley; Indiana University, Bloomington, IN 47405, 1320 E. 10th St. E170, United States; email: jenlrile@indiana.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Woodruff J.; Pardo B.; Dannenberg R.","Woodruff, John (15841051800); Pardo, Bryan (10242155400); Dannenberg, Roger (7003266250)","15841051800; 10242155400; 7003266250","Remixing stereo music with score-informed source separation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","59","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449136628&partnerID=40&md5=2d5b5d70f327274bbf448faed4d8c1c0","Music Technology, School of Music, Northwestern University, Evanston, IL 60208, United States; Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Woodruff J., Music Technology, School of Music, Northwestern University, Evanston, IL 60208, United States; Pardo B., Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208, United States; Dannenberg R., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Musicians and recording engineers are often interested in manipulating and processing individual instrumental parts within an existing recording to create a remix of the recording. When individual source tracks for a stereo mixture are unavailable, remixing is typically difficult or impossible, since one cannot isolate the individual parts. We describe a method of informed source separation that uses knowledge of the written score and spatial information from an anechoic, stereo mixture to isolate individual sound sources, allowing remixing of stereo mixtures without access to the original source tracks. This method is tested on a corpus of string quartet performances, artificially created using Bach four-part chorale harmonizations and sample violin, viola and cello recordings. System performance is compared in cases where the algorithm has knowledge of the score and those in which it operates blindly. The results show that source separation performance is markedly improved when the algorithm has access to a well-aligned score. © 2006 University of Victoria.","Music; Score alignment; Source separation","Algorithms; Information retrieval; Mixtures; Source separation; Music; Separation performance; Sound source; Spatial informations; Stereo music; Well-aligned; Audio recordings","J. Woodruff; Music Technology, School of Music, Northwestern University, Evanston, IL 60208, United States; email: j-woodruff@northwestern.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Cont A.","Cont, Arshia (12344985300)","12344985300","Realtime multiple pitch observation using sparse non-negative constraints","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449658600&partnerID=40&md5=29fcd64ed6b5db127bbf9c1d05ebc8bc","Ircam, Realtime Applications Team, Paris, France; Center for Research in Computing and the Arts, UCSD, San Diego, United States","Cont A., Ircam, Realtime Applications Team, Paris, France, Center for Research in Computing and the Arts, UCSD, San Diego, United States","In this paper we introduce a new approach for realtime multiple pitch observation of musical instruments. The proposed algorithm is quite different from others in the literature both in its purpose and approach. It is destined not for continuous multiple f0 recognition but rather for projection of the ongoing spectrum to learned pitch templates. The decomposition algorithm on the other hand, does not compromise signal processing models for pitches and consists of an algorithm for efficient decomposition of a spectrum using known pitch structures and based on sparse non-negative constraints. After introducing the algorithm along with evaluations, a real-time implementation of the algorithm is provided for free download for the MaxMSP realtime programming environment. © 2006 University of Victoria.","Machine Learning; Multiple-pitch observation; Non-negative Matrix Factorization; Sparseness constraints","Face recognition; Information retrieval; Learning systems; Real time control; Signal processing; Decomposition algorithm; Efficient decomposition; Multiple-pitch observation; Nonnegative matrix factorization; Real time; Real time programming; Real-time implementations; Sparseness constraints; Algorithms","A. Cont; Ircam, Realtime Applications Team, Paris, France; email: cont@ircam.fr","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Cunningham S.J.; Bainbridge D.; Falconer A.","Cunningham, Sally Jo (7201937110); Bainbridge, David (8756864800); Falconer, Annette (55582514000)","7201937110; 8756864800; 55582514000","'More of an art than a science': Supporting the creation of playlists and mixes","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","63","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873424976&partnerID=40&md5=e340307d171e84e98ea68eec998def3e","Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; not available, Hamilton, 17 Ernest Road, New Zealand","Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Bainbridge D., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Falconer A., not available, Hamilton, 17 Ernest Road, New Zealand","This paper presents an analysis of how people construct playlists and mixes. Interviews with practitioners and postings made to a web site are analyzed using a grounded theory approach to extract themes and categorizations. The information sought is often encapsulated as music information retrieval tasks, albeit not as the traditional ""known item search"" paradigm. The collated data is analyzed and trends identified and discussed in relation to music information retrieval algorithms that could help support such activity. © 2006 University of Victoria.","Mix CD; Music information behavior; Playlists","Grounded theory approach; Known item searches; Music information; Music information retrieval; Playlists; Information retrieval","S.J. Cunningham; Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Dhanaraj R.; Logan B.","Dhanaraj, Ruth (55585304800); Logan, Beth (7202196551)","55585304800; 7202196551","Automatic prediction of hit songs","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873556775&partnerID=40&md5=b2a699b2d076553956644982c2ecb272","Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States","Dhanaraj R., Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States; Logan B., Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States","We explore the automatic analysis of music to identify likely hit songs. We extract both acoustic and lyric information from each song and separate hits from non-hits using standard classifiers, specifically Support Vector Machines and boosting classifiers. Our features are based on global sounds learnt in an unsupervised fashion from acoustic data or global topics learnt from a lyrics database. Experiments on a corpus of 1700 songs demonstrate performance that is much better than random. The lyricbased features are slightly more useful than the acoustic features in correctly identifying hit songs. Concatenating the two features does not produce significant improvements. Analysis of the lyric-based features shows that the absence of certain semantic information indicates that a song is more likely to be a hit. © 2005 Queen Mary, University of London.","Hit song detection; Music classification","Acoustic data; Acoustic features; Automatic analysis; Automatic prediction; Boosting classifiers; Music classification; Semantic information; Information retrieval","R. Dhanaraj; Research Science Institute Intern, Hewlett Packard Labs., One Cambridge Center, Cambridge MA, United States; email: ruthdhan@mit.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Jang J.-S.R.; Hsu C.-L.; Lee H.-R.","Jang, Jyh-Shing Roger (7402965041); Hsu, Chao-Ling (47061226300); Lee, Hong-Ru (8230742600)","7402965041; 47061226300; 8230742600","Continuous HMM and its enhancement for singing/humming query retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873554994&partnerID=40&md5=1e3e938df824f9ffe17390a815b3bbcb","Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan","Jang J.-S.R., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Hsu C.-L., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; Lee H.-R., Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan","The use of HMM (Hidden Markov Models) for speech recognition has been successful for various applications in the past decades. However, the use of continuous HMM (CHMM) for melody recognition via acoustic input (MRAI for short), or the so-called query by singing/humming, has seldom been reported, partly due to the difference in acoustic characteristics between speech and singing/humming inputs. This paper will derive the formula of CHMM training for frame-based MRAI. In particular, we shall propose enhancement to CHMM and demonstrate that with the enhancement scheme, CHMM can compare favourably with DTW in both efficiency and effectiveness. © 2005 Queen Mary, University of London.","Continuous HMM; HMM; Melody recognition via acoustic input (MRAI); Query by singing/ humming; Speech recognition","Hidden Markov models; Information retrieval; Acoustic characteristic; Continuous HMM; Efficiency and effectiveness; Frame-based; HMM; Melody recognition; Query retrieval; Query-by-singing; Speech recognition","J.-S.R. Jang; Multimedia Information Retrieval Laboratory, Computer Science Department, National Tsing Hua University, Hsinchu, Taiwan; email: jang@wayne.cs.nthu.edu.tw","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Loscos A.; Wang Y.; Jie W.; Boo J.","Loscos, Alex (16245952700); Wang, Ye (36103845200); Jie, Wei (55583169300); Boo, Jonathan (55583215300)","16245952700; 36103845200; 55583169300; 55583215300","Low level descriptors for automatic violin transcription","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849011819&partnerID=40&md5=475ec4fa14e104d1456334ce07e799b2","MTG, Universitat Pompeu Fabra, Spain; National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore","Loscos A., MTG, Universitat Pompeu Fabra, Spain, National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Wang Y., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Jie W., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore; Boo J., National University of Singapore, School of Computing (SoC), Singapore 117543, Singapore","On top of previous work in automatic violin transcription we present a set of straight forward low level descriptors for assisting the transcription techniques and saving computational cost. Proposed descriptors have been tested against a database of 1500 violin notes and double stops. © 2006 University of Victoria.","Automatic transcription; Violin","Information retrieval; Musical instruments; Automatic transcription; Computational costs; Descriptors; Low level descriptors; Violin; Transcription","A. Loscos; MTG, Universitat Pompeu Fabra, Spain; email: aloscos@iua.upf.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Madsen S.T.; Widmer G.","Madsen, Søren Tjagvad (12344652200); Widmer, Gerhard (7004342843)","12344652200; 7004342843","Separating voices in MIDI","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50349094309&partnerID=40&md5=225f902de686cbaa2bd84f5fe8edb61f","Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenbergerstraße 69, Austria","Madsen S.T., Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenbergerstraße 69, Austria","This paper presents an algorithm for converting midi events into logical voices. The algorithm is fundamentally based on the pitch proximity principle. New heuristics are introduced and evaluated in order to handle unsolved situations. The algorithm is tested on ground truth data: inventions and fugues by J. S. Bach. Due to its left to right processing it also runs on real time input. © 2006 University of Victoria.","Stream separation; Voice separation","Information retrieval; Separation; Ground truth data; Real time; Voice separation; Algorithms","S.T. Madsen; Austrian Research Institute for Artificial Intelligence, A-1010 Vienna, Freyung 6/6, Austria; email: soren.madsen@ofai.at","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Fiebrink R.; McKay C.; Fujinaga I.","Fiebrink, Rebecca (36095844900); McKay, Cory (14033215600); Fujinaga, Ichiro (9038140900)","36095844900; 14033215600; 9038140900","Combining D2K and JGAP for efficient feature weighting for classification tasks in music information retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873563446&partnerID=40&md5=3dceca48d525728557974e5c0e6cbd0b","Music Technology, McGill University, Montreal, Canada","Fiebrink R., Music Technology, McGill University, Montreal, Canada; McKay C., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","Music classification continues to be an important component of music information retrieval research. An underutilized tool for improving the performance of classifiers is feature weighting. A major reason for its unpopularity, despite its benefits, is the potentially infinite calculation time it requires to achieve optimal results. Genetic algorithms offer potentially sub-optimal but reasonable solutions at much reduced calculation time, yet they are still quite costly. We investigate the advantages of implementing genetic algorithms in a parallel computing environment to make feature weighting an affordable instrument for researchers in MIR. © 2005 Queen Mary, University of London.","Classification; D2K; Feature weighting; Parallel computing","Classification (of information); Genetic algorithms; Optimization; Parallel architectures; Parallel processing systems; Calculation time; Classification tasks; D2K; Feature weighting; Music classification; Music information retrieval; Optimal results; Parallel-computing environment; Performance of classifier; Reduced calculation; Information retrieval","R. Fiebrink; Music Technology, McGill University, Montreal, Canada; email: rebecca.fiebrink@mail.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Poliner G.E.; Ellis D.P.W.","Poliner, Graham E. (15080995300); Ellis, Daniel P.W. (13609089200)","15080995300; 13609089200","A classification approach to melody transcription","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","38","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873560057&partnerID=40&md5=462d9ae85877840d4b57a73a164621a8","LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States","Poliner G.E., LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States; Ellis D.P.W., LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States","Melodies provide an important conceptual summarization of polyphonic audio. The extraction of melodic content has practical applications ranging from content-based audio retrieval to the analysis of musical structure. In contrast to previous transcription systems based on a model of the harmonic (or periodic) structure of musical pitches, we present a classification-based system for performing automatic melody transcription that makes no assumptions beyond what is learned from its training data. We evaluate the success of our algorithm by predicting the melody of the ISMIR 2004 Melody Competition evaluation set and on newly-generated test data. We show that a Support Vector Machine melodic classifier produces results comparable to state of the art model-based transcription systems. © 2005 Queen Mary, University of London.","Classification; Melody transcription","Information retrieval; Transcription; Audio retrieval; Classification approach; Content-based; Musical pitch; Musical structures; State of the art; Test data; Training data; Classification (of information)","G.E. Poliner; LabROSA, Dept. of Electrical Engineering, Columbia University, New York NY 10027, United States; email: graham@ee.columbia.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kapur A.; McWalter R.I.; Tzanetakis G.","Kapur, Ajay (13609187300); McWalter, Richard I. (16550369900); Tzanetakis, George (6602262192)","13609187300; 16550369900; 6602262192","New music interfaces for rhythm-based retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873541438&partnerID=40&md5=de5bad91098c185fc5e49249b04b6d2a","University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","Kapur A., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; McWalter R.I., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; Tzanetakis G., University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","In the majority of existing work in music information retrieval (MIR) the user interacts with the system using standard desktop components such as the keyboard, mouse or sometimes microphone input. It is our belief that moving away from the desktop to more physically tangible ways of interacting can lead to novel ways of thinking about MIR. In this paper, we report on our work in utilizing new non-standard interfaces for MIR purposes. One of the most important but frequently neglected ways of characterizing and retrieving music is through rhythmic information. We concentrate on rhythmic information both as user input and as means for retrieval. Algorithms and experiments for rhythm-based information retrieval of music, drum loops and indian tabla thekas are described. This work targets expert users such as DJs and musicians which tend to be more curious about new technologies and therefore can serve as catalysts for accelerating the adoption of MIR techniques. In addition, we describe how the proposed rhythm-based interfaces can assist in the annotation and preservation of perfomance practice. © 2005 Queen Mary, University of London.","Controllers; Live performance; Rhythm analysis; User interfaces","Controllers; Information retrieval; Expert users; Live performance; Music information retrieval; New music interfaces; New technologies; Perfomance; Rhythm analysis; Standard desktop; User input; Ways of thinking; User interfaces","A. Kapur; University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; email: ajay@ece.uvic.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Fujihara H.; Kitahara T.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.","Fujihara, Hiromasa (16068753300); Kitahara, Tetsuro (7201371361); Goto, Masataka (7403505330); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","16068753300; 7201371361; 7403505330; 35577813100; 7402000772; 7102397930","Singer identification based on accompaniment sound reduction and reliable frame selection","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","50","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873533890&partnerID=40&md5=5e6d6a2a6868331d8a3b7782113dcecd","Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","Fujihara H., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper describes a method for automatic singer identification from polyphonic musical audio signals including sounds of various instruments. Because singing voices play an important role in musical pieces with a vocal part, the identification of singer names is useful for music information retrieval systems. The main problem in automatically identifying singers is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection. The former method makes it possible to identify the singer of a singing voice after reducing accompaniment sounds. It first extracts harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by those components. The latter method then judges whether each frame of the obtained melody is reliable (i.e. little influenced by accompaniment sound) or not by using two Gaussian mixture models for vocal and non-vocal frames. It enables the singer identification using only reliable vocal portions of musical pieces. Experimental results with forty popular-music songs by ten singers showed that our method was able to reduce the influences of accompaniment sounds and achieved an accuracy of 95%, while the accuracy for a conventional method was 53%. © 2005 Queen Mary, University of London.","Artist identification; Melody extraction; Similarity-based MIR; Singer identification; Singing detection","Conventional methods; Frame selection; Gaussian Mixture Model; Harmonic components; Melody extractions; Music information retrieval; Musical audio signal; Musical pieces; Negative influence; Similarity-based MIR; Singing voices; Sinusoidal model; Sound mixtures; Sound reduction; Information retrieval systems","H. Fujihara; Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: fujihara@kuis.kyoto-u.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Müller M.; Kurth F.; Clausen M.","Müller, Meinard (7404689873); Kurth, Frank (56240850200); Clausen, Michael (56225233200)","7404689873; 56240850200; 56225233200","Audio matching via chroma-based statistical features","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","169","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873540864&partnerID=40&md5=83e4d9abac077b4b94ee723957789f78","Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany","Müller M., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Kurth F., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Clausen M., Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany","In this paper, we describe an efficient method for audio matching which performs effectively for a wide range of classical music. The basic goal of audio matching can be described as follows: consider an audio database containing several CD recordings for one and the same piece of music interpreted by various musicians. Then, given a short query audio clip of one interpretation, the goal is to automatically retrieve the corresponding excerpts from the other interpretations. To solve this problem, we introduce a new type of chroma-based audio feature that strongly correlates to the harmonic progression of the audio signal. Our feature shows a high degree of robustness to variations in parameters such as dynamics, timbre, articulation, and local tempo deviations. As another contribution, we describe a robust matching procedure, which allows to handle global tempo variations. Finally, we give a detailed account on our experiments, which have been carried out on a database of more than 110 hours of audio comprising a wide range of classical music. © 2005 Queen Mary, University of London.","Audio matching; Chroma feature; Music identification","Audio recordings; Information retrieval; Query processing; Audio clips; Audio database; Audio features; Audio matching; Audio signal; Chroma features; Classical musics; Degree of robustness; Music identification; Robust matching; Statistical features; Audio acoustics","M. Müller; Universität Bonn, Institut für Informatik III, D-53117 Bonn, Römerstr. 164, Germany; email: meinard@cs.uni-bonn.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Li M.; Sleep R.","Li, Ming (56994195900); Sleep, Ronan (6603308689)","56994195900; 6603308689","Genre classification via an LZ78-based string kernel","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873559624&partnerID=40&md5=cc6c4f01e4c0432ad239c35768b2918a","School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom","Li M., School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom; Sleep R., School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom","We develop the notion of normalized information distance (NID) [7] into a kernel distance suitable for use with a Support Vector Machine classifier, and demonstrate its use for an audio genre classification task. Our classification scheme involves a relatively small number of low-level audio features, is efficient to compute, yet generates an accuracy which compares well with recent works. © 2005 Queen Mary, University of London.","Classification; Genre; LZ78 String Kernel; SVM","Classification (of information); Information retrieval; Audio features; Classification scheme; Genre; Genre classification; Information distance; Kernel distance; String Kernel; Support vector machine classifiers; SVM; Support vector machines","M. Li; School of Computing Sciences, University of East Anglia, Norwich NR47TJ, United Kingdom; email: mli@cmp.uea.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Norowi N.M.; Doraisamy S.; Wirza R.","Norowi, Noris Mohd (24766399900); Doraisamy, Shyamala (24765904400); Wirza, Rahmita (35614233000)","24766399900; 24765904400; 35614233000","Factors affecting automatic genre classification: An investigation incorporating non-western musical forms","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873543336&partnerID=40&md5=848ddf6c074b9bad6fc8cc90bd36be64","Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia","Norowi N.M., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia; Doraisamy S., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia; Wirza R., Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia","The number of studies investigating automated genre classification is growing following the increasing amounts of digital audio data available. The underlying techniques to perform automated genre classification in general include feature extraction and classification. In this study, MARSYAS was used to extract audio features and the suite of tools available in WEKA was used for the classification. This study investigates the factors affecting automated genre classification. As for the dataset, most studies in this area work with western genres and traditional Malay music is incorporated in this study. Eight genres were introduced; Dikir Barat, Etnik Sabah, Inang, Joget, Keroncong, Tumbuk Kalang, Wayang Kulit, and Zapin. A total of 417 tracks from various Audio Compact Discs were collected and used as the dataset. Results show that various factors such as the musical features extracted, classifiers employed, the size of the dataset, excerpt length, excerpt location and test set parameters improve classification results. © 2005 Queen Mary, University of London.","Feature extraction; Genre classification; Music information retrieval; Traditional malay music","Automation; Feature extraction; Information retrieval; Statistical tests; Audio features; Automatic genre classification; Classification results; Digital audio; Feature extraction and classification; Genre classification; Music information retrieval; Musical features; Test sets; Traditional malay music; Classification (of information)","N.M. Norowi; Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400, Selangor, Malaysia; email: noris@fsktm.upm.edu.my","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pauws S.; Van De Wijdeven S.","Pauws, Steffen (11240480500); Van De Wijdeven, Sander (55585232200)","11240480500; 55585232200","User evaluation of a new interactive playlist generation concept","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","23","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873535158&partnerID=40&md5=62d050ead4aeb73fcddb7380e012e2c1","Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Pauws S., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; Van De Wijdeven S., Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands","Selecting the 'right' songs and putting them in the 'right' order are key to a great music listening or dance experience. 'SatisFly' is an interactive playlist generation system in which the user can tell what kind of songs should be contained in what order in the playlist, while she navigates through the music collection. The system uses constraint satisfaction to generate a playlist that meets all user wishes. In a user evaluation, it was found that users created high-quality playlists in a swift way and with little effort using the system, while still having complete control on their music choices. The novel interactive way of creating a playlist, while browsing through the music collection, was highly appreciated. Ease of navigation through a music collection is still an issue that needs further attention. © 2005 Queen Mary, University of London.","Constraint satisfaction; Playlist generation; User evaluation","Audio recordings; Information retrieval; Complete control; Constraint Satisfaction; Generation systems; High quality; Interactive way; Music collection; Playlist generation; System use; User evaluations; Sound recording","S. Pauws; Philips Research, 5656 AA Eindhoven, Prof. Holstlaan 4, Netherlands; email: steffen.pauws@philips.com","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Adams N.; Marquez D.; Wakefield G.","Adams, Norman (14009865000); Marquez, Daniela (55584971400); Wakefield, Gregory (35612367100)","14009865000; 55584971400; 35612367100","Iterative deepening for melody alignment and retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","26","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873542125&partnerID=40&md5=b50b45492ac9883f33f5aac3cb0ace8c","Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States","Adams N., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States; Marquez D., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States; Wakefield G., Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States","For melodic theme retrieval there is a fundamental tradeoff between retrieval performance and retrieval speed. Melodic representations of large dimension yield the best retrieval performance, but at high computational cost, and vice versa. In the present work we explore the use of iterative deepening to achieve robust retrieval performance, but without the accompanying computational burden. In particular, we propose the use of a smooth pitch contour that facilitates query and target representations of variable length. We implement an iterative query-by-humming system that yields a dramatic increase in speed, without degrading performance compared to contemporary retrieval systems. Furthermore, we expand the conventional iterative framework to retain the alignment paths found in each iteration. These alignment paths are used to adapt the alignment window of subsequent iterations, further expediting retrieval without degrading performance. © 2005 Queen Mary, University of London.","DTW; Iterative deepening; Melody retrieval","Alignment; Computational burden; Computational costs; DTW; Iterative deepening; Iterative framework; Large dimensions; Melody retrieval; Pitch contours; Query-by-humming system; Retrieval performance; Retrieval speed; Retrieval systems; Target representation; Variable length; Information retrieval","N. Adams; Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, 48109, 1101 Beal Ave., United States; email: nhadams@umich.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Corthaut N.; Govaerts S.; Duval E.","Corthaut, Nik (35748332800); Govaerts, Sten (7801629780); Duval, Erik (7006487422)","35748332800; 7801629780; 7006487422","Moody tunes: The rockanango project","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849161806&partnerID=40&md5=16d1f887b66ef4c5906879595bd04521","K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","Corthaut N., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Govaerts S., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; Duval E., K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium","Wouldn' t it be nice if we had a tool that could offer people the right music for a specific time and place? For HORECA (hotel, restaurants and cafés) businesses, providing appropriate music is often not just nice, but essential. Typically this boils down to music that matches a certain situation on desired atmospheres, this will be defined as a musical context (MC). The developed tool, a music player, meeting the specific needs of HORECA, allows creation and management of those contexts. The user creates a musical context by selecting a number of appropriate atmospheres and can fine-tune the context with additional musical properties. The atmospheres are defined by a group of music experts, composed of DJ' s, music teachers, musicians, etc., who also manually annotate the properties of all musical content. To assist the music experts, a specially developed tool allows them to categorise and annotate the songs and evaluate their results. We provide insight on how we constructed and implemented our metadata schema and look at some existing schemas. The evaluation shows the economic value of such a system in the specific context of a HORECA business. © 2006 University of Victoria.","Context; Metadata; Multimedia systems; Music information retrieval","Information retrieval; Multimedia systems; Context; Economic values; Metadata schema; Music information retrieval; Music players; Specific time; Metadata","N. Corthaut; K.U. Leuven, B-3001 Heverlee, Celestijnenlaan 200A, Belgium; email: Nik.Corthaut@cs.kuleuven.be","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Mörchen F.; Ultsch A.; Nöcker M.; Stamm C.","Mörchen, Fabian (14831508600); Ultsch, Alfred (55915441200); Nöcker, Mario (55585156300); Stamm, Christian (55585542000)","14831508600; 55915441200; 55585156300; 55585542000","Databionic visualization of music collections according to perceptual distance","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","58","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873562110&partnerID=40&md5=c18db285bd6697131a126b4ded7e62a7","Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany","Mörchen F., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Ultsch A., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Nöcker M., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; Stamm C., Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany","We describe the MusicMiner system for organizing large collections of music with databionic mining techniques. Low level audio features are extracted from the raw audio data on short time windows during which the sound is assumed to be stationary. Static and temporal statistics were consistently and systematically used for aggregation of low level features to form high level features. A supervised feature selection targeted to model perceptual distance between different sounding music lead to a small set of non-redundant sound features. Clustering and visualization based on these feature vectors can discover emergent structures in collections of music. Visualization based on Emergent Self-Organizing Maps in particular enables the unsupervised discovery of timbrally consistent clusters that may or may not correspond to musical genres and artists. We demonstrate the visualizations capabilities of the U-Map, displaying local sound differences based on the new audio features. An intuitive browsing of large music collections is offered based on the paradigm of topographic maps. The user can navigate the sound space and interact with the maps to play music or show the context of a song. © 2005 Queen Mary, University of London.","Audio features; Clustering; Music similarity; Perception; Visualization","Audio recordings; Conformal mapping; Flow visualization; Information retrieval; Maps; Sensory perception; Visualization; Audio data; Audio features; Clustering; Emergent structure; Feature vectors; High-level features; Large music collections; Low level; Low-level features; Mining techniques; Music collection; Music similarity; Musical genre; Non-redundant; Perceptual distance; Short time windows; Sound spaces; Topographic map; Audio acoustics","F. Mörchen; Data Bionics Research Group, Philipps-University Marburg, 35032 Marburg, Germany; email: fabian@informatik.uni-marburg.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Homburg H.; Mierswa I.; Möller B.; Morik K.; Wurst M.","Homburg, Helge (55585252100); Mierswa, Ingo (56128662600); Möller, Bülent (55584944300); Morik, Katharina (6701443861); Wurst, Michael (13405022200)","55585252100; 56128662600; 55584944300; 6701443861; 13405022200","A benchmark dataset for audio classification and clustering","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","92","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873546054&partnerID=40&md5=9ec26b9cab5b262b6196d1e481cce5f5","University of Dortmund, AI Unit, 44221 Dortmund, Germany","Homburg H., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Mierswa I., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Möller B., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Morik K., University of Dortmund, AI Unit, 44221 Dortmund, Germany; Wurst M., University of Dortmund, AI Unit, 44221 Dortmund, Germany","We present a freely available benchmark dataset for audio classification and clustering. This dataset consists of 10 seconds samples of 1886 songs obtained from the Garageband site. Beside the audio clips themselves, textual meta data is provided for the individual songs. The songs are classified into 9 genres. In addition to the genre information, our dataset also consists of 24 hierarchical cluster models created manually by a group of users. This enables a user centric evaluation of audio classification and clustering algorithms and gives researchers the opportunity to test the performance of their methods on heterogeneous data. We first give a motivation for assembling our benchmark dataset. Then we describe the dataset and its elements in more detail. Finally, we present some initial results using a set of audio features generated by a feature construction approach. © 2005 Queen Mary, University of London.","Audio classification; Audio clustering; Benchmark dataset; Meta learning","Audio acoustics; Hierarchical systems; Information retrieval; Audio classification; Audio clips; Audio clustering; Audio features; Benchmark datasets; Feature construction; Heterogeneous data; Hierarchical clusters; Metalearning; User-centric evaluations; Clustering algorithms","","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pampalk E.; Flexer A.; Widmer G.","Pampalk, Elias (6507297042); Flexer, Arthur (7004555682); Widmer, Gerhard (7004342843)","6507297042; 7004555682; 7004342843","Improvements of audio-based music similarity and genre classificaton","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","196","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873545114&partnerID=40&md5=aa02fafe088cccdada6e7b8282ac532b","Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Department of Computational Perception, Johannes Kepler University, Linz, Austria","Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria; Flexer A., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), 1010 Vienna, Freyung 6/6, Austria, Department of Computational Perception, Johannes Kepler University, Linz, Austria","Audio-based music similarity measures can be applied to automatically generate playlists or recommendations. In this paper spectral similarity is combined with complementary information from fluctuation patterns including two new descriptors derived thereof. The performance is evaluated in a series of experiments on four music collections. The evaluations are based on genre classification, assuming that very similar tracks belong to the same genre. The main findings are that, (1) although the improvements are substantial on two of the four collections our extensive experiments confirm earlier findings that we are approaching the limit of how far we can get using simple audio statistics. (2)We have found that evaluating similarity through genre classification is biased by the music collection (and genre taxonomy) used. Furthermore, (3) in a cross validation no pieces from the same artist should be in both training and test set. © 2005 Queen Mary, University of London.","","Audio recordings; Experiments; Information retrieval; Audio-based; Cross validation; Descriptors; Genre classification; Music collection; Music similarity; Spectral similarity; Test sets; Motion compensation","","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bosma M.; Veltkamp R.C.; Wiering F.","Bosma, Martijn (55583635500); Veltkamp, Remco C. (7003421646); Wiering, Frans (8976178100)","55583635500; 7003421646; 8976178100","Muugle: A modular music information retrieval framework","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350486820&partnerID=40&md5=43b93507757b91854eda76716c84d2d9","Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands","Bosma M., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands; Veltkamp R.C., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands; Wiering F., Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands","Muugle (Musical Utrecht University Global Lookup Engine) is a modular framework that allows the comparison of different MIR techniques and usability studies. A system overview and a discussion of a pilot usability experiment are given. A demo version of the framework can be found on http://give-lab.cs.uu.nl/ muugle. © 2006 University of Victoria.","Framework; Music information retrieval","Framework; Lookup engines; Modular framework; Music information retrieval; Usability studies; Information retrieval","M. Bosma; Utrecht University, 3584 CH Utrecht, Padualaan 14, De Uithof, Netherlands; email: mbosma@cs.uu.nl","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Li Y.; Wang D.","Li, Yipeng (18042281500); Wang, DeLiang (7407070944)","18042281500; 7407070944","Singing voice separation from monaural recordings","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-41649099242&partnerID=40&md5=b10f7985353c8ab6674070e20b2d0be9","Department of Computer Science and Engineering, Ohio State University, United States; Department of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","Li Y., Department of Computer Science and Engineering, Ohio State University, United States; Wang D., Department of Computer Science and Engineering, Center for Cognitive Science, Ohio State University, United States","Separating singing voice from music accompaniment has wide applications in areas such as automatic lyrics recognition and alignment, singer identification, and music information retrieval. Compared to the extensive studies of speech separation, singing voice separation has been little explored. We propose a system to separate singing voice from music accompaniment from monaural recordings. The system has three stages. The singing voice detection stage partitions and classifies an input into vocal and non-vocal portions. Then the predominant pitch detection stage detects the pitch contour of the singing voice for vocal portions. Finally the separation stage uses the detected pitch contour to group the time-frequency segments of the singing voice. Quantitative results show that the system performs well in singing voice separation. © 2006 University of Victoria.","Predominant pitch detection; Singing voice detection; Singing voice separation","Audio recordings; Information retrieval; Music information retrieval; Pitch contours; Pitch detection; Quantitative result; Singing voice detection; Speech separation; Time frequency; Voice separation; Separation","Y. Li; Department of Computer Science and Engineering, Ohio State University, United States; email: liyip@cse.ohio-state.edu","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Lerch A.","Lerch, Alexander (22034963000)","22034963000","On the requirement of automatic tuning frequency estimation","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866326330&partnerID=40&md5=68bf4c494611f2c58499cf50b1f5a268","","","The deviation of the tuning frequency from the standard tuning frequency 440 Hz is evaluated for a database of classical music. It is discussed if and under what circumstances such a deviation may affect the robustness of pitch-based systems for musical content analysis. © 2006 University of Victoria.","Concert pitch; Detuning; Tuning frequency","Automatic tuning; Concert pitch; Content analysis; Detunings; Tuning frequency; Information retrieval","A. Lerch; Berlin, Germany; email: lerch@zplane.de","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Hosoya T.; Suzuki M.; Ito A.; Makino S.","Hosoya, Toru (15765019200); Suzuki, Motoyuki (56037004700); Ito, Akinori (7403722531); Makino, Shozo (7403067655)","15765019200; 56037004700; 7403722531; 7403067655","Lyrics recognition from a singing voice based on finite state automaton for music information retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","29","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873550660&partnerID=40&md5=04d67e9945ffa79c90f690afe7a65cde","Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan","Hosoya T., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Suzuki M., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Ito A., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; Makino S., Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan","Recently, several music information retrieval (MIR) systems have been developed which retrieve musical pieces by the user's singing voice. All of these systems use only the melody information for retrieval. Although the lyrics information is useful for retrieval, there have been few attempts to exploit lyrics in the user's input. In order to develop a MIR system that uses lyrics and melody information, lyrics recognition is needed. Lyrics recognition from a singing voice is achieved by similar technology to that of speech recognition. The difference between lyrics recognition and general speech recognition is that the input lyrics are a part of the lyrics of songs in a database. To exploit linguistic constraints maximally, we described the recognition grammar using a finite state automaton (FSA) that accepts only lyrics in the database. In addition, we carried out a ""singing voice adaptation"" using a speaker adaptation technique. In our experimental results, about 86% retrieval accuracy was obtained. © 2005 Queen Mary, University of London.","FSA; Lyrics recognition; MIR","Information retrieval; Speech recognition; FSA; Linguistic constraints; Lyrics recognition; MIR; Music information retrieval; Musical pieces; Retrieval accuracy; Singing voices; Speaker adaptation; Search engines","T. Hosoya; Graduate School of Engineering, Tohoku University, Aramaki, Aoba-ku Sendai, 980-8579, 6-6-05, Aoba, Japan; email: thosoya@makino.ecei.tohoku.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Eck D.; Casagrande N.","Eck, Douglas (12141444300); Casagrande, Norman (15080733700)","12141444300; 15080733700","Finding meter in music using an autocorrelation phase matrix and shannon entropy","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","19","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873555550&partnerID=40&md5=e78387d927d4cfac7ff69ce90ca901ff","University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","Eck D., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; Casagrande N., University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada","This paper introduces a novel way to detect metrical structure in music. We introduce a way to compute autocorrelation such that the distribution of energy in phase space is preserved in a matrix. The resulting autocorrelation phase matrix is useful for several tasks involving metrical structure. First we can use the matrix to enhance standard autocorrelation by calculating the Shannon entropy at each lag. This approach yields improved results for autocorrelationbased tempo induction. Second, we can efficiently search the matrix for combinations of lags that suggest particular metrical hierarchies. This approach yields a good model for predicting the meter of a piece of music. Finally we can use the phase information in the matrix to align a candidate meter with music, making it possible to perform beat induction with an autocorrelation-based model. We present results for several meter prediction and tempo induction datasets, demonstrating that the approach is competitive with models designed specifically for these tasks. We also present preliminary beat induction results on a small set of artificial patterns. © 2005 Queen Mary, University of London.","Autocorrelation; Beat induction; Entropy; Meter prediction; Tempo induction","Entropy; Forecasting; Information retrieval; Phase space methods; Beat induction; In-phase; Phase information; Phase matrix; Shannon entropy; Tempo induction; Autocorrelation","D. Eck; University of Montreal, Department of Computer Science, Montreal, QC H3C 3J7, CP 6128, Succ. Centre-Ville, Canada; email: eckdoug@iro.umontreal.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Fiebrink R.; Fujinaga I.","Fiebrink, Rebecca (36095844900); Fujinaga, Ichiro (9038140900)","36095844900; 9038140900","Feature selection pitfalls and music classification","2006","ISMIR 2006 - 7th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749096588&partnerID=40&md5=8b2dbc0deff54851ea1a1fc7aa315c73","Music Technology, McGill University, Montreal, Canada","Fiebrink R., Music Technology, McGill University, Montreal, Canada; Fujinaga I., Music Technology, McGill University, Montreal, Canada","Previous work has employed an approach to the evaluation of wrapper feature selection methods that may overstate their ability to improve classification accuracy, because of a phenomenon akin to overfitting. This paper discusses this phenomenon in the context of recent work in machine learning, demonstrates that previous work in MIR has indeed exaggerated the efficacy of feature selection for music classification, and presents new testing providing a more realistic analysis of feature selection's impact on music classification accuracy. © 2006 University of Victoria.","Classification; Feature selection","Classification (of information); Feature extraction; Classification accuracy; Feature selection methods; Music classification; Overfitting; Information retrieval","R. Fiebrink; Music Technology, McGill University, Montreal, Canada; email: rfiebrink@acm.org","","7th International Conference on Music Information Retrieval, ISMIR 2006","8 October 2006 through 12 October 2006","Victoria, BC","95392"
"Celma O.; Ramírez M.; Herrera P.","Celma, Óscar (12804596800); Ramírez, Miquel (23398785800); Herrera, Perfecto (24824250300)","12804596800; 23398785800; 24824250300","Foafing the Music: A music recommendation system based on RSS feeds and user preferences","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","63","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449799861&partnerID=40&md5=cf4b7ab0f9456330da96c4e20f388001","Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain","Celma O., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain; Ramírez M., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain; Herrera P., Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain","In this paper we give an overview of the Foafing the Music system. The system uses the Friend of a Friend (FOAF) and Rich Site Summary (RSS) vocabularies for recommending music to a user, depending on her musical tastes. Music information (new album releases, related artists' news and available audio) is gathered from thousands of RSS feeds .an XML format for syndicating Web content. On the other hand, FOAF documents are used to define user preferences. The presented system provides music discovery by means of: user profiling .defined in the user's FOAF description., context-based information .extracted from music related RSS feeds. and content-based descriptions .extracted from the audio itself © 2005 Queen Mary, University of London.","","Information retrieval; RSS; Content-based; Context-based information; Music information; Music Recommendation System; RSS feeds; System use; User profiling; Web content; XML format; Audio acoustics","O. Celma; Music Technology Group, Universitat Pompeu Fabra, 08003 Barcelona, Pg. Circumval lacio 8, Spain; email: ocelma@iua.upf.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Grund C.M.","Grund, Cynthia M. (7006384605)","7006384605","Music information retrieval, memory and culture: Some philosophical remarks","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873436506&partnerID=40&md5=0c3a386c0c52ce7c9b74bf58c7d48c99","Institute of Philosophy, Education and the Study of Religions Philosophy, University of Southern Denmark, Odense, Denmark","Grund C.M., Institute of Philosophy, Education and the Study of Religions Philosophy, University of Southern Denmark, Odense, Denmark","The burgeoning field of Music Information Retrieval (MIR) raises issues which are of interest within traditional areas of discussion in philosophy of music and of philosophy of culture in general. The purpose of this paper is twofold: the first goal is to highlight and briefly discuss a selection of these issues, while the second is to make a case for increased mutual awareness of each other on the parts of MIR and of humanistic research. Many traditional debates within the latter receive infusions of new perspectives from MIR, while research within MIR could be fruitfully pointed in directions suggested by questions of interest within traditional research in the humanities, e.g. the relationship of individual memory to cultural memory, issues regarding crosscultural understanding and the importance of authenticity in artistic contexts. © 2005 Queen Mary, University of London.","Authenticity; Culture; Ethics; Memory; Philosophy of music","Cell culture; Data storage equipment; Ontology; Philosophical aspects; Research; Authenticity; Cultural memory; Ethics; Music information retrieval; Mutual awareness; Philosophy of music; Information retrieval","C.M. Grund; Institute of Philosophy, Education and the Study of Religions Philosophy, University of Southern Denmark, Odense, Denmark; email: cmgrund@filos.sdu.dk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bertin N.; De Cheveigné A.","Bertin, Nancy (26424256400); De Cheveigné, Alain (7004036663)","26424256400; 7004036663","Scalable metadata and quick retrieval of audio signals","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949181052&partnerID=40&md5=8592bf4555c0f1493a209f0be2f68910","Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France","Bertin N., Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France; De Cheveigné A., Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France","Audio search algorithms have reached a degree of speed and accuracy that allows them to search efficiently within large databases of audio. For speed, algorithms generally depend on precalculated indexing metadata. Unfortunately, the size of the metadata follows the same exponential trend as the audio data itself, and this may lead to an exponential increase in storage cost and search time. The concept of scalable metadata has been introduced to allow metadata to adjust to such trends and alleviate the effects of forseeable increases of data and metadata size. Here, we argue that scalability fits the needs of the hierarchical structures that allow fast search, and illustrate this by adapting a state-of-the-art search algorithm to a scalable indexing structure. Scalability allows search algorithms to adapt to the increase of database size without loss of performance. © 2005 Queen Mary, University of London.","Audio retrieval; Indexing; Scalability; Scalable metadata; Search","Digital storage; Indexing (of information); Information retrieval; Learning algorithms; Scalability; Audio data; Audio retrieval; Audio searches; Audio signal; Data and metadata; Database size; Exponential increase; Exponential trends; Fast search; Hierarchical structures; Indexing structures; Large database; Loss of performance; Search; Search Algorithms; Search time; Storage costs; Metadata","N. Bertin; Equipe Audition, CNRS UMR 8581, ENS (DEC), 75005 Paris, 29, rue d'Ulm, France; email: Nancy.Bertin@ens.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Orio N.; Neve G.","Orio, Nicola (6507928255); Neve, Giovanna (13006287700)","6507928255; 13006287700","Experiments on segmentation techniques for music documents indexing","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751506470&partnerID=40&md5=03bdbff7a4fa72c8b5b8b6091818e7f7","Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy","Orio N., Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy; Neve G., Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy","This paper presents an overview of different approaches to melody segmentation aimed at extracting music lexical units, which can be used as content descriptors of music documents. Four approaches have been implemented and compared on a test collection of real documents and queries, showing their impact on index term size and on retrieval effectiveness. From the results, simple but extensive approaches seem to give better performances than more sophisticated segmentation algorithms. © 2005 Queen Mary, University of London.","Indexing; Melodic Segmentation","Information retrieval; Better performance; Content descriptors; Index terms; Lexical unit; Retrieval effectiveness; Segmentation algorithms; Segmentation techniques; Test Collection; Indexing (of information)","N. Orio; Department of Information Engineering, 35131 Padova, Via Gradenigo, 6/A, Italy; email: orio@dei.unipd.it","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Schedl M.; Knees P.; Widmer G.","Schedl, Markus (8684865900); Knees, Peter (8219023200); Widmer, Gerhard (7004342843)","8684865900; 8219023200; 7004342843","Discovering and visualizing prototypical artists by web-based co-occurrence analysis","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873445415&partnerID=40&md5=5bda80a942d051358ce2487cc9a9f876","Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria; Austrian Research Institute for Artificial Intelligence (ÖFAI), A-1010 Vienna, Austria","Schedl M., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (ÖFAI), A-1010 Vienna, Austria; Knees P., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (ÖFAI), A-1010 Vienna, Austria","Detecting artists that can be considered as prototypes for particular genres or styles of music is an interesting task. In this paper, we present an approach that ranks artists according to their prototypicality. To calculate such a ranking, we use asymmetric similarity matrices obtained via co-occurrence analysis of artist names on web pages. We demonstrate our approach on a data set containing 224 artists from 14 genres and evaluate the results using the rank correlation between the prototypicality ranking and a ranking obtained by page counts of search queries to Google that contain artist and genre. High positive rank correlations are achieved for nearly all genres of the data set. Furthermore, we elaborate a visualization method that illustrates similarities between artists using the prototypes of all genres as reference points. On the whole, we show how to create a prototypicality ranking and use it, together with a similarity matrix, to visualize a music repository. © 2005 Queen Mary, University of London.","Asymmetric artist similarity; Co-occurrence analysis; Prototypical artist detection; Visualization; Web mining","Flow visualization; Information retrieval; Visualization; Artist similarities; Co-occurrence analysis; Data set; Prototypicality; Rank correlation; Reference points; Search queries; Similarity matrix; Visualization method; Web Mining; Websites","M. Schedl; Department of Computational Perception, Johannes Kepler University (JKU), A-4040 Linz, Austria; email: markus.schedl@jku.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pardo B.; Sanghi M.","Pardo, Bryan (10242155400); Sanghi, Manan (8912371300)","10242155400; 8912371300","Polyphonic musical sequence alignment for database search","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46749093627&partnerID=40&md5=a86dae1c8bc714ae1970736800a417fd","Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States","Pardo B., Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States; Sanghi M., Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States","Finding the best matching database target to a melodic query has been of great interest in the music IR world. The string alignment paradigm works well for this task when comparing a monophonic query to a database of monophonic pieces. However, most tonal music is polyphonic, with multiple concurrent musical lines. Such pieces are not adequately represented as strings. Moreover, users often represent polyphonic pieces in their queries by skipping from one part (the soprano) to another (the bass). Current string matching approaches are not designed to handle this situation. This paper outlines approaches to extending string alignment that allow measuring similarity between a monophonic query and a polyphonic piece. These approaches are compared using synthetic queries on a database of Bach pieces. Results indicate that when a monophonic query is drawn from multiple parts in the target, a method which explicitly takes the multi-part structure of a piece into account significantly outperforms the one that does not. © 2005 Queen Mary, University of London.","","Alignment; Information retrieval; Query languages; Search engines; Best matching; Database searches; Measuring similarities; Multiple parts; One parts; Sequence alignments; String alignment; String matching; Tonal music; Query processing","B. Pardo; Computer Science Department, Northwestern University, Evanston, IL, 1890 Maple Ave., United States; email: pardo@northwestern.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Amatriain X.; Massaguer J.; Garcia D.; Mosquera I.","Amatriain, Xavier (23048860200); Massaguer, Jordi (6603092129); Garcia, David (57213360830); Mosquera, Ismael (55585486000)","23048860200; 6603092129; 57213360830; 55585486000","The CLAM Annotator: A cross-platform audio descriptors editing tool","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949223170&partnerID=40&md5=454c258e2f647b8a51a863f6e11ca844","CREATE, University of California, Santa Barbara CA 93106, United States; Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain","Amatriain X., CREATE, University of California, Santa Barbara CA 93106, United States; Massaguer J., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain; Garcia D., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain; Mosquera I., Universitat Pompeu Fabra, Barcelona, Psg. Circumvalacio, 18, Spain","This paper presents the CLAM Annotator tool. This application has been developed in the context of the CLAM framework and can be used to manually edit any previously computed audio descriptors. The application offers a convenient GUI that allows to edit low-level frame descriptors, global descriptors of any kind and segmentation marks. It is designed in such a way that the interface adapts itself to a user-defined schema, offering possibilities to a large range of applications. © 2005 Queen Mary, University of London.","Annotating tool; Audio descriptors; XML","Information retrieval; Shellfish; XML; Cross-platform; Descriptors; Editing tools; Global Descriptors; User-defined schema; Molluscs","X. Amatriain; CREATE, University of California, Santa Barbara CA 93106, United States; email: xavier@create.ucsb.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Bray S.; Tzanetakis G.","Bray, Stuart (55585443000); Tzanetakis, George (6602262192)","55585443000; 6602262192","Distributed audio feature extraction for music","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247202817&partnerID=40&md5=3308ea01bdc0ebf7eb8bcb2078d3aa5b","Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","Bray S., Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; Tzanetakis G., Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada","One of the important challenges facing music information retrieval (MIR) of audio signals is scaling analysis algorithms to large collections. Typically, analysis of audio signals utilizes sophisticated signal processing and machine learning techniques that require significant computational resources. Therefore, audio MIR is an area were computational resources are a significant bottleneck. For example, the number of pieces utilized in the majority of existing work in audio MIR is at most a few thousand files. Computing audio features over thousands files can sometimes take days of processing. In this paper, we describe how Marsyas-0.2, a free software framework for audio analysis and synthesis can be used to rapidly implement efficient distributed audio analysis algorithms. The framework is based on a dataflow architecture which facilitates partitioning of audio computations over multiple computers. Experimental results demonstrating the effectiveness of the proposed approach are presented. © 2005 Queen Mary, University of London.","Dataflow networks; Distributed processing; Large-scale music information retrieval","Algorithms; Computer architecture; Computer programming; Feature extraction; Learning systems; Signal processing; Audio analysis; Audio analysis and synthesis; Audio feature extraction; Audio features; Audio signal; Computational resources; Data-flow architectures; Dataflow; Distributed processing; Free software; Machine learning techniques; Multiple computers; Music information retrieval; Scaling analysis; Information retrieval","S. Bray; Computer Science Department, University of Victoria, Victoria BC, 3800 Finnerty Rd., Canada; email: sbray@csc.uvic.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cantone D.; Cristofaro S.; Faro S.","Cantone, Domenico (55892604800); Cristofaro, Salvatore (8881779200); Faro, Simone (8881779300)","55892604800; 8881779200; 8881779300","On tuning the (δ, α)-SEQUENTIAL-SAMPLING algorithm for δ-approximate matching with α-bounded gaps in musical sequences","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049084711&partnerID=40&md5=2b473e9297c7c9be29fa38b536705867","Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","Cantone D., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Cristofaro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; Faro S., Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy","We present a very efficient variant of the (δ, α)- SEQUENTIAL-SAMPLING algorithm, recently introduced by the authors, for the δ-approximate string matching problemwith α-bounded gaps, which often arises in many questions on musical information retrieval and musical analysis. Though it retains the same worst-case O(mn)-time and O(mα)-space complexity of its progenitor to compute the number of distinct δ-approximate α-gapped occurrences of a pattern of lengthmat each position in a text of length n, our new variant achieves an average O(n)-time complexity in practical cases. Extensive experimentations indicate that our algorithm is more efficient than existing solutions for the same problem, especially in the case of long patterns. © 2005 Queen Mary, University of London.","Approximate string matching; Experimental algorithms; Musical information retrieval","Learning algorithms; Approximate matching; Approximate string matching; Experimental algorithms; Musical analysis; Musical information retrieval; Space complexity; Time complexity; Information retrieval","D. Cantone; Università di Catania, Dipartimento di Matematica e Informatica, I-95125 Catania, Viale Andrea Doria 6, Italy; email: cantone@dmi.unict.it","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kurth F.; Müller M.; Damm D.; Fremerey C.; Ribbrock A.; Clausen M.","Kurth, Frank (56240850200); Müller, Meinard (7404689873); Damm, David (57197094801); Fremerey, Christian (23396821400); Ribbrock, Andreas (22433406800); Clausen, Michael (56225233200)","56240850200; 7404689873; 57197094801; 23396821400; 22433406800; 56225233200","SyncPlayer - An advanced system for multimodal music access","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349282433&partnerID=40&md5=261538d120abf7fb94658b604594fbdd","Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany","Kurth F., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Müller M., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Damm D., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Fremerey C., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Ribbrock A., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; Clausen M., Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany","In this paper, we present the SyncPlayer system for multimodal presentation of high quality audio and associated music-related data. Using the SyncPlayer client interface, a user may play back an audio recording that is locally available on his computer. The recording is then identified by the SyncPlayer server, a process which is performed entirely content-based. Subsequently, the server delivers music-related data like scores or lyrics to the client, which are then displayed synchronously with audio playback using a multimodal visualization plug-in. In addition to visualization, the system provides functionality for contentbased music retrieval and semi-manual content annotation. To the best of our knowledge, our system is moreover the first to systematically exploit automatically generated synchronization data for content-based symbolic browsing in high quality audio recordings. SyncPlayer has already proved to be a valuable tool for evaluating algorithms in MIR research on a larger scale. In this paper, we describe the technical background of the SyncPlayer framework in detail. We also give an overview of the underlying MIR techniques of audio matching, music synchronization, and text-based retrieval that are incorporated in the current version of the system. © 2005 Queen Mary, University of London.","MIR systems and infrastructure; Multimodal interfaces and music access; Synchronization","Audio acoustics; Data visualization; Information retrieval; Synchronization; Visualization; Advanced systems; Audio matching; Automatically generated; Client interface; Content annotation; Content-based; Content-based music retrieval; Evaluating algorithms; High-quality audio; Mir systems and infrastructures; Multi-modal; Multi-modal interfaces; Multi-modal visualization; Music synchronizations; Plug-ins; Synchronization data; Technical background; Text-based retrieval; Audio recordings","F. Kurth; Universität Bonn, Institut F Ür Informatik III, D-53117 Bonn, Römerstr. 164, Germany; email: frank@cs.uni-bonn.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Doets P.J.O.; Lagendijk R.L.","Doets, P.J.O. (6506320913); Lagendijk, R.L. (56969610500)","6506320913; 56969610500","Extracting quality parameters for compressed audio from fingerprints","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645683098&partnerID=40&md5=239de6c7937197a95fb07c74250be055","Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands","Doets P.J.O., Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands; Lagendijk R.L., Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands","An audio fingerprint is a compact yet very robust representation of the perceptually relevant parts of audio content. It can be used to identify audio, even when of severely distorted. Audio compression causes small changes in the fingerprint. We aim to exploit these small fingerprint differences due to compression to assess the perceptual quality of the compressed audio file. Analysis shows that for uncorrelated signals the Bit Error Rate (BER) is approximately inversely proportional to the square root of the Signal-to-Noise Ratio (SNR) of the signal. Experiments using real music confirm this relation. Further experiments show how the various local spectral characteristics cause a large variation in the behavior of the fingerprint difference as a function of SNR or the bitrate set for compression. © 2005 Queen Mary, University of London.","","Experiments; Information retrieval; Audio compression; Audio content; Audio files; Audio fingerprint; Bit rates; Perceptual quality; Quality parameters; Signaltonoise ratio (SNR); Spectral characteristics; Square roots; Uncorrelated signals; Audio signal processing","P.J.O. Doets; Dept. of Mediamatics, Information and Communication Theory Group, Faculty of Electrical Engineering,Mathematics and Computer Science, Delft University of Technology, 2600 GA Delft, P.O. Box 5031, Netherlands; email: p.j.doets@ewi.tudelft.nl","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Saito S.; Kameoka H.; Nishimoto T.; Sagayama S.","Saito, Shoichiro (55464073400); Kameoka, Hirokazu (7006771405); Nishimoto, Takuya (7102868523); Sagayama, Shigeki (7004859104)","55464073400; 7006771405; 7102868523; 7004859104","Specmurt analysis of multi-pitch music signals with adaptive estimation of common harmonic structure","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449098327&partnerID=40&md5=b5ba07704da73a3e0b2e4b3dddae9725","Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan","Saito S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Nishimoto T., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan","This paper describes a multi-pitch analysis method using specmurt analysis with iterative estimation of the quasioptimal common harmonic structure function. Specmurt analysis (Sagayama et al., 2004) is based upon the idea that superimposed harmonic structure pattern can be expressed as a convolution of two components, a fundamental frequency distribution and a 'common harmonic structure' function if each underlying tone component has similar harmonic structure pattern. As proved in our previous work (Sagayama et al., 2004) inappropriate common structure function leads to inaccurate analysis results. The iterative algorithm proposed in this paper automatically chooses a proper structure, which results in finding concurrent multiple fundamental frequencies and reduces the dependency on heuristically chosen initial common harmonic structure. The experimental evaluation showed promising results. © 2005 Queen Mary, University of London.","Audio feature extraction; Specmurt analysis; Visualization of the fundamental frequency","Algorithms; Feature extraction; Harmonic analysis; Information retrieval; Iterative methods; Natural frequencies; Adaptive estimation; Analysis method; Audio feature extraction; Common structures; Experimental evaluation; Fundamental frequencies; Harmonic structures; Iterative algorithm; Iterative estimation; Multi pitches; Music signals; Quasi-optimal; Specmurt analysis; Two-component; Harmonic functions","S. Saito; Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1, Hongo, Japan; email: saito@hil.t.u-tokyo.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lobb R.; Bell T.; Bainbridge D.","Lobb, Richard (7005660583); Bell, Tim (7402501556); Bainbridge, David (8756864800)","7005660583; 7402501556; 8756864800","Fast capture of sheet music for an agile digital music library","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866006591&partnerID=40&md5=f708d1250b088cbecf2015cc60f66bbd","Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; Department of Computer Science, University of Waikato, Hamilton, New Zealand","Lobb R., Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; Bell T., Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; Bainbridge D., Department of Computer Science, University of Waikato, Hamilton, New Zealand","A personal digital music library needs to be .agile., that is, it needs to make it easy to capture and index material on the fly. A digital camera is a particularly effective way of achieving this, but there are several issues with the quality of the captured image, including distortions in the shape of the image due to the camera not being aligned properly with the page, non-planarity of the page, lens distortion from close-up shots, and inconsistent lighting across the page. In this paper we explore ways to improve the quality of music images captured by a digital camera or an inexpensive scanner, where the user is not expected to pay a lot of attention to the process. Such pre-processing will significantly aid Music Information Retrieval indexing through Optical Music Recognition, for example. The research presented here is primarily based around using a Fast Fourier Transform (FFT) to determine the orientation of the page. We find that a windowed FFT is effective at correcting rotational errors, and we make significant progress towards removing perspective distortion introduced by the camera not being parallel with the music. © 2005 Queen Mary, University of London.","Digital camera; FFT; Image capture","Digital cameras; Fast Fourier transforms; Information retrieval; Video cameras; Digital music libraries; Image captures; Index material; Lens distortion; Music information retrieval; On the flies; Optical music recognition; Perspective distortion; Pre-processing; Rotational errors; Windowed-FFT; Digital libraries","R. Lobb; Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand; email: richard.lobb@canterbury.ac.nz","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Chordia P.","Chordia, Parag (24723695700)","24723695700","Segmentation and recognition of tabla strokes","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","30","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872732949&partnerID=40&md5=0446ce7792037f891ccde6a773fc6852","CCRMA, Stanford University, Stanford CA 94305, 660 Lomita Dr., United States","Chordia P., CCRMA, Stanford University, Stanford CA 94305, 660 Lomita Dr., United States","A system that segments and labels tabla strokes from real performances is described. Performance is evaluated on a large database taken from three performers under different recording conditions, containing a total of 16,834 strokes. The current work extends previous work by Gillet and Richard (2003) on categorizing tabla strokes, by using a larger, more diverse database that includes their data as a benchmark, and by testing neural networks and treebased classification methods. First, the time-domain signal was segmented using complex-domain thresholding that looked for sudden changes in amplitude and phase discontinuities. At the optimal point on the ROC curve, false positives were less than 1% and false negatives were less than 2%. Then, classification was performed using a multivariate Gaussian model (mv gauss) as well as non-parametric techniques such as probabilistic neural networks (pnn), feed-forward neural networks (ffnn), and tree-based classifiers. Two evaluation protocols were used. The first used 10-fold cross validation. The recognition rate averaged over several experiments that contained 10-15 classes was 92% for the mv gauss, 94% for the ffnn and pnn, and 84% for the tree based classifier. To test generalization, a more difficult independent evaluation was undertaken in which no test strokes came from the same recording as the training strokes. The average recognition rate over a wide variety of test conditions was 76% for the mv gauss, 83% for the ffnn, 76% for the pnn, and 66% for the tree classifier. © 2005 Queen Mary, University of London.","Automatic transcription; Instrument recognition; Tabla; Timbre","Audio recordings; Forestry; Information retrieval; Neural networks; Automatic transcription; Classification methods; Cross validation; Evaluation protocol; False negatives; False positive; Instrument recognition; Large database; Multivariate gaussian models; Non-parametric techniques; Optimal points; Phase-discontinuities; Probabilistic neural networks; Recognition rates; ROC curves; Sudden change; Tabla; Test condition; Thresholding; Timbre; Time-domain signal; Tree classifiers; Tree-based; Gaussian distribution","P. Chordia; CCRMA, Stanford University, Stanford CA 94305, 660 Lomita Dr., United States; email: pchordia@ccrma.stanford.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cahill M.; Ó Maidín D.","Cahill, Margaret (55582788400); Ó Maidín, Donncha (55584898200)","55582788400; 55584898200","Melodic similarity algorithms - Using similarity ratings for development and early evaluation","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","7","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949607193&partnerID=40&md5=5278af8ca26074b98a633ba038be3d71","Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","Cahill M., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; Ó Maidín D., Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland","This paper focuses on gathering similarity ratings for use in the construction, optimization and evaluation of melodic similarity algorithms. The approach involves conducting listening experiments to gather these ratings for a piece in Theme and Variation form. © 2005 Queen Mary, University of London.","Algorithm; Listening experiments; Melodic similarity; Perception; Score; Similarity ratings","Experiments; Information retrieval; Sensory perception; Early evaluation; Melodic similarity; Score; Algorithms","M. Cahill; Centre for Computational Musicology and Computer Music, Department of Computer Science and Information Systems, University of Limerick, Ireland; email: margaret.cahill@ul.ie","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Wood G.; O'Keefe S.","Wood, Gavin (57198391373); O'Keefe, Simon (14424319000)","57198391373; 14424319000","On techniques for content-based visual annotation to aid intra-track music navigation","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547703973&partnerID=40&md5=12fbd0e0b9db83abbae07905700edd6d","University of York, York YO10 5DD, United Kingdom","Wood G., University of York, York YO10 5DD, United Kingdom; O'Keefe S., University of York, York YO10 5DD, United Kingdom","Despite the fact that people are increasingly listening to music electronically, the core interface of the common tools for playing the music have had very little improvement. In particular the tools for intra-track navigation have remained basically static, not taking advantage of recent studies into the field of audio jisting, summarising and segmentation. We introduce a novel mechanism for musical audio linear summarisation and modify a widely used open source media player to utilise several music information retrieval techniques directly in the graphical user interface. With a broad range of music, we provide a qualitative discussion on several techniques used for contentbased music information retrieval and perform quantitative investigation to their usefulness. © 2005 Queen Mary, University of London.","","Graphical user interfaces; Information retrieval; Content-based; Core interfaces; Media players; Music information retrieval; Musical audio; Open sources; Quantitative investigation; Techniques used; Visual annotations; Audio acoustics","G. Wood; University of York, York YO10 5DD, United Kingdom; email: gav@cs.york.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Isaacson E.","Isaacson, Eric (7003783976)","7003783976","What you see is what you get: On visualizing music","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","32","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952690386&partnerID=40&md5=a9d66a5128a2ef57a7d093dbf30af595","Indiana University, School of Music, Department of Music Theory, Bloomington, IN 47405, United States","Isaacson E., Indiana University, School of Music, Department of Music Theory, Bloomington, IN 47405, United States","Though music is fundamentally an aural phenomenon, we often communicate about music through visual means. The paper examines a number of visualization techniques developed for music, focusing especially on those developed for music analysis by specialists in the field, but also looking at some less successful approaches. It is hoped that, by presenting them in this way, those in the MIR community will develop a greater awareness of the kinds of musical problems music scholars are concerned with, and might lend a hand toward addressing them © 2005 Queen Mary, University of London.","Analysis; Harmony; Visualization","Flow visualization; Analysis; Harmony; Music analysis; Visualization technique; Information retrieval","E. Isaacson; Indiana University, School of Music, Department of Music Theory, Bloomington, IN 47405, United States; email: isaacso@indiana.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Dannenberg R.B.","Dannenberg, Roger B. (7003266250)","7003266250","Toward automated holistic beat tracking, music analysis, and understanding","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","22","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547404622&partnerID=40&md5=11b7d0de98814d778cde87afd5b3c6cb","School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Dannenberg R.B., School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Most music processing attempts to focus on one particular feature or structural element such as pitch, beat location, tempo, or genre. This hierarchical approach, in which music is separated into elements that are analyzed independently, is convenient for the scientific researcher, but is at odds with intuition about music perception. Music is interconnected at many levels, and the interplay of melody, harmony, and rhythm are important in perception. As a first step toward more holistic music analysis, music structure is used to constrain a beat tracking program. With structural information, the simple beat tracker, working with audio input, shows a large improvement. The implications of this work for other music analysis problems are discussed. © 2005 Queen Mary, University of London.","Analysis; Beat tracking; Music structure; Tempo","Analysis; Audio input; Beat tracking; Hierarchical approach; Music analysis; Music perception; Music structures; Structural elements; Structural information; Tempo; Information retrieval","R.B. Dannenberg; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, United States; email: rbd@cs.cmu.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Dixon S.; Widmer G.","Dixon, Simon (57203056378); Widmer, Gerhard (7004342843)","57203056378; 7004342843","MATCH: A music alignment tool chest","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","128","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870607867&partnerID=40&md5=34fcd410b321e8a2ca98c92533b33d4a","Austrian Research Institute for Artificial Intelligence, Vienna 1010, Freyung 6/6, Austria; Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Altenberger Str 69, Austria","Dixon S., Austrian Research Institute for Artificial Intelligence, Vienna 1010, Freyung 6/6, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Altenberger Str 69, Austria","We present MATCH, a toolkit for aligning audio recordings of different renditions of the same piece of music, based on an efficient implementation of a dynamic time warping algorithm. A forward path estimation algorithm constrains the alignment path so that dynamic time warping can be performed with time and space costs that are linear in the size of the audio files. Frames of audio are represented by a positive spectral difference vector, which emphasises note onsets in the alignment process. In tests with Classical and Romantic piano music, the average alignment error was 41ms (median 20ms), with only 2 out of 683 test cases failing to align. The software is useful for content-based indexing of audio files and for the study of performance interpretation; it can also be used in real-time for tracking live performances. The toolkit also provides functions for displaying the cost matrix, the forward and backward paths, and any metadata associated with the recordings, which can be shown in real time as the alignment is computed. © 2005 Queen Mary, University of London.","Audio alignment; Content-based indexing; Dynamic time warping; Music performance analysis","Algorithms; Alignment; Audio acoustics; Indexing (of information); Information retrieval; Metadata; Alignment error; Audio alignments; Audio files; Content-based indexing; Cost matrices; Dynamic time warping; Dynamic time warping algorithms; Efficient implementation; Forward-and-backward; Music performance; Path estimation; Piano music; Real time; Spectral differences; Test case; Audio recordings","S. Dixon; Austrian Research Institute for Artificial Intelligence, Vienna 1010, Freyung 6/6, Austria; email: simon@ofai.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kameoka H.; Nishimoto T.; Sagayama S.","Kameoka, Hirokazu (7006771405); Nishimoto, Takuya (7102868523); Sagayama, Shigeki (7004859104)","7006771405; 7102868523; 7004859104","Harmonic-temporal-structured clustering via deterministic annealing EM algorithm for audio feature extraction","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","15","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846191496&partnerID=40&md5=5e5b21f78c997c621c3c5568b2505136","Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","Kameoka H., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Nishimoto T., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; Sagayama S., Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan","This paper proposes ""harmonic-temporal structured clustering (HTC) method"", that allows simultaneous estimation of pitch, intensity, onset, duration, etc., of each underlying source in multi-stream audio signal, which we expect to be an effective feature extraction for MIR systems. STC decomposes the energy patterns diffused in timefrequency space, i.e., a time series of power spectrum, into distinct clusters such that each of them is originated from a single sound stream. It becomes clear that the problem is equivalent to geometrically approximating the observed time series of power spectrum by superimposed harmonictemporal structured models (HTMs), whose parameters are directly associated with the specific acoustic characteristics. The update equations in DA(Deterministic Annealing) EM algorithm for the optimal parameter convergence are derived by formulating the model with Gaussian kernel representation. The experiment showed promising results, and verified the potential of the proposed method. © 2005 Queen Mary, University of London.","Audio feature extraction; Harmonic-temporal structured clustering; Multi-pitch estimation","Algorithms; Information retrieval; Power spectrum; Rapid thermal annealing; Time series; Acoustic characteristic; Audio feature extraction; Audio signal; Deterministic annealing; Deterministic Annealing EM algorithms; EM algorithms; Energy patterns; Gaussian kernels; Harmonic-temporal structured clustering; Multi-pitch estimations; Multi-stream; Optimal parameter; Simultaneous estimation; Sound streams; Structured model; Time-frequency space; Feature extraction","H. Kameoka; Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, 113-8656, 7-3-1 Hongo, Japan; email: kameoka@hil.t.u-tokyo.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Heydarian P.; Reiss J.D.","Heydarian, Peyman (25627773400); Reiss, Joshua D. (10140139100)","25627773400; 10140139100","The Persian music and the Santur instrument","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949101135&partnerID=40&md5=8262a054397da849c0e633a59f5f83cf","Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom","Heydarian P., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom; Reiss J.D., Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom","Persian music has had a profound effect on various Eastern musical cultures, and also influenced Southern European and Northern African music. The Santur, a hammered dulcimer, is one of the most important instruments in Persia. In this paper, Persian music and the Santur instrument are explained and analysed. Techniques for fundamental frequency detection are applied to data acquired from the Santur and results are reported. © 2005 Queen Mary, University of London.","Dastgàh; Dulcimer; Fundamental frequency detection; Interval; Iranian music; Non-western musicology; Persian; Pitch; Quartertone; Santoor; Santur","Information retrieval; Natural frequencies; Dulcimer; Fundamental frequency detections; Interval; Iranian music; Non-western musicology; Persians; Pitch; Quartertone; Santoor; Santur; Instruments","P. Heydarian; Centre for Digital Music, Queen Mary, University of London, London E1 4NS, Mile End Road, United Kingdom; email: peyman.heydarian@elec.qmul.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Muñoz-Expósito J.E.; Garcia-Galán S.; Ruiz-Reyes N.; Vera-Candeas P.; Rivas-Peña F.","Muñoz-Expósito, J.E. (15725953900); Garcia-Galán, S. (15724913400); Ruiz-Reyes, N. (8906410300); Vera-Candeas, P. (57216710350); Rivas-Peña, F. (55257689000)","15725953900; 15724913400; 8906410300; 57216710350; 55257689000","Speech/music discrimination using a single Warped LPC-based feature","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350641966&partnerID=40&md5=6a3b91c396463e2f9f916a0720a02ce4","Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain","Muñoz-Expósito J.E., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Garcia-Galán S., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Ruiz-Reyes N., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Vera-Candeas P., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; Rivas-Peña F., Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain","Automatic discrimination of speech and music is an important tool in many multimedia applications. The paper presents a low complexity but effective approach for speech/music discrimination, which exploits only one simple feature, called Warped LPC-based Spectral Centroid (WLPC-SC). A three-component Gaussian Mixture Model (GMM) classifier is used because it showed a slightly better performance than other Statistical Pattern Recognition (SPR) classifiers. Comparison between WLPC-SC and the timbral features proposed in Tzanetakis and Cook (2002) is performed, aiming to assess the good discriminatory power of the proposed feature. Experimental results reveal that our speech/music discriminator is robust and fast, making it suitable for real-time multimedia applications. © 2005 Queen Mary, University of London.","GMM; LPC; Spectral centroid; Speech/music discrimination","Information retrieval; Better performance; Discriminatory power; Effective approaches; Gaussian Mixture Model; GMM; LPC; Multimedia applications; Realtime multimedia; Spectral centroid; Speech/music discrimination; Statistical pattern recognition; Three-component; Speech recognition","J.E. Muñoz-Expósito; Electronics and Telecommunication Engineering Department, University of Jaén, Polytechnic School, 23700 Linares, Jaén, C/ Alfonso X el Sabio, 28, Spain; email: jemunoz@ujaen.es","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Weyde T.; Datzko C.","Weyde, Tillman (24476899500); Datzko, Christian (55585554200)","24476899500; 55585554200","Efficient melody retrieval with motif contour classes","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64049088858&partnerID=40&md5=32d5e3063f6703807f0ddd7f725cd22a","City University, School of Informatics, Department of Computing, London, United Kingdom; University of Osnabrück, Research Department of Music and Media Technology, Osnabrück, Germany","Weyde T., City University, School of Informatics, Department of Computing, London, United Kingdom; Datzko C., University of Osnabrück, Research Department of Music and Media Technology, Osnabrück, Germany","This paper describes the use of motif contour classes for efficient retrieval of melodies from music collections. Instead of extracting incipits or themes, complete monophonic pieces are indexed for their motifs, using classes of motif contours. Similarity relations between these classes can be used for a very efficient search. This can serve as a first level search, which can be refined by using more computationally intensive comparisons on its results. The model introduced has been implemented and tested using the MUSITECH framework. We present empirical and analytical results on the retrieval quality, the complexity, and quality/efficiency trade-off. © 2005 Queen Mary, University of London.","Melodic similarity; Melody retrieval; Motivic analysis; Retrieval efficiency","Analytical results; Melodic similarity; Melody retrieval; Motivic analysis; Music collection; Retrieval efficiency; Retrieval quality; Similarity relations; Information retrieval","T. Weyde; City University, School of Informatics, Department of Computing, London, United Kingdom; email: t.e.weyde@city.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Flexer A.; Pampalk E.; Widmer G.","Flexer, Arthur (7004555682); Pampalk, Elias (6507297042); Widmer, Gerhard (7004342843)","7004555682; 6507297042; 7004342843","Novelty detection based on spectral similarity of songs","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644630695&partnerID=40&md5=9fe873b081217e4b76b6082180c19cd3","Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, A-1010 Vienna, Freyung 6/2, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria; Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenberger Str. 69, Austria","Flexer A., Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, A-1010 Vienna, Freyung 6/2, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria; Pampalk E., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria; Widmer G., Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Freyung 6 /6, Austria, Department of Computational Perception, Johannes Kepler University, A-4040 Linz, Altenberger Str. 69, Austria","We are introducing novelty detection, i.e. the automatic identification of new or unknown data not covered by the training data, to the field of music information retrieval. Two methods for novelty detection - one based solely on the similarity information and one also utilizing genre label information - are evaluated within the context of genre classification based on spectral similarity. Both are shown to perform equally well. © 2005 Queen Mary, University of London.","Genre classification; Novelty detection; Spectral similarity","Automation; Automatic identification; Genre classification; Label information; Music information retrieval; Novelty detection; Similarity informations; Spectral similarity; Training data; Information retrieval","A. Flexer; Institute of Medical Cybernetics and Artificial Intelligence, Center for Brain Research, Medical University of Vienna, A-1010 Vienna, Freyung 6/2, Austria; email: arthur@ai.univie.ac.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Raphael C.","Raphael, Christopher (7004214964)","7004214964","A graphical model for recognizing sung melodies","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846186295&partnerID=40&md5=694072e99159d33b922a8608cf5be8e6","School of Informatics, Indiana Univ., Bloomington, IN 47408, United States","Raphael C., School of Informatics, Indiana Univ., Bloomington, IN 47408, United States","A method is presented for automatic transcription of sung melodic fragments to score-like representation, including metric values and pitch. A joint model for pitch, rhythm, segmentation, and tempo is defined for a sung fragment. We then discuss the identification of the globally optimal musical transcription, given the observed audio data. A post process estimates the location of the tonic, so the transcription can be presented into they key of C. Experimental results are presented for a small test collection. © 2005 Queen Mary, University of London.","Graphical models; Monophonic music recognition","Information retrieval; Speech recognition; Transcription; Audio data; Automatic transcription; GraphicaL model; Joint models; Metric values; Monophonic music; Musical transcription; Post process; Test Collection; Graphic methods","C. Raphael; School of Informatics, Indiana Univ., Bloomington, IN 47408, United States; email: craphael@indiana.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pikrakis A.; Theodoridis S.","Pikrakis, Aggelos (6507232714); Theodoridis, Sergios (7004236721)","6507232714; 7004236721","A novel HMM approach to melody spotting in rawaudio recordings","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750995368&partnerID=40&md5=9e9c7176f5154ba7361a2ef079ea4f62","Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece","Pikrakis A., Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece; Theodoridis S., Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece","This paper presents a melody spotting system based on Variable Duration Hidden Markov Models (VDHMM's), capable of locating monophonic melodies in a database of raw audio recordings. The audio recordings may either contain a single instrument performing in solo mode, or an ensemble of instruments where one of the instruments has a leading role. The melody to be spotted is presented to the system as a sequence of note durations and music intervals. In the sequel, this sequence is treated as a pattern prototype and based on it, a VDHMM is constructed. The probabilities of the associated VDHMM are determined according to a set of rules that account (a) for the allowable note duration flexibility and (b) with possible structural deviations from the prototype pattern. In addition, for each raw audio recording in the database, a sequence of note durations and music intervals is extracted by means of a multi pitch tracking algorithm. These sequences are subsequently fed as input to the constructed VDHMM that models the pattern to be located. The VDHMM employs an enhanced Viterbi algorithm, previously introduced by the authors, in order to account for pitch tracking errors and performance improvisations of the instrument players. For each audio recording in the database, the best-state sequence generated by the enhanced Viterbi algorithm is further post-processed in order to locate occurrences of the melody which is searched. Our method has been successfully tested with a variety of cello recordings in the context ofWestern Classical music, as well as with Greek traditional multi-instrument recordings, in which clarinet has a leading role. © 2005 Queen Mary, University of London.","Melody spotting; Variable Duration Hidden Markov Models","Database systems; Hidden Markov models; Information retrieval; Instrument errors; Instruments; Viterbi algorithm; Classical musics; Melody spotting; Multi pitches; Pitch-tracking; Prototype patterns; Set of rules; Structural deviations; Audio recordings","A. Pikrakis; Dept. of Informatics and Telecommunications, University of Athens, TYPA Buildings, 15784, Athens, Panepistimioupolis, Greece; email: pikrakis@di.uoa.gr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Tanghe K.; Lesaffre M.; Degroeve S.; Leman M.; De Baets B.; Martens J.-P.","Tanghe, Koen (55545970400); Lesaffre, Micheline (6507560640); Degroeve, Sven (6505827636); Leman, Marc (6603703642); De Baets, Bernard (55664779600); Martens, Jean-Pierre (7201836932)","55545970400; 6507560640; 6505827636; 6603703642; 55664779600; 7201836932","Collecting ground truth annotations for drum detection in polyphonic music","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35648949771&partnerID=40&md5=a52ece37446fcefb46acf2bb8400f3b8","IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Department of Electronics and Information Systems, Ghent University, Belgium","Tanghe K., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; Lesaffre M., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; Degroeve S., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Leman M., IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; De Baets B., Department of Applied Mathematics, Biometrics and Process Control, Ghent University, Belgium; Martens J.-P., Department of Electronics and Information Systems, Ghent University, Belgium","In order to train and test algorithms that can automatically detect drum events in polyphonic music, ground truth data is needed. This paper describes a setup used for gathering manual annotations for 49 real-world music fragments containing different drum event types. Apart from the drum events, the beat was also annotated. The annotators were experienced drummers or percussionists. This paper is primarily aimed towards other drum detection researchers, but might also be of interest to others dealing with automatic music analysis, manual annotation and data gathering. Its purpose is threefold: providing annotation data for algorithm training and evaluation, describing a practical way of setting up a drum annotation task, and reporting issues that came up during the annotation sessions while at the same time providing some thoughts on important points that could be taken into account when setting up similar tasks in the future. © 2005 Queen Mary, University of London.","Annotation; Data gathering; Drum detection","Information retrieval; Annotation; Data gathering; Event Types; Ground truth; Ground truth data; Manual annotation; Music analysis; Polyphonic music; Real-world; Test algorithms; Algorithms","K. Tanghe; IPEM, Department of Musicology, Ghent University, 9000 Ghent, Blandijnberg 2, Belgium; email: Koen.Tanghe@UGent.be","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Aucouturier J.-J.; Pachet F.","Aucouturier, Jean-Julien (9943558100); Pachet, François (6701441655)","9943558100; 6701441655","Ringomatic: A real-time interactive drummer using constraint-satisfaction and drum sound descriptors","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872720146&partnerID=40&md5=5833a9523be1057cdef90139a1f9b922","SONY CSL Paris, 75005 Paris, 6, rue Amyot, France","Aucouturier J.-J., SONY CSL Paris, 75005 Paris, 6, rue Amyot, France; Pachet F., SONY CSL Paris, 75005 Paris, 6, rue Amyot, France","We describe a real-time musical agent that generates an audio drum-track by concatenating audio segments automatically extracted from pre-existing musical files. The drum-track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata automatically extracted from the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We report on several drum track audio descriptors designed for the system. We also describe a basic mecanism for controlling the tradeoff between the agent's autonomy and reactivity, which we illustrate with experiments made in the context of a virtual duet between the system and a human pianist. © 2005 Queen Mary, University of London.","Concatenative synthesis; Constraint satisfaction; Drumtrack; Interaction; Metadata","Information retrieval; Metadata; Virtual reality; Best match; Constraint Satisfaction; Descriptors; Drumtrack; Interaction; Local search; Audio acoustics","J.-J. Aucouturier; SONY CSL Paris, 75005 Paris, 6, rue Amyot, France; email: jj@csl.sony.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Pickens J.; Iliopoulos C.","Pickens, Jeremy (7005535299); Iliopoulos, Costas (7004240640)","7005535299; 7004240640","Markov random fields and maximum entropy modeling for music information retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","11","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849091441&partnerID=40&md5=a795ebe6b7862b706ae0f6badaa7b66a","Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","Pickens J., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Iliopoulos C., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom","Music information retrieval is characterized by a number of various user information needs. Systems are being developed that allow searchers to find melodies, rhythms, genres, and singers or artists, to name but a few. At the heart of all these systems is the need to find models or measures that answer the question .how similar are two given pieces of music.. However, similarity has a variety of meanings depending on the nature of the system being developed. More importantly, the features extracted from a music source are often either single-dimensional (i.e.: only pitch, or only rhythm, or only timbre) or else assumed to be orthogonal. In this paper we present a framework for developing systems which combine a wide variety of non-independent features without having to make the independence assumption. As evidence of effectiveness, we evaluate the system on the polyphonic theme similarity task over symbolic data. Nevertheless, we emphasize that the framework is general, and can handle a range of music information retrieval tasks. © 2005 Queen Mary, University of London.","Music modeling; Random fields","Computer music; Markov processes; Independence assumption; Markov Random Fields; Maximum entropy modeling; Music information retrieval; Random fields; Symbolic data; User information need; Information retrieval","J. Pickens; Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; email: jeremy@dcs.kcl.ac.uk","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cunningham S.J.; Downie J.S.; Bainbridge D.","Cunningham, Sally Jo (7201937110); Downie, J. Stephen (7102932568); Bainbridge, David (8756864800)","7201937110; 7102932568; 8756864800","The pain, the pain: Modelling music information behavior and the songs we hate","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","20","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845923163&partnerID=40&md5=2fb1c821f2e0744fe585a7bed93d50a7","Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; GSLIS, University of Illinois at Urbana-Champaign, United States","Cunningham S.J., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; Downie J.S., GSLIS, University of Illinois at Urbana-Champaign, United States; Bainbridge D., Dept. of Computer Science, University of Waikato, Hamilton, New Zealand","The paper presents a grounded theory analysis of 395 user responses to the survey question, ""What is the worst song ever?"" Important factors uncovered include: lyric quality, the ""earworm"" effect, voice quality, the influence of associated music videos, over-exposure, perceptions of pretentiousness, and associations with unpleasant personal experiences. © 2005 Queen Mary, University of London.","Music information behaviour; Music recommender systems; User study","Information retrieval; Grounded theory; Music information; Music recommender systems; Music video; Personal experience; User study; Voice quality; Computer music","S.J. Cunningham; Dept. of Computer Science, University of Waikato, Hamilton, New Zealand; email: sallyjo@cs.waikato.ac.nz","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Dalitz C.; Karsten T.","Dalitz, Christoph (6602222732); Karsten, Thomas (55585283500)","6602222732; 55585283500","Using the Gamera framework for building a lute tablature recognition system","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","10","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849156559&partnerID=40&md5=bc4b89e57b70f48b05a4cfda0ceaa01a","Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany","Dalitz C., Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany; Karsten T., Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany","In this article we describe an optical recognition system for historic lute tablature prints that we have built with the aid of the Gamera toolkit for document analysis and recognition. We give recognition rates for various historic sources and show that our system works quite well on printed tablature sources using movable types. For engraved and manuscript sources, we discuss some principal current limitations of our system and Gamera. © 2005 Queen Mary, University of London.","Lute tablature; Optical music recognition","Current limitation; Document analysis; Gamera framework; Lute tablature; Optical music recognition; Optical recognition systems; Recognition rates; Recognition systems; Information retrieval","C. Dalitz; Niederrhein University of Applied Sciences, 47805 Krefeld, Reinarzstr. 49, Germany; email: christoph.dalitz@hsnr.de","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Yu Y.; Watanabe C.; Joe K.","Yu, Yi (36731861100); Watanabe, Chiemi (7103333389); Joe, Kazuki (23477621000)","36731861100; 7103333389; 23477621000","Towards a fast and efficient match algorithm for content-based music retrieval on acoustic data","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68149143823&partnerID=40&md5=2b3329e6b79a3707407c9fbd7f215331","Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan","Yu Y., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan; Watanabe C., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan; Joe K., Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan","In this paper we present a fast and efficient match algorithm, which consists of two key techniques: Spectral Correlation Based Feature Merge(SCBFM) and Two-Step Retrieval(TSR). SCBFM can remove the redundant information. In consequence, the resulting feature sequence has a smaller size, requiring less storage and computation. In addition, most of the tempo variation is removed; thus a much simpler sequence match method can be adopted. Also, TSR relies on the characteristics of Mel-Frequency Cepstral Coefficient(MFCC), where the precise match in the second step depends on the first step to filter out most of the dissimilar references with only the low order MFCC feature. As a result, the whole retrieval speed can be further improved. The experimental evaluation verifies that SCBFM-TSR yields more meaningful results in comparatively short time. The experiment results are analyzed with a theoretical approach that seeks to find the relation between Spectral Correlation(SC) threshold and storage, computation. © 2005 Queen Mary, University of London.","Content based music retrieval; Dynamic programming; Feature merge; Prefiltering; Spectral correlation","Algorithms; Dynamic programming; Information retrieval; Speech recognition; Acoustic data; Content-based music retrieval; Experimental evaluation; Feature merge; Feature sequence; Key techniques; Low order; Mel-frequency cepstral coefficients; Pre-filtering; Redundant informations; Retrieval speed; Spectral correlation; Theoretical approach; Digital storage","Y. Yu; Graduate School of Humanity and Science, Nara Women's University, Nara 630-8506, Kitauoya nishi-machi, Japan; email: yuyi@ics.nara-wu.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Gillet O.; Richard G.","Gillet, Olivier (56616219800); Richard, Gaël (57195915952)","56616219800; 57195915952","Drum track transcription of polyphonic music using noise subspace projection","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947708506&partnerID=40&md5=b9469f731b3f1695eea1fdc368d4b6ef","GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France","Gillet O., GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France; Richard G., GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France","This paper presents a novel drum transcription system for polyphonic music. The use of a band-wise harmonic/noise decomposition allows the suppression of the deterministic part of the signal, which is mainly contributed by nonrhythmic instruments. The transcription is then performed on the residual noise signal, which contains most of the rhythmic information. This signal is segmented, and the events associated to each onset are classified by support vector machines (SVM) with probabilistic outputs. The features used for classification are directly extracted from the sub-band signals. An additional pre-processing stage in which the instances are reclassified using a localized model was also tested. This transcription method is evaluated on ten test sequences, each of them being performed by two drummers and being available with different mixing settings. The whole system achieves precision and recall rates of 84% for the bass drum and snare drum detection tasks. © 2005 Queen Mary, University of London.","Drum transcription; Highresolution methods; Rhythm analysis","Information retrieval; Support vector machines; Detection tasks; High-resolution methods; Noise subspace; Polyphonic music; Pre-processing; Precision and recall; Probabilistic output; Residual noise; Rhythm analysis; Subbands; Test sequence; Transcription methods; Transcription","O. Gillet; GET / Télécom Paris, CNRS, LTCI, 75014 Paris, 37 rue Dareau, France; email: olivier.gillet@enst.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Hu N.; Dannenberg R.B.","Hu, Ning (36882056900); Dannenberg, Roger B. (7003266250)","36882056900; 7003266250","A bootstrap method for training an accurate audio segmenter","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955537796&partnerID=40&md5=516d9e2aa06f56e6ef3f947e992e3d40","Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States","Hu N., Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States; Dannenberg R.B., Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States","Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and handlabeling is quite difficult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are first used to find the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to refine the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data. © 2005 Queen Mary, University of London.","Audio-to-score alignment; Bootstrap; Music audio segmentation; Note onset detection","Alignment; Audio systems; Information retrieval; Iterative methods; Audio alignments; Audio data; Audio segmentation; Bootstrap; Bootstrap approach; Bootstrap method; IMPROVE-A; Initial estimate; Note onset detections; Note segmentation; Segmenter; Symbolic music representation; Synthetic data; Training example; Training methods; Training process; Audio acoustics","N. Hu; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, 5000 Forbes Ave., United States; email: ninghu@cs.cmu.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Knees P.; Schedl M.; Widmer G.","Knees, Peter (8219023200); Schedl, Markus (8684865900); Widmer, Gerhard (7004342843)","8219023200; 8684865900; 7004342843","Multiple lyrics alignment: Automatic retrieval of song lyrics","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","24","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43249122908&partnerID=40&md5=cfc6f0bab1fbdb2c858f656b1b4d18f5","Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria; Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Austria","Knees P., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria; Schedl M., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Austria; Widmer G., Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria, Austrian Research Institute for Artificial Intelligence (OFAI), A-1010 Vienna, Austria","We present an approach to automatically retrieve and extract lyrics of arbitrary songs from the Internet. It is intended to provide easy and convenient access to lyrics for users, as well as a basis for further research based on lyrics, e.g. semantic analysis. Due to the fact that many lyrics found on the web suffer from individual errors like typos, we make use of multiple versions from different sources to eliminate mistakes. This is accomplished by Multiple Sequence Alignment. The different sites are aligned and examined for matching sequences of words, finding those parts on the pages that are likely to contain the lyrics. This provides a means to find the most probable version of lyrics, i.e. a version with highest consensus among different sources. © 2005 Queen Mary, University of London.","Lyrics; Multiple sequence alignment; Web mining","Information retrieval; Automatic retrieval; Lyrics; Multiple sequence alignments; Semantic analysis; Web Mining; Semantics","P. Knees; Department of Computational Perception, Johannes Kepler University Linz, A-4040 Linz, Austria; email: peter.knees@jku.at","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Liang W.; Zhang S.; Xu B.","Liang, Wei (57682931700); Zhang, Shuwu (35754160500); Xu, Bo (37022633100)","57682931700; 35754160500; 37022633100","A histogram algorithm for fast audio retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547544593&partnerID=40&md5=cb3254b08e00dd507aeed58ddbbe6dda","Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","Liang W., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Zhang S., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; Xu B., Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China","This paper describes a fast audio detection method for specific audio retrieval in the AV stream. The method is a histogram matching algorithm based on structural and perceptual features. This algorithm extracts audio features based on human perception on the sound scene and locates the special audio clip by fast histogram matching. Experimental results based on the advertisement detection in TV program showed that the algorithm can achieve a very high overall precision and recall rate both about 97% with very fast search time about 1/40 on real time. © 2005 Queen Mary, University of London.","Audio retrieval; Histogram","Algorithms; Graphic methods; Advertisement detections; Audio clips; Audio detection; Audio features; Audio retrieval; AV stream; Fast search; Histogram; Histogram matching; Human perception; Perceptual feature; Precision and recall; Real time; TV programs; Information retrieval","W. Liang; Institute of Automation, Chinese Academy of Sciences, Beijing, 100080, China; email: wliang@hitic.ia.ac.cn","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Sapp C.S.","Sapp, Craig Stuart (14032002500)","14032002500","Online database of scores in the Humdrum file format","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","40","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149277175&partnerID=40&md5=7e4741ca077d5801931bd478d7746a7a","Center for Computer Assisted Research in the Humanities, Stanford University, United States; Centre for the History and Analysis of Recorded Music, Royal Holloway, University of London, United Kingdom","Sapp C.S., Center for Computer Assisted Research in the Humanities, Stanford University, United States, Centre for the History and Analysis of Recorded Music, Royal Holloway, University of London, United Kingdom","KernScores, an online library of musical data currently consisting of over 5 million notes, has been created to assist projects dealing with the computational analysis of musical scores. The online scores are in a format suitable for processing with the Humdrum Toolkit for Music Research, but the website also provides automatic translations into several other popular data formats for digital musical scores. © 2005 Queen Mary, University of London.","Computational music analysis; Digital scores; Humdrum toolkit; Musical data","Digital libraries; Automatic translation; Computational analysis; Digital score; File formats; Humdrum toolkit; Music analysis; Musical data; Musical score; Online database; Information retrieval","C.S. Sapp; Center for Computer Assisted Research in the Humanities, Stanford University, United States; email: craig@ccrma.stanford.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lee J.H.; Downie J.S.; Cunningham S.J.","Lee, Jin Ha (57190797465); Downie, J. Stephen (7102932568); Cunningham, Sally Jo (7201937110)","57190797465; 7102932568; 7201937110","Challenges in cross-cultural/multilingual music information seeking","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","41","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052526600&partnerID=40&md5=7939e907c47da945068a575ebb465734","Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Dept. of Computer Science, University of Waikato, New Zealand","Lee J.H., Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Downie J.S., Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; Cunningham S.J., Dept. of Computer Science, University of Waikato, New Zealand","Understanding and meeting the needs of a broad range of music users across different cultures and languages are central in designing a global music digital library. This exploratory study examines cross-cultural/multilingual music information seeking behaviors and reveals some important characteristics of these behaviors by analyzing 107 authentic music information queries from a Korean knowledge search portal Naver (knowledge) iN and 150 queries from Google Answers website. We conclude that new sets of access points must be developed to accommodate music queries that cross cultural or language boundaries. © 2005 Queen Mary, University of London.","Cross-cultural music information seeking; Korean users; Multilingual music information seeking; User behaviors","Behavioral research; Digital libraries; Information retrieval; Information use; Portals; Access points; Exploratory studies; Korean users; Language boundaries; Music digital libraries; Music information; User behaviors; Search engines","J.H. Lee; Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign, United States; email: jinlee1@uiuc.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Kitahara T.; Goto M.; Komatani K.; Ogata T.; Okuno H.G.","Kitahara, Tetsuro (7201371361); Goto, Masataka (7403505330); Komatani, Kazunori (35577813100); Ogata, Tetsuya (7402000772); Okuno, Hiroshi G. (7102397930)","7201371361; 7403505330; 35577813100; 7402000772; 7102397930","Instrument identification in polyphonic music: Feature weighting with mixed sounds, pitch-dependent timbre modeling, and use of musical context","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","17","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846174759&partnerID=40&md5=e0df96474b1e9b7c05db0e4826d64e8d","Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan","Kitahara T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Goto M., National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki 305-8568, Japan; Komatani K., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Ogata T., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; Okuno H.G., Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan","This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identification (MII) is an improtant task in music information retrieval because MII results make it possible to automatically retrieving certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the first issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identified, the a priori probablity of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8% to 85.8% for trio music and from 65.5% to 91.1% for duo music. © 2005 Queen Mary, University of London.","F0-dependent multivariate normal distribution; Mixedsound template; MPEG-7; Musical context; Musical instrument identification","Information retrieval; Mixtures; Motion Picture Experts Group standards; Musical instruments; Normal distribution; Mixedsound template; MPEG-7; Multi-variate normal distributions; Musical context; Musical instrument identification; Computer music","T. Kitahara; Dept. of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto 606-8501, Japan; email: kitahara@kuis.kyoto-u.ac.jp","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Toiviainen P.; Eerola T.","Toiviainen, Petri (6602829513); Eerola, Tuomas (6602209042)","6602829513; 6602209042","Classification of musical metre with autocorrelation and discriminant functions","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949088981&partnerID=40&md5=8a74fe6a07b54b7d68577a4111716b98","Department of Music, University of Jyväskylä, Finland","Toiviainen P., Department of Music, University of Jyväskylä, Finland; Eerola T., Department of Music, University of Jyväskylä, Finland","The performance of autocorrelation-based metre induction was tested with two large collections of folk melodies, consisting of approximately 13,000 melodies in MIDI file format, for which the correct metres were available. The analysis included a number of melodic accents assumed to contribute to metric structure. The performance was measured by the proportion of melodies whose metre was correctly classified by Multiple Discriminant Analysis. Overall, the method predicted notated metre with an accuracy of 75 % for classification into nine categories of metre. The most frequent confusions were made within the groups of duple and triple/ compound metres, whereas confusions across these groups where significantly less frequent. In addition to note onset locations and note durations, Thomassen's melodic accent was found to be an important predictor of notated metre. © 2005 Queen Mary, University of London.","Autocorrelation; Classification; Metre","Classification (of information); Discriminant analysis; Information retrieval; Discriminant functions; Metre; MIDI files; Multiple discriminant analysis; Autocorrelation","P. Toiviainen; Department of Music, University of Jyväskylä, Finland; email: ptoiviai@campus.jyu.fi","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Sinyor E.; McKay C.; Fiebrink R.; McEnnis D.; Fujinaga I.","Sinyor, Elliot (55566206600); McKay, Cory (14033215600); Fiebrink, Rebecca (36095844900); McEnnis, Daniel (16234097400); Fujinaga, Ichiro (9038140900)","55566206600; 14033215600; 36095844900; 16234097400; 9038140900","Beatbox classification using ACE","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","13","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872711201&partnerID=40&md5=6692b5d5a141197ec86f69aa8d606b12","Music Technology, McGill University, Montreal, QC, Canada","Sinyor E., Music Technology, McGill University, Montreal, QC, Canada; McKay C., Music Technology, McGill University, Montreal, QC, Canada; Fiebrink R., Music Technology, McGill University, Montreal, QC, Canada; McEnnis D., Music Technology, McGill University, Montreal, QC, Canada; Fujinaga I., Music Technology, McGill University, Montreal, QC, Canada","This paper describes the use of the Autonomous Classification Engine (ACE) to classify beatboxing (vocal percussion) sounds. A set of unvoiced percussion sounds belonging to five classes (bass drum, open hihat, closed hihat and two types of snare drum) were recorded and manually segmented. ACE was used to compare various classification techniques, both with and without feature selection. The best result was 95.55% accuracy using AdaBoost with C4.5 decision tress. © 2005 Queen Mary, University of London.","ACE; Beatboxing; Classification; Feature selection","Classification (of information); Feature extraction; Information retrieval; ACE; Beatboxing; Classification technique; Adaptive boosting","E. Sinyor; Music Technology, McGill University, Montreal, QC, Canada; email: elliot.sinyor@mail.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Lampropoulos A.S.; Lampropoulou P.S.; Tsihrintzis G.A.","Lampropoulos, Aristomenis S. (8206350300); Lampropoulou, Paraskevi S. (14625356000); Tsihrintzis, George A. (7003361233)","8206350300; 14625356000; 7003361233","Musical genre classification enhanced by improved source separation techniques","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","18","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750683676&partnerID=40&md5=59dd2ca05a93a12fcce461e41b0c0ea6","University of Piraeus, Department of Informatics, Piraeus 185 34, Greece","Lampropoulos A.S., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece; Lampropoulou P.S., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece; Tsihrintzis G.A., University of Piraeus, Department of Informatics, Piraeus 185 34, Greece","We present a system for musical genre classification based on audio features extracted from signals which correspond to distinct musical instrument sources. For the separation of the musical sources, we propose an innovative technique in which the convolutive sparse coding algorithm is applied to several portions of the audio signal. The system is evaluated and its performance is assessed. © 2005 Queen Mary, University of London.","Convolutive sparse coding; Musical genre classification; Source separation","Information retrieval; Source separation; Audio features; Audio signal; Innovative techniques; Musical genre classification; Separation techniques; Sparse coding; Signal analysis","A.S. Lampropoulos; University of Piraeus, Department of Informatics, Piraeus 185 34, Greece; email: arislamp@unipi.gr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Cambouropoulos E.; Crochemore M.; Iliopoulos C.; Mohamed M.; Sagot M.-F.","Cambouropoulos, Emilios (9632551100); Crochemore, Maxime (35576964400); Iliopoulos, Costas (7004240640); Mohamed, Manal (9634866600); Sagot, Marie-France (6701533535)","9632551100; 35576964400; 7004240640; 9634866600; 6701533535","A pattern extraction algorithm for abstract melodic representations that allow partial overlapping of intervallic categories","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","16","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847137305&partnerID=40&md5=5d943b87c919f337b40c3837505bc083","Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece; Institut Gaspard-Monge, University of Marne-la-Vallée, 77454 Marne-la-Vallée cedex 2, France; Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; INRIA Rhône-Alpes, Université Claude Bernard, 69622 Villeurbanne cedex, 43 Bd du 11 novembre 1918, France","Cambouropoulos E., Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece; Crochemore M., Institut Gaspard-Monge, University of Marne-la-Vallée, 77454 Marne-la-Vallée cedex 2, France, Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Iliopoulos C., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Mohamed M., Department of Computer Science, King's College London, London WC2R 2LS, United Kingdom; Sagot M.-F., INRIA Rhône-Alpes, Université Claude Bernard, 69622 Villeurbanne cedex, 43 Bd du 11 novembre 1918, France","This paper proposes an efficient pattern extraction algorithm that can be applied on melodic sequences that are represented as strings of abstract intervallic symbols; the melodic representation introduces special ""don't care"" symbols for intervals that may belong to two partially overlapping intervallic categories. As a special case the well established ""step-leap"" representation is examined. In the step-leap representation, each melodic diatonic interval is classified as a step (±s), a leap (±l) or a unison (u). Binary don't care symbols are introduced to represent the possible overlapping between the various abstract categories e.g. * = s, * = l and # = -s, # = -l. For such a sequence, we are interested in finding maximal repeating pairs and repetitions with a hole (two matching subsequences separated with an intervening non-matching symbol). We propose an O(n + d(n - d) + z)-time algorithm for computing all such repetitions in a given sequence x = x[1..n] with d binary don't care symbols, where z is the output size. © 2005 Queen Mary, University of London.","Don't care; Lowest common ancestor; Repetitions; String; Suffix tree","Extraction; Information retrieval; Don't-cares; Lowest common ancestors; Repetitions; String; Suffix-trees; Algorithms","E. Cambouropoulos; Department of Music Studies, University of Thessaloniki, 540006, Thessaloniki, Greece; email: emilios@mus.auth.gr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Izmirli O.","Izmirli, Özgür (6507754258)","6507754258","Tonal similarity from audio using a template based attractor model","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64949107887&partnerID=40&md5=ce9650cad2508ff62b8024930473a767","Center for Arts and Technology, Connecticut College, New London CT, 270 Mohegan Ave., United States","Izmirli O., Center for Arts and Technology, Connecticut College, New London CT, 270 Mohegan Ave., United States","A model that calculates similarity of tonal evolution among pieces in an audio database is presented. The model employs a template based key finding algorithm. This algorithm is used in a sliding window fashion to obtain a sequence of tonal center estimates that delineate the trajectory of tonal evolution in tonal space. A chroma based representation is used to capture tonality information. Templates are formed from instrument sounds weighted according to pitch distribution profiles. For each window in the input audio, the chroma based representation is interpreted with respect to the precalculated templates that serve as attractor points in tonal space. This leads to a discretization in both time and tonal space making the output representation compact. Local and global variations in tempo are accounted for using dynamic time warping that employs a special type of music theoretical distance measure. Evaluation is given in two stages. The first is evaluation of the key finding model to assess its performance in key finding for raw audio input. The second is based on cross validation testing for pieces that have multiple performances in the database to determine the success of recall by distance. © 2005 Queen Mary, University of London.","Dynamic time warping; Key finding; Tonal similarity; Tonal space","Audio database; Audio input; Cross validation; Discretizations; Distance measure; Distribution profiles; Dynamic time warping; Finding algorithm; Global variations; Is evaluations; Key finding; Sliding Window; Template-based; Tonal similarity; Tonal space; Information retrieval","O. Izmirli; Center for Arts and Technology, Connecticut College, New London CT, 270 Mohegan Ave., United States; email: oizm@conncoll.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Knopke I.","Knopke, Ian (24381706300)","24381706300","Geospatial location of music and sound files for music information retrieval","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861011184&partnerID=40&md5=fda945790c07974255f257fe2d481f7e","McGill Music Technology, Montréal, QC, Canada","Knopke I., McGill Music Technology, Montréal, QC, Canada","A relatively new avenue of Web-based information retrieval research, intended to semantically improve information extraction, is the idea of using geographical information to accurately locate resources. This paper introduces a technique for locating sound and music files geographically. It uses information extracted from the Web relating to audio resources and combines it with geospatial location data to provide new information about audio usage in various countries. The results presented here illustrate the enormous potential for MIR to use the vast amount of audio materials on the Web within a physical and geographical context. Statistics of audio usage around the world are provided, as well as examples of other applications of these techniques. © 2005 Queen Mary, University of London.","AROOOGA; CIA; Geospatial; GIS; ISMIR; Mapping; McGill; Music; Semantic; Web crawler; World Wide Web","Geographic information systems; Mapping; Semantic Web; Semantics; World Wide Web; AROOOGA; CIA; Geo-spatial; ISMIR; McGill; Music; Web crawlers; Information retrieval","I. Knopke; McGill Music Technology, Montréal, QC, Canada; email: ian.knopke@mail.mcgill.ca","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Typke R.; Wiering F.; Veltkamp R.C.","Typke, Rainer (6507237569); Wiering, Frans (8976178100); Veltkamp, Remco C. (7003421646)","6507237569; 8976178100; 7003421646","A survey of music information retrieval systems","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","187","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649146511&partnerID=40&md5=5ce0f7cc7b0d3048cae77fe825fe59d1","Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands","Typke R., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands; Wiering F., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands; Veltkamp R.C., Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands","This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks. © 2005 Queen Mary, University of London.","Indexing; Matching; MIR","Algorithms; Audio acoustics; Content based retrieval; Indexing (of information); Information retrieval; Information retrieval systems; Surveys; Content-based; Data collection; Indexing methods; Matching; Matching algorithm; MIR; Music information retrieval; Music notation; Image matching","R. Typke; Universiteit Utrecht, 3584CH Utrecht, Padualaan 14, De Uithof, Netherlands; email: rainer.typke@cs.uu.nl","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Parker C.","Parker, Charles (56403966500)","56403966500","Applications of binary classification and adaptive boosting to the query-by-humming problem","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64849093376&partnerID=40&md5=e484effc5522be136b5c49dde66a4bbd","Oregon State University, 102 Dearborn Hall, Corvallis, OR 97333, United States","Parker C., Oregon State University, 102 Dearborn Hall, Corvallis, OR 97333, United States","In the. query-by-humming. problem, we attempt to retrieve a speci c song from a target set based on a sung query. Recent evaluations of query-by-humming systems show that the state-of-the-art algorithm is a simple dynamic programming-based interval matching technique. Other techniques based on hidden Markov models are far more expensive computationally and do not appear to offer significant increases in performance. Here, we borrow techniques from artificial intelligence to create an algorithm able to outperform the current state-of-the-art with only a negligible increase in running time.© 2005 Queen Mary, University of London.","Articial intelligence; Melodic retrieval; Sequence alignment","Adaptive boosting; Algorithms; Artificial intelligence; Hidden Markov models; Articial intelligence; Binary classification; Matching techniques; Melodic retrieval; Query-by-humming; Query-by-humming system; Running time; Sequence alignments; State-of-the-art algorithms; Information retrieval","C. Parker; Oregon State University, 102 Dearborn Hall, Corvallis, OR 97333, United States; email: parker@cs.orst.edu","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Karydis I.; Nanopoulos A.; Katsaros D.; Manolopoulos Y.; Papadopoulos A.","Karydis, Ioannis (36950839300); Nanopoulos, Alexandros (6603555418); Katsaros, Dimitrios (7005894738); Manolopoulos, Yannis (54397260800); Papadopoulos, Apostolos (7101944599)","36950839300; 6603555418; 7005894738; 54397260800; 7101944599","Content-based music information retrieval in wireless ad-hoc networks","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","5","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66149111533&partnerID=40&md5=f39212feb53fdb77266d73ff5faae240","Aristotle University, Thessaloniki 54124, Greece","Karydis I., Aristotle University, Thessaloniki 54124, Greece; Nanopoulos A., Aristotle University, Thessaloniki 54124, Greece; Katsaros D., Aristotle University, Thessaloniki 54124, Greece; Manolopoulos Y., Aristotle University, Thessaloniki 54124, Greece; Papadopoulos A., Aristotle University, Thessaloniki 54124, Greece","This paper, introduces the application of Content-Based Music Information Retrieval (CBMIR) in wireless ad-hoc networks. We investigate for the first time the challenges posed by the wireless medium and recognise the factors that require optimisation. We propose novel techniques, which attain a significant reduction in both response times and traffic, compared to naive approaches. Extensive experimental results illustrate the appropriateness and efficiency of the proposed method in this bandwidth-starving and volatile, due to mobility, environment. © 2005 Queen Mary, University of London.","Content-based similarity; Music Information Retrieval; P2P; Wireless ad-hoc","Wireless ad hoc networks; Content-based; Music information retrieval; Novel techniques; Optimisations; P2P; Wireless medium; Information retrieval","I. Karydis; Aristotle University, Thessaloniki 54124, Greece; email: karydis@delab.csd.auth.gr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Meredith D.; Wiggins G.A.","Meredith, David (57125703100); Wiggins, Geraint A. (14032393700)","57125703100; 14032393700","Comparing pitch spelling algorithms","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","6","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748496003&partnerID=40&md5=244131775376f60bd3f7538da3fba0ab","Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom","Meredith D., Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom; Wiggins G.A., Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom","A pitch spelling algorithm predicts the pitch names of the notes in a musical passage when given the onset-time, MIDI note number and possibly the duration and voice of each note. Various versions of the algorithms of Longuet-Higgins, Cambouropoulos, Temperley and Sleator, Chew and Chen, and Meredith were run on a corpus containing 195972 notes, equally divided between eight classical and baroque composers. The standard deviation of the accuracies achieved by each algorithm over the eight composers was used as a measure of its style dependence (SD). Meredith's ps1303 was the most accurate algorithm, spelling 99.43% of the notes correctly (SD = 0.54). The best version of Chew and Chen's algorithm was the least dependent on style (SD = 0.35) and spelt 99.15% of the notes correctly. A new version of Cambouropoulos's algorithm, combining features of all three versions described by Cambouropoulos himself, also spelt 99.15% of the notes correctly (SD = 0.47). The best version of Temperley and Sleator's algorithm spelt 97.79% of the notes correctly, but nearly 70% of its errors were due to a single sudden enharmonic change. Longuet-Higgins's algorithm spelt 98.21% of the notes correctly (SD = 1.79) but only when it processed the music a voice at a time. © 2005 Queen Mary, University of London.","Algorithms; Evaluation; Pitch spelling; Transcription","Information retrieval; Transcription; Evaluation; Onset-time; Pitch spelling; Pitch spelling algorithms; Standard deviation; Algorithms","D. Meredith; Department of Computing, Goldsmiths' College, University of London, London, SE14 6NW, New Cross, United Kingdom; email: dave@titanmusic.com","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Scaringella N.; Zoia G.","Scaringella, Nicolas (12761231600); Zoia, Giorgio (6602980333)","12761231600; 6602980333","On the modeling of time information for automatic genre recognition systems in audio signals","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","28","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644624152&partnerID=40&md5=ce1f1039928d5f7c94d40f93f57fe706","Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland","Scaringella N., Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland; Zoia G., Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland","The creation of huge databases coming from both restoration of existing analogue archives and new content is demanding fast and more and more reliable tools for content analysis and description, to be used for searches, content queries and interactive access. In that context, musical genres are crucial descriptors since they have been widely used for years to organize music catalogues, libraries and shops. Despite their use musical genres remain poorly defined concepts which make of the automatic classification problem a non-trivial task. Most automatic genre classification models rely on the same pattern recognition architecture: extracting features from chunks of audio signal and classifying features independently. In this paper, we focus instead on the low-level temporal relationships between chunks when classifying audio signals in terms of genre; in other words, we investigate means to model short-term time structures from context information in music segments to consolidate classification consistency by reducing ambiguities. A detailed comparative analysis of five different time modelling schemes is provided and classification results are reported for a database of 1400 songs evenly distributed over 7 genres. © 2005 Queen Mary, University of London.","Content analysis and indexing; Features extraction; Machine learning; Musical genres","Automatic indexing; Learning systems; Pattern recognition; Audio signal; Automatic classification; Automatic genre classification; Classification consistency; Classification results; Comparative analysis; Content analysis; Content queries; Context information; Descriptors; Extracting features; Features extraction; Music segments; Musical genre; Non-trivial tasks; Recognition systems; Temporal relationships; Time information; Information retrieval","N. Scaringella; Signal Processing Institute (ITS-LTS3), École Polytechnique Fédérale de Lausanne EPFL, Lausanne, CH-1015, Switzerland; email: nicolas.scaringella@epfl.ch","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
"Roy P.; Aucouturier J.-J.; Pachet F.; Beurivé A.","Roy, Pierre (55506419000); Aucouturier, Jean-Julien (9943558100); Pachet, François (6701441655); Beurivé, Anthony (14032993800)","55506419000; 9943558100; 6701441655; 14032993800","Exploiting the tradeoff between precision and cpu-time to speed up nearest neighbor search","2005","ISMIR 2005 - 6th International Conference on Music Information Retrieval","12","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047196692&partnerID=40&md5=b8deee40b9f8cb1b63b0b94a8a947be3","SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France","Roy P., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Aucouturier J.-J., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Pachet F., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; Beurivé A., SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France","We describe an incremental filtering algorithm to quickly compute the N nearest neighbors according to a similarity measure in a metric space. The algorithm exploits an intrinsic property of a large class of similarity measures for which some parameter p has a positive influence both on the precision and the cpu cost (precision-cputime tradeoff ). The algorithm uses successive approximations of the measure to compute first cheap distances on the whole set of possible items, thenmore andmore expensivemeasures on smaller and smaller sets. We illustrate the algorithm on the case of a timbre similarity algorithm, which compares gaussian mixture models using a Monte Carlo approximation of the Kullback-Leibler distance, where p is the number of points drawn from the distributions. We describe several Monte Carlo algorithmic variants, which improve the convergence speed of the approximation. On this problem, the algorithm performs more than 30 times faster than the naive approach. © 2005 Queen Mary, University of London.","Large databases; Nearest neighbor; Similarity measure; Timbre","Approximation theory; Communication channels (information theory); Information retrieval; Monte Carlo methods; Algorithmic variants; Convergence speed; Filtering algorithm; Gaussian Mixture Model; Intrinsic property; Kullback-Leibler distance; Large database; Metric spaces; MONTE CARLO; Monte-carlo approximations; Nearest Neighbor search; Nearest neighbors; Similarity measure; Speed up; Successive approximations; Timbre; Timbre similarities; Approximation algorithms","P. Roy; SONY Computer Science Laboratory Paris, 75005 Paris, 6, rue Amyot, France; email: roy@csl.sony.fr","","6th International Conference on Music Information Retrieval, ISMIR 2005","11 September 2005 through 15 September 2005","London","95391"
